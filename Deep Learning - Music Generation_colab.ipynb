{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "train_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik1mJGuhQipH"
      },
      "source": [
        "# Music Generation using Deep Learning"
      ],
      "id": "ik1mJGuhQipH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvHFRlm9Qr59"
      },
      "source": [
        "## Importing all the required libraries"
      ],
      "id": "AvHFRlm9Qr59"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peaceful-reliance"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding, GRU"
      ],
      "id": "peaceful-reliance",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63J_DtQbpTMG"
      },
      "source": [
        "Mount  the drive to read input data and to save files to the directory"
      ],
      "id": "63J_DtQbpTMG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HLmV6XiXNLp",
        "outputId": "69c5c74e-1678-4564-b4f8-fb0524d4bcb6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "3HLmV6XiXNLp",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOcoZO1hQzgx"
      },
      "source": [
        "## Create required folders in your google drive\n",
        "#### Main Folder - Deep_Learning_Project\n",
        "#### Sub folders - 1) data\n",
        "####               2) model\n",
        "####                 a) GRU_iteration1\n",
        "####                 b) LSTM_iteration2"
      ],
      "id": "fOcoZO1hQzgx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "functioning-paradise"
      },
      "source": [
        "DATA_DIR = '/content/drive/My Drive/Deep_Learning_Project/data/'\n",
        "MODEL_DIR = '/content/drive/My Drive/Deep_Learning_Project/model/'\n",
        "filename = 'input.txt'"
      ],
      "id": "functioning-paradise",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chemical-peninsula"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "SEQ_LENGTH = 64"
      ],
      "id": "chemical-peninsula",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjjjijYHRwNq"
      },
      "source": [
        "## Defining Batches"
      ],
      "id": "DjjjijYHRwNq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oriental-vocabulary"
      },
      "source": [
        "# In this function we are creating batches with sequence length of 64 chars. \n",
        "# X consists of indices of characters for each of the sequence\n",
        "# Y is a tensor which is a on hot encoded value corresponding to the index of next character\n",
        "def read_batches(T, vocab_size):\n",
        "    length = T.shape[0]; #129,665\n",
        "    batch_chars = int(length / BATCH_SIZE); # 8,104\n",
        "\n",
        "    for start in range(0, batch_chars - SEQ_LENGTH, SEQ_LENGTH): # (0, 8040, 64)\n",
        "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH)) # 16X64\n",
        "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, vocab_size)) # 16X64X86\n",
        "        for batch_idx in range(0, BATCH_SIZE): # (0,16)\n",
        "            for i in range(0, SEQ_LENGTH): #(0,64)\n",
        "                X[batch_idx, i] = T[batch_chars * batch_idx + start + i] # \n",
        "                Y[batch_idx, i, T[batch_chars * batch_idx + start + i + 1]] = 1\n",
        "        yield X, Y"
      ],
      "id": "oriental-vocabulary",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concrete-reservation"
      },
      "source": [
        "# Final Model - Multi stack LSTM Model"
      ],
      "id": "concrete-reservation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrO3yeqqO38Z"
      },
      "source": [
        "## Model Architecture"
      ],
      "id": "hrO3yeqqO38Z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rational-front"
      },
      "source": [
        "def save_weights(epoch, model):\n",
        "    if not os.path.exists(MODEL_DIR):\n",
        "        os.makedirs(MODEL_DIR)\n",
        "    model.save_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n",
        "\n",
        "def load_weights(epoch, model):\n",
        "    model.load_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n",
        "\n",
        "###########################################################################################################################\n",
        "## The hidden state has multiple LSTM units(256). Same input goes to all the LSTM units, each of the LSTM will learn different \n",
        "## aspects of the input characters. Output of each LSTM goes to next timestamp\n",
        "\n",
        "## return_sequences=True --- By default this parameter is false for LSTM. If we make it True, for every LSTM unit\n",
        "##                           if we want it to generate an output we have to set this parameter to True. \n",
        "##                           Usually for many-many artchitectures this parameter is true\n",
        "\n",
        "## TimeDistributed Dense layer --- After every timestep if we want to create a dense layer then we use this parameter. \n",
        "##                                 It is similar to an MLP with 86 neurons i.e. vocab_size\n",
        "\n",
        "## stateful=True --- Default value is False. BUt if True then we are giving output of 'Batch 1_First row' as input to 'Batch 2_First row'\n",
        "##                   Essentialluy RNN will learn data from 0 to 8103 with continuity\n",
        "\n",
        "## LSTM Documentation: https://keras.io/api/layers/recurrent_layers/lstm/\n",
        "##                     https://keras.io/models/sequential/\n",
        "###########################################################################################################################\n",
        "\n",
        "def build_model(batch_size, seq_len, vocab_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 512, batch_input_shape=(batch_size, seq_len)))\n",
        "    for i in range(3):\n",
        "        model.add(LSTM(256, return_sequences=True, stateful=True))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(TimeDistributed(Dense(vocab_size))) \n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    return model\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "    #model = build_model(16, 64, 50)\n",
        "    #model.summary()"
      ],
      "id": "rational-front",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JT_MMumR6n2"
      },
      "source": [
        "## Model Training function"
      ],
      "id": "4JT_MMumR6n2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "little-acting"
      },
      "source": [
        "#### text has the data pressent in input.txt file, # of epochs is 100, after every 10 epochs we are saving the model\n",
        "def train(text, epochs=100, save_freq=10):\n",
        "\n",
        "    # Sorting the characters in the text file and assigning index numbers to each character. \n",
        "    # char_to_idx will be a dictionary of unique characters as key and index number is the value pair of dictionary\n",
        "    # To summarize we are comverting each character to a numerical index\n",
        "    \n",
        "    char_to_idx = { ch: i for (i, ch) in enumerate(sorted(list(set(text)))) }\n",
        "    print(\"Number of unique characters: \" + str(len(char_to_idx))) #86\n",
        "\n",
        "    ## Saving the char_to_idx to a json file\n",
        "    with open(os.path.join(DATA_DIR, 'char_to_idx.json'), 'w') as f:\n",
        "        json.dump(char_to_idx, f)\n",
        "\n",
        "    #3 Here we are creating index to character mapping, i.e. given an index we want to get the character for that index\n",
        "    idx_to_char = { i: ch for (ch, i) in char_to_idx.items() }\n",
        "    vocab_size = len(char_to_idx)\n",
        "\n",
        "    ######################################\n",
        "    ######### Model architecture #########\n",
        "    ######################################\n",
        "    \n",
        "    model = build_model(BATCH_SIZE, SEQ_LENGTH, vocab_size)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    ###################################\n",
        "    ###### Train data generation ######\n",
        "    ###################################\n",
        "    \n",
        "    #convert complete text into numerical indices\n",
        "    T = np.asarray([char_to_idx[c] for c in text], dtype=np.int64)\n",
        "\n",
        "    print(\"Length of text:\" + str(T.size)) #129,665\n",
        "\n",
        "    steps_per_epoch = (len(text) / BATCH_SIZE - 1) / SEQ_LENGTH  \n",
        "    \n",
        "    epc,losses, accs = [], [], []\n",
        "    ### This for loop will run for 100 epochs\n",
        "    for epoch in range(epochs):\n",
        "        print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
        "        \n",
        "        \n",
        "\n",
        "        # For each epoch it will generate a batch of X , Y values. For each batch we will train the model. \n",
        "        for i, (X, Y) in enumerate(read_batches(T, vocab_size)):\n",
        "            \n",
        "            #print(X);\n",
        "            \n",
        "            ## Details about train_on_batch here: https://keras.io/models/sequential/\n",
        "            loss, acc = model.train_on_batch(X, Y)\n",
        "            print('Batch {}: loss = {}, acc = {}'.format(i + 1, loss, acc))\n",
        "            epc.append(epoch + 1)\n",
        "            losses.append(loss)\n",
        "            accs.append(acc)\n",
        "        \n",
        "      \n",
        "        # Saving the model after every 10 epochs\n",
        "        if (epoch + 1) % save_freq == 0:\n",
        "            save_weights(epoch + 1, model)\n",
        "            print('Saved checkpoint to', 'weights.{}.h5'.format(epoch + 1))\n",
        "    \n",
        "    df1 = pd.DataFrame(epc)\n",
        "    df2 = pd.DataFrame(losses)\n",
        "    df3 = pd.DataFrame(accs)\n",
        "    frames = [df1,df2, df3]\n",
        "\n",
        "    result = pd.concat(frames, axis =1)\n",
        "    result.columns=['Epoch','Losses','Accuracy']\n",
        "\n",
        "    grouped_multiple = result.groupby(['Epoch'], as_index=False).agg({'Losses': 'mean'\n",
        "                                              ,'Accuracy':'mean'})\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return grouped_multiple\n"
      ],
      "id": "little-acting",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scientific-scope"
      },
      "source": [
        "### Code for building a LSTM model to generate sample sequence of characters"
      ],
      "id": "scientific-scope"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geological-poker"
      },
      "source": [
        "def build_model_seq_gen(unique_chars):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(unique_chars, 512, batch_input_shape=(1, 1)))\n",
        "    for i in range(3):\n",
        "        model.add(LSTM(256, return_sequences=(i != 2), stateful=True))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(unique_chars))\n",
        "    model.add(Activation('softmax'))\n",
        "    plot_model(model, to_file='model.png')\n",
        "    return model"
      ],
      "id": "geological-poker",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Ecl9ikSNVs"
      },
      "source": [
        "## Code for Sample character sequence generator"
      ],
      "id": "i7Ecl9ikSNVs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coordinated-guide"
      },
      "source": [
        "def sample_seq_generator(epoch_num, character_index, seq_length):\n",
        "    with open(os.path.join(DATA_DIR, 'char_to_idx.json')) as f:\n",
        "        char_to_index = json.load(f)\n",
        "    index_to_char = {i:ch for ch, i in char_to_index.items()}\n",
        "    vocab_size = len(index_to_char)\n",
        "\n",
        "    ##########################################################\n",
        "    ######### Sequence generator model architecture  #########\n",
        "    ##########################################################\n",
        "    \n",
        "    model = build_model_seq_gen(vocab_size)\n",
        "    model.load_weights(MODEL_DIR + 'weights.{}.h5'.format(epoch_num))\n",
        "     \n",
        "    sequence_index = [character_index]\n",
        "    \n",
        "    for _ in range(seq_length):\n",
        "        batch = np.zeros((1, 1))\n",
        "        batch[0, 0] = sequence_index[-1]\n",
        "        \n",
        "        predicted_probs = model.predict(batch).reshape(-1)\n",
        "        #print(predicted_probs)\n",
        "        sample = np.random.choice(range(vocab_size), size = 1, p = predicted_probs)\n",
        "        \n",
        "        sequence_index.append(sample[0])\n",
        "    \n",
        "    seq = ''.join(index_to_char[c] for c in sequence_index)\n",
        "    return seq"
      ],
      "id": "coordinated-guide",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e16f7qs-SRwP"
      },
      "source": [
        "## Code to call Train function"
      ],
      "id": "e16f7qs-SRwP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pretty-conditioning",
        "outputId": "df98b659-49a0-48da-d832-485f78c13a07"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    epochs = 100\n",
        "    freq = 10\n",
        "    \n",
        "    ### Calling the train function to read the data from input.txt file \n",
        "    Train_epoch_loss_acc = train(open(os.path.join(DATA_DIR, filename)).read(), epochs, freq)"
      ],
      "id": "pretty-conditioning",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch 21: loss = 0.40900716185569763, acc = 0.8642578125\n",
            "Batch 22: loss = 0.38351723551750183, acc = 0.8544921875\n",
            "Batch 23: loss = 0.38637539744377136, acc = 0.857421875\n",
            "Batch 24: loss = 0.3574186861515045, acc = 0.8818359375\n",
            "Batch 25: loss = 0.3797234296798706, acc = 0.869140625\n",
            "Batch 26: loss = 0.3453875780105591, acc = 0.8828125\n",
            "Batch 27: loss = 0.40357375144958496, acc = 0.8759765625\n",
            "Batch 28: loss = 0.37052303552627563, acc = 0.869140625\n",
            "Batch 29: loss = 0.3803356885910034, acc = 0.8603515625\n",
            "Batch 30: loss = 0.3422233462333679, acc = 0.880859375\n",
            "Batch 31: loss = 0.3837091028690338, acc = 0.8759765625\n",
            "Batch 32: loss = 0.4639955759048462, acc = 0.8447265625\n",
            "Batch 33: loss = 0.33672571182250977, acc = 0.89453125\n",
            "Batch 34: loss = 0.3752266764640808, acc = 0.8779296875\n",
            "Batch 35: loss = 0.3481239080429077, acc = 0.8828125\n",
            "Batch 36: loss = 0.3128468990325928, acc = 0.90234375\n",
            "Batch 37: loss = 0.31583595275878906, acc = 0.890625\n",
            "Batch 38: loss = 0.3454834818840027, acc = 0.8896484375\n",
            "Batch 39: loss = 0.3466418981552124, acc = 0.896484375\n",
            "Batch 40: loss = 0.368741899728775, acc = 0.875\n",
            "Batch 41: loss = 0.3401256501674652, acc = 0.8876953125\n",
            "Batch 42: loss = 0.3616357147693634, acc = 0.8828125\n",
            "Batch 43: loss = 0.3729482591152191, acc = 0.8720703125\n",
            "Batch 44: loss = 0.32004350423812866, acc = 0.8935546875\n",
            "Batch 45: loss = 0.32307183742523193, acc = 0.884765625\n",
            "Batch 46: loss = 0.34178030490875244, acc = 0.8916015625\n",
            "Batch 47: loss = 0.35502806305885315, acc = 0.8701171875\n",
            "Batch 48: loss = 0.3199852406978607, acc = 0.892578125\n",
            "Batch 49: loss = 0.34290778636932373, acc = 0.890625\n",
            "Batch 50: loss = 0.30963554978370667, acc = 0.8994140625\n",
            "Batch 51: loss = 0.33200371265411377, acc = 0.88671875\n",
            "Batch 52: loss = 0.36992233991622925, acc = 0.8740234375\n",
            "Batch 53: loss = 0.3662988841533661, acc = 0.87109375\n",
            "Batch 54: loss = 0.3019953966140747, acc = 0.8984375\n",
            "Batch 55: loss = 0.3048175573348999, acc = 0.8984375\n",
            "Batch 56: loss = 0.38180074095726013, acc = 0.865234375\n",
            "Batch 57: loss = 0.3827016353607178, acc = 0.8681640625\n",
            "Batch 58: loss = 0.37405315041542053, acc = 0.859375\n",
            "Batch 59: loss = 0.31656280159950256, acc = 0.904296875\n",
            "Batch 60: loss = 0.3512168526649475, acc = 0.8837890625\n",
            "Batch 61: loss = 0.3747016489505768, acc = 0.880859375\n",
            "Batch 62: loss = 0.42237770557403564, acc = 0.8671875\n",
            "Batch 63: loss = 0.3420857787132263, acc = 0.8759765625\n",
            "Batch 64: loss = 0.3126947283744812, acc = 0.8955078125\n",
            "Batch 65: loss = 0.3975353538990021, acc = 0.869140625\n",
            "Batch 66: loss = 0.36161747574806213, acc = 0.8828125\n",
            "Batch 67: loss = 0.36127734184265137, acc = 0.88671875\n",
            "Batch 68: loss = 0.353218674659729, acc = 0.8759765625\n",
            "Batch 69: loss = 0.3196505606174469, acc = 0.890625\n",
            "Batch 70: loss = 0.3977079391479492, acc = 0.8603515625\n",
            "Batch 71: loss = 0.36758744716644287, acc = 0.8759765625\n",
            "Batch 72: loss = 0.3533177971839905, acc = 0.8759765625\n",
            "Batch 73: loss = 0.40197473764419556, acc = 0.8671875\n",
            "Batch 74: loss = 0.3849372863769531, acc = 0.8701171875\n",
            "Batch 75: loss = 0.44058459997177124, acc = 0.8642578125\n",
            "Batch 76: loss = 0.3847351670265198, acc = 0.8681640625\n",
            "Batch 77: loss = 0.29344478249549866, acc = 0.90234375\n",
            "Batch 78: loss = 0.3459319472312927, acc = 0.8828125\n",
            "Batch 79: loss = 0.34655478596687317, acc = 0.8779296875\n",
            "Batch 80: loss = 0.3618558347225189, acc = 0.8798828125\n",
            "Batch 81: loss = 0.3618590831756592, acc = 0.8681640625\n",
            "Batch 82: loss = 0.34580498933792114, acc = 0.888671875\n",
            "Batch 83: loss = 0.3506406545639038, acc = 0.8837890625\n",
            "Batch 84: loss = 0.34233543276786804, acc = 0.880859375\n",
            "Batch 85: loss = 0.39337363839149475, acc = 0.8701171875\n",
            "Batch 86: loss = 0.34406307339668274, acc = 0.873046875\n",
            "Batch 87: loss = 0.39806199073791504, acc = 0.8740234375\n",
            "Batch 88: loss = 0.3834060728549957, acc = 0.876953125\n",
            "Batch 89: loss = 0.36226364970207214, acc = 0.8759765625\n",
            "Batch 90: loss = 0.3856697678565979, acc = 0.869140625\n",
            "Batch 91: loss = 0.40180081129074097, acc = 0.8623046875\n",
            "Batch 92: loss = 0.37644055485725403, acc = 0.873046875\n",
            "Batch 93: loss = 0.37583473324775696, acc = 0.87109375\n",
            "Batch 94: loss = 0.331500381231308, acc = 0.8828125\n",
            "Batch 95: loss = 0.3615100383758545, acc = 0.8828125\n",
            "Batch 96: loss = 0.4088742434978485, acc = 0.8642578125\n",
            "Batch 97: loss = 0.36605656147003174, acc = 0.8837890625\n",
            "Batch 98: loss = 0.3623572587966919, acc = 0.8857421875\n",
            "Batch 99: loss = 0.4025404751300812, acc = 0.8623046875\n",
            "Batch 100: loss = 0.3853726387023926, acc = 0.861328125\n",
            "Batch 101: loss = 0.3437982499599457, acc = 0.880859375\n",
            "Batch 102: loss = 0.3881399929523468, acc = 0.8740234375\n",
            "Batch 103: loss = 0.3500700891017914, acc = 0.8857421875\n",
            "Batch 104: loss = 0.3618105351924896, acc = 0.8857421875\n",
            "Batch 105: loss = 0.3205171823501587, acc = 0.884765625\n",
            "Batch 106: loss = 0.3612201511859894, acc = 0.8740234375\n",
            "Batch 107: loss = 0.35635989904403687, acc = 0.8818359375\n",
            "Batch 108: loss = 0.32982027530670166, acc = 0.8798828125\n",
            "Batch 109: loss = 0.35881489515304565, acc = 0.87890625\n",
            "Batch 110: loss = 0.3294239640235901, acc = 0.8916015625\n",
            "Batch 111: loss = 0.374514102935791, acc = 0.8671875\n",
            "Batch 112: loss = 0.3874398469924927, acc = 0.8779296875\n",
            "Batch 113: loss = 0.3888393044471741, acc = 0.87109375\n",
            "Batch 114: loss = 0.37901902198791504, acc = 0.875\n",
            "Batch 115: loss = 0.37745431065559387, acc = 0.8740234375\n",
            "Batch 116: loss = 0.4094933569431305, acc = 0.8583984375\n",
            "Batch 117: loss = 0.3637954294681549, acc = 0.8837890625\n",
            "Batch 118: loss = 0.3193858861923218, acc = 0.8994140625\n",
            "Batch 119: loss = 0.34149596095085144, acc = 0.8857421875\n",
            "Batch 120: loss = 0.33416274189949036, acc = 0.8916015625\n",
            "Batch 121: loss = 0.3288085162639618, acc = 0.8828125\n",
            "Batch 122: loss = 0.3436436653137207, acc = 0.8876953125\n",
            "Batch 123: loss = 0.38690224289894104, acc = 0.87109375\n",
            "Batch 124: loss = 0.41437840461730957, acc = 0.85546875\n",
            "Batch 125: loss = 0.36750054359436035, acc = 0.87109375\n",
            "Batch 126: loss = 0.3913673758506775, acc = 0.865234375\n",
            "\n",
            "Epoch 63/100\n",
            "Batch 1: loss = 0.49351805448532104, acc = 0.8486328125\n",
            "Batch 2: loss = 0.35878419876098633, acc = 0.876953125\n",
            "Batch 3: loss = 0.3738415241241455, acc = 0.873046875\n",
            "Batch 4: loss = 0.3590530455112457, acc = 0.880859375\n",
            "Batch 5: loss = 0.3908701539039612, acc = 0.8662109375\n",
            "Batch 6: loss = 0.37827375531196594, acc = 0.8681640625\n",
            "Batch 7: loss = 0.3500964045524597, acc = 0.8740234375\n",
            "Batch 8: loss = 0.35639286041259766, acc = 0.87890625\n",
            "Batch 9: loss = 0.3743605315685272, acc = 0.876953125\n",
            "Batch 10: loss = 0.29975733160972595, acc = 0.8994140625\n",
            "Batch 11: loss = 0.3506617546081543, acc = 0.87890625\n",
            "Batch 12: loss = 0.36545196175575256, acc = 0.8740234375\n",
            "Batch 13: loss = 0.3315715789794922, acc = 0.8876953125\n",
            "Batch 14: loss = 0.34801214933395386, acc = 0.8896484375\n",
            "Batch 15: loss = 0.35208559036254883, acc = 0.8798828125\n",
            "Batch 16: loss = 0.35761457681655884, acc = 0.880859375\n",
            "Batch 17: loss = 0.3593203127384186, acc = 0.8876953125\n",
            "Batch 18: loss = 0.37939316034317017, acc = 0.87890625\n",
            "Batch 19: loss = 0.3509766161441803, acc = 0.875\n",
            "Batch 20: loss = 0.32464689016342163, acc = 0.8916015625\n",
            "Batch 21: loss = 0.3627374470233917, acc = 0.8876953125\n",
            "Batch 22: loss = 0.34715721011161804, acc = 0.8798828125\n",
            "Batch 23: loss = 0.3736974596977234, acc = 0.8720703125\n",
            "Batch 24: loss = 0.3440007269382477, acc = 0.8896484375\n",
            "Batch 25: loss = 0.3947444558143616, acc = 0.880859375\n",
            "Batch 26: loss = 0.3261556625366211, acc = 0.890625\n",
            "Batch 27: loss = 0.4218561053276062, acc = 0.8583984375\n",
            "Batch 28: loss = 0.3178502917289734, acc = 0.890625\n",
            "Batch 29: loss = 0.3828780949115753, acc = 0.873046875\n",
            "Batch 30: loss = 0.369512140750885, acc = 0.8681640625\n",
            "Batch 31: loss = 0.41312360763549805, acc = 0.8623046875\n",
            "Batch 32: loss = 0.39353352785110474, acc = 0.8662109375\n",
            "Batch 33: loss = 0.33425602316856384, acc = 0.880859375\n",
            "Batch 34: loss = 0.35374629497528076, acc = 0.873046875\n",
            "Batch 35: loss = 0.3493589162826538, acc = 0.890625\n",
            "Batch 36: loss = 0.2975836396217346, acc = 0.890625\n",
            "Batch 37: loss = 0.3053266704082489, acc = 0.8955078125\n",
            "Batch 38: loss = 0.34166058897972107, acc = 0.8876953125\n",
            "Batch 39: loss = 0.35241129994392395, acc = 0.8935546875\n",
            "Batch 40: loss = 0.33916425704956055, acc = 0.8955078125\n",
            "Batch 41: loss = 0.31350237131118774, acc = 0.8779296875\n",
            "Batch 42: loss = 0.34314388036727905, acc = 0.8837890625\n",
            "Batch 43: loss = 0.3747933506965637, acc = 0.873046875\n",
            "Batch 44: loss = 0.32492804527282715, acc = 0.9033203125\n",
            "Batch 45: loss = 0.3145102262496948, acc = 0.8994140625\n",
            "Batch 46: loss = 0.30944037437438965, acc = 0.8974609375\n",
            "Batch 47: loss = 0.31804031133651733, acc = 0.900390625\n",
            "Batch 48: loss = 0.32098644971847534, acc = 0.89453125\n",
            "Batch 49: loss = 0.3482132852077484, acc = 0.8837890625\n",
            "Batch 50: loss = 0.3032880425453186, acc = 0.8984375\n",
            "Batch 51: loss = 0.353215754032135, acc = 0.88671875\n",
            "Batch 52: loss = 0.37840160727500916, acc = 0.8662109375\n",
            "Batch 53: loss = 0.36650389432907104, acc = 0.875\n",
            "Batch 54: loss = 0.31058600544929504, acc = 0.900390625\n",
            "Batch 55: loss = 0.2926510274410248, acc = 0.908203125\n",
            "Batch 56: loss = 0.36029982566833496, acc = 0.8876953125\n",
            "Batch 57: loss = 0.3719403147697449, acc = 0.876953125\n",
            "Batch 58: loss = 0.3942180871963501, acc = 0.8671875\n",
            "Batch 59: loss = 0.2967424690723419, acc = 0.8994140625\n",
            "Batch 60: loss = 0.3544023334980011, acc = 0.8857421875\n",
            "Batch 61: loss = 0.37996095418930054, acc = 0.873046875\n",
            "Batch 62: loss = 0.3932408094406128, acc = 0.8681640625\n",
            "Batch 63: loss = 0.3629433810710907, acc = 0.892578125\n",
            "Batch 64: loss = 0.31325453519821167, acc = 0.8994140625\n",
            "Batch 65: loss = 0.36454108357429504, acc = 0.8779296875\n",
            "Batch 66: loss = 0.3732215166091919, acc = 0.8740234375\n",
            "Batch 67: loss = 0.32084131240844727, acc = 0.8935546875\n",
            "Batch 68: loss = 0.3568541705608368, acc = 0.876953125\n",
            "Batch 69: loss = 0.3256670832633972, acc = 0.896484375\n",
            "Batch 70: loss = 0.3801499009132385, acc = 0.865234375\n",
            "Batch 71: loss = 0.35806718468666077, acc = 0.8798828125\n",
            "Batch 72: loss = 0.33088648319244385, acc = 0.890625\n",
            "Batch 73: loss = 0.3623158931732178, acc = 0.880859375\n",
            "Batch 74: loss = 0.36760857701301575, acc = 0.876953125\n",
            "Batch 75: loss = 0.4279042184352875, acc = 0.8525390625\n",
            "Batch 76: loss = 0.34307295083999634, acc = 0.8935546875\n",
            "Batch 77: loss = 0.3055061101913452, acc = 0.8974609375\n",
            "Batch 78: loss = 0.3720260262489319, acc = 0.87890625\n",
            "Batch 79: loss = 0.32283318042755127, acc = 0.8857421875\n",
            "Batch 80: loss = 0.31718844175338745, acc = 0.8994140625\n",
            "Batch 81: loss = 0.371584415435791, acc = 0.873046875\n",
            "Batch 82: loss = 0.33538568019866943, acc = 0.8818359375\n",
            "Batch 83: loss = 0.34555256366729736, acc = 0.876953125\n",
            "Batch 84: loss = 0.3316802382469177, acc = 0.8818359375\n",
            "Batch 85: loss = 0.3985505998134613, acc = 0.8603515625\n",
            "Batch 86: loss = 0.3490321934223175, acc = 0.8828125\n",
            "Batch 87: loss = 0.34778788685798645, acc = 0.8671875\n",
            "Batch 88: loss = 0.41371071338653564, acc = 0.857421875\n",
            "Batch 89: loss = 0.32910171151161194, acc = 0.90234375\n",
            "Batch 90: loss = 0.399046927690506, acc = 0.859375\n",
            "Batch 91: loss = 0.382742702960968, acc = 0.8642578125\n",
            "Batch 92: loss = 0.3948746621608734, acc = 0.86328125\n",
            "Batch 93: loss = 0.31798678636550903, acc = 0.892578125\n",
            "Batch 94: loss = 0.30163225531578064, acc = 0.8974609375\n",
            "Batch 95: loss = 0.3577425479888916, acc = 0.8779296875\n",
            "Batch 96: loss = 0.38919782638549805, acc = 0.875\n",
            "Batch 97: loss = 0.35873937606811523, acc = 0.87890625\n",
            "Batch 98: loss = 0.37109506130218506, acc = 0.876953125\n",
            "Batch 99: loss = 0.39839085936546326, acc = 0.8642578125\n",
            "Batch 100: loss = 0.37704962491989136, acc = 0.8701171875\n",
            "Batch 101: loss = 0.360665500164032, acc = 0.873046875\n",
            "Batch 102: loss = 0.35730093717575073, acc = 0.8955078125\n",
            "Batch 103: loss = 0.3655704855918884, acc = 0.876953125\n",
            "Batch 104: loss = 0.3209463357925415, acc = 0.892578125\n",
            "Batch 105: loss = 0.3329818546772003, acc = 0.8857421875\n",
            "Batch 106: loss = 0.3268468379974365, acc = 0.8876953125\n",
            "Batch 107: loss = 0.3825761675834656, acc = 0.87109375\n",
            "Batch 108: loss = 0.32094675302505493, acc = 0.8935546875\n",
            "Batch 109: loss = 0.37917113304138184, acc = 0.859375\n",
            "Batch 110: loss = 0.32194027304649353, acc = 0.8955078125\n",
            "Batch 111: loss = 0.3664185404777527, acc = 0.87109375\n",
            "Batch 112: loss = 0.3462187349796295, acc = 0.8896484375\n",
            "Batch 113: loss = 0.3662140965461731, acc = 0.8759765625\n",
            "Batch 114: loss = 0.38051503896713257, acc = 0.87109375\n",
            "Batch 115: loss = 0.34904226660728455, acc = 0.8798828125\n",
            "Batch 116: loss = 0.354569673538208, acc = 0.875\n",
            "Batch 117: loss = 0.3680881857872009, acc = 0.873046875\n",
            "Batch 118: loss = 0.34042346477508545, acc = 0.8798828125\n",
            "Batch 119: loss = 0.2985180914402008, acc = 0.896484375\n",
            "Batch 120: loss = 0.3240060806274414, acc = 0.88671875\n",
            "Batch 121: loss = 0.34600478410720825, acc = 0.8828125\n",
            "Batch 122: loss = 0.32435715198516846, acc = 0.890625\n",
            "Batch 123: loss = 0.35292112827301025, acc = 0.8818359375\n",
            "Batch 124: loss = 0.4285763204097748, acc = 0.845703125\n",
            "Batch 125: loss = 0.39652931690216064, acc = 0.8642578125\n",
            "Batch 126: loss = 0.3761748671531677, acc = 0.876953125\n",
            "\n",
            "Epoch 64/100\n",
            "Batch 1: loss = 0.4366086721420288, acc = 0.875\n",
            "Batch 2: loss = 0.3768315017223358, acc = 0.87109375\n",
            "Batch 3: loss = 0.36575257778167725, acc = 0.875\n",
            "Batch 4: loss = 0.32526183128356934, acc = 0.888671875\n",
            "Batch 5: loss = 0.36334502696990967, acc = 0.876953125\n",
            "Batch 6: loss = 0.3671797215938568, acc = 0.8798828125\n",
            "Batch 7: loss = 0.3399314284324646, acc = 0.87890625\n",
            "Batch 8: loss = 0.3658266067504883, acc = 0.89453125\n",
            "Batch 9: loss = 0.32421186566352844, acc = 0.896484375\n",
            "Batch 10: loss = 0.3343360722064972, acc = 0.8916015625\n",
            "Batch 11: loss = 0.37574824690818787, acc = 0.873046875\n",
            "Batch 12: loss = 0.3673228323459625, acc = 0.8779296875\n",
            "Batch 13: loss = 0.35023602843284607, acc = 0.884765625\n",
            "Batch 14: loss = 0.3468524217605591, acc = 0.896484375\n",
            "Batch 15: loss = 0.3187124729156494, acc = 0.8994140625\n",
            "Batch 16: loss = 0.3340708017349243, acc = 0.8828125\n",
            "Batch 17: loss = 0.3156406283378601, acc = 0.8984375\n",
            "Batch 18: loss = 0.3896440863609314, acc = 0.8671875\n",
            "Batch 19: loss = 0.33803504705429077, acc = 0.8974609375\n",
            "Batch 20: loss = 0.32794705033302307, acc = 0.89453125\n",
            "Batch 21: loss = 0.3984971046447754, acc = 0.8671875\n",
            "Batch 22: loss = 0.3434499502182007, acc = 0.87890625\n",
            "Batch 23: loss = 0.3921724557876587, acc = 0.861328125\n",
            "Batch 24: loss = 0.36690813302993774, acc = 0.8779296875\n",
            "Batch 25: loss = 0.37975215911865234, acc = 0.873046875\n",
            "Batch 26: loss = 0.3256065249443054, acc = 0.8896484375\n",
            "Batch 27: loss = 0.4154875576496124, acc = 0.8671875\n",
            "Batch 28: loss = 0.38475173711776733, acc = 0.857421875\n",
            "Batch 29: loss = 0.3723675608634949, acc = 0.8720703125\n",
            "Batch 30: loss = 0.34349325299263, acc = 0.884765625\n",
            "Batch 31: loss = 0.3810194432735443, acc = 0.8701171875\n",
            "Batch 32: loss = 0.3904833197593689, acc = 0.86328125\n",
            "Batch 33: loss = 0.3323502242565155, acc = 0.8857421875\n",
            "Batch 34: loss = 0.3588505983352661, acc = 0.8837890625\n",
            "Batch 35: loss = 0.3247189521789551, acc = 0.8984375\n",
            "Batch 36: loss = 0.32327884435653687, acc = 0.8974609375\n",
            "Batch 37: loss = 0.2902367115020752, acc = 0.9052734375\n",
            "Batch 38: loss = 0.3324531316757202, acc = 0.892578125\n",
            "Batch 39: loss = 0.33098626136779785, acc = 0.8935546875\n",
            "Batch 40: loss = 0.31517061591148376, acc = 0.8916015625\n",
            "Batch 41: loss = 0.3282346725463867, acc = 0.890625\n",
            "Batch 42: loss = 0.37067633867263794, acc = 0.8798828125\n",
            "Batch 43: loss = 0.3581470847129822, acc = 0.875\n",
            "Batch 44: loss = 0.3104883134365082, acc = 0.900390625\n",
            "Batch 45: loss = 0.3115353286266327, acc = 0.89453125\n",
            "Batch 46: loss = 0.30036601424217224, acc = 0.908203125\n",
            "Batch 47: loss = 0.3439449667930603, acc = 0.880859375\n",
            "Batch 48: loss = 0.33254560828208923, acc = 0.88671875\n",
            "Batch 49: loss = 0.3618836998939514, acc = 0.876953125\n",
            "Batch 50: loss = 0.3044120967388153, acc = 0.8994140625\n",
            "Batch 51: loss = 0.3430584967136383, acc = 0.8798828125\n",
            "Batch 52: loss = 0.35547471046447754, acc = 0.8720703125\n",
            "Batch 53: loss = 0.3526638448238373, acc = 0.8837890625\n",
            "Batch 54: loss = 0.29416367411613464, acc = 0.8984375\n",
            "Batch 55: loss = 0.3251458406448364, acc = 0.892578125\n",
            "Batch 56: loss = 0.3471219539642334, acc = 0.873046875\n",
            "Batch 57: loss = 0.36322423815727234, acc = 0.873046875\n",
            "Batch 58: loss = 0.3993074893951416, acc = 0.859375\n",
            "Batch 59: loss = 0.2751918435096741, acc = 0.91015625\n",
            "Batch 60: loss = 0.33662527799606323, acc = 0.88671875\n",
            "Batch 61: loss = 0.3416447639465332, acc = 0.8955078125\n",
            "Batch 62: loss = 0.3800186514854431, acc = 0.87109375\n",
            "Batch 63: loss = 0.3344736695289612, acc = 0.8896484375\n",
            "Batch 64: loss = 0.303699254989624, acc = 0.9013671875\n",
            "Batch 65: loss = 0.3860951066017151, acc = 0.876953125\n",
            "Batch 66: loss = 0.3542832136154175, acc = 0.875\n",
            "Batch 67: loss = 0.3536660373210907, acc = 0.8876953125\n",
            "Batch 68: loss = 0.4052828550338745, acc = 0.865234375\n",
            "Batch 69: loss = 0.30250105261802673, acc = 0.8974609375\n",
            "Batch 70: loss = 0.3708925247192383, acc = 0.87890625\n",
            "Batch 71: loss = 0.3721819519996643, acc = 0.8779296875\n",
            "Batch 72: loss = 0.35849273204803467, acc = 0.87890625\n",
            "Batch 73: loss = 0.38487106561660767, acc = 0.87109375\n",
            "Batch 74: loss = 0.37407711148262024, acc = 0.880859375\n",
            "Batch 75: loss = 0.4300529658794403, acc = 0.86328125\n",
            "Batch 76: loss = 0.3870563507080078, acc = 0.8662109375\n",
            "Batch 77: loss = 0.3022722005844116, acc = 0.90234375\n",
            "Batch 78: loss = 0.35244953632354736, acc = 0.8896484375\n",
            "Batch 79: loss = 0.31898197531700134, acc = 0.892578125\n",
            "Batch 80: loss = 0.3269835412502289, acc = 0.8837890625\n",
            "Batch 81: loss = 0.37456223368644714, acc = 0.869140625\n",
            "Batch 82: loss = 0.3431926965713501, acc = 0.8837890625\n",
            "Batch 83: loss = 0.34797203540802, acc = 0.8779296875\n",
            "Batch 84: loss = 0.38349196314811707, acc = 0.8681640625\n",
            "Batch 85: loss = 0.37292298674583435, acc = 0.880859375\n",
            "Batch 86: loss = 0.3608209192752838, acc = 0.876953125\n",
            "Batch 87: loss = 0.31356382369995117, acc = 0.89453125\n",
            "Batch 88: loss = 0.4154268503189087, acc = 0.865234375\n",
            "Batch 89: loss = 0.31912195682525635, acc = 0.892578125\n",
            "Batch 90: loss = 0.3720255196094513, acc = 0.8681640625\n",
            "Batch 91: loss = 0.3853221535682678, acc = 0.876953125\n",
            "Batch 92: loss = 0.34871360659599304, acc = 0.8828125\n",
            "Batch 93: loss = 0.3139509856700897, acc = 0.8974609375\n",
            "Batch 94: loss = 0.2986013889312744, acc = 0.90625\n",
            "Batch 95: loss = 0.34244751930236816, acc = 0.884765625\n",
            "Batch 96: loss = 0.38446223735809326, acc = 0.857421875\n",
            "Batch 97: loss = 0.3301456868648529, acc = 0.890625\n",
            "Batch 98: loss = 0.3489457964897156, acc = 0.89453125\n",
            "Batch 99: loss = 0.37689316272735596, acc = 0.8759765625\n",
            "Batch 100: loss = 0.41646480560302734, acc = 0.8603515625\n",
            "Batch 101: loss = 0.36871597170829773, acc = 0.873046875\n",
            "Batch 102: loss = 0.3728829026222229, acc = 0.8701171875\n",
            "Batch 103: loss = 0.36416545510292053, acc = 0.875\n",
            "Batch 104: loss = 0.3109436631202698, acc = 0.8984375\n",
            "Batch 105: loss = 0.3352402448654175, acc = 0.890625\n",
            "Batch 106: loss = 0.33514177799224854, acc = 0.888671875\n",
            "Batch 107: loss = 0.33560657501220703, acc = 0.880859375\n",
            "Batch 108: loss = 0.34616267681121826, acc = 0.880859375\n",
            "Batch 109: loss = 0.369878351688385, acc = 0.873046875\n",
            "Batch 110: loss = 0.3013240098953247, acc = 0.900390625\n",
            "Batch 111: loss = 0.3791598081588745, acc = 0.87109375\n",
            "Batch 112: loss = 0.34309789538383484, acc = 0.8876953125\n",
            "Batch 113: loss = 0.33604586124420166, acc = 0.8876953125\n",
            "Batch 114: loss = 0.3633226454257965, acc = 0.8779296875\n",
            "Batch 115: loss = 0.3637717366218567, acc = 0.8828125\n",
            "Batch 116: loss = 0.38054192066192627, acc = 0.8759765625\n",
            "Batch 117: loss = 0.3718752861022949, acc = 0.884765625\n",
            "Batch 118: loss = 0.32102006673812866, acc = 0.8916015625\n",
            "Batch 119: loss = 0.343037873506546, acc = 0.876953125\n",
            "Batch 120: loss = 0.3306499123573303, acc = 0.8935546875\n",
            "Batch 121: loss = 0.3115842044353485, acc = 0.8935546875\n",
            "Batch 122: loss = 0.2850339114665985, acc = 0.9052734375\n",
            "Batch 123: loss = 0.32325297594070435, acc = 0.8876953125\n",
            "Batch 124: loss = 0.40484893321990967, acc = 0.8662109375\n",
            "Batch 125: loss = 0.3482416272163391, acc = 0.8935546875\n",
            "Batch 126: loss = 0.3686036467552185, acc = 0.8818359375\n",
            "\n",
            "Epoch 65/100\n",
            "Batch 1: loss = 0.47235825657844543, acc = 0.8544921875\n",
            "Batch 2: loss = 0.37903356552124023, acc = 0.875\n",
            "Batch 3: loss = 0.37665218114852905, acc = 0.869140625\n",
            "Batch 4: loss = 0.34379085898399353, acc = 0.8828125\n",
            "Batch 5: loss = 0.3610033690929413, acc = 0.8876953125\n",
            "Batch 6: loss = 0.35209745168685913, acc = 0.8779296875\n",
            "Batch 7: loss = 0.34696030616760254, acc = 0.8759765625\n",
            "Batch 8: loss = 0.3850194811820984, acc = 0.865234375\n",
            "Batch 9: loss = 0.33822381496429443, acc = 0.8798828125\n",
            "Batch 10: loss = 0.31185680627822876, acc = 0.9052734375\n",
            "Batch 11: loss = 0.3489886522293091, acc = 0.888671875\n",
            "Batch 12: loss = 0.3372386395931244, acc = 0.87890625\n",
            "Batch 13: loss = 0.3773845434188843, acc = 0.8759765625\n",
            "Batch 14: loss = 0.3475520610809326, acc = 0.8857421875\n",
            "Batch 15: loss = 0.33037203550338745, acc = 0.8896484375\n",
            "Batch 16: loss = 0.3610767126083374, acc = 0.880859375\n",
            "Batch 17: loss = 0.3369349539279938, acc = 0.8984375\n",
            "Batch 18: loss = 0.37056565284729004, acc = 0.8740234375\n",
            "Batch 19: loss = 0.33358079195022583, acc = 0.8896484375\n",
            "Batch 20: loss = 0.35830777883529663, acc = 0.8779296875\n",
            "Batch 21: loss = 0.37094584107398987, acc = 0.8779296875\n",
            "Batch 22: loss = 0.3471532166004181, acc = 0.8740234375\n",
            "Batch 23: loss = 0.3816676139831543, acc = 0.8857421875\n",
            "Batch 24: loss = 0.3336934447288513, acc = 0.888671875\n",
            "Batch 25: loss = 0.36000847816467285, acc = 0.8837890625\n",
            "Batch 26: loss = 0.29340073466300964, acc = 0.9033203125\n",
            "Batch 27: loss = 0.4014686942100525, acc = 0.869140625\n",
            "Batch 28: loss = 0.34005308151245117, acc = 0.876953125\n",
            "Batch 29: loss = 0.3705536127090454, acc = 0.8681640625\n",
            "Batch 30: loss = 0.3158571124076843, acc = 0.89453125\n",
            "Batch 31: loss = 0.3529028296470642, acc = 0.8828125\n",
            "Batch 32: loss = 0.40033459663391113, acc = 0.873046875\n",
            "Batch 33: loss = 0.32415276765823364, acc = 0.8935546875\n",
            "Batch 34: loss = 0.3632802665233612, acc = 0.884765625\n",
            "Batch 35: loss = 0.30448028445243835, acc = 0.91015625\n",
            "Batch 36: loss = 0.333451509475708, acc = 0.8857421875\n",
            "Batch 37: loss = 0.2804214358329773, acc = 0.9033203125\n",
            "Batch 38: loss = 0.3155556321144104, acc = 0.904296875\n",
            "Batch 39: loss = 0.337280809879303, acc = 0.888671875\n",
            "Batch 40: loss = 0.31212761998176575, acc = 0.89453125\n",
            "Batch 41: loss = 0.3113432228565216, acc = 0.9013671875\n",
            "Batch 42: loss = 0.32268625497817993, acc = 0.890625\n",
            "Batch 43: loss = 0.3582479953765869, acc = 0.876953125\n",
            "Batch 44: loss = 0.3065548241138458, acc = 0.8955078125\n",
            "Batch 45: loss = 0.2680358290672302, acc = 0.9013671875\n",
            "Batch 46: loss = 0.3242584466934204, acc = 0.8837890625\n",
            "Batch 47: loss = 0.310909628868103, acc = 0.89453125\n",
            "Batch 48: loss = 0.2983609735965729, acc = 0.9072265625\n",
            "Batch 49: loss = 0.33794745802879333, acc = 0.896484375\n",
            "Batch 50: loss = 0.29594242572784424, acc = 0.9033203125\n",
            "Batch 51: loss = 0.3214937448501587, acc = 0.8896484375\n",
            "Batch 52: loss = 0.36446142196655273, acc = 0.876953125\n",
            "Batch 53: loss = 0.3544524908065796, acc = 0.8798828125\n",
            "Batch 54: loss = 0.2804466485977173, acc = 0.9140625\n",
            "Batch 55: loss = 0.284795880317688, acc = 0.9130859375\n",
            "Batch 56: loss = 0.34749433398246765, acc = 0.87890625\n",
            "Batch 57: loss = 0.38505440950393677, acc = 0.8623046875\n",
            "Batch 58: loss = 0.38966765999794006, acc = 0.869140625\n",
            "Batch 59: loss = 0.29565927386283875, acc = 0.9013671875\n",
            "Batch 60: loss = 0.3357495963573456, acc = 0.8837890625\n",
            "Batch 61: loss = 0.27877601981163025, acc = 0.9130859375\n",
            "Batch 62: loss = 0.37264004349708557, acc = 0.8798828125\n",
            "Batch 63: loss = 0.33758434653282166, acc = 0.8916015625\n",
            "Batch 64: loss = 0.32979512214660645, acc = 0.9013671875\n",
            "Batch 65: loss = 0.3721212148666382, acc = 0.8798828125\n",
            "Batch 66: loss = 0.33936792612075806, acc = 0.888671875\n",
            "Batch 67: loss = 0.32343876361846924, acc = 0.8876953125\n",
            "Batch 68: loss = 0.3390710949897766, acc = 0.884765625\n",
            "Batch 69: loss = 0.3283195197582245, acc = 0.8857421875\n",
            "Batch 70: loss = 0.3741524815559387, acc = 0.8671875\n",
            "Batch 71: loss = 0.38282397389411926, acc = 0.8701171875\n",
            "Batch 72: loss = 0.33106252551078796, acc = 0.8955078125\n",
            "Batch 73: loss = 0.3469002842903137, acc = 0.8779296875\n",
            "Batch 74: loss = 0.38530319929122925, acc = 0.8681640625\n",
            "Batch 75: loss = 0.41658681631088257, acc = 0.8671875\n",
            "Batch 76: loss = 0.3393747806549072, acc = 0.8876953125\n",
            "Batch 77: loss = 0.34919577836990356, acc = 0.876953125\n",
            "Batch 78: loss = 0.32553109526634216, acc = 0.90234375\n",
            "Batch 79: loss = 0.3261202871799469, acc = 0.892578125\n",
            "Batch 80: loss = 0.34670746326446533, acc = 0.8798828125\n",
            "Batch 81: loss = 0.3444925844669342, acc = 0.8857421875\n",
            "Batch 82: loss = 0.35301151871681213, acc = 0.87890625\n",
            "Batch 83: loss = 0.3495015501976013, acc = 0.884765625\n",
            "Batch 84: loss = 0.36320918798446655, acc = 0.8837890625\n",
            "Batch 85: loss = 0.3701515793800354, acc = 0.873046875\n",
            "Batch 86: loss = 0.3406684696674347, acc = 0.8857421875\n",
            "Batch 87: loss = 0.31920284032821655, acc = 0.888671875\n",
            "Batch 88: loss = 0.400980144739151, acc = 0.876953125\n",
            "Batch 89: loss = 0.3539053201675415, acc = 0.87890625\n",
            "Batch 90: loss = 0.35710379481315613, acc = 0.876953125\n",
            "Batch 91: loss = 0.3964235186576843, acc = 0.861328125\n",
            "Batch 92: loss = 0.34771090745925903, acc = 0.8798828125\n",
            "Batch 93: loss = 0.32996052503585815, acc = 0.890625\n",
            "Batch 94: loss = 0.3058219254016876, acc = 0.9052734375\n",
            "Batch 95: loss = 0.34357357025146484, acc = 0.8935546875\n",
            "Batch 96: loss = 0.37160199880599976, acc = 0.8681640625\n",
            "Batch 97: loss = 0.3606998920440674, acc = 0.8857421875\n",
            "Batch 98: loss = 0.335581511259079, acc = 0.89453125\n",
            "Batch 99: loss = 0.3935014605522156, acc = 0.880859375\n",
            "Batch 100: loss = 0.37412935495376587, acc = 0.857421875\n",
            "Batch 101: loss = 0.3160661458969116, acc = 0.8974609375\n",
            "Batch 102: loss = 0.3738228976726532, acc = 0.876953125\n",
            "Batch 103: loss = 0.35189089179039, acc = 0.875\n",
            "Batch 104: loss = 0.31223827600479126, acc = 0.900390625\n",
            "Batch 105: loss = 0.329488605260849, acc = 0.8837890625\n",
            "Batch 106: loss = 0.33814001083374023, acc = 0.8818359375\n",
            "Batch 107: loss = 0.34555575251579285, acc = 0.888671875\n",
            "Batch 108: loss = 0.3047904968261719, acc = 0.8916015625\n",
            "Batch 109: loss = 0.3829949200153351, acc = 0.8564453125\n",
            "Batch 110: loss = 0.3276432752609253, acc = 0.8896484375\n",
            "Batch 111: loss = 0.352225661277771, acc = 0.8798828125\n",
            "Batch 112: loss = 0.33707359433174133, acc = 0.888671875\n",
            "Batch 113: loss = 0.33798351883888245, acc = 0.8896484375\n",
            "Batch 114: loss = 0.3839800953865051, acc = 0.8798828125\n",
            "Batch 115: loss = 0.33825886249542236, acc = 0.8857421875\n",
            "Batch 116: loss = 0.3730621635913849, acc = 0.8779296875\n",
            "Batch 117: loss = 0.34881356358528137, acc = 0.8857421875\n",
            "Batch 118: loss = 0.2822067141532898, acc = 0.9052734375\n",
            "Batch 119: loss = 0.29932790994644165, acc = 0.8974609375\n",
            "Batch 120: loss = 0.3011706471443176, acc = 0.892578125\n",
            "Batch 121: loss = 0.3285978138446808, acc = 0.8740234375\n",
            "Batch 122: loss = 0.3102721571922302, acc = 0.900390625\n",
            "Batch 123: loss = 0.33897343277931213, acc = 0.892578125\n",
            "Batch 124: loss = 0.38558417558670044, acc = 0.8740234375\n",
            "Batch 125: loss = 0.3577357530593872, acc = 0.87890625\n",
            "Batch 126: loss = 0.3722054660320282, acc = 0.87109375\n",
            "\n",
            "Epoch 66/100\n",
            "Batch 1: loss = 0.4087724983692169, acc = 0.8759765625\n",
            "Batch 2: loss = 0.3835591971874237, acc = 0.876953125\n",
            "Batch 3: loss = 0.3300396203994751, acc = 0.8916015625\n",
            "Batch 4: loss = 0.3610800504684448, acc = 0.87890625\n",
            "Batch 5: loss = 0.33795490860939026, acc = 0.8876953125\n",
            "Batch 6: loss = 0.35342299938201904, acc = 0.884765625\n",
            "Batch 7: loss = 0.33781373500823975, acc = 0.8818359375\n",
            "Batch 8: loss = 0.35328322649002075, acc = 0.8818359375\n",
            "Batch 9: loss = 0.32692447304725647, acc = 0.88671875\n",
            "Batch 10: loss = 0.30635640025138855, acc = 0.90234375\n",
            "Batch 11: loss = 0.3551742434501648, acc = 0.87109375\n",
            "Batch 12: loss = 0.33545759320259094, acc = 0.888671875\n",
            "Batch 13: loss = 0.3232932388782501, acc = 0.890625\n",
            "Batch 14: loss = 0.36696580052375793, acc = 0.8740234375\n",
            "Batch 15: loss = 0.2926490604877472, acc = 0.9052734375\n",
            "Batch 16: loss = 0.3529583215713501, acc = 0.8916015625\n",
            "Batch 17: loss = 0.3536049425601959, acc = 0.8916015625\n",
            "Batch 18: loss = 0.3481757938861847, acc = 0.88671875\n",
            "Batch 19: loss = 0.33228299021720886, acc = 0.8974609375\n",
            "Batch 20: loss = 0.31953591108322144, acc = 0.8984375\n",
            "Batch 21: loss = 0.362210750579834, acc = 0.88671875\n",
            "Batch 22: loss = 0.3326893746852875, acc = 0.8818359375\n",
            "Batch 23: loss = 0.33368512988090515, acc = 0.880859375\n",
            "Batch 24: loss = 0.32250452041625977, acc = 0.896484375\n",
            "Batch 25: loss = 0.35551977157592773, acc = 0.8818359375\n",
            "Batch 26: loss = 0.34182754158973694, acc = 0.8837890625\n",
            "Batch 27: loss = 0.365455687046051, acc = 0.875\n",
            "Batch 28: loss = 0.31189289689064026, acc = 0.892578125\n",
            "Batch 29: loss = 0.3695830702781677, acc = 0.8779296875\n",
            "Batch 30: loss = 0.33652836084365845, acc = 0.8876953125\n",
            "Batch 31: loss = 0.37673479318618774, acc = 0.8759765625\n",
            "Batch 32: loss = 0.3945646584033966, acc = 0.859375\n",
            "Batch 33: loss = 0.3277939558029175, acc = 0.8857421875\n",
            "Batch 34: loss = 0.3772777020931244, acc = 0.8779296875\n",
            "Batch 35: loss = 0.3384198248386383, acc = 0.890625\n",
            "Batch 36: loss = 0.27708861231803894, acc = 0.916015625\n",
            "Batch 37: loss = 0.2962270975112915, acc = 0.9052734375\n",
            "Batch 38: loss = 0.3240041434764862, acc = 0.8955078125\n",
            "Batch 39: loss = 0.302404522895813, acc = 0.9013671875\n",
            "Batch 40: loss = 0.3294399380683899, acc = 0.8955078125\n",
            "Batch 41: loss = 0.3007620573043823, acc = 0.90234375\n",
            "Batch 42: loss = 0.33873167634010315, acc = 0.880859375\n",
            "Batch 43: loss = 0.33745479583740234, acc = 0.876953125\n",
            "Batch 44: loss = 0.2998259961605072, acc = 0.9033203125\n",
            "Batch 45: loss = 0.28046372532844543, acc = 0.912109375\n",
            "Batch 46: loss = 0.2704951763153076, acc = 0.916015625\n",
            "Batch 47: loss = 0.3344705402851105, acc = 0.8876953125\n",
            "Batch 48: loss = 0.3035673499107361, acc = 0.8935546875\n",
            "Batch 49: loss = 0.30683764815330505, acc = 0.904296875\n",
            "Batch 50: loss = 0.29555660486221313, acc = 0.90625\n",
            "Batch 51: loss = 0.3103727698326111, acc = 0.8935546875\n",
            "Batch 52: loss = 0.35236257314682007, acc = 0.873046875\n",
            "Batch 53: loss = 0.3565981090068817, acc = 0.865234375\n",
            "Batch 54: loss = 0.3048642873764038, acc = 0.89453125\n",
            "Batch 55: loss = 0.3124234676361084, acc = 0.89453125\n",
            "Batch 56: loss = 0.3420148193836212, acc = 0.884765625\n",
            "Batch 57: loss = 0.3553275763988495, acc = 0.869140625\n",
            "Batch 58: loss = 0.3693154454231262, acc = 0.875\n",
            "Batch 59: loss = 0.2954050302505493, acc = 0.90625\n",
            "Batch 60: loss = 0.3243895471096039, acc = 0.89453125\n",
            "Batch 61: loss = 0.2985909581184387, acc = 0.90625\n",
            "Batch 62: loss = 0.37846457958221436, acc = 0.8671875\n",
            "Batch 63: loss = 0.3227626383304596, acc = 0.9033203125\n",
            "Batch 64: loss = 0.3157373368740082, acc = 0.904296875\n",
            "Batch 65: loss = 0.3538728952407837, acc = 0.880859375\n",
            "Batch 66: loss = 0.3195590078830719, acc = 0.890625\n",
            "Batch 67: loss = 0.3641830086708069, acc = 0.888671875\n",
            "Batch 68: loss = 0.3326953649520874, acc = 0.8779296875\n",
            "Batch 69: loss = 0.32153427600860596, acc = 0.8876953125\n",
            "Batch 70: loss = 0.38946160674095154, acc = 0.87109375\n",
            "Batch 71: loss = 0.35935503244400024, acc = 0.884765625\n",
            "Batch 72: loss = 0.3094910681247711, acc = 0.8896484375\n",
            "Batch 73: loss = 0.33595865964889526, acc = 0.884765625\n",
            "Batch 74: loss = 0.34843602776527405, acc = 0.8828125\n",
            "Batch 75: loss = 0.39625364542007446, acc = 0.853515625\n",
            "Batch 76: loss = 0.37163621187210083, acc = 0.87109375\n",
            "Batch 77: loss = 0.32107025384902954, acc = 0.8876953125\n",
            "Batch 78: loss = 0.3278779089450836, acc = 0.8955078125\n",
            "Batch 79: loss = 0.2889402508735657, acc = 0.890625\n",
            "Batch 80: loss = 0.3115779161453247, acc = 0.8876953125\n",
            "Batch 81: loss = 0.36957159638404846, acc = 0.8779296875\n",
            "Batch 82: loss = 0.33985814452171326, acc = 0.87890625\n",
            "Batch 83: loss = 0.32795029878616333, acc = 0.873046875\n",
            "Batch 84: loss = 0.3422548472881317, acc = 0.8828125\n",
            "Batch 85: loss = 0.3585665225982666, acc = 0.875\n",
            "Batch 86: loss = 0.30920636653900146, acc = 0.8896484375\n",
            "Batch 87: loss = 0.3196336627006531, acc = 0.8974609375\n",
            "Batch 88: loss = 0.34935835003852844, acc = 0.888671875\n",
            "Batch 89: loss = 0.3318037986755371, acc = 0.8837890625\n",
            "Batch 90: loss = 0.38805127143859863, acc = 0.8681640625\n",
            "Batch 91: loss = 0.348325252532959, acc = 0.876953125\n",
            "Batch 92: loss = 0.35414043068885803, acc = 0.875\n",
            "Batch 93: loss = 0.31034356355667114, acc = 0.9033203125\n",
            "Batch 94: loss = 0.3199383318424225, acc = 0.8798828125\n",
            "Batch 95: loss = 0.33546891808509827, acc = 0.884765625\n",
            "Batch 96: loss = 0.3844452500343323, acc = 0.87109375\n",
            "Batch 97: loss = 0.3459060788154602, acc = 0.8955078125\n",
            "Batch 98: loss = 0.3321623206138611, acc = 0.8779296875\n",
            "Batch 99: loss = 0.41925519704818726, acc = 0.8681640625\n",
            "Batch 100: loss = 0.3719766139984131, acc = 0.8759765625\n",
            "Batch 101: loss = 0.32791176438331604, acc = 0.8876953125\n",
            "Batch 102: loss = 0.32607847452163696, acc = 0.888671875\n",
            "Batch 103: loss = 0.35537606477737427, acc = 0.88671875\n",
            "Batch 104: loss = 0.3066807687282562, acc = 0.896484375\n",
            "Batch 105: loss = 0.2951202392578125, acc = 0.9052734375\n",
            "Batch 106: loss = 0.32079556584358215, acc = 0.8984375\n",
            "Batch 107: loss = 0.3278341293334961, acc = 0.892578125\n",
            "Batch 108: loss = 0.3117961883544922, acc = 0.90234375\n",
            "Batch 109: loss = 0.33647724986076355, acc = 0.8828125\n",
            "Batch 110: loss = 0.3047659397125244, acc = 0.896484375\n",
            "Batch 111: loss = 0.3412977457046509, acc = 0.888671875\n",
            "Batch 112: loss = 0.3568679094314575, acc = 0.8779296875\n",
            "Batch 113: loss = 0.3495613634586334, acc = 0.876953125\n",
            "Batch 114: loss = 0.35204997658729553, acc = 0.8828125\n",
            "Batch 115: loss = 0.3552839457988739, acc = 0.880859375\n",
            "Batch 116: loss = 0.35426193475723267, acc = 0.875\n",
            "Batch 117: loss = 0.3472602069377899, acc = 0.875\n",
            "Batch 118: loss = 0.3335043787956238, acc = 0.8818359375\n",
            "Batch 119: loss = 0.30428820848464966, acc = 0.8994140625\n",
            "Batch 120: loss = 0.3082040846347809, acc = 0.900390625\n",
            "Batch 121: loss = 0.3133322298526764, acc = 0.8994140625\n",
            "Batch 122: loss = 0.31034189462661743, acc = 0.8837890625\n",
            "Batch 123: loss = 0.3373938202857971, acc = 0.8759765625\n",
            "Batch 124: loss = 0.3727475106716156, acc = 0.8623046875\n",
            "Batch 125: loss = 0.36066150665283203, acc = 0.88671875\n",
            "Batch 126: loss = 0.3753580152988434, acc = 0.87109375\n",
            "\n",
            "Epoch 67/100\n",
            "Batch 1: loss = 0.44489869475364685, acc = 0.859375\n",
            "Batch 2: loss = 0.3658401668071747, acc = 0.8818359375\n",
            "Batch 3: loss = 0.3259124457836151, acc = 0.8984375\n",
            "Batch 4: loss = 0.327300488948822, acc = 0.8994140625\n",
            "Batch 5: loss = 0.34493839740753174, acc = 0.88671875\n",
            "Batch 6: loss = 0.3229072690010071, acc = 0.8974609375\n",
            "Batch 7: loss = 0.3315944969654083, acc = 0.880859375\n",
            "Batch 8: loss = 0.35314223170280457, acc = 0.87890625\n",
            "Batch 9: loss = 0.34597116708755493, acc = 0.8896484375\n",
            "Batch 10: loss = 0.30353572964668274, acc = 0.8984375\n",
            "Batch 11: loss = 0.3523295819759369, acc = 0.8896484375\n",
            "Batch 12: loss = 0.33317047357559204, acc = 0.8779296875\n",
            "Batch 13: loss = 0.3035297691822052, acc = 0.900390625\n",
            "Batch 14: loss = 0.2945256233215332, acc = 0.9013671875\n",
            "Batch 15: loss = 0.327208012342453, acc = 0.8828125\n",
            "Batch 16: loss = 0.3405706286430359, acc = 0.888671875\n",
            "Batch 17: loss = 0.3229183554649353, acc = 0.8935546875\n",
            "Batch 18: loss = 0.3398628234863281, acc = 0.8984375\n",
            "Batch 19: loss = 0.35144734382629395, acc = 0.884765625\n",
            "Batch 20: loss = 0.34745997190475464, acc = 0.880859375\n",
            "Batch 21: loss = 0.34739214181900024, acc = 0.8974609375\n",
            "Batch 22: loss = 0.32777953147888184, acc = 0.8798828125\n",
            "Batch 23: loss = 0.3622291386127472, acc = 0.87890625\n",
            "Batch 24: loss = 0.3272758722305298, acc = 0.888671875\n",
            "Batch 25: loss = 0.3474009335041046, acc = 0.880859375\n",
            "Batch 26: loss = 0.32789546251296997, acc = 0.8896484375\n",
            "Batch 27: loss = 0.38412055373191833, acc = 0.869140625\n",
            "Batch 28: loss = 0.33457791805267334, acc = 0.8837890625\n",
            "Batch 29: loss = 0.350642591714859, acc = 0.8896484375\n",
            "Batch 30: loss = 0.3281227946281433, acc = 0.896484375\n",
            "Batch 31: loss = 0.37004753947257996, acc = 0.8798828125\n",
            "Batch 32: loss = 0.36046290397644043, acc = 0.8828125\n",
            "Batch 33: loss = 0.3185214400291443, acc = 0.8876953125\n",
            "Batch 34: loss = 0.34657496213912964, acc = 0.8740234375\n",
            "Batch 35: loss = 0.3413199186325073, acc = 0.884765625\n",
            "Batch 36: loss = 0.27010706067085266, acc = 0.900390625\n",
            "Batch 37: loss = 0.29027095437049866, acc = 0.90234375\n",
            "Batch 38: loss = 0.33115360140800476, acc = 0.8857421875\n",
            "Batch 39: loss = 0.33940011262893677, acc = 0.8798828125\n",
            "Batch 40: loss = 0.32751473784446716, acc = 0.88671875\n",
            "Batch 41: loss = 0.3207521438598633, acc = 0.8916015625\n",
            "Batch 42: loss = 0.3183210492134094, acc = 0.90625\n",
            "Batch 43: loss = 0.3674357831478119, acc = 0.876953125\n",
            "Batch 44: loss = 0.2923614978790283, acc = 0.9033203125\n",
            "Batch 45: loss = 0.26806867122650146, acc = 0.9150390625\n",
            "Batch 46: loss = 0.3034013509750366, acc = 0.900390625\n",
            "Batch 47: loss = 0.32820481061935425, acc = 0.8828125\n",
            "Batch 48: loss = 0.32169756293296814, acc = 0.8876953125\n",
            "Batch 49: loss = 0.3116433620452881, acc = 0.9033203125\n",
            "Batch 50: loss = 0.266348659992218, acc = 0.919921875\n",
            "Batch 51: loss = 0.2941032350063324, acc = 0.90234375\n",
            "Batch 52: loss = 0.3360666036605835, acc = 0.884765625\n",
            "Batch 53: loss = 0.3214702010154724, acc = 0.892578125\n",
            "Batch 54: loss = 0.2774421274662018, acc = 0.90234375\n",
            "Batch 55: loss = 0.281679630279541, acc = 0.91015625\n",
            "Batch 56: loss = 0.3546034097671509, acc = 0.88671875\n",
            "Batch 57: loss = 0.35299280285835266, acc = 0.875\n",
            "Batch 58: loss = 0.37051844596862793, acc = 0.8681640625\n",
            "Batch 59: loss = 0.2915438413619995, acc = 0.8994140625\n",
            "Batch 60: loss = 0.33921560645103455, acc = 0.8837890625\n",
            "Batch 61: loss = 0.3370322585105896, acc = 0.8876953125\n",
            "Batch 62: loss = 0.37158656120300293, acc = 0.8818359375\n",
            "Batch 63: loss = 0.3162789046764374, acc = 0.8916015625\n",
            "Batch 64: loss = 0.29218268394470215, acc = 0.9052734375\n",
            "Batch 65: loss = 0.38390541076660156, acc = 0.873046875\n",
            "Batch 66: loss = 0.3292011618614197, acc = 0.89453125\n",
            "Batch 67: loss = 0.3318118155002594, acc = 0.8798828125\n",
            "Batch 68: loss = 0.3548586070537567, acc = 0.873046875\n",
            "Batch 69: loss = 0.2835094630718231, acc = 0.9013671875\n",
            "Batch 70: loss = 0.3655971586704254, acc = 0.8818359375\n",
            "Batch 71: loss = 0.3550759553909302, acc = 0.8701171875\n",
            "Batch 72: loss = 0.33483946323394775, acc = 0.8837890625\n",
            "Batch 73: loss = 0.37452030181884766, acc = 0.880859375\n",
            "Batch 74: loss = 0.37980759143829346, acc = 0.87109375\n",
            "Batch 75: loss = 0.3886532783508301, acc = 0.8701171875\n",
            "Batch 76: loss = 0.3555145263671875, acc = 0.8701171875\n",
            "Batch 77: loss = 0.33409383893013, acc = 0.8828125\n",
            "Batch 78: loss = 0.34614068269729614, acc = 0.876953125\n",
            "Batch 79: loss = 0.3171350359916687, acc = 0.888671875\n",
            "Batch 80: loss = 0.31033456325531006, acc = 0.8896484375\n",
            "Batch 81: loss = 0.3475545346736908, acc = 0.8779296875\n",
            "Batch 82: loss = 0.3566615879535675, acc = 0.87890625\n",
            "Batch 83: loss = 0.3606359660625458, acc = 0.876953125\n",
            "Batch 84: loss = 0.35585907101631165, acc = 0.88671875\n",
            "Batch 85: loss = 0.39866381883621216, acc = 0.861328125\n",
            "Batch 86: loss = 0.31125113368034363, acc = 0.8974609375\n",
            "Batch 87: loss = 0.3224633038043976, acc = 0.8974609375\n",
            "Batch 88: loss = 0.41453519463539124, acc = 0.857421875\n",
            "Batch 89: loss = 0.3224073350429535, acc = 0.896484375\n",
            "Batch 90: loss = 0.3821330666542053, acc = 0.8681640625\n",
            "Batch 91: loss = 0.34461739659309387, acc = 0.8828125\n",
            "Batch 92: loss = 0.36937427520751953, acc = 0.892578125\n",
            "Batch 93: loss = 0.30197346210479736, acc = 0.8935546875\n",
            "Batch 94: loss = 0.33460351824760437, acc = 0.8837890625\n",
            "Batch 95: loss = 0.3228400945663452, acc = 0.8857421875\n",
            "Batch 96: loss = 0.35479021072387695, acc = 0.8837890625\n",
            "Batch 97: loss = 0.32383137941360474, acc = 0.90234375\n",
            "Batch 98: loss = 0.3464623689651489, acc = 0.8876953125\n",
            "Batch 99: loss = 0.3432287871837616, acc = 0.8974609375\n",
            "Batch 100: loss = 0.36905670166015625, acc = 0.8720703125\n",
            "Batch 101: loss = 0.3272031247615814, acc = 0.88671875\n",
            "Batch 102: loss = 0.3458402454853058, acc = 0.876953125\n",
            "Batch 103: loss = 0.35067445039749146, acc = 0.884765625\n",
            "Batch 104: loss = 0.30384957790374756, acc = 0.9013671875\n",
            "Batch 105: loss = 0.2818230986595154, acc = 0.9091796875\n",
            "Batch 106: loss = 0.3418729901313782, acc = 0.888671875\n",
            "Batch 107: loss = 0.3259637653827667, acc = 0.900390625\n",
            "Batch 108: loss = 0.3305165767669678, acc = 0.8955078125\n",
            "Batch 109: loss = 0.3178490698337555, acc = 0.890625\n",
            "Batch 110: loss = 0.2913130223751068, acc = 0.9130859375\n",
            "Batch 111: loss = 0.30221545696258545, acc = 0.9013671875\n",
            "Batch 112: loss = 0.34408921003341675, acc = 0.8896484375\n",
            "Batch 113: loss = 0.3083747625350952, acc = 0.8955078125\n",
            "Batch 114: loss = 0.3452889025211334, acc = 0.87890625\n",
            "Batch 115: loss = 0.3458176851272583, acc = 0.8759765625\n",
            "Batch 116: loss = 0.3723541498184204, acc = 0.8623046875\n",
            "Batch 117: loss = 0.34126508235931396, acc = 0.8974609375\n",
            "Batch 118: loss = 0.30176764726638794, acc = 0.8974609375\n",
            "Batch 119: loss = 0.3376281261444092, acc = 0.876953125\n",
            "Batch 120: loss = 0.301129549741745, acc = 0.8935546875\n",
            "Batch 121: loss = 0.2995705306529999, acc = 0.8984375\n",
            "Batch 122: loss = 0.32038331031799316, acc = 0.890625\n",
            "Batch 123: loss = 0.3361809551715851, acc = 0.896484375\n",
            "Batch 124: loss = 0.3714892566204071, acc = 0.8671875\n",
            "Batch 125: loss = 0.3565438687801361, acc = 0.888671875\n",
            "Batch 126: loss = 0.36938703060150146, acc = 0.8720703125\n",
            "\n",
            "Epoch 68/100\n",
            "Batch 1: loss = 0.4322294592857361, acc = 0.8701171875\n",
            "Batch 2: loss = 0.33100855350494385, acc = 0.888671875\n",
            "Batch 3: loss = 0.32229673862457275, acc = 0.89453125\n",
            "Batch 4: loss = 0.31703561544418335, acc = 0.8916015625\n",
            "Batch 5: loss = 0.3367410898208618, acc = 0.8896484375\n",
            "Batch 6: loss = 0.3338637948036194, acc = 0.89453125\n",
            "Batch 7: loss = 0.3350755572319031, acc = 0.8828125\n",
            "Batch 8: loss = 0.3557718098163605, acc = 0.8857421875\n",
            "Batch 9: loss = 0.3752773404121399, acc = 0.8740234375\n",
            "Batch 10: loss = 0.3092014491558075, acc = 0.8935546875\n",
            "Batch 11: loss = 0.33714547753334045, acc = 0.8828125\n",
            "Batch 12: loss = 0.3347185552120209, acc = 0.884765625\n",
            "Batch 13: loss = 0.32898399233818054, acc = 0.890625\n",
            "Batch 14: loss = 0.3289019465446472, acc = 0.896484375\n",
            "Batch 15: loss = 0.2872985303401947, acc = 0.90625\n",
            "Batch 16: loss = 0.3516244888305664, acc = 0.8798828125\n",
            "Batch 17: loss = 0.33757326006889343, acc = 0.892578125\n",
            "Batch 18: loss = 0.35030868649482727, acc = 0.8818359375\n",
            "Batch 19: loss = 0.3790353536605835, acc = 0.87109375\n",
            "Batch 20: loss = 0.3087647557258606, acc = 0.892578125\n",
            "Batch 21: loss = 0.3432522714138031, acc = 0.888671875\n",
            "Batch 22: loss = 0.32845357060432434, acc = 0.8759765625\n",
            "Batch 23: loss = 0.36070716381073, acc = 0.876953125\n",
            "Batch 24: loss = 0.331424355506897, acc = 0.8916015625\n",
            "Batch 25: loss = 0.3353480398654938, acc = 0.8818359375\n",
            "Batch 26: loss = 0.30581265687942505, acc = 0.9013671875\n",
            "Batch 27: loss = 0.3796105682849884, acc = 0.87109375\n",
            "Batch 28: loss = 0.32865726947784424, acc = 0.8896484375\n",
            "Batch 29: loss = 0.35812926292419434, acc = 0.876953125\n",
            "Batch 30: loss = 0.30452942848205566, acc = 0.9013671875\n",
            "Batch 31: loss = 0.3388012945652008, acc = 0.890625\n",
            "Batch 32: loss = 0.35329481959342957, acc = 0.8720703125\n",
            "Batch 33: loss = 0.3004029095172882, acc = 0.8994140625\n",
            "Batch 34: loss = 0.322389155626297, acc = 0.892578125\n",
            "Batch 35: loss = 0.3467717170715332, acc = 0.8857421875\n",
            "Batch 36: loss = 0.3107941150665283, acc = 0.8916015625\n",
            "Batch 37: loss = 0.27392807602882385, acc = 0.90234375\n",
            "Batch 38: loss = 0.312601238489151, acc = 0.9072265625\n",
            "Batch 39: loss = 0.30582404136657715, acc = 0.900390625\n",
            "Batch 40: loss = 0.3291721045970917, acc = 0.9013671875\n",
            "Batch 41: loss = 0.3032180368900299, acc = 0.888671875\n",
            "Batch 42: loss = 0.31427237391471863, acc = 0.89453125\n",
            "Batch 43: loss = 0.3543052673339844, acc = 0.880859375\n",
            "Batch 44: loss = 0.29329177737236023, acc = 0.9111328125\n",
            "Batch 45: loss = 0.2837044894695282, acc = 0.9033203125\n",
            "Batch 46: loss = 0.2630603313446045, acc = 0.9091796875\n",
            "Batch 47: loss = 0.29907718300819397, acc = 0.8994140625\n",
            "Batch 48: loss = 0.3137071132659912, acc = 0.892578125\n",
            "Batch 49: loss = 0.3289906978607178, acc = 0.8896484375\n",
            "Batch 50: loss = 0.3159332573413849, acc = 0.8916015625\n",
            "Batch 51: loss = 0.32323893904685974, acc = 0.8955078125\n",
            "Batch 52: loss = 0.30440813302993774, acc = 0.900390625\n",
            "Batch 53: loss = 0.30718106031417847, acc = 0.892578125\n",
            "Batch 54: loss = 0.25130322575569153, acc = 0.9208984375\n",
            "Batch 55: loss = 0.2964123785495758, acc = 0.9052734375\n",
            "Batch 56: loss = 0.2889958322048187, acc = 0.900390625\n",
            "Batch 57: loss = 0.3130733370780945, acc = 0.890625\n",
            "Batch 58: loss = 0.35370227694511414, acc = 0.873046875\n",
            "Batch 59: loss = 0.27585500478744507, acc = 0.9091796875\n",
            "Batch 60: loss = 0.34235671162605286, acc = 0.8857421875\n",
            "Batch 61: loss = 0.32709231972694397, acc = 0.8876953125\n",
            "Batch 62: loss = 0.3515075147151947, acc = 0.880859375\n",
            "Batch 63: loss = 0.32397517561912537, acc = 0.8798828125\n",
            "Batch 64: loss = 0.30676811933517456, acc = 0.89453125\n",
            "Batch 65: loss = 0.3464588224887848, acc = 0.8828125\n",
            "Batch 66: loss = 0.31215977668762207, acc = 0.9013671875\n",
            "Batch 67: loss = 0.2918339967727661, acc = 0.9013671875\n",
            "Batch 68: loss = 0.3078106939792633, acc = 0.8974609375\n",
            "Batch 69: loss = 0.3221627175807953, acc = 0.900390625\n",
            "Batch 70: loss = 0.37081390619277954, acc = 0.8759765625\n",
            "Batch 71: loss = 0.3188305199146271, acc = 0.8955078125\n",
            "Batch 72: loss = 0.29132556915283203, acc = 0.9052734375\n",
            "Batch 73: loss = 0.3795718848705292, acc = 0.87109375\n",
            "Batch 74: loss = 0.365244597196579, acc = 0.87890625\n",
            "Batch 75: loss = 0.3870418965816498, acc = 0.8671875\n",
            "Batch 76: loss = 0.3364773392677307, acc = 0.892578125\n",
            "Batch 77: loss = 0.3409394323825836, acc = 0.873046875\n",
            "Batch 78: loss = 0.3201119899749756, acc = 0.8955078125\n",
            "Batch 79: loss = 0.30988752841949463, acc = 0.89453125\n",
            "Batch 80: loss = 0.2995462119579315, acc = 0.8974609375\n",
            "Batch 81: loss = 0.3475938141345978, acc = 0.87109375\n",
            "Batch 82: loss = 0.32289138436317444, acc = 0.8916015625\n",
            "Batch 83: loss = 0.36516809463500977, acc = 0.876953125\n",
            "Batch 84: loss = 0.33311858773231506, acc = 0.890625\n",
            "Batch 85: loss = 0.38628730177879333, acc = 0.85546875\n",
            "Batch 86: loss = 0.30763208866119385, acc = 0.90625\n",
            "Batch 87: loss = 0.3030751943588257, acc = 0.8955078125\n",
            "Batch 88: loss = 0.38177239894866943, acc = 0.8798828125\n",
            "Batch 89: loss = 0.329062819480896, acc = 0.892578125\n",
            "Batch 90: loss = 0.32147905230522156, acc = 0.890625\n",
            "Batch 91: loss = 0.3570515215396881, acc = 0.8828125\n",
            "Batch 92: loss = 0.3542739748954773, acc = 0.8837890625\n",
            "Batch 93: loss = 0.2934310734272003, acc = 0.8994140625\n",
            "Batch 94: loss = 0.2830657958984375, acc = 0.900390625\n",
            "Batch 95: loss = 0.33168867230415344, acc = 0.888671875\n",
            "Batch 96: loss = 0.3582574427127838, acc = 0.8623046875\n",
            "Batch 97: loss = 0.3431353271007538, acc = 0.8837890625\n",
            "Batch 98: loss = 0.318778932094574, acc = 0.896484375\n",
            "Batch 99: loss = 0.335257887840271, acc = 0.884765625\n",
            "Batch 100: loss = 0.3345160484313965, acc = 0.8857421875\n",
            "Batch 101: loss = 0.3330153524875641, acc = 0.8798828125\n",
            "Batch 102: loss = 0.34830719232559204, acc = 0.8837890625\n",
            "Batch 103: loss = 0.31700506806373596, acc = 0.888671875\n",
            "Batch 104: loss = 0.29502126574516296, acc = 0.8974609375\n",
            "Batch 105: loss = 0.2910013496875763, acc = 0.9052734375\n",
            "Batch 106: loss = 0.31019726395606995, acc = 0.900390625\n",
            "Batch 107: loss = 0.337103009223938, acc = 0.873046875\n",
            "Batch 108: loss = 0.3134024143218994, acc = 0.8876953125\n",
            "Batch 109: loss = 0.33669716119766235, acc = 0.8828125\n",
            "Batch 110: loss = 0.3004007637500763, acc = 0.90234375\n",
            "Batch 111: loss = 0.3176650106906891, acc = 0.8935546875\n",
            "Batch 112: loss = 0.34489989280700684, acc = 0.880859375\n",
            "Batch 113: loss = 0.32681965827941895, acc = 0.8935546875\n",
            "Batch 114: loss = 0.3146663010120392, acc = 0.8955078125\n",
            "Batch 115: loss = 0.3766191601753235, acc = 0.880859375\n",
            "Batch 116: loss = 0.35456618666648865, acc = 0.8798828125\n",
            "Batch 117: loss = 0.31056803464889526, acc = 0.904296875\n",
            "Batch 118: loss = 0.281882107257843, acc = 0.9013671875\n",
            "Batch 119: loss = 0.3052757680416107, acc = 0.9013671875\n",
            "Batch 120: loss = 0.2983715534210205, acc = 0.90234375\n",
            "Batch 121: loss = 0.31720274686813354, acc = 0.8857421875\n",
            "Batch 122: loss = 0.3184886872768402, acc = 0.884765625\n",
            "Batch 123: loss = 0.29879552125930786, acc = 0.904296875\n",
            "Batch 124: loss = 0.353030800819397, acc = 0.88671875\n",
            "Batch 125: loss = 0.36138394474983215, acc = 0.876953125\n",
            "Batch 126: loss = 0.35669466853141785, acc = 0.8759765625\n",
            "\n",
            "Epoch 69/100\n",
            "Batch 1: loss = 0.39746761322021484, acc = 0.876953125\n",
            "Batch 2: loss = 0.3402646780014038, acc = 0.88671875\n",
            "Batch 3: loss = 0.3276904821395874, acc = 0.884765625\n",
            "Batch 4: loss = 0.3075384795665741, acc = 0.8955078125\n",
            "Batch 5: loss = 0.3797439634799957, acc = 0.8876953125\n",
            "Batch 6: loss = 0.3268097937107086, acc = 0.8837890625\n",
            "Batch 7: loss = 0.3433730900287628, acc = 0.8896484375\n",
            "Batch 8: loss = 0.3307913541793823, acc = 0.888671875\n",
            "Batch 9: loss = 0.339613676071167, acc = 0.892578125\n",
            "Batch 10: loss = 0.30345970392227173, acc = 0.896484375\n",
            "Batch 11: loss = 0.3405149281024933, acc = 0.8779296875\n",
            "Batch 12: loss = 0.32687532901763916, acc = 0.90234375\n",
            "Batch 13: loss = 0.30959194898605347, acc = 0.890625\n",
            "Batch 14: loss = 0.2932389974594116, acc = 0.8984375\n",
            "Batch 15: loss = 0.3020658791065216, acc = 0.9033203125\n",
            "Batch 16: loss = 0.32384952902793884, acc = 0.88671875\n",
            "Batch 17: loss = 0.3339381217956543, acc = 0.884765625\n",
            "Batch 18: loss = 0.34638574719429016, acc = 0.88671875\n",
            "Batch 19: loss = 0.31982630491256714, acc = 0.8896484375\n",
            "Batch 20: loss = 0.3506709337234497, acc = 0.8818359375\n",
            "Batch 21: loss = 0.33068791031837463, acc = 0.8955078125\n",
            "Batch 22: loss = 0.3142148554325104, acc = 0.8916015625\n",
            "Batch 23: loss = 0.3356533646583557, acc = 0.880859375\n",
            "Batch 24: loss = 0.32361847162246704, acc = 0.9072265625\n",
            "Batch 25: loss = 0.30389535427093506, acc = 0.900390625\n",
            "Batch 26: loss = 0.29215943813323975, acc = 0.8974609375\n",
            "Batch 27: loss = 0.3538895845413208, acc = 0.880859375\n",
            "Batch 28: loss = 0.31159502267837524, acc = 0.8857421875\n",
            "Batch 29: loss = 0.34296882152557373, acc = 0.87890625\n",
            "Batch 30: loss = 0.32635995745658875, acc = 0.8896484375\n",
            "Batch 31: loss = 0.3349454402923584, acc = 0.8974609375\n",
            "Batch 32: loss = 0.3464968204498291, acc = 0.8828125\n",
            "Batch 33: loss = 0.3235585689544678, acc = 0.896484375\n",
            "Batch 34: loss = 0.32754966616630554, acc = 0.876953125\n",
            "Batch 35: loss = 0.3357190191745758, acc = 0.884765625\n",
            "Batch 36: loss = 0.26428720355033875, acc = 0.90625\n",
            "Batch 37: loss = 0.28502047061920166, acc = 0.9130859375\n",
            "Batch 38: loss = 0.30235645174980164, acc = 0.9033203125\n",
            "Batch 39: loss = 0.33884304761886597, acc = 0.884765625\n",
            "Batch 40: loss = 0.30514413118362427, acc = 0.8955078125\n",
            "Batch 41: loss = 0.30830246210098267, acc = 0.8896484375\n",
            "Batch 42: loss = 0.3246214985847473, acc = 0.8798828125\n",
            "Batch 43: loss = 0.32905787229537964, acc = 0.8818359375\n",
            "Batch 44: loss = 0.27955907583236694, acc = 0.90625\n",
            "Batch 45: loss = 0.289259135723114, acc = 0.904296875\n",
            "Batch 46: loss = 0.3015645444393158, acc = 0.900390625\n",
            "Batch 47: loss = 0.30704450607299805, acc = 0.888671875\n",
            "Batch 48: loss = 0.3181746006011963, acc = 0.88671875\n",
            "Batch 49: loss = 0.2980231046676636, acc = 0.8994140625\n",
            "Batch 50: loss = 0.2787982225418091, acc = 0.916015625\n",
            "Batch 51: loss = 0.31027793884277344, acc = 0.890625\n",
            "Batch 52: loss = 0.3147766590118408, acc = 0.8974609375\n",
            "Batch 53: loss = 0.2861696779727936, acc = 0.90234375\n",
            "Batch 54: loss = 0.25198349356651306, acc = 0.9111328125\n",
            "Batch 55: loss = 0.260287880897522, acc = 0.916015625\n",
            "Batch 56: loss = 0.32078075408935547, acc = 0.88671875\n",
            "Batch 57: loss = 0.3675532639026642, acc = 0.86328125\n",
            "Batch 58: loss = 0.36319950222969055, acc = 0.8798828125\n",
            "Batch 59: loss = 0.290711909532547, acc = 0.900390625\n",
            "Batch 60: loss = 0.32742586731910706, acc = 0.888671875\n",
            "Batch 61: loss = 0.3146641254425049, acc = 0.8984375\n",
            "Batch 62: loss = 0.34398266673088074, acc = 0.888671875\n",
            "Batch 63: loss = 0.296109676361084, acc = 0.8984375\n",
            "Batch 64: loss = 0.29134443402290344, acc = 0.8974609375\n",
            "Batch 65: loss = 0.3098582327365875, acc = 0.8984375\n",
            "Batch 66: loss = 0.3144405484199524, acc = 0.8896484375\n",
            "Batch 67: loss = 0.324696809053421, acc = 0.8876953125\n",
            "Batch 68: loss = 0.3342006504535675, acc = 0.8818359375\n",
            "Batch 69: loss = 0.2606794238090515, acc = 0.9140625\n",
            "Batch 70: loss = 0.3468591570854187, acc = 0.88671875\n",
            "Batch 71: loss = 0.3484252095222473, acc = 0.880859375\n",
            "Batch 72: loss = 0.29641467332839966, acc = 0.90234375\n",
            "Batch 73: loss = 0.3455260396003723, acc = 0.8955078125\n",
            "Batch 74: loss = 0.36850300431251526, acc = 0.875\n",
            "Batch 75: loss = 0.36386728286743164, acc = 0.8779296875\n",
            "Batch 76: loss = 0.3291723132133484, acc = 0.8916015625\n",
            "Batch 77: loss = 0.3024921715259552, acc = 0.8916015625\n",
            "Batch 78: loss = 0.32350414991378784, acc = 0.890625\n",
            "Batch 79: loss = 0.3186212480068207, acc = 0.88671875\n",
            "Batch 80: loss = 0.31029143929481506, acc = 0.884765625\n",
            "Batch 81: loss = 0.342250257730484, acc = 0.88671875\n",
            "Batch 82: loss = 0.3217860758304596, acc = 0.8876953125\n",
            "Batch 83: loss = 0.3360678255558014, acc = 0.8779296875\n",
            "Batch 84: loss = 0.32277098298072815, acc = 0.8837890625\n",
            "Batch 85: loss = 0.3500596284866333, acc = 0.8818359375\n",
            "Batch 86: loss = 0.30687975883483887, acc = 0.8974609375\n",
            "Batch 87: loss = 0.29775723814964294, acc = 0.900390625\n",
            "Batch 88: loss = 0.34031590819358826, acc = 0.892578125\n",
            "Batch 89: loss = 0.2951207160949707, acc = 0.9091796875\n",
            "Batch 90: loss = 0.33207839727401733, acc = 0.8837890625\n",
            "Batch 91: loss = 0.3627128303050995, acc = 0.87890625\n",
            "Batch 92: loss = 0.3285810947418213, acc = 0.8916015625\n",
            "Batch 93: loss = 0.29138779640197754, acc = 0.8994140625\n",
            "Batch 94: loss = 0.2862819731235504, acc = 0.8984375\n",
            "Batch 95: loss = 0.3329140245914459, acc = 0.8798828125\n",
            "Batch 96: loss = 0.37184223532676697, acc = 0.8720703125\n",
            "Batch 97: loss = 0.3053012490272522, acc = 0.900390625\n",
            "Batch 98: loss = 0.33328208327293396, acc = 0.8896484375\n",
            "Batch 99: loss = 0.34660130739212036, acc = 0.896484375\n",
            "Batch 100: loss = 0.3266105055809021, acc = 0.87890625\n",
            "Batch 101: loss = 0.3284938633441925, acc = 0.8857421875\n",
            "Batch 102: loss = 0.3539552390575409, acc = 0.8818359375\n",
            "Batch 103: loss = 0.3420710861682892, acc = 0.884765625\n",
            "Batch 104: loss = 0.298353374004364, acc = 0.9072265625\n",
            "Batch 105: loss = 0.29112204909324646, acc = 0.9033203125\n",
            "Batch 106: loss = 0.3051110804080963, acc = 0.9033203125\n",
            "Batch 107: loss = 0.32199084758758545, acc = 0.8984375\n",
            "Batch 108: loss = 0.3269348740577698, acc = 0.89453125\n",
            "Batch 109: loss = 0.3081338405609131, acc = 0.8935546875\n",
            "Batch 110: loss = 0.2969760000705719, acc = 0.9013671875\n",
            "Batch 111: loss = 0.341813862323761, acc = 0.884765625\n",
            "Batch 112: loss = 0.3298247754573822, acc = 0.8876953125\n",
            "Batch 113: loss = 0.2836504876613617, acc = 0.91015625\n",
            "Batch 114: loss = 0.3180423676967621, acc = 0.892578125\n",
            "Batch 115: loss = 0.3270024359226227, acc = 0.890625\n",
            "Batch 116: loss = 0.3184576630592346, acc = 0.888671875\n",
            "Batch 117: loss = 0.32212603092193604, acc = 0.904296875\n",
            "Batch 118: loss = 0.3163495361804962, acc = 0.900390625\n",
            "Batch 119: loss = 0.31891804933547974, acc = 0.88671875\n",
            "Batch 120: loss = 0.3237980604171753, acc = 0.890625\n",
            "Batch 121: loss = 0.2696547508239746, acc = 0.91015625\n",
            "Batch 122: loss = 0.2892918586730957, acc = 0.900390625\n",
            "Batch 123: loss = 0.3104695677757263, acc = 0.8935546875\n",
            "Batch 124: loss = 0.3679537773132324, acc = 0.8740234375\n",
            "Batch 125: loss = 0.3427025377750397, acc = 0.87890625\n",
            "Batch 126: loss = 0.36527496576309204, acc = 0.8720703125\n",
            "\n",
            "Epoch 70/100\n",
            "Batch 1: loss = 0.4115287661552429, acc = 0.8779296875\n",
            "Batch 2: loss = 0.3429484963417053, acc = 0.875\n",
            "Batch 3: loss = 0.33893856406211853, acc = 0.8896484375\n",
            "Batch 4: loss = 0.29763999581336975, acc = 0.9091796875\n",
            "Batch 5: loss = 0.31077060103416443, acc = 0.8984375\n",
            "Batch 6: loss = 0.35704272985458374, acc = 0.88671875\n",
            "Batch 7: loss = 0.2979338765144348, acc = 0.8994140625\n",
            "Batch 8: loss = 0.34622466564178467, acc = 0.890625\n",
            "Batch 9: loss = 0.3062145709991455, acc = 0.8984375\n",
            "Batch 10: loss = 0.30019819736480713, acc = 0.9033203125\n",
            "Batch 11: loss = 0.3157101571559906, acc = 0.8896484375\n",
            "Batch 12: loss = 0.30906009674072266, acc = 0.900390625\n",
            "Batch 13: loss = 0.2987533509731293, acc = 0.8984375\n",
            "Batch 14: loss = 0.3158170282840729, acc = 0.8984375\n",
            "Batch 15: loss = 0.2870367765426636, acc = 0.9033203125\n",
            "Batch 16: loss = 0.3576667904853821, acc = 0.8837890625\n",
            "Batch 17: loss = 0.31049108505249023, acc = 0.9033203125\n",
            "Batch 18: loss = 0.33650869131088257, acc = 0.880859375\n",
            "Batch 19: loss = 0.31652137637138367, acc = 0.90234375\n",
            "Batch 20: loss = 0.35885530710220337, acc = 0.8798828125\n",
            "Batch 21: loss = 0.3379420340061188, acc = 0.880859375\n",
            "Batch 22: loss = 0.29303640127182007, acc = 0.90234375\n",
            "Batch 23: loss = 0.34271594882011414, acc = 0.8779296875\n",
            "Batch 24: loss = 0.32144880294799805, acc = 0.8935546875\n",
            "Batch 25: loss = 0.3240754306316376, acc = 0.900390625\n",
            "Batch 26: loss = 0.2901647686958313, acc = 0.904296875\n",
            "Batch 27: loss = 0.35078051686286926, acc = 0.8818359375\n",
            "Batch 28: loss = 0.3355844020843506, acc = 0.8857421875\n",
            "Batch 29: loss = 0.31289583444595337, acc = 0.8896484375\n",
            "Batch 30: loss = 0.31127703189849854, acc = 0.890625\n",
            "Batch 31: loss = 0.3750041425228119, acc = 0.888671875\n",
            "Batch 32: loss = 0.36313679814338684, acc = 0.8779296875\n",
            "Batch 33: loss = 0.2870538830757141, acc = 0.908203125\n",
            "Batch 34: loss = 0.31143155694007874, acc = 0.88671875\n",
            "Batch 35: loss = 0.3149489760398865, acc = 0.900390625\n",
            "Batch 36: loss = 0.25312286615371704, acc = 0.9072265625\n",
            "Batch 37: loss = 0.28023433685302734, acc = 0.9033203125\n",
            "Batch 38: loss = 0.2955997586250305, acc = 0.9052734375\n",
            "Batch 39: loss = 0.3057541847229004, acc = 0.8974609375\n",
            "Batch 40: loss = 0.3054286241531372, acc = 0.8955078125\n",
            "Batch 41: loss = 0.292377769947052, acc = 0.896484375\n",
            "Batch 42: loss = 0.32689228653907776, acc = 0.8935546875\n",
            "Batch 43: loss = 0.3318331837654114, acc = 0.8974609375\n",
            "Batch 44: loss = 0.2788749039173126, acc = 0.9140625\n",
            "Batch 45: loss = 0.2721724510192871, acc = 0.9033203125\n",
            "Batch 46: loss = 0.2708612084388733, acc = 0.916015625\n",
            "Batch 47: loss = 0.2794772982597351, acc = 0.90234375\n",
            "Batch 48: loss = 0.3034786581993103, acc = 0.896484375\n",
            "Batch 49: loss = 0.30562108755111694, acc = 0.90234375\n",
            "Batch 50: loss = 0.2914969325065613, acc = 0.8984375\n",
            "Batch 51: loss = 0.3186267912387848, acc = 0.8984375\n",
            "Batch 52: loss = 0.3130950331687927, acc = 0.890625\n",
            "Batch 53: loss = 0.3023030757904053, acc = 0.8984375\n",
            "Batch 54: loss = 0.2616172730922699, acc = 0.9072265625\n",
            "Batch 55: loss = 0.28804999589920044, acc = 0.90234375\n",
            "Batch 56: loss = 0.31959953904151917, acc = 0.884765625\n",
            "Batch 57: loss = 0.34371840953826904, acc = 0.8876953125\n",
            "Batch 58: loss = 0.34913113713264465, acc = 0.8759765625\n",
            "Batch 59: loss = 0.26440635323524475, acc = 0.8994140625\n",
            "Batch 60: loss = 0.340982049703598, acc = 0.8857421875\n",
            "Batch 61: loss = 0.29819604754447937, acc = 0.8994140625\n",
            "Batch 62: loss = 0.34371432662010193, acc = 0.8876953125\n",
            "Batch 63: loss = 0.3043290972709656, acc = 0.9013671875\n",
            "Batch 64: loss = 0.28113970160484314, acc = 0.90625\n",
            "Batch 65: loss = 0.31337735056877136, acc = 0.8935546875\n",
            "Batch 66: loss = 0.3171212375164032, acc = 0.888671875\n",
            "Batch 67: loss = 0.30640730261802673, acc = 0.892578125\n",
            "Batch 68: loss = 0.3091731071472168, acc = 0.8974609375\n",
            "Batch 69: loss = 0.2862687408924103, acc = 0.900390625\n",
            "Batch 70: loss = 0.3177333474159241, acc = 0.888671875\n",
            "Batch 71: loss = 0.289186030626297, acc = 0.908203125\n",
            "Batch 72: loss = 0.28799542784690857, acc = 0.9052734375\n",
            "Batch 73: loss = 0.3399302363395691, acc = 0.884765625\n",
            "Batch 74: loss = 0.35121792554855347, acc = 0.8818359375\n",
            "Batch 75: loss = 0.36819446086883545, acc = 0.8642578125\n",
            "Batch 76: loss = 0.33512645959854126, acc = 0.8896484375\n",
            "Batch 77: loss = 0.3027905225753784, acc = 0.8896484375\n",
            "Batch 78: loss = 0.31696027517318726, acc = 0.896484375\n",
            "Batch 79: loss = 0.29805421829223633, acc = 0.8974609375\n",
            "Batch 80: loss = 0.30030742287635803, acc = 0.896484375\n",
            "Batch 81: loss = 0.32322603464126587, acc = 0.8935546875\n",
            "Batch 82: loss = 0.29929178953170776, acc = 0.8984375\n",
            "Batch 83: loss = 0.31731104850769043, acc = 0.8876953125\n",
            "Batch 84: loss = 0.31014305353164673, acc = 0.8837890625\n",
            "Batch 85: loss = 0.3626134991645813, acc = 0.8701171875\n",
            "Batch 86: loss = 0.2993096113204956, acc = 0.8994140625\n",
            "Batch 87: loss = 0.3103017210960388, acc = 0.89453125\n",
            "Batch 88: loss = 0.3545284867286682, acc = 0.8876953125\n",
            "Batch 89: loss = 0.30467209219932556, acc = 0.8984375\n",
            "Batch 90: loss = 0.3465271294116974, acc = 0.8798828125\n",
            "Batch 91: loss = 0.32096630334854126, acc = 0.888671875\n",
            "Batch 92: loss = 0.3287614583969116, acc = 0.8798828125\n",
            "Batch 93: loss = 0.3020564019680023, acc = 0.890625\n",
            "Batch 94: loss = 0.28988730907440186, acc = 0.904296875\n",
            "Batch 95: loss = 0.3474268913269043, acc = 0.892578125\n",
            "Batch 96: loss = 0.34016990661621094, acc = 0.8818359375\n",
            "Batch 97: loss = 0.33041998744010925, acc = 0.90234375\n",
            "Batch 98: loss = 0.3196074664592743, acc = 0.8974609375\n",
            "Batch 99: loss = 0.3457382321357727, acc = 0.8798828125\n",
            "Batch 100: loss = 0.3258030414581299, acc = 0.8857421875\n",
            "Batch 101: loss = 0.29745253920555115, acc = 0.8984375\n",
            "Batch 102: loss = 0.3809219002723694, acc = 0.8671875\n",
            "Batch 103: loss = 0.35070016980171204, acc = 0.8828125\n",
            "Batch 104: loss = 0.3131754398345947, acc = 0.884765625\n",
            "Batch 105: loss = 0.3098616600036621, acc = 0.8984375\n",
            "Batch 106: loss = 0.29999327659606934, acc = 0.9013671875\n",
            "Batch 107: loss = 0.3320813477039337, acc = 0.8916015625\n",
            "Batch 108: loss = 0.2981807589530945, acc = 0.89453125\n",
            "Batch 109: loss = 0.3309710621833801, acc = 0.88671875\n",
            "Batch 110: loss = 0.28187477588653564, acc = 0.904296875\n",
            "Batch 111: loss = 0.31973201036453247, acc = 0.8896484375\n",
            "Batch 112: loss = 0.3166401982307434, acc = 0.90234375\n",
            "Batch 113: loss = 0.32911109924316406, acc = 0.8896484375\n",
            "Batch 114: loss = 0.310964435338974, acc = 0.8984375\n",
            "Batch 115: loss = 0.32703569531440735, acc = 0.892578125\n",
            "Batch 116: loss = 0.322953999042511, acc = 0.896484375\n",
            "Batch 117: loss = 0.29884573817253113, acc = 0.896484375\n",
            "Batch 118: loss = 0.27071666717529297, acc = 0.9072265625\n",
            "Batch 119: loss = 0.28281792998313904, acc = 0.9111328125\n",
            "Batch 120: loss = 0.2908612787723541, acc = 0.9052734375\n",
            "Batch 121: loss = 0.32873788475990295, acc = 0.888671875\n",
            "Batch 122: loss = 0.27803105115890503, acc = 0.91015625\n",
            "Batch 123: loss = 0.3258058428764343, acc = 0.8916015625\n",
            "Batch 124: loss = 0.3667265772819519, acc = 0.875\n",
            "Batch 125: loss = 0.33514660596847534, acc = 0.88671875\n",
            "Batch 126: loss = 0.3607633709907532, acc = 0.8779296875\n",
            "Saved checkpoint to weights.70.h5\n",
            "\n",
            "Epoch 71/100\n",
            "Batch 1: loss = 0.4149954319000244, acc = 0.8681640625\n",
            "Batch 2: loss = 0.32220780849456787, acc = 0.87890625\n",
            "Batch 3: loss = 0.33864903450012207, acc = 0.896484375\n",
            "Batch 4: loss = 0.28555622696876526, acc = 0.9091796875\n",
            "Batch 5: loss = 0.3191007375717163, acc = 0.8935546875\n",
            "Batch 6: loss = 0.3055911958217621, acc = 0.8955078125\n",
            "Batch 7: loss = 0.29292982816696167, acc = 0.9033203125\n",
            "Batch 8: loss = 0.34040772914886475, acc = 0.892578125\n",
            "Batch 9: loss = 0.31820592284202576, acc = 0.8974609375\n",
            "Batch 10: loss = 0.29338425397872925, acc = 0.90625\n",
            "Batch 11: loss = 0.2935144305229187, acc = 0.8974609375\n",
            "Batch 12: loss = 0.2959904074668884, acc = 0.90234375\n",
            "Batch 13: loss = 0.30006372928619385, acc = 0.8935546875\n",
            "Batch 14: loss = 0.28502434492111206, acc = 0.90625\n",
            "Batch 15: loss = 0.2959102988243103, acc = 0.90234375\n",
            "Batch 16: loss = 0.3284626007080078, acc = 0.892578125\n",
            "Batch 17: loss = 0.2907543480396271, acc = 0.908203125\n",
            "Batch 18: loss = 0.2993406355381012, acc = 0.900390625\n",
            "Batch 19: loss = 0.3125511109828949, acc = 0.890625\n",
            "Batch 20: loss = 0.30199599266052246, acc = 0.8994140625\n",
            "Batch 21: loss = 0.2954695522785187, acc = 0.8935546875\n",
            "Batch 22: loss = 0.2940638065338135, acc = 0.8955078125\n",
            "Batch 23: loss = 0.33089330792427063, acc = 0.8876953125\n",
            "Batch 24: loss = 0.3169563412666321, acc = 0.8994140625\n",
            "Batch 25: loss = 0.32624953985214233, acc = 0.8935546875\n",
            "Batch 26: loss = 0.3043529689311981, acc = 0.896484375\n",
            "Batch 27: loss = 0.34588050842285156, acc = 0.8798828125\n",
            "Batch 28: loss = 0.29610541462898254, acc = 0.9013671875\n",
            "Batch 29: loss = 0.31306976079940796, acc = 0.9033203125\n",
            "Batch 30: loss = 0.30523017048835754, acc = 0.90625\n",
            "Batch 31: loss = 0.3572610020637512, acc = 0.8779296875\n",
            "Batch 32: loss = 0.37767118215560913, acc = 0.8662109375\n",
            "Batch 33: loss = 0.2847961485385895, acc = 0.8994140625\n",
            "Batch 34: loss = 0.30684953927993774, acc = 0.8876953125\n",
            "Batch 35: loss = 0.3127806782722473, acc = 0.90234375\n",
            "Batch 36: loss = 0.27014827728271484, acc = 0.904296875\n",
            "Batch 37: loss = 0.2760125696659088, acc = 0.9130859375\n",
            "Batch 38: loss = 0.2816992998123169, acc = 0.9072265625\n",
            "Batch 39: loss = 0.3240640163421631, acc = 0.8935546875\n",
            "Batch 40: loss = 0.323284775018692, acc = 0.890625\n",
            "Batch 41: loss = 0.30177274346351624, acc = 0.89453125\n",
            "Batch 42: loss = 0.3076157569885254, acc = 0.9013671875\n",
            "Batch 43: loss = 0.3243976831436157, acc = 0.89453125\n",
            "Batch 44: loss = 0.2833118438720703, acc = 0.9013671875\n",
            "Batch 45: loss = 0.2775222659111023, acc = 0.90234375\n",
            "Batch 46: loss = 0.28082501888275146, acc = 0.90625\n",
            "Batch 47: loss = 0.30606797337532043, acc = 0.8935546875\n",
            "Batch 48: loss = 0.2719806432723999, acc = 0.9033203125\n",
            "Batch 49: loss = 0.30759140849113464, acc = 0.904296875\n",
            "Batch 50: loss = 0.27711233496665955, acc = 0.90625\n",
            "Batch 51: loss = 0.29177576303482056, acc = 0.9140625\n",
            "Batch 52: loss = 0.3122684955596924, acc = 0.9013671875\n",
            "Batch 53: loss = 0.3042360544204712, acc = 0.900390625\n",
            "Batch 54: loss = 0.27467143535614014, acc = 0.9033203125\n",
            "Batch 55: loss = 0.2774547338485718, acc = 0.908203125\n",
            "Batch 56: loss = 0.33210480213165283, acc = 0.888671875\n",
            "Batch 57: loss = 0.31144389510154724, acc = 0.8974609375\n",
            "Batch 58: loss = 0.3508117198944092, acc = 0.8798828125\n",
            "Batch 59: loss = 0.2813880145549774, acc = 0.9052734375\n",
            "Batch 60: loss = 0.3368789553642273, acc = 0.88671875\n",
            "Batch 61: loss = 0.29942071437835693, acc = 0.8955078125\n",
            "Batch 62: loss = 0.33922436833381653, acc = 0.890625\n",
            "Batch 63: loss = 0.2624204158782959, acc = 0.9208984375\n",
            "Batch 64: loss = 0.28581249713897705, acc = 0.9013671875\n",
            "Batch 65: loss = 0.3295322358608246, acc = 0.884765625\n",
            "Batch 66: loss = 0.29187700152397156, acc = 0.9033203125\n",
            "Batch 67: loss = 0.3312966525554657, acc = 0.8857421875\n",
            "Batch 68: loss = 0.33334970474243164, acc = 0.8798828125\n",
            "Batch 69: loss = 0.2719535231590271, acc = 0.9013671875\n",
            "Batch 70: loss = 0.3399496376514435, acc = 0.884765625\n",
            "Batch 71: loss = 0.33895057439804077, acc = 0.8828125\n",
            "Batch 72: loss = 0.30319100618362427, acc = 0.90625\n",
            "Batch 73: loss = 0.34335577487945557, acc = 0.8837890625\n",
            "Batch 74: loss = 0.31263071298599243, acc = 0.8984375\n",
            "Batch 75: loss = 0.34104597568511963, acc = 0.8779296875\n",
            "Batch 76: loss = 0.32075974345207214, acc = 0.8935546875\n",
            "Batch 77: loss = 0.27571409940719604, acc = 0.9033203125\n",
            "Batch 78: loss = 0.30675822496414185, acc = 0.892578125\n",
            "Batch 79: loss = 0.31639689207077026, acc = 0.8974609375\n",
            "Batch 80: loss = 0.2950979769229889, acc = 0.8984375\n",
            "Batch 81: loss = 0.2998557388782501, acc = 0.9091796875\n",
            "Batch 82: loss = 0.29529571533203125, acc = 0.90234375\n",
            "Batch 83: loss = 0.31207433342933655, acc = 0.8857421875\n",
            "Batch 84: loss = 0.304009348154068, acc = 0.8984375\n",
            "Batch 85: loss = 0.3266025483608246, acc = 0.8876953125\n",
            "Batch 86: loss = 0.3114146292209625, acc = 0.888671875\n",
            "Batch 87: loss = 0.2847612500190735, acc = 0.900390625\n",
            "Batch 88: loss = 0.3441377878189087, acc = 0.888671875\n",
            "Batch 89: loss = 0.2984529733657837, acc = 0.900390625\n",
            "Batch 90: loss = 0.3215433359146118, acc = 0.8994140625\n",
            "Batch 91: loss = 0.34242987632751465, acc = 0.8828125\n",
            "Batch 92: loss = 0.34559792280197144, acc = 0.8779296875\n",
            "Batch 93: loss = 0.27458974719047546, acc = 0.9150390625\n",
            "Batch 94: loss = 0.3062348961830139, acc = 0.89453125\n",
            "Batch 95: loss = 0.3465576767921448, acc = 0.8759765625\n",
            "Batch 96: loss = 0.3214397132396698, acc = 0.8837890625\n",
            "Batch 97: loss = 0.3047909736633301, acc = 0.892578125\n",
            "Batch 98: loss = 0.30255118012428284, acc = 0.904296875\n",
            "Batch 99: loss = 0.3222319781780243, acc = 0.8837890625\n",
            "Batch 100: loss = 0.3386072814464569, acc = 0.876953125\n",
            "Batch 101: loss = 0.32736435532569885, acc = 0.892578125\n",
            "Batch 102: loss = 0.3421574532985687, acc = 0.87109375\n",
            "Batch 103: loss = 0.3187163472175598, acc = 0.8896484375\n",
            "Batch 104: loss = 0.2745884656906128, acc = 0.90625\n",
            "Batch 105: loss = 0.31406083703041077, acc = 0.892578125\n",
            "Batch 106: loss = 0.2904539108276367, acc = 0.900390625\n",
            "Batch 107: loss = 0.3191157579421997, acc = 0.8974609375\n",
            "Batch 108: loss = 0.2927415072917938, acc = 0.8955078125\n",
            "Batch 109: loss = 0.33868035674095154, acc = 0.890625\n",
            "Batch 110: loss = 0.3109140694141388, acc = 0.888671875\n",
            "Batch 111: loss = 0.3433612585067749, acc = 0.8828125\n",
            "Batch 112: loss = 0.3304600119590759, acc = 0.8896484375\n",
            "Batch 113: loss = 0.3351632356643677, acc = 0.8876953125\n",
            "Batch 114: loss = 0.3228359520435333, acc = 0.8984375\n",
            "Batch 115: loss = 0.328500360250473, acc = 0.8935546875\n",
            "Batch 116: loss = 0.3100687265396118, acc = 0.904296875\n",
            "Batch 117: loss = 0.3235647976398468, acc = 0.888671875\n",
            "Batch 118: loss = 0.28343456983566284, acc = 0.9013671875\n",
            "Batch 119: loss = 0.3134463429450989, acc = 0.8857421875\n",
            "Batch 120: loss = 0.27936774492263794, acc = 0.90625\n",
            "Batch 121: loss = 0.30335402488708496, acc = 0.896484375\n",
            "Batch 122: loss = 0.29365625977516174, acc = 0.9033203125\n",
            "Batch 123: loss = 0.29298362135887146, acc = 0.9033203125\n",
            "Batch 124: loss = 0.3383899927139282, acc = 0.875\n",
            "Batch 125: loss = 0.3548254370689392, acc = 0.875\n",
            "Batch 126: loss = 0.3305214047431946, acc = 0.8916015625\n",
            "\n",
            "Epoch 72/100\n",
            "Batch 1: loss = 0.42442336678504944, acc = 0.859375\n",
            "Batch 2: loss = 0.34326785802841187, acc = 0.8837890625\n",
            "Batch 3: loss = 0.3328796625137329, acc = 0.890625\n",
            "Batch 4: loss = 0.31720027327537537, acc = 0.8994140625\n",
            "Batch 5: loss = 0.33442461490631104, acc = 0.892578125\n",
            "Batch 6: loss = 0.32338035106658936, acc = 0.8857421875\n",
            "Batch 7: loss = 0.3073444962501526, acc = 0.896484375\n",
            "Batch 8: loss = 0.3265226483345032, acc = 0.8837890625\n",
            "Batch 9: loss = 0.30863749980926514, acc = 0.896484375\n",
            "Batch 10: loss = 0.2781267762184143, acc = 0.908203125\n",
            "Batch 11: loss = 0.32881227135658264, acc = 0.892578125\n",
            "Batch 12: loss = 0.30261531472206116, acc = 0.8984375\n",
            "Batch 13: loss = 0.3013964891433716, acc = 0.896484375\n",
            "Batch 14: loss = 0.30361559987068176, acc = 0.8916015625\n",
            "Batch 15: loss = 0.30731433629989624, acc = 0.888671875\n",
            "Batch 16: loss = 0.3120706081390381, acc = 0.8916015625\n",
            "Batch 17: loss = 0.30502524971961975, acc = 0.900390625\n",
            "Batch 18: loss = 0.31572985649108887, acc = 0.88671875\n",
            "Batch 19: loss = 0.318198025226593, acc = 0.8935546875\n",
            "Batch 20: loss = 0.3015150725841522, acc = 0.89453125\n",
            "Batch 21: loss = 0.37671831250190735, acc = 0.876953125\n",
            "Batch 22: loss = 0.28227996826171875, acc = 0.9033203125\n",
            "Batch 23: loss = 0.3320913314819336, acc = 0.8857421875\n",
            "Batch 24: loss = 0.32613229751586914, acc = 0.884765625\n",
            "Batch 25: loss = 0.2940250635147095, acc = 0.9072265625\n",
            "Batch 26: loss = 0.33032169938087463, acc = 0.8876953125\n",
            "Batch 27: loss = 0.35126668214797974, acc = 0.8759765625\n",
            "Batch 28: loss = 0.31018543243408203, acc = 0.892578125\n",
            "Batch 29: loss = 0.3064117431640625, acc = 0.8876953125\n",
            "Batch 30: loss = 0.3098028600215912, acc = 0.8974609375\n",
            "Batch 31: loss = 0.3315728008747101, acc = 0.876953125\n",
            "Batch 32: loss = 0.3368496000766754, acc = 0.8837890625\n",
            "Batch 33: loss = 0.2883312404155731, acc = 0.912109375\n",
            "Batch 34: loss = 0.2976377308368683, acc = 0.89453125\n",
            "Batch 35: loss = 0.2802750766277313, acc = 0.9052734375\n",
            "Batch 36: loss = 0.27426645159721375, acc = 0.9033203125\n",
            "Batch 37: loss = 0.26492583751678467, acc = 0.912109375\n",
            "Batch 38: loss = 0.2712803781032562, acc = 0.9150390625\n",
            "Batch 39: loss = 0.2951650619506836, acc = 0.8955078125\n",
            "Batch 40: loss = 0.28089776635169983, acc = 0.9013671875\n",
            "Batch 41: loss = 0.29560035467147827, acc = 0.8955078125\n",
            "Batch 42: loss = 0.31923580169677734, acc = 0.8916015625\n",
            "Batch 43: loss = 0.331636905670166, acc = 0.8916015625\n",
            "Batch 44: loss = 0.2677026093006134, acc = 0.9140625\n",
            "Batch 45: loss = 0.288271427154541, acc = 0.904296875\n",
            "Batch 46: loss = 0.255993127822876, acc = 0.916015625\n",
            "Batch 47: loss = 0.2999560832977295, acc = 0.8994140625\n",
            "Batch 48: loss = 0.28523361682891846, acc = 0.916015625\n",
            "Batch 49: loss = 0.2882816791534424, acc = 0.908203125\n",
            "Batch 50: loss = 0.2635785937309265, acc = 0.912109375\n",
            "Batch 51: loss = 0.3032664358615875, acc = 0.8994140625\n",
            "Batch 52: loss = 0.33863386511802673, acc = 0.8828125\n",
            "Batch 53: loss = 0.30680567026138306, acc = 0.888671875\n",
            "Batch 54: loss = 0.24679720401763916, acc = 0.91796875\n",
            "Batch 55: loss = 0.28701087832450867, acc = 0.9033203125\n",
            "Batch 56: loss = 0.318766325712204, acc = 0.8876953125\n",
            "Batch 57: loss = 0.31940293312072754, acc = 0.8896484375\n",
            "Batch 58: loss = 0.3423587679862976, acc = 0.8818359375\n",
            "Batch 59: loss = 0.242015078663826, acc = 0.9189453125\n",
            "Batch 60: loss = 0.3114486336708069, acc = 0.90234375\n",
            "Batch 61: loss = 0.27973538637161255, acc = 0.9091796875\n",
            "Batch 62: loss = 0.3086693286895752, acc = 0.90625\n",
            "Batch 63: loss = 0.29956501722335815, acc = 0.9013671875\n",
            "Batch 64: loss = 0.28901052474975586, acc = 0.9072265625\n",
            "Batch 65: loss = 0.3373550474643707, acc = 0.884765625\n",
            "Batch 66: loss = 0.29999393224716187, acc = 0.9033203125\n",
            "Batch 67: loss = 0.29187172651290894, acc = 0.8994140625\n",
            "Batch 68: loss = 0.32509657740592957, acc = 0.880859375\n",
            "Batch 69: loss = 0.2597310543060303, acc = 0.9140625\n",
            "Batch 70: loss = 0.3560692071914673, acc = 0.8818359375\n",
            "Batch 71: loss = 0.28565528988838196, acc = 0.8994140625\n",
            "Batch 72: loss = 0.2953333854675293, acc = 0.9033203125\n",
            "Batch 73: loss = 0.3275519907474518, acc = 0.89453125\n",
            "Batch 74: loss = 0.3308156132698059, acc = 0.8994140625\n",
            "Batch 75: loss = 0.35212549567222595, acc = 0.8818359375\n",
            "Batch 76: loss = 0.3019910454750061, acc = 0.896484375\n",
            "Batch 77: loss = 0.2999647855758667, acc = 0.880859375\n",
            "Batch 78: loss = 0.32026559114456177, acc = 0.892578125\n",
            "Batch 79: loss = 0.2990753650665283, acc = 0.892578125\n",
            "Batch 80: loss = 0.26603052020072937, acc = 0.9091796875\n",
            "Batch 81: loss = 0.31505703926086426, acc = 0.8896484375\n",
            "Batch 82: loss = 0.30522969365119934, acc = 0.896484375\n",
            "Batch 83: loss = 0.2913227677345276, acc = 0.900390625\n",
            "Batch 84: loss = 0.3025168180465698, acc = 0.89453125\n",
            "Batch 85: loss = 0.3533936142921448, acc = 0.8828125\n",
            "Batch 86: loss = 0.2948456406593323, acc = 0.8974609375\n",
            "Batch 87: loss = 0.3089028000831604, acc = 0.888671875\n",
            "Batch 88: loss = 0.30893418192863464, acc = 0.884765625\n",
            "Batch 89: loss = 0.29541200399398804, acc = 0.8984375\n",
            "Batch 90: loss = 0.31096500158309937, acc = 0.89453125\n",
            "Batch 91: loss = 0.32761576771736145, acc = 0.880859375\n",
            "Batch 92: loss = 0.3177645206451416, acc = 0.896484375\n",
            "Batch 93: loss = 0.2913662791252136, acc = 0.900390625\n",
            "Batch 94: loss = 0.2760460078716278, acc = 0.896484375\n",
            "Batch 95: loss = 0.2808510959148407, acc = 0.896484375\n",
            "Batch 96: loss = 0.3614726960659027, acc = 0.87890625\n",
            "Batch 97: loss = 0.32102489471435547, acc = 0.8974609375\n",
            "Batch 98: loss = 0.3074861466884613, acc = 0.8974609375\n",
            "Batch 99: loss = 0.3065272867679596, acc = 0.8916015625\n",
            "Batch 100: loss = 0.2998884320259094, acc = 0.8916015625\n",
            "Batch 101: loss = 0.3233990967273712, acc = 0.8818359375\n",
            "Batch 102: loss = 0.32660526037216187, acc = 0.8896484375\n",
            "Batch 103: loss = 0.2784620523452759, acc = 0.900390625\n",
            "Batch 104: loss = 0.27039703726768494, acc = 0.908203125\n",
            "Batch 105: loss = 0.30078378319740295, acc = 0.908203125\n",
            "Batch 106: loss = 0.3205234408378601, acc = 0.8955078125\n",
            "Batch 107: loss = 0.29564929008483887, acc = 0.896484375\n",
            "Batch 108: loss = 0.30676522850990295, acc = 0.8837890625\n",
            "Batch 109: loss = 0.31472986936569214, acc = 0.8935546875\n",
            "Batch 110: loss = 0.2690175771713257, acc = 0.904296875\n",
            "Batch 111: loss = 0.31095099449157715, acc = 0.90234375\n",
            "Batch 112: loss = 0.2879486680030823, acc = 0.908203125\n",
            "Batch 113: loss = 0.30498871207237244, acc = 0.8974609375\n",
            "Batch 114: loss = 0.3136901259422302, acc = 0.896484375\n",
            "Batch 115: loss = 0.31172168254852295, acc = 0.8916015625\n",
            "Batch 116: loss = 0.3251512050628662, acc = 0.89453125\n",
            "Batch 117: loss = 0.31881704926490784, acc = 0.890625\n",
            "Batch 118: loss = 0.28662779927253723, acc = 0.90234375\n",
            "Batch 119: loss = 0.2994905114173889, acc = 0.8994140625\n",
            "Batch 120: loss = 0.2664463520050049, acc = 0.9091796875\n",
            "Batch 121: loss = 0.3202706575393677, acc = 0.88671875\n",
            "Batch 122: loss = 0.26593196392059326, acc = 0.9072265625\n",
            "Batch 123: loss = 0.30718743801116943, acc = 0.8955078125\n",
            "Batch 124: loss = 0.32593199610710144, acc = 0.896484375\n",
            "Batch 125: loss = 0.3194217085838318, acc = 0.884765625\n",
            "Batch 126: loss = 0.3337574005126953, acc = 0.8837890625\n",
            "\n",
            "Epoch 73/100\n",
            "Batch 1: loss = 0.4108092188835144, acc = 0.8701171875\n",
            "Batch 2: loss = 0.367736279964447, acc = 0.87890625\n",
            "Batch 3: loss = 0.3258287310600281, acc = 0.896484375\n",
            "Batch 4: loss = 0.283185750246048, acc = 0.9052734375\n",
            "Batch 5: loss = 0.3156278431415558, acc = 0.9033203125\n",
            "Batch 6: loss = 0.3063364326953888, acc = 0.8935546875\n",
            "Batch 7: loss = 0.2915436327457428, acc = 0.8984375\n",
            "Batch 8: loss = 0.28792816400527954, acc = 0.908203125\n",
            "Batch 9: loss = 0.30471593141555786, acc = 0.89453125\n",
            "Batch 10: loss = 0.2770758271217346, acc = 0.9072265625\n",
            "Batch 11: loss = 0.2745426595211029, acc = 0.908203125\n",
            "Batch 12: loss = 0.28037500381469727, acc = 0.90234375\n",
            "Batch 13: loss = 0.2828320264816284, acc = 0.904296875\n",
            "Batch 14: loss = 0.33060184121131897, acc = 0.8896484375\n",
            "Batch 15: loss = 0.2772897481918335, acc = 0.90625\n",
            "Batch 16: loss = 0.32530033588409424, acc = 0.890625\n",
            "Batch 17: loss = 0.29076001048088074, acc = 0.8974609375\n",
            "Batch 18: loss = 0.28824272751808167, acc = 0.8955078125\n",
            "Batch 19: loss = 0.3030332028865814, acc = 0.9052734375\n",
            "Batch 20: loss = 0.3137691020965576, acc = 0.890625\n",
            "Batch 21: loss = 0.30023911595344543, acc = 0.900390625\n",
            "Batch 22: loss = 0.3470079004764557, acc = 0.884765625\n",
            "Batch 23: loss = 0.28029197454452515, acc = 0.9013671875\n",
            "Batch 24: loss = 0.30633479356765747, acc = 0.8994140625\n",
            "Batch 25: loss = 0.3064700663089752, acc = 0.896484375\n",
            "Batch 26: loss = 0.28758564591407776, acc = 0.90625\n",
            "Batch 27: loss = 0.3467523455619812, acc = 0.8828125\n",
            "Batch 28: loss = 0.2874751389026642, acc = 0.9072265625\n",
            "Batch 29: loss = 0.29265424609184265, acc = 0.9130859375\n",
            "Batch 30: loss = 0.3124496638774872, acc = 0.900390625\n",
            "Batch 31: loss = 0.32090386748313904, acc = 0.89453125\n",
            "Batch 32: loss = 0.324996680021286, acc = 0.892578125\n",
            "Batch 33: loss = 0.255360871553421, acc = 0.919921875\n",
            "Batch 34: loss = 0.3154774010181427, acc = 0.8857421875\n",
            "Batch 35: loss = 0.31179535388946533, acc = 0.890625\n",
            "Batch 36: loss = 0.2681022584438324, acc = 0.9189453125\n",
            "Batch 37: loss = 0.27474772930145264, acc = 0.9140625\n",
            "Batch 38: loss = 0.3057847321033478, acc = 0.89453125\n",
            "Batch 39: loss = 0.2864278554916382, acc = 0.9111328125\n",
            "Batch 40: loss = 0.303068608045578, acc = 0.8994140625\n",
            "Batch 41: loss = 0.2829943299293518, acc = 0.9013671875\n",
            "Batch 42: loss = 0.3298722803592682, acc = 0.8896484375\n",
            "Batch 43: loss = 0.31858789920806885, acc = 0.8896484375\n",
            "Batch 44: loss = 0.2640436589717865, acc = 0.9189453125\n",
            "Batch 45: loss = 0.2709559202194214, acc = 0.904296875\n",
            "Batch 46: loss = 0.2635587155818939, acc = 0.912109375\n",
            "Batch 47: loss = 0.31943005323410034, acc = 0.8984375\n",
            "Batch 48: loss = 0.272163987159729, acc = 0.9033203125\n",
            "Batch 49: loss = 0.293118417263031, acc = 0.90234375\n",
            "Batch 50: loss = 0.25819358229637146, acc = 0.9111328125\n",
            "Batch 51: loss = 0.28274646401405334, acc = 0.8984375\n",
            "Batch 52: loss = 0.27864477038383484, acc = 0.904296875\n",
            "Batch 53: loss = 0.2808741629123688, acc = 0.904296875\n",
            "Batch 54: loss = 0.2479812055826187, acc = 0.9140625\n",
            "Batch 55: loss = 0.30588653683662415, acc = 0.8974609375\n",
            "Batch 56: loss = 0.2806037664413452, acc = 0.896484375\n",
            "Batch 57: loss = 0.3160732388496399, acc = 0.88671875\n",
            "Batch 58: loss = 0.3352500796318054, acc = 0.8837890625\n",
            "Batch 59: loss = 0.2545386850833893, acc = 0.9140625\n",
            "Batch 60: loss = 0.3219050467014313, acc = 0.896484375\n",
            "Batch 61: loss = 0.2867960035800934, acc = 0.9091796875\n",
            "Batch 62: loss = 0.3419659435749054, acc = 0.8818359375\n",
            "Batch 63: loss = 0.29285919666290283, acc = 0.8974609375\n",
            "Batch 64: loss = 0.26100391149520874, acc = 0.9169921875\n",
            "Batch 65: loss = 0.3465735912322998, acc = 0.8857421875\n",
            "Batch 66: loss = 0.30936625599861145, acc = 0.9013671875\n",
            "Batch 67: loss = 0.3333870768547058, acc = 0.8896484375\n",
            "Batch 68: loss = 0.3282809853553772, acc = 0.88671875\n",
            "Batch 69: loss = 0.27269357442855835, acc = 0.9072265625\n",
            "Batch 70: loss = 0.32659927010536194, acc = 0.88671875\n",
            "Batch 71: loss = 0.2962988018989563, acc = 0.892578125\n",
            "Batch 72: loss = 0.29469117522239685, acc = 0.900390625\n",
            "Batch 73: loss = 0.3331332206726074, acc = 0.9072265625\n",
            "Batch 74: loss = 0.3359316885471344, acc = 0.88671875\n",
            "Batch 75: loss = 0.3574223518371582, acc = 0.8798828125\n",
            "Batch 76: loss = 0.293579638004303, acc = 0.896484375\n",
            "Batch 77: loss = 0.30801627039909363, acc = 0.892578125\n",
            "Batch 78: loss = 0.30447885394096375, acc = 0.8974609375\n",
            "Batch 79: loss = 0.2921244204044342, acc = 0.9013671875\n",
            "Batch 80: loss = 0.2554628252983093, acc = 0.916015625\n",
            "Batch 81: loss = 0.30743953585624695, acc = 0.890625\n",
            "Batch 82: loss = 0.30998119711875916, acc = 0.8994140625\n",
            "Batch 83: loss = 0.31449973583221436, acc = 0.8876953125\n",
            "Batch 84: loss = 0.2959947884082794, acc = 0.9033203125\n",
            "Batch 85: loss = 0.3191196322441101, acc = 0.8857421875\n",
            "Batch 86: loss = 0.2989267110824585, acc = 0.900390625\n",
            "Batch 87: loss = 0.26677027344703674, acc = 0.9091796875\n",
            "Batch 88: loss = 0.3154100775718689, acc = 0.8876953125\n",
            "Batch 89: loss = 0.2795413136482239, acc = 0.9130859375\n",
            "Batch 90: loss = 0.3470749855041504, acc = 0.884765625\n",
            "Batch 91: loss = 0.2885594666004181, acc = 0.8955078125\n",
            "Batch 92: loss = 0.3346303105354309, acc = 0.892578125\n",
            "Batch 93: loss = 0.26543787121772766, acc = 0.912109375\n",
            "Batch 94: loss = 0.2882552444934845, acc = 0.8984375\n",
            "Batch 95: loss = 0.30932825803756714, acc = 0.8994140625\n",
            "Batch 96: loss = 0.3422689735889435, acc = 0.880859375\n",
            "Batch 97: loss = 0.33130204677581787, acc = 0.890625\n",
            "Batch 98: loss = 0.2820211350917816, acc = 0.904296875\n",
            "Batch 99: loss = 0.33365321159362793, acc = 0.890625\n",
            "Batch 100: loss = 0.284721702337265, acc = 0.9033203125\n",
            "Batch 101: loss = 0.2823764979839325, acc = 0.8994140625\n",
            "Batch 102: loss = 0.34122979640960693, acc = 0.888671875\n",
            "Batch 103: loss = 0.2996688783168793, acc = 0.896484375\n",
            "Batch 104: loss = 0.25564146041870117, acc = 0.919921875\n",
            "Batch 105: loss = 0.2560639977455139, acc = 0.916015625\n",
            "Batch 106: loss = 0.282895028591156, acc = 0.9111328125\n",
            "Batch 107: loss = 0.30722591280937195, acc = 0.9033203125\n",
            "Batch 108: loss = 0.29101797938346863, acc = 0.8994140625\n",
            "Batch 109: loss = 0.3230014443397522, acc = 0.8994140625\n",
            "Batch 110: loss = 0.3144528269767761, acc = 0.90625\n",
            "Batch 111: loss = 0.30504071712493896, acc = 0.9052734375\n",
            "Batch 112: loss = 0.2867460250854492, acc = 0.90234375\n",
            "Batch 113: loss = 0.3181280791759491, acc = 0.88671875\n",
            "Batch 114: loss = 0.2896089553833008, acc = 0.8994140625\n",
            "Batch 115: loss = 0.33769118785858154, acc = 0.880859375\n",
            "Batch 116: loss = 0.3396829664707184, acc = 0.8955078125\n",
            "Batch 117: loss = 0.3103889226913452, acc = 0.890625\n",
            "Batch 118: loss = 0.2747425436973572, acc = 0.91015625\n",
            "Batch 119: loss = 0.2956341803073883, acc = 0.8916015625\n",
            "Batch 120: loss = 0.25515449047088623, acc = 0.91015625\n",
            "Batch 121: loss = 0.29924476146698, acc = 0.90625\n",
            "Batch 122: loss = 0.2939003109931946, acc = 0.90234375\n",
            "Batch 123: loss = 0.28748658299446106, acc = 0.892578125\n",
            "Batch 124: loss = 0.35401859879493713, acc = 0.873046875\n",
            "Batch 125: loss = 0.3609822988510132, acc = 0.873046875\n",
            "Batch 126: loss = 0.3187579810619354, acc = 0.8984375\n",
            "\n",
            "Epoch 74/100\n",
            "Batch 1: loss = 0.36960551142692566, acc = 0.890625\n",
            "Batch 2: loss = 0.3114907741546631, acc = 0.8935546875\n",
            "Batch 3: loss = 0.32374370098114014, acc = 0.8896484375\n",
            "Batch 4: loss = 0.309888631105423, acc = 0.904296875\n",
            "Batch 5: loss = 0.3101791739463806, acc = 0.8994140625\n",
            "Batch 6: loss = 0.3313199281692505, acc = 0.8818359375\n",
            "Batch 7: loss = 0.3011469542980194, acc = 0.8974609375\n",
            "Batch 8: loss = 0.3230900168418884, acc = 0.8896484375\n",
            "Batch 9: loss = 0.33278900384902954, acc = 0.900390625\n",
            "Batch 10: loss = 0.28017476201057434, acc = 0.9052734375\n",
            "Batch 11: loss = 0.3132556080818176, acc = 0.8916015625\n",
            "Batch 12: loss = 0.28601545095443726, acc = 0.9091796875\n",
            "Batch 13: loss = 0.2727578282356262, acc = 0.90625\n",
            "Batch 14: loss = 0.28375259041786194, acc = 0.9072265625\n",
            "Batch 15: loss = 0.265868216753006, acc = 0.9072265625\n",
            "Batch 16: loss = 0.3323640823364258, acc = 0.8984375\n",
            "Batch 17: loss = 0.28276216983795166, acc = 0.9091796875\n",
            "Batch 18: loss = 0.3357318043708801, acc = 0.888671875\n",
            "Batch 19: loss = 0.29850244522094727, acc = 0.9052734375\n",
            "Batch 20: loss = 0.322250634431839, acc = 0.8984375\n",
            "Batch 21: loss = 0.31772947311401367, acc = 0.8994140625\n",
            "Batch 22: loss = 0.29898515343666077, acc = 0.8935546875\n",
            "Batch 23: loss = 0.2826950252056122, acc = 0.9091796875\n",
            "Batch 24: loss = 0.2811373174190521, acc = 0.9091796875\n",
            "Batch 25: loss = 0.35496169328689575, acc = 0.884765625\n",
            "Batch 26: loss = 0.2791106104850769, acc = 0.9013671875\n",
            "Batch 27: loss = 0.32893869280815125, acc = 0.8896484375\n",
            "Batch 28: loss = 0.2773546576499939, acc = 0.91015625\n",
            "Batch 29: loss = 0.31348979473114014, acc = 0.896484375\n",
            "Batch 30: loss = 0.3021688163280487, acc = 0.900390625\n",
            "Batch 31: loss = 0.3187514543533325, acc = 0.8857421875\n",
            "Batch 32: loss = 0.35980361700057983, acc = 0.884765625\n",
            "Batch 33: loss = 0.27864497900009155, acc = 0.912109375\n",
            "Batch 34: loss = 0.30284419655799866, acc = 0.900390625\n",
            "Batch 35: loss = 0.30154913663864136, acc = 0.9033203125\n",
            "Batch 36: loss = 0.2479187399148941, acc = 0.916015625\n",
            "Batch 37: loss = 0.27109381556510925, acc = 0.91796875\n",
            "Batch 38: loss = 0.2527982294559479, acc = 0.9189453125\n",
            "Batch 39: loss = 0.27846044301986694, acc = 0.9052734375\n",
            "Batch 40: loss = 0.2868598699569702, acc = 0.90625\n",
            "Batch 41: loss = 0.3071921169757843, acc = 0.900390625\n",
            "Batch 42: loss = 0.29589876532554626, acc = 0.900390625\n",
            "Batch 43: loss = 0.311110258102417, acc = 0.8974609375\n",
            "Batch 44: loss = 0.2652846872806549, acc = 0.9150390625\n",
            "Batch 45: loss = 0.2692602574825287, acc = 0.9169921875\n",
            "Batch 46: loss = 0.22241902351379395, acc = 0.9150390625\n",
            "Batch 47: loss = 0.27819594740867615, acc = 0.8974609375\n",
            "Batch 48: loss = 0.2684350311756134, acc = 0.9052734375\n",
            "Batch 49: loss = 0.30570292472839355, acc = 0.9033203125\n",
            "Batch 50: loss = 0.25057452917099, acc = 0.9150390625\n",
            "Batch 51: loss = 0.29022446274757385, acc = 0.90234375\n",
            "Batch 52: loss = 0.2925108075141907, acc = 0.90234375\n",
            "Batch 53: loss = 0.32374730706214905, acc = 0.884765625\n",
            "Batch 54: loss = 0.23988625407218933, acc = 0.9208984375\n",
            "Batch 55: loss = 0.28588172793388367, acc = 0.9033203125\n",
            "Batch 56: loss = 0.29285651445388794, acc = 0.90234375\n",
            "Batch 57: loss = 0.29025569558143616, acc = 0.89453125\n",
            "Batch 58: loss = 0.3119770884513855, acc = 0.8974609375\n",
            "Batch 59: loss = 0.2740594148635864, acc = 0.9072265625\n",
            "Batch 60: loss = 0.31583356857299805, acc = 0.880859375\n",
            "Batch 61: loss = 0.30897292494773865, acc = 0.900390625\n",
            "Batch 62: loss = 0.31726205348968506, acc = 0.8974609375\n",
            "Batch 63: loss = 0.28130045533180237, acc = 0.9033203125\n",
            "Batch 64: loss = 0.27524125576019287, acc = 0.908203125\n",
            "Batch 65: loss = 0.3137362003326416, acc = 0.90234375\n",
            "Batch 66: loss = 0.2852551341056824, acc = 0.904296875\n",
            "Batch 67: loss = 0.29231590032577515, acc = 0.8955078125\n",
            "Batch 68: loss = 0.31394606828689575, acc = 0.8876953125\n",
            "Batch 69: loss = 0.2690103352069855, acc = 0.9091796875\n",
            "Batch 70: loss = 0.2878890931606293, acc = 0.904296875\n",
            "Batch 71: loss = 0.3159438371658325, acc = 0.8837890625\n",
            "Batch 72: loss = 0.2591518461704254, acc = 0.91015625\n",
            "Batch 73: loss = 0.31659650802612305, acc = 0.8935546875\n",
            "Batch 74: loss = 0.30909785628318787, acc = 0.8984375\n",
            "Batch 75: loss = 0.37910494208335876, acc = 0.8701171875\n",
            "Batch 76: loss = 0.297626256942749, acc = 0.8994140625\n",
            "Batch 77: loss = 0.26624634861946106, acc = 0.9052734375\n",
            "Batch 78: loss = 0.27889755368232727, acc = 0.9052734375\n",
            "Batch 79: loss = 0.2802690863609314, acc = 0.904296875\n",
            "Batch 80: loss = 0.2921549379825592, acc = 0.8974609375\n",
            "Batch 81: loss = 0.29600387811660767, acc = 0.8935546875\n",
            "Batch 82: loss = 0.26446497440338135, acc = 0.90625\n",
            "Batch 83: loss = 0.2969619631767273, acc = 0.896484375\n",
            "Batch 84: loss = 0.29427796602249146, acc = 0.89453125\n",
            "Batch 85: loss = 0.33431801199913025, acc = 0.8779296875\n",
            "Batch 86: loss = 0.2970001697540283, acc = 0.9033203125\n",
            "Batch 87: loss = 0.2690902352333069, acc = 0.916015625\n",
            "Batch 88: loss = 0.32951635122299194, acc = 0.8974609375\n",
            "Batch 89: loss = 0.2891308665275574, acc = 0.90234375\n",
            "Batch 90: loss = 0.3361048698425293, acc = 0.8896484375\n",
            "Batch 91: loss = 0.31556612253189087, acc = 0.888671875\n",
            "Batch 92: loss = 0.3105536699295044, acc = 0.904296875\n",
            "Batch 93: loss = 0.28271934390068054, acc = 0.900390625\n",
            "Batch 94: loss = 0.2735791802406311, acc = 0.9111328125\n",
            "Batch 95: loss = 0.28887397050857544, acc = 0.904296875\n",
            "Batch 96: loss = 0.3242219388484955, acc = 0.9013671875\n",
            "Batch 97: loss = 0.30938130617141724, acc = 0.8984375\n",
            "Batch 98: loss = 0.3230750262737274, acc = 0.890625\n",
            "Batch 99: loss = 0.29841795563697815, acc = 0.896484375\n",
            "Batch 100: loss = 0.32367202639579773, acc = 0.880859375\n",
            "Batch 101: loss = 0.28727230429649353, acc = 0.8876953125\n",
            "Batch 102: loss = 0.2835409641265869, acc = 0.896484375\n",
            "Batch 103: loss = 0.31000226736068726, acc = 0.8916015625\n",
            "Batch 104: loss = 0.2879801094532013, acc = 0.90234375\n",
            "Batch 105: loss = 0.30410265922546387, acc = 0.9052734375\n",
            "Batch 106: loss = 0.32816922664642334, acc = 0.890625\n",
            "Batch 107: loss = 0.2893573045730591, acc = 0.900390625\n",
            "Batch 108: loss = 0.2859686315059662, acc = 0.8955078125\n",
            "Batch 109: loss = 0.28807055950164795, acc = 0.90234375\n",
            "Batch 110: loss = 0.2966498136520386, acc = 0.8974609375\n",
            "Batch 111: loss = 0.311021625995636, acc = 0.8876953125\n",
            "Batch 112: loss = 0.2984200716018677, acc = 0.8916015625\n",
            "Batch 113: loss = 0.3034003674983978, acc = 0.896484375\n",
            "Batch 114: loss = 0.3022114038467407, acc = 0.8984375\n",
            "Batch 115: loss = 0.28978073596954346, acc = 0.90234375\n",
            "Batch 116: loss = 0.28293001651763916, acc = 0.904296875\n",
            "Batch 117: loss = 0.3385997712612152, acc = 0.8896484375\n",
            "Batch 118: loss = 0.2909935712814331, acc = 0.900390625\n",
            "Batch 119: loss = 0.2848009169101715, acc = 0.9111328125\n",
            "Batch 120: loss = 0.2760061025619507, acc = 0.90625\n",
            "Batch 121: loss = 0.28262314200401306, acc = 0.8974609375\n",
            "Batch 122: loss = 0.2889350652694702, acc = 0.8984375\n",
            "Batch 123: loss = 0.30644890666007996, acc = 0.890625\n",
            "Batch 124: loss = 0.329681932926178, acc = 0.880859375\n",
            "Batch 125: loss = 0.3335426151752472, acc = 0.8935546875\n",
            "Batch 126: loss = 0.35822781920433044, acc = 0.8818359375\n",
            "\n",
            "Epoch 75/100\n",
            "Batch 1: loss = 0.3737652897834778, acc = 0.8779296875\n",
            "Batch 2: loss = 0.3133537471294403, acc = 0.8896484375\n",
            "Batch 3: loss = 0.31765079498291016, acc = 0.888671875\n",
            "Batch 4: loss = 0.3096180260181427, acc = 0.90234375\n",
            "Batch 5: loss = 0.3189949095249176, acc = 0.884765625\n",
            "Batch 6: loss = 0.28404563665390015, acc = 0.9111328125\n",
            "Batch 7: loss = 0.3005574941635132, acc = 0.8935546875\n",
            "Batch 8: loss = 0.2792714238166809, acc = 0.91015625\n",
            "Batch 9: loss = 0.2849333882331848, acc = 0.908203125\n",
            "Batch 10: loss = 0.28573352098464966, acc = 0.91015625\n",
            "Batch 11: loss = 0.31353676319122314, acc = 0.896484375\n",
            "Batch 12: loss = 0.31402522325515747, acc = 0.892578125\n",
            "Batch 13: loss = 0.28708234429359436, acc = 0.90234375\n",
            "Batch 14: loss = 0.2787802517414093, acc = 0.9130859375\n",
            "Batch 15: loss = 0.2569131851196289, acc = 0.9150390625\n",
            "Batch 16: loss = 0.2824260890483856, acc = 0.908203125\n",
            "Batch 17: loss = 0.308837354183197, acc = 0.900390625\n",
            "Batch 18: loss = 0.30607858300209045, acc = 0.8994140625\n",
            "Batch 19: loss = 0.2875029444694519, acc = 0.9091796875\n",
            "Batch 20: loss = 0.29156553745269775, acc = 0.89453125\n",
            "Batch 21: loss = 0.28904443979263306, acc = 0.8994140625\n",
            "Batch 22: loss = 0.30284151434898376, acc = 0.89453125\n",
            "Batch 23: loss = 0.3116697669029236, acc = 0.8818359375\n",
            "Batch 24: loss = 0.29032012820243835, acc = 0.900390625\n",
            "Batch 25: loss = 0.32810893654823303, acc = 0.890625\n",
            "Batch 26: loss = 0.30323997139930725, acc = 0.9033203125\n",
            "Batch 27: loss = 0.31162360310554504, acc = 0.8994140625\n",
            "Batch 28: loss = 0.26993370056152344, acc = 0.9140625\n",
            "Batch 29: loss = 0.3129534125328064, acc = 0.900390625\n",
            "Batch 30: loss = 0.28998345136642456, acc = 0.8974609375\n",
            "Batch 31: loss = 0.31553635001182556, acc = 0.8935546875\n",
            "Batch 32: loss = 0.3513970375061035, acc = 0.880859375\n",
            "Batch 33: loss = 0.28817039728164673, acc = 0.91015625\n",
            "Batch 34: loss = 0.3082122206687927, acc = 0.890625\n",
            "Batch 35: loss = 0.258236289024353, acc = 0.923828125\n",
            "Batch 36: loss = 0.22278442978858948, acc = 0.93359375\n",
            "Batch 37: loss = 0.2761882245540619, acc = 0.912109375\n",
            "Batch 38: loss = 0.27844199538230896, acc = 0.9052734375\n",
            "Batch 39: loss = 0.2833866775035858, acc = 0.90234375\n",
            "Batch 40: loss = 0.28186896443367004, acc = 0.91796875\n",
            "Batch 41: loss = 0.26589906215667725, acc = 0.9111328125\n",
            "Batch 42: loss = 0.2658472955226898, acc = 0.90625\n",
            "Batch 43: loss = 0.2844051420688629, acc = 0.9072265625\n",
            "Batch 44: loss = 0.2871227264404297, acc = 0.91015625\n",
            "Batch 45: loss = 0.27921026945114136, acc = 0.9013671875\n",
            "Batch 46: loss = 0.23372352123260498, acc = 0.92578125\n",
            "Batch 47: loss = 0.25255364179611206, acc = 0.904296875\n",
            "Batch 48: loss = 0.27220189571380615, acc = 0.912109375\n",
            "Batch 49: loss = 0.26633134484291077, acc = 0.908203125\n",
            "Batch 50: loss = 0.2610211670398712, acc = 0.908203125\n",
            "Batch 51: loss = 0.26849979162216187, acc = 0.8984375\n",
            "Batch 52: loss = 0.29515668749809265, acc = 0.8955078125\n",
            "Batch 53: loss = 0.2795717120170593, acc = 0.9072265625\n",
            "Batch 54: loss = 0.2412947714328766, acc = 0.9130859375\n",
            "Batch 55: loss = 0.25952547788619995, acc = 0.9189453125\n",
            "Batch 56: loss = 0.31384316086769104, acc = 0.8955078125\n",
            "Batch 57: loss = 0.33549201488494873, acc = 0.8876953125\n",
            "Batch 58: loss = 0.3090212643146515, acc = 0.8974609375\n",
            "Batch 59: loss = 0.2325119972229004, acc = 0.9267578125\n",
            "Batch 60: loss = 0.27388474345207214, acc = 0.912109375\n",
            "Batch 61: loss = 0.2679775059223175, acc = 0.9267578125\n",
            "Batch 62: loss = 0.2874909043312073, acc = 0.9072265625\n",
            "Batch 63: loss = 0.2541240453720093, acc = 0.9169921875\n",
            "Batch 64: loss = 0.2359059900045395, acc = 0.916015625\n",
            "Batch 65: loss = 0.25180962681770325, acc = 0.90625\n",
            "Batch 66: loss = 0.2975142300128937, acc = 0.90625\n",
            "Batch 67: loss = 0.31276246905326843, acc = 0.888671875\n",
            "Batch 68: loss = 0.2665938436985016, acc = 0.908203125\n",
            "Batch 69: loss = 0.2495066374540329, acc = 0.9169921875\n",
            "Batch 70: loss = 0.2965073585510254, acc = 0.8994140625\n",
            "Batch 71: loss = 0.2843051552772522, acc = 0.8994140625\n",
            "Batch 72: loss = 0.30128034949302673, acc = 0.900390625\n",
            "Batch 73: loss = 0.3140544295310974, acc = 0.8974609375\n",
            "Batch 74: loss = 0.29488512873649597, acc = 0.90234375\n",
            "Batch 75: loss = 0.3692842721939087, acc = 0.876953125\n",
            "Batch 76: loss = 0.31291642785072327, acc = 0.8994140625\n",
            "Batch 77: loss = 0.2781228721141815, acc = 0.90234375\n",
            "Batch 78: loss = 0.2820959985256195, acc = 0.9130859375\n",
            "Batch 79: loss = 0.2459140419960022, acc = 0.921875\n",
            "Batch 80: loss = 0.253553181886673, acc = 0.9130859375\n",
            "Batch 81: loss = 0.27926942706108093, acc = 0.90625\n",
            "Batch 82: loss = 0.2841758131980896, acc = 0.912109375\n",
            "Batch 83: loss = 0.28309962153434753, acc = 0.916015625\n",
            "Batch 84: loss = 0.2922661304473877, acc = 0.89453125\n",
            "Batch 85: loss = 0.3099316954612732, acc = 0.8955078125\n",
            "Batch 86: loss = 0.28009098768234253, acc = 0.90234375\n",
            "Batch 87: loss = 0.27381831407546997, acc = 0.912109375\n",
            "Batch 88: loss = 0.3512094020843506, acc = 0.8779296875\n",
            "Batch 89: loss = 0.3060963749885559, acc = 0.8896484375\n",
            "Batch 90: loss = 0.3009723722934723, acc = 0.890625\n",
            "Batch 91: loss = 0.32494568824768066, acc = 0.8974609375\n",
            "Batch 92: loss = 0.3064148426055908, acc = 0.900390625\n",
            "Batch 93: loss = 0.2731010913848877, acc = 0.9033203125\n",
            "Batch 94: loss = 0.26908180117607117, acc = 0.9052734375\n",
            "Batch 95: loss = 0.3221500515937805, acc = 0.888671875\n",
            "Batch 96: loss = 0.3420524001121521, acc = 0.8857421875\n",
            "Batch 97: loss = 0.30737414956092834, acc = 0.904296875\n",
            "Batch 98: loss = 0.28965985774993896, acc = 0.90625\n",
            "Batch 99: loss = 0.27884867787361145, acc = 0.908203125\n",
            "Batch 100: loss = 0.31629881262779236, acc = 0.8818359375\n",
            "Batch 101: loss = 0.2971517741680145, acc = 0.8994140625\n",
            "Batch 102: loss = 0.3139539957046509, acc = 0.888671875\n",
            "Batch 103: loss = 0.2892058193683624, acc = 0.9033203125\n",
            "Batch 104: loss = 0.26792487502098083, acc = 0.9072265625\n",
            "Batch 105: loss = 0.269782692193985, acc = 0.9111328125\n",
            "Batch 106: loss = 0.31171905994415283, acc = 0.900390625\n",
            "Batch 107: loss = 0.29157984256744385, acc = 0.8974609375\n",
            "Batch 108: loss = 0.27567800879478455, acc = 0.904296875\n",
            "Batch 109: loss = 0.32963132858276367, acc = 0.884765625\n",
            "Batch 110: loss = 0.2754540741443634, acc = 0.91015625\n",
            "Batch 111: loss = 0.2802201509475708, acc = 0.9072265625\n",
            "Batch 112: loss = 0.30228161811828613, acc = 0.8916015625\n",
            "Batch 113: loss = 0.28557905554771423, acc = 0.8994140625\n",
            "Batch 114: loss = 0.29184621572494507, acc = 0.8984375\n",
            "Batch 115: loss = 0.319728285074234, acc = 0.8955078125\n",
            "Batch 116: loss = 0.2740032374858856, acc = 0.900390625\n",
            "Batch 117: loss = 0.3259139657020569, acc = 0.8876953125\n",
            "Batch 118: loss = 0.29469171166419983, acc = 0.8984375\n",
            "Batch 119: loss = 0.26084718108177185, acc = 0.9189453125\n",
            "Batch 120: loss = 0.27201735973358154, acc = 0.91015625\n",
            "Batch 121: loss = 0.2880588471889496, acc = 0.9033203125\n",
            "Batch 122: loss = 0.28017836809158325, acc = 0.90234375\n",
            "Batch 123: loss = 0.26277077198028564, acc = 0.9208984375\n",
            "Batch 124: loss = 0.3301558494567871, acc = 0.8896484375\n",
            "Batch 125: loss = 0.331938236951828, acc = 0.8916015625\n",
            "Batch 126: loss = 0.2872350811958313, acc = 0.900390625\n",
            "\n",
            "Epoch 76/100\n",
            "Batch 1: loss = 0.3529931902885437, acc = 0.8896484375\n",
            "Batch 2: loss = 0.3211389482021332, acc = 0.890625\n",
            "Batch 3: loss = 0.305096834897995, acc = 0.8984375\n",
            "Batch 4: loss = 0.2607668936252594, acc = 0.9140625\n",
            "Batch 5: loss = 0.32388344407081604, acc = 0.88671875\n",
            "Batch 6: loss = 0.3264867663383484, acc = 0.89453125\n",
            "Batch 7: loss = 0.324888676404953, acc = 0.896484375\n",
            "Batch 8: loss = 0.3129102289676666, acc = 0.8984375\n",
            "Batch 9: loss = 0.29342731833457947, acc = 0.908203125\n",
            "Batch 10: loss = 0.2597019374370575, acc = 0.9150390625\n",
            "Batch 11: loss = 0.31688904762268066, acc = 0.904296875\n",
            "Batch 12: loss = 0.30936214327812195, acc = 0.8955078125\n",
            "Batch 13: loss = 0.27127206325531006, acc = 0.9072265625\n",
            "Batch 14: loss = 0.25857099890708923, acc = 0.9169921875\n",
            "Batch 15: loss = 0.24704138934612274, acc = 0.919921875\n",
            "Batch 16: loss = 0.2870127856731415, acc = 0.9072265625\n",
            "Batch 17: loss = 0.25699108839035034, acc = 0.921875\n",
            "Batch 18: loss = 0.31090739369392395, acc = 0.89453125\n",
            "Batch 19: loss = 0.2807232141494751, acc = 0.90234375\n",
            "Batch 20: loss = 0.32727867364883423, acc = 0.884765625\n",
            "Batch 21: loss = 0.30237719416618347, acc = 0.8955078125\n",
            "Batch 22: loss = 0.27942463755607605, acc = 0.9033203125\n",
            "Batch 23: loss = 0.2832372188568115, acc = 0.8984375\n",
            "Batch 24: loss = 0.29328060150146484, acc = 0.8984375\n",
            "Batch 25: loss = 0.31708958745002747, acc = 0.8896484375\n",
            "Batch 26: loss = 0.2682128846645355, acc = 0.9072265625\n",
            "Batch 27: loss = 0.30752652883529663, acc = 0.904296875\n",
            "Batch 28: loss = 0.29097557067871094, acc = 0.900390625\n",
            "Batch 29: loss = 0.2927510142326355, acc = 0.904296875\n",
            "Batch 30: loss = 0.30040058493614197, acc = 0.9111328125\n",
            "Batch 31: loss = 0.30006304383277893, acc = 0.900390625\n",
            "Batch 32: loss = 0.3326733112335205, acc = 0.87890625\n",
            "Batch 33: loss = 0.279082328081131, acc = 0.91015625\n",
            "Batch 34: loss = 0.3050934374332428, acc = 0.8994140625\n",
            "Batch 35: loss = 0.305950403213501, acc = 0.90625\n",
            "Batch 36: loss = 0.26664289832115173, acc = 0.8994140625\n",
            "Batch 37: loss = 0.24262672662734985, acc = 0.91796875\n",
            "Batch 38: loss = 0.295784592628479, acc = 0.896484375\n",
            "Batch 39: loss = 0.2883604168891907, acc = 0.8994140625\n",
            "Batch 40: loss = 0.27194857597351074, acc = 0.904296875\n",
            "Batch 41: loss = 0.2765325605869293, acc = 0.904296875\n",
            "Batch 42: loss = 0.27883380651474, acc = 0.8955078125\n",
            "Batch 43: loss = 0.30118194222450256, acc = 0.908203125\n",
            "Batch 44: loss = 0.25486668944358826, acc = 0.9140625\n",
            "Batch 45: loss = 0.27613040804862976, acc = 0.90625\n",
            "Batch 46: loss = 0.24624207615852356, acc = 0.9130859375\n",
            "Batch 47: loss = 0.2769353687763214, acc = 0.904296875\n",
            "Batch 48: loss = 0.28535932302474976, acc = 0.908203125\n",
            "Batch 49: loss = 0.2795829474925995, acc = 0.9140625\n",
            "Batch 50: loss = 0.2612442672252655, acc = 0.9130859375\n",
            "Batch 51: loss = 0.25932377576828003, acc = 0.908203125\n",
            "Batch 52: loss = 0.29818323254585266, acc = 0.8994140625\n",
            "Batch 53: loss = 0.2868443727493286, acc = 0.9091796875\n",
            "Batch 54: loss = 0.2549059987068176, acc = 0.9072265625\n",
            "Batch 55: loss = 0.26232215762138367, acc = 0.921875\n",
            "Batch 56: loss = 0.26916104555130005, acc = 0.9072265625\n",
            "Batch 57: loss = 0.3208473026752472, acc = 0.89453125\n",
            "Batch 58: loss = 0.29973000288009644, acc = 0.9013671875\n",
            "Batch 59: loss = 0.23206987977027893, acc = 0.9296875\n",
            "Batch 60: loss = 0.3080202341079712, acc = 0.904296875\n",
            "Batch 61: loss = 0.2684490978717804, acc = 0.919921875\n",
            "Batch 62: loss = 0.2952114939689636, acc = 0.9052734375\n",
            "Batch 63: loss = 0.2549736797809601, acc = 0.92578125\n",
            "Batch 64: loss = 0.24601052701473236, acc = 0.9228515625\n",
            "Batch 65: loss = 0.31354284286499023, acc = 0.9033203125\n",
            "Batch 66: loss = 0.28881704807281494, acc = 0.8994140625\n",
            "Batch 67: loss = 0.30668529868125916, acc = 0.8916015625\n",
            "Batch 68: loss = 0.2838706374168396, acc = 0.8955078125\n",
            "Batch 69: loss = 0.2610718607902527, acc = 0.9033203125\n",
            "Batch 70: loss = 0.31577086448669434, acc = 0.8935546875\n",
            "Batch 71: loss = 0.31415295600891113, acc = 0.89453125\n",
            "Batch 72: loss = 0.2698429822921753, acc = 0.904296875\n",
            "Batch 73: loss = 0.31101831793785095, acc = 0.8994140625\n",
            "Batch 74: loss = 0.29994601011276245, acc = 0.900390625\n",
            "Batch 75: loss = 0.3411920368671417, acc = 0.8818359375\n",
            "Batch 76: loss = 0.29918941855430603, acc = 0.8984375\n",
            "Batch 77: loss = 0.27834784984588623, acc = 0.9150390625\n",
            "Batch 78: loss = 0.25997787714004517, acc = 0.9130859375\n",
            "Batch 79: loss = 0.2838849127292633, acc = 0.900390625\n",
            "Batch 80: loss = 0.27229198813438416, acc = 0.9111328125\n",
            "Batch 81: loss = 0.313538134098053, acc = 0.8955078125\n",
            "Batch 82: loss = 0.28753921389579773, acc = 0.900390625\n",
            "Batch 83: loss = 0.2656557559967041, acc = 0.9072265625\n",
            "Batch 84: loss = 0.301943302154541, acc = 0.8916015625\n",
            "Batch 85: loss = 0.3180813193321228, acc = 0.8935546875\n",
            "Batch 86: loss = 0.2794691026210785, acc = 0.9072265625\n",
            "Batch 87: loss = 0.29657262563705444, acc = 0.9013671875\n",
            "Batch 88: loss = 0.33728253841400146, acc = 0.884765625\n",
            "Batch 89: loss = 0.2828892171382904, acc = 0.900390625\n",
            "Batch 90: loss = 0.3200211226940155, acc = 0.884765625\n",
            "Batch 91: loss = 0.3105317950248718, acc = 0.888671875\n",
            "Batch 92: loss = 0.3100473880767822, acc = 0.8955078125\n",
            "Batch 93: loss = 0.2934298515319824, acc = 0.8935546875\n",
            "Batch 94: loss = 0.2686349153518677, acc = 0.90625\n",
            "Batch 95: loss = 0.2813934087753296, acc = 0.9033203125\n",
            "Batch 96: loss = 0.33485323190689087, acc = 0.87890625\n",
            "Batch 97: loss = 0.28917810320854187, acc = 0.916015625\n",
            "Batch 98: loss = 0.28741157054901123, acc = 0.90234375\n",
            "Batch 99: loss = 0.29287418723106384, acc = 0.9130859375\n",
            "Batch 100: loss = 0.29626843333244324, acc = 0.8955078125\n",
            "Batch 101: loss = 0.2631448209285736, acc = 0.9169921875\n",
            "Batch 102: loss = 0.3059597909450531, acc = 0.8798828125\n",
            "Batch 103: loss = 0.3009283244609833, acc = 0.8974609375\n",
            "Batch 104: loss = 0.2619338929653168, acc = 0.9091796875\n",
            "Batch 105: loss = 0.2443903684616089, acc = 0.9189453125\n",
            "Batch 106: loss = 0.28989696502685547, acc = 0.9013671875\n",
            "Batch 107: loss = 0.3178061246871948, acc = 0.89453125\n",
            "Batch 108: loss = 0.2717515826225281, acc = 0.908203125\n",
            "Batch 109: loss = 0.2904192805290222, acc = 0.904296875\n",
            "Batch 110: loss = 0.2646067142486572, acc = 0.908203125\n",
            "Batch 111: loss = 0.3086996376514435, acc = 0.8955078125\n",
            "Batch 112: loss = 0.3050471842288971, acc = 0.8994140625\n",
            "Batch 113: loss = 0.3044061064720154, acc = 0.892578125\n",
            "Batch 114: loss = 0.31392741203308105, acc = 0.8896484375\n",
            "Batch 115: loss = 0.3126795291900635, acc = 0.8876953125\n",
            "Batch 116: loss = 0.284241646528244, acc = 0.9013671875\n",
            "Batch 117: loss = 0.29949551820755005, acc = 0.890625\n",
            "Batch 118: loss = 0.27662011981010437, acc = 0.900390625\n",
            "Batch 119: loss = 0.24303656816482544, acc = 0.919921875\n",
            "Batch 120: loss = 0.24314388632774353, acc = 0.9228515625\n",
            "Batch 121: loss = 0.2607105076313019, acc = 0.912109375\n",
            "Batch 122: loss = 0.2712653577327728, acc = 0.912109375\n",
            "Batch 123: loss = 0.2903464436531067, acc = 0.8984375\n",
            "Batch 124: loss = 0.33225762844085693, acc = 0.888671875\n",
            "Batch 125: loss = 0.31396767497062683, acc = 0.888671875\n",
            "Batch 126: loss = 0.2925657629966736, acc = 0.904296875\n",
            "\n",
            "Epoch 77/100\n",
            "Batch 1: loss = 0.39984387159347534, acc = 0.87109375\n",
            "Batch 2: loss = 0.3202823996543884, acc = 0.8837890625\n",
            "Batch 3: loss = 0.32140809297561646, acc = 0.890625\n",
            "Batch 4: loss = 0.2766410708427429, acc = 0.8994140625\n",
            "Batch 5: loss = 0.3135415315628052, acc = 0.888671875\n",
            "Batch 6: loss = 0.3144230842590332, acc = 0.8876953125\n",
            "Batch 7: loss = 0.28204330801963806, acc = 0.9072265625\n",
            "Batch 8: loss = 0.2957124710083008, acc = 0.8994140625\n",
            "Batch 9: loss = 0.3183571994304657, acc = 0.888671875\n",
            "Batch 10: loss = 0.29451775550842285, acc = 0.9013671875\n",
            "Batch 11: loss = 0.29968586564064026, acc = 0.8984375\n",
            "Batch 12: loss = 0.30199509859085083, acc = 0.8955078125\n",
            "Batch 13: loss = 0.28451696038246155, acc = 0.9052734375\n",
            "Batch 14: loss = 0.27330055832862854, acc = 0.9130859375\n",
            "Batch 15: loss = 0.24685218930244446, acc = 0.9189453125\n",
            "Batch 16: loss = 0.30304819345474243, acc = 0.9033203125\n",
            "Batch 17: loss = 0.29542025923728943, acc = 0.90625\n",
            "Batch 18: loss = 0.2829788327217102, acc = 0.90625\n",
            "Batch 19: loss = 0.29631489515304565, acc = 0.9052734375\n",
            "Batch 20: loss = 0.2884926497936249, acc = 0.900390625\n",
            "Batch 21: loss = 0.30688923597335815, acc = 0.89453125\n",
            "Batch 22: loss = 0.29852938652038574, acc = 0.8935546875\n",
            "Batch 23: loss = 0.3144747316837311, acc = 0.8896484375\n",
            "Batch 24: loss = 0.2691374123096466, acc = 0.9111328125\n",
            "Batch 25: loss = 0.29847320914268494, acc = 0.896484375\n",
            "Batch 26: loss = 0.2886289656162262, acc = 0.8984375\n",
            "Batch 27: loss = 0.341117799282074, acc = 0.8828125\n",
            "Batch 28: loss = 0.28934845328330994, acc = 0.8916015625\n",
            "Batch 29: loss = 0.27903878688812256, acc = 0.90234375\n",
            "Batch 30: loss = 0.28886735439300537, acc = 0.896484375\n",
            "Batch 31: loss = 0.32167869806289673, acc = 0.8857421875\n",
            "Batch 32: loss = 0.30511337518692017, acc = 0.890625\n",
            "Batch 33: loss = 0.2503756284713745, acc = 0.9169921875\n",
            "Batch 34: loss = 0.26837217807769775, acc = 0.9091796875\n",
            "Batch 35: loss = 0.2641480267047882, acc = 0.9130859375\n",
            "Batch 36: loss = 0.2412908375263214, acc = 0.9111328125\n",
            "Batch 37: loss = 0.2591094374656677, acc = 0.921875\n",
            "Batch 38: loss = 0.25909385085105896, acc = 0.9228515625\n",
            "Batch 39: loss = 0.26546797156333923, acc = 0.919921875\n",
            "Batch 40: loss = 0.2679787874221802, acc = 0.91796875\n",
            "Batch 41: loss = 0.2530515789985657, acc = 0.9140625\n",
            "Batch 42: loss = 0.27935346961021423, acc = 0.9072265625\n",
            "Batch 43: loss = 0.3047557771205902, acc = 0.8994140625\n",
            "Batch 44: loss = 0.2647140324115753, acc = 0.9208984375\n",
            "Batch 45: loss = 0.2590945065021515, acc = 0.904296875\n",
            "Batch 46: loss = 0.2563149333000183, acc = 0.9169921875\n",
            "Batch 47: loss = 0.2713056206703186, acc = 0.9111328125\n",
            "Batch 48: loss = 0.24513410031795502, acc = 0.923828125\n",
            "Batch 49: loss = 0.2928559482097626, acc = 0.90234375\n",
            "Batch 50: loss = 0.26539450883865356, acc = 0.9091796875\n",
            "Batch 51: loss = 0.2752602696418762, acc = 0.9052734375\n",
            "Batch 52: loss = 0.2725604474544525, acc = 0.9072265625\n",
            "Batch 53: loss = 0.30048248171806335, acc = 0.900390625\n",
            "Batch 54: loss = 0.232553169131279, acc = 0.9208984375\n",
            "Batch 55: loss = 0.2804185748100281, acc = 0.9111328125\n",
            "Batch 56: loss = 0.28331422805786133, acc = 0.8955078125\n",
            "Batch 57: loss = 0.31987178325653076, acc = 0.8876953125\n",
            "Batch 58: loss = 0.31959107518196106, acc = 0.89453125\n",
            "Batch 59: loss = 0.2730202078819275, acc = 0.916015625\n",
            "Batch 60: loss = 0.2771444320678711, acc = 0.916015625\n",
            "Batch 61: loss = 0.2733270227909088, acc = 0.9072265625\n",
            "Batch 62: loss = 0.31996864080429077, acc = 0.8896484375\n",
            "Batch 63: loss = 0.24641481041908264, acc = 0.923828125\n",
            "Batch 64: loss = 0.26460209488868713, acc = 0.9130859375\n",
            "Batch 65: loss = 0.3036256432533264, acc = 0.892578125\n",
            "Batch 66: loss = 0.2774505615234375, acc = 0.9140625\n",
            "Batch 67: loss = 0.30739516019821167, acc = 0.8994140625\n",
            "Batch 68: loss = 0.2916697859764099, acc = 0.89453125\n",
            "Batch 69: loss = 0.2647494077682495, acc = 0.91015625\n",
            "Batch 70: loss = 0.3063730001449585, acc = 0.8984375\n",
            "Batch 71: loss = 0.3120446503162384, acc = 0.8994140625\n",
            "Batch 72: loss = 0.2703227996826172, acc = 0.8994140625\n",
            "Batch 73: loss = 0.302862286567688, acc = 0.8994140625\n",
            "Batch 74: loss = 0.3013814091682434, acc = 0.9072265625\n",
            "Batch 75: loss = 0.3430396318435669, acc = 0.8828125\n",
            "Batch 76: loss = 0.2911602854728699, acc = 0.9072265625\n",
            "Batch 77: loss = 0.2732907831668854, acc = 0.91015625\n",
            "Batch 78: loss = 0.2653670310974121, acc = 0.904296875\n",
            "Batch 79: loss = 0.2621476650238037, acc = 0.9130859375\n",
            "Batch 80: loss = 0.2740916311740875, acc = 0.9013671875\n",
            "Batch 81: loss = 0.3041045665740967, acc = 0.8984375\n",
            "Batch 82: loss = 0.30383142828941345, acc = 0.90234375\n",
            "Batch 83: loss = 0.25812405347824097, acc = 0.9208984375\n",
            "Batch 84: loss = 0.2766258418560028, acc = 0.89453125\n",
            "Batch 85: loss = 0.34163451194763184, acc = 0.892578125\n",
            "Batch 86: loss = 0.2960924208164215, acc = 0.888671875\n",
            "Batch 87: loss = 0.2794167399406433, acc = 0.908203125\n",
            "Batch 88: loss = 0.3075825870037079, acc = 0.900390625\n",
            "Batch 89: loss = 0.2743242681026459, acc = 0.9091796875\n",
            "Batch 90: loss = 0.31570345163345337, acc = 0.8974609375\n",
            "Batch 91: loss = 0.29877710342407227, acc = 0.896484375\n",
            "Batch 92: loss = 0.3193128705024719, acc = 0.9013671875\n",
            "Batch 93: loss = 0.2956273555755615, acc = 0.900390625\n",
            "Batch 94: loss = 0.25159311294555664, acc = 0.9130859375\n",
            "Batch 95: loss = 0.2893352508544922, acc = 0.8994140625\n",
            "Batch 96: loss = 0.31074124574661255, acc = 0.8974609375\n",
            "Batch 97: loss = 0.300052285194397, acc = 0.9013671875\n",
            "Batch 98: loss = 0.283619225025177, acc = 0.9072265625\n",
            "Batch 99: loss = 0.30662721395492554, acc = 0.8857421875\n",
            "Batch 100: loss = 0.28633442521095276, acc = 0.890625\n",
            "Batch 101: loss = 0.2812820374965668, acc = 0.896484375\n",
            "Batch 102: loss = 0.28630688786506653, acc = 0.8974609375\n",
            "Batch 103: loss = 0.2966840863227844, acc = 0.8984375\n",
            "Batch 104: loss = 0.27094224095344543, acc = 0.91015625\n",
            "Batch 105: loss = 0.25475725531578064, acc = 0.908203125\n",
            "Batch 106: loss = 0.2678735852241516, acc = 0.908203125\n",
            "Batch 107: loss = 0.3039720058441162, acc = 0.89453125\n",
            "Batch 108: loss = 0.2813194990158081, acc = 0.8994140625\n",
            "Batch 109: loss = 0.29083752632141113, acc = 0.9013671875\n",
            "Batch 110: loss = 0.2736932039260864, acc = 0.908203125\n",
            "Batch 111: loss = 0.2913619577884674, acc = 0.896484375\n",
            "Batch 112: loss = 0.27794912457466125, acc = 0.908203125\n",
            "Batch 113: loss = 0.29432788491249084, acc = 0.900390625\n",
            "Batch 114: loss = 0.263398677110672, acc = 0.9130859375\n",
            "Batch 115: loss = 0.2730325162410736, acc = 0.9072265625\n",
            "Batch 116: loss = 0.3027570843696594, acc = 0.8955078125\n",
            "Batch 117: loss = 0.2782607674598694, acc = 0.90234375\n",
            "Batch 118: loss = 0.28172338008880615, acc = 0.9072265625\n",
            "Batch 119: loss = 0.26503387093544006, acc = 0.912109375\n",
            "Batch 120: loss = 0.2556041479110718, acc = 0.921875\n",
            "Batch 121: loss = 0.29134705662727356, acc = 0.9111328125\n",
            "Batch 122: loss = 0.292611300945282, acc = 0.8994140625\n",
            "Batch 123: loss = 0.270040363073349, acc = 0.919921875\n",
            "Batch 124: loss = 0.2940851151943207, acc = 0.8974609375\n",
            "Batch 125: loss = 0.30852630734443665, acc = 0.8984375\n",
            "Batch 126: loss = 0.30082786083221436, acc = 0.9130859375\n",
            "\n",
            "Epoch 78/100\n",
            "Batch 1: loss = 0.3447112441062927, acc = 0.896484375\n",
            "Batch 2: loss = 0.30081111192703247, acc = 0.900390625\n",
            "Batch 3: loss = 0.29675015807151794, acc = 0.908203125\n",
            "Batch 4: loss = 0.27603116631507874, acc = 0.91015625\n",
            "Batch 5: loss = 0.3370048403739929, acc = 0.8876953125\n",
            "Batch 6: loss = 0.2985738515853882, acc = 0.89453125\n",
            "Batch 7: loss = 0.262625128030777, acc = 0.9111328125\n",
            "Batch 8: loss = 0.2900848090648651, acc = 0.9052734375\n",
            "Batch 9: loss = 0.30013561248779297, acc = 0.8955078125\n",
            "Batch 10: loss = 0.2726311683654785, acc = 0.91015625\n",
            "Batch 11: loss = 0.29107022285461426, acc = 0.90234375\n",
            "Batch 12: loss = 0.2956022620201111, acc = 0.91015625\n",
            "Batch 13: loss = 0.2857629358768463, acc = 0.8984375\n",
            "Batch 14: loss = 0.2728417217731476, acc = 0.904296875\n",
            "Batch 15: loss = 0.24563869833946228, acc = 0.9169921875\n",
            "Batch 16: loss = 0.2694539427757263, acc = 0.9228515625\n",
            "Batch 17: loss = 0.3249649703502655, acc = 0.890625\n",
            "Batch 18: loss = 0.29424557089805603, acc = 0.9033203125\n",
            "Batch 19: loss = 0.27920591831207275, acc = 0.9091796875\n",
            "Batch 20: loss = 0.26379743218421936, acc = 0.904296875\n",
            "Batch 21: loss = 0.3068835735321045, acc = 0.8994140625\n",
            "Batch 22: loss = 0.24464291334152222, acc = 0.916015625\n",
            "Batch 23: loss = 0.29621750116348267, acc = 0.908203125\n",
            "Batch 24: loss = 0.2634623646736145, acc = 0.91796875\n",
            "Batch 25: loss = 0.29456403851509094, acc = 0.900390625\n",
            "Batch 26: loss = 0.2843165099620819, acc = 0.9033203125\n",
            "Batch 27: loss = 0.3090320825576782, acc = 0.8896484375\n",
            "Batch 28: loss = 0.2655402719974518, acc = 0.904296875\n",
            "Batch 29: loss = 0.2883446216583252, acc = 0.9033203125\n",
            "Batch 30: loss = 0.28761613368988037, acc = 0.8994140625\n",
            "Batch 31: loss = 0.27917027473449707, acc = 0.9091796875\n",
            "Batch 32: loss = 0.29563358426094055, acc = 0.90234375\n",
            "Batch 33: loss = 0.27474889159202576, acc = 0.896484375\n",
            "Batch 34: loss = 0.2973732054233551, acc = 0.9052734375\n",
            "Batch 35: loss = 0.2939019203186035, acc = 0.9033203125\n",
            "Batch 36: loss = 0.2329391986131668, acc = 0.9208984375\n",
            "Batch 37: loss = 0.25414010882377625, acc = 0.916015625\n",
            "Batch 38: loss = 0.3008863925933838, acc = 0.90234375\n",
            "Batch 39: loss = 0.2842193841934204, acc = 0.90234375\n",
            "Batch 40: loss = 0.31464770436286926, acc = 0.89453125\n",
            "Batch 41: loss = 0.23290084302425385, acc = 0.9150390625\n",
            "Batch 42: loss = 0.28949105739593506, acc = 0.8984375\n",
            "Batch 43: loss = 0.3115953505039215, acc = 0.8896484375\n",
            "Batch 44: loss = 0.2618536949157715, acc = 0.9140625\n",
            "Batch 45: loss = 0.2425500452518463, acc = 0.919921875\n",
            "Batch 46: loss = 0.2297322154045105, acc = 0.9208984375\n",
            "Batch 47: loss = 0.23634542524814606, acc = 0.9248046875\n",
            "Batch 48: loss = 0.2420247197151184, acc = 0.921875\n",
            "Batch 49: loss = 0.2588227689266205, acc = 0.9140625\n",
            "Batch 50: loss = 0.21586649119853973, acc = 0.9248046875\n",
            "Batch 51: loss = 0.25000205636024475, acc = 0.9111328125\n",
            "Batch 52: loss = 0.2560729384422302, acc = 0.9111328125\n",
            "Batch 53: loss = 0.2630409598350525, acc = 0.91015625\n",
            "Batch 54: loss = 0.2252928614616394, acc = 0.927734375\n",
            "Batch 55: loss = 0.24345175921916962, acc = 0.9267578125\n",
            "Batch 56: loss = 0.26470574736595154, acc = 0.9150390625\n",
            "Batch 57: loss = 0.28286975622177124, acc = 0.9052734375\n",
            "Batch 58: loss = 0.31506815552711487, acc = 0.9052734375\n",
            "Batch 59: loss = 0.2510998845100403, acc = 0.90625\n",
            "Batch 60: loss = 0.2730584740638733, acc = 0.9052734375\n",
            "Batch 61: loss = 0.23935085535049438, acc = 0.92578125\n",
            "Batch 62: loss = 0.31264424324035645, acc = 0.8916015625\n",
            "Batch 63: loss = 0.2733164429664612, acc = 0.912109375\n",
            "Batch 64: loss = 0.2614244520664215, acc = 0.916015625\n",
            "Batch 65: loss = 0.2672280967235565, acc = 0.916015625\n",
            "Batch 66: loss = 0.29987335205078125, acc = 0.89453125\n",
            "Batch 67: loss = 0.27992165088653564, acc = 0.9072265625\n",
            "Batch 68: loss = 0.29189425706863403, acc = 0.896484375\n",
            "Batch 69: loss = 0.25937750935554504, acc = 0.91015625\n",
            "Batch 70: loss = 0.31422320008277893, acc = 0.9013671875\n",
            "Batch 71: loss = 0.29258763790130615, acc = 0.904296875\n",
            "Batch 72: loss = 0.27045342326164246, acc = 0.9052734375\n",
            "Batch 73: loss = 0.30656197667121887, acc = 0.8974609375\n",
            "Batch 74: loss = 0.2924017310142517, acc = 0.9013671875\n",
            "Batch 75: loss = 0.3657432198524475, acc = 0.865234375\n",
            "Batch 76: loss = 0.2982965111732483, acc = 0.9013671875\n",
            "Batch 77: loss = 0.26176488399505615, acc = 0.9072265625\n",
            "Batch 78: loss = 0.2615651488304138, acc = 0.92578125\n",
            "Batch 79: loss = 0.24971216917037964, acc = 0.919921875\n",
            "Batch 80: loss = 0.27439945936203003, acc = 0.8994140625\n",
            "Batch 81: loss = 0.2589768171310425, acc = 0.904296875\n",
            "Batch 82: loss = 0.2952879071235657, acc = 0.8994140625\n",
            "Batch 83: loss = 0.2696246802806854, acc = 0.916015625\n",
            "Batch 84: loss = 0.2970166802406311, acc = 0.900390625\n",
            "Batch 85: loss = 0.31328386068344116, acc = 0.8896484375\n",
            "Batch 86: loss = 0.29753628373146057, acc = 0.908203125\n",
            "Batch 87: loss = 0.25960537791252136, acc = 0.9111328125\n",
            "Batch 88: loss = 0.29902008175849915, acc = 0.900390625\n",
            "Batch 89: loss = 0.2987064719200134, acc = 0.9033203125\n",
            "Batch 90: loss = 0.3222912847995758, acc = 0.8876953125\n",
            "Batch 91: loss = 0.2660694122314453, acc = 0.9033203125\n",
            "Batch 92: loss = 0.31534242630004883, acc = 0.8935546875\n",
            "Batch 93: loss = 0.2665787935256958, acc = 0.9033203125\n",
            "Batch 94: loss = 0.2636508345603943, acc = 0.9169921875\n",
            "Batch 95: loss = 0.2757683992385864, acc = 0.908203125\n",
            "Batch 96: loss = 0.3128695487976074, acc = 0.9033203125\n",
            "Batch 97: loss = 0.28626981377601624, acc = 0.9091796875\n",
            "Batch 98: loss = 0.30681145191192627, acc = 0.9052734375\n",
            "Batch 99: loss = 0.28146928548812866, acc = 0.9033203125\n",
            "Batch 100: loss = 0.2987574338912964, acc = 0.8984375\n",
            "Batch 101: loss = 0.2981739044189453, acc = 0.9033203125\n",
            "Batch 102: loss = 0.2766556739807129, acc = 0.8974609375\n",
            "Batch 103: loss = 0.2917585074901581, acc = 0.900390625\n",
            "Batch 104: loss = 0.27881741523742676, acc = 0.90625\n",
            "Batch 105: loss = 0.2544786036014557, acc = 0.919921875\n",
            "Batch 106: loss = 0.26654252409935, acc = 0.908203125\n",
            "Batch 107: loss = 0.2919175624847412, acc = 0.9033203125\n",
            "Batch 108: loss = 0.2971154451370239, acc = 0.8994140625\n",
            "Batch 109: loss = 0.28151455521583557, acc = 0.9033203125\n",
            "Batch 110: loss = 0.2704809308052063, acc = 0.9091796875\n",
            "Batch 111: loss = 0.308051735162735, acc = 0.89453125\n",
            "Batch 112: loss = 0.27178990840911865, acc = 0.9052734375\n",
            "Batch 113: loss = 0.2498626559972763, acc = 0.916015625\n",
            "Batch 114: loss = 0.2958180904388428, acc = 0.9033203125\n",
            "Batch 115: loss = 0.2722935676574707, acc = 0.91015625\n",
            "Batch 116: loss = 0.282757967710495, acc = 0.9072265625\n",
            "Batch 117: loss = 0.29036280512809753, acc = 0.9013671875\n",
            "Batch 118: loss = 0.24786587059497833, acc = 0.921875\n",
            "Batch 119: loss = 0.28594547510147095, acc = 0.9033203125\n",
            "Batch 120: loss = 0.2299310564994812, acc = 0.9208984375\n",
            "Batch 121: loss = 0.26693668961524963, acc = 0.896484375\n",
            "Batch 122: loss = 0.26410627365112305, acc = 0.90625\n",
            "Batch 123: loss = 0.2742993235588074, acc = 0.908203125\n",
            "Batch 124: loss = 0.2957838773727417, acc = 0.9013671875\n",
            "Batch 125: loss = 0.3010123670101166, acc = 0.8955078125\n",
            "Batch 126: loss = 0.2901601791381836, acc = 0.90625\n",
            "\n",
            "Epoch 79/100\n",
            "Batch 1: loss = 0.3822632431983948, acc = 0.873046875\n",
            "Batch 2: loss = 0.29612910747528076, acc = 0.9072265625\n",
            "Batch 3: loss = 0.29595014452934265, acc = 0.9033203125\n",
            "Batch 4: loss = 0.2697085440158844, acc = 0.908203125\n",
            "Batch 5: loss = 0.282230943441391, acc = 0.9072265625\n",
            "Batch 6: loss = 0.3044404089450836, acc = 0.8896484375\n",
            "Batch 7: loss = 0.29373347759246826, acc = 0.900390625\n",
            "Batch 8: loss = 0.3010988235473633, acc = 0.9013671875\n",
            "Batch 9: loss = 0.2794831097126007, acc = 0.9052734375\n",
            "Batch 10: loss = 0.229999378323555, acc = 0.919921875\n",
            "Batch 11: loss = 0.2778702974319458, acc = 0.9033203125\n",
            "Batch 12: loss = 0.2771461308002472, acc = 0.90625\n",
            "Batch 13: loss = 0.24830098450183868, acc = 0.9091796875\n",
            "Batch 14: loss = 0.2752382457256317, acc = 0.9111328125\n",
            "Batch 15: loss = 0.26722443103790283, acc = 0.91015625\n",
            "Batch 16: loss = 0.3012983798980713, acc = 0.8984375\n",
            "Batch 17: loss = 0.2944960296154022, acc = 0.9169921875\n",
            "Batch 18: loss = 0.2956736385822296, acc = 0.9013671875\n",
            "Batch 19: loss = 0.27211716771125793, acc = 0.9150390625\n",
            "Batch 20: loss = 0.27928686141967773, acc = 0.900390625\n",
            "Batch 21: loss = 0.27593377232551575, acc = 0.9072265625\n",
            "Batch 22: loss = 0.2462245374917984, acc = 0.9169921875\n",
            "Batch 23: loss = 0.28920161724090576, acc = 0.8984375\n",
            "Batch 24: loss = 0.32170194387435913, acc = 0.890625\n",
            "Batch 25: loss = 0.29858100414276123, acc = 0.9033203125\n",
            "Batch 26: loss = 0.2881491482257843, acc = 0.904296875\n",
            "Batch 27: loss = 0.3091132938861847, acc = 0.904296875\n",
            "Batch 28: loss = 0.2987092435359955, acc = 0.8955078125\n",
            "Batch 29: loss = 0.28032124042510986, acc = 0.9052734375\n",
            "Batch 30: loss = 0.2863767743110657, acc = 0.908203125\n",
            "Batch 31: loss = 0.2778935432434082, acc = 0.912109375\n",
            "Batch 32: loss = 0.27802300453186035, acc = 0.9111328125\n",
            "Batch 33: loss = 0.26899734139442444, acc = 0.9052734375\n",
            "Batch 34: loss = 0.3123764097690582, acc = 0.8984375\n",
            "Batch 35: loss = 0.2987942099571228, acc = 0.8994140625\n",
            "Batch 36: loss = 0.24810293316841125, acc = 0.9169921875\n",
            "Batch 37: loss = 0.2509549558162689, acc = 0.923828125\n",
            "Batch 38: loss = 0.2645757794380188, acc = 0.91015625\n",
            "Batch 39: loss = 0.26666659116744995, acc = 0.916015625\n",
            "Batch 40: loss = 0.2711850106716156, acc = 0.91015625\n",
            "Batch 41: loss = 0.24054116010665894, acc = 0.91015625\n",
            "Batch 42: loss = 0.27852869033813477, acc = 0.9013671875\n",
            "Batch 43: loss = 0.27827778458595276, acc = 0.9013671875\n",
            "Batch 44: loss = 0.23584267497062683, acc = 0.9208984375\n",
            "Batch 45: loss = 0.2685602903366089, acc = 0.9228515625\n",
            "Batch 46: loss = 0.20143014192581177, acc = 0.931640625\n",
            "Batch 47: loss = 0.2411181628704071, acc = 0.9208984375\n",
            "Batch 48: loss = 0.2789731025695801, acc = 0.9052734375\n",
            "Batch 49: loss = 0.248764306306839, acc = 0.91015625\n",
            "Batch 50: loss = 0.23863399028778076, acc = 0.9228515625\n",
            "Batch 51: loss = 0.25079792737960815, acc = 0.9189453125\n",
            "Batch 52: loss = 0.2834024429321289, acc = 0.8974609375\n",
            "Batch 53: loss = 0.25876864790916443, acc = 0.908203125\n",
            "Batch 54: loss = 0.26924586296081543, acc = 0.912109375\n",
            "Batch 55: loss = 0.25095587968826294, acc = 0.9150390625\n",
            "Batch 56: loss = 0.2536112666130066, acc = 0.9150390625\n",
            "Batch 57: loss = 0.28290611505508423, acc = 0.9150390625\n",
            "Batch 58: loss = 0.2958894968032837, acc = 0.900390625\n",
            "Batch 59: loss = 0.26166218519210815, acc = 0.908203125\n",
            "Batch 60: loss = 0.30688294768333435, acc = 0.8935546875\n",
            "Batch 61: loss = 0.256080687046051, acc = 0.90234375\n",
            "Batch 62: loss = 0.2839876115322113, acc = 0.9208984375\n",
            "Batch 63: loss = 0.2573811709880829, acc = 0.9140625\n",
            "Batch 64: loss = 0.2777673304080963, acc = 0.9052734375\n",
            "Batch 65: loss = 0.2868344485759735, acc = 0.9140625\n",
            "Batch 66: loss = 0.2480156123638153, acc = 0.921875\n",
            "Batch 67: loss = 0.30767783522605896, acc = 0.8935546875\n",
            "Batch 68: loss = 0.27466529607772827, acc = 0.904296875\n",
            "Batch 69: loss = 0.25497764348983765, acc = 0.923828125\n",
            "Batch 70: loss = 0.28125226497650146, acc = 0.9052734375\n",
            "Batch 71: loss = 0.28567078709602356, acc = 0.91015625\n",
            "Batch 72: loss = 0.26745280623435974, acc = 0.9091796875\n",
            "Batch 73: loss = 0.28865647315979004, acc = 0.9013671875\n",
            "Batch 74: loss = 0.27086102962493896, acc = 0.904296875\n",
            "Batch 75: loss = 0.33101212978363037, acc = 0.88671875\n",
            "Batch 76: loss = 0.3111514151096344, acc = 0.8994140625\n",
            "Batch 77: loss = 0.26088768243789673, acc = 0.923828125\n",
            "Batch 78: loss = 0.2488483041524887, acc = 0.9130859375\n",
            "Batch 79: loss = 0.24998952448368073, acc = 0.9169921875\n",
            "Batch 80: loss = 0.23687759041786194, acc = 0.9228515625\n",
            "Batch 81: loss = 0.28584417700767517, acc = 0.916015625\n",
            "Batch 82: loss = 0.2878498435020447, acc = 0.904296875\n",
            "Batch 83: loss = 0.27966320514678955, acc = 0.9033203125\n",
            "Batch 84: loss = 0.27245718240737915, acc = 0.90625\n",
            "Batch 85: loss = 0.31227168440818787, acc = 0.89453125\n",
            "Batch 86: loss = 0.2910483777523041, acc = 0.9013671875\n",
            "Batch 87: loss = 0.2770920395851135, acc = 0.9130859375\n",
            "Batch 88: loss = 0.28814277052879333, acc = 0.8974609375\n",
            "Batch 89: loss = 0.23885409533977509, acc = 0.921875\n",
            "Batch 90: loss = 0.29066672921180725, acc = 0.8974609375\n",
            "Batch 91: loss = 0.28130948543548584, acc = 0.896484375\n",
            "Batch 92: loss = 0.27374333143234253, acc = 0.9150390625\n",
            "Batch 93: loss = 0.24978815019130707, acc = 0.9228515625\n",
            "Batch 94: loss = 0.24636030197143555, acc = 0.912109375\n",
            "Batch 95: loss = 0.2891971468925476, acc = 0.91015625\n",
            "Batch 96: loss = 0.29725274443626404, acc = 0.9013671875\n",
            "Batch 97: loss = 0.3039593994617462, acc = 0.912109375\n",
            "Batch 98: loss = 0.23539680242538452, acc = 0.9208984375\n",
            "Batch 99: loss = 0.30517834424972534, acc = 0.890625\n",
            "Batch 100: loss = 0.3096473515033722, acc = 0.8994140625\n",
            "Batch 101: loss = 0.2765849828720093, acc = 0.900390625\n",
            "Batch 102: loss = 0.290874183177948, acc = 0.9033203125\n",
            "Batch 103: loss = 0.3184361457824707, acc = 0.88671875\n",
            "Batch 104: loss = 0.24145090579986572, acc = 0.916015625\n",
            "Batch 105: loss = 0.26323920488357544, acc = 0.9140625\n",
            "Batch 106: loss = 0.25899389386177063, acc = 0.9130859375\n",
            "Batch 107: loss = 0.27082204818725586, acc = 0.91796875\n",
            "Batch 108: loss = 0.28302618861198425, acc = 0.89453125\n",
            "Batch 109: loss = 0.28817734122276306, acc = 0.904296875\n",
            "Batch 110: loss = 0.26736220717430115, acc = 0.90625\n",
            "Batch 111: loss = 0.27191051840782166, acc = 0.9052734375\n",
            "Batch 112: loss = 0.3000434637069702, acc = 0.904296875\n",
            "Batch 113: loss = 0.2905271351337433, acc = 0.8935546875\n",
            "Batch 114: loss = 0.31025391817092896, acc = 0.890625\n",
            "Batch 115: loss = 0.27169668674468994, acc = 0.9072265625\n",
            "Batch 116: loss = 0.27364635467529297, acc = 0.9072265625\n",
            "Batch 117: loss = 0.24715250730514526, acc = 0.9150390625\n",
            "Batch 118: loss = 0.27138397097587585, acc = 0.90625\n",
            "Batch 119: loss = 0.2405688315629959, acc = 0.9208984375\n",
            "Batch 120: loss = 0.24176841974258423, acc = 0.91796875\n",
            "Batch 121: loss = 0.22984465956687927, acc = 0.927734375\n",
            "Batch 122: loss = 0.23720012605190277, acc = 0.9228515625\n",
            "Batch 123: loss = 0.2693730592727661, acc = 0.91015625\n",
            "Batch 124: loss = 0.30819234251976013, acc = 0.888671875\n",
            "Batch 125: loss = 0.29832395911216736, acc = 0.8955078125\n",
            "Batch 126: loss = 0.28312867879867554, acc = 0.912109375\n",
            "\n",
            "Epoch 80/100\n",
            "Batch 1: loss = 0.3399244248867035, acc = 0.89453125\n",
            "Batch 2: loss = 0.29354867339134216, acc = 0.900390625\n",
            "Batch 3: loss = 0.2995759844779968, acc = 0.91015625\n",
            "Batch 4: loss = 0.27187293767929077, acc = 0.9130859375\n",
            "Batch 5: loss = 0.2871173322200775, acc = 0.8994140625\n",
            "Batch 6: loss = 0.3000413179397583, acc = 0.9052734375\n",
            "Batch 7: loss = 0.2750760316848755, acc = 0.9052734375\n",
            "Batch 8: loss = 0.30226218700408936, acc = 0.8955078125\n",
            "Batch 9: loss = 0.2669459283351898, acc = 0.91015625\n",
            "Batch 10: loss = 0.26012465357780457, acc = 0.9140625\n",
            "Batch 11: loss = 0.24802862107753754, acc = 0.9150390625\n",
            "Batch 12: loss = 0.24911649525165558, acc = 0.904296875\n",
            "Batch 13: loss = 0.2462444305419922, acc = 0.9169921875\n",
            "Batch 14: loss = 0.253370076417923, acc = 0.9140625\n",
            "Batch 15: loss = 0.24809327721595764, acc = 0.9189453125\n",
            "Batch 16: loss = 0.3153752088546753, acc = 0.8896484375\n",
            "Batch 17: loss = 0.2971525192260742, acc = 0.9013671875\n",
            "Batch 18: loss = 0.29173409938812256, acc = 0.8955078125\n",
            "Batch 19: loss = 0.29188889265060425, acc = 0.9052734375\n",
            "Batch 20: loss = 0.3200194537639618, acc = 0.880859375\n",
            "Batch 21: loss = 0.30708009004592896, acc = 0.8916015625\n",
            "Batch 22: loss = 0.2599070072174072, acc = 0.919921875\n",
            "Batch 23: loss = 0.2814282178878784, acc = 0.904296875\n",
            "Batch 24: loss = 0.2793358564376831, acc = 0.9091796875\n",
            "Batch 25: loss = 0.2848645746707916, acc = 0.896484375\n",
            "Batch 26: loss = 0.25122782588005066, acc = 0.9169921875\n",
            "Batch 27: loss = 0.2965121269226074, acc = 0.896484375\n",
            "Batch 28: loss = 0.28445300459861755, acc = 0.8876953125\n",
            "Batch 29: loss = 0.2874113917350769, acc = 0.9111328125\n",
            "Batch 30: loss = 0.24187442660331726, acc = 0.9189453125\n",
            "Batch 31: loss = 0.28116416931152344, acc = 0.908203125\n",
            "Batch 32: loss = 0.3008762300014496, acc = 0.8984375\n",
            "Batch 33: loss = 0.2680092453956604, acc = 0.9091796875\n",
            "Batch 34: loss = 0.3020921051502228, acc = 0.890625\n",
            "Batch 35: loss = 0.26319223642349243, acc = 0.9111328125\n",
            "Batch 36: loss = 0.23475511372089386, acc = 0.9287109375\n",
            "Batch 37: loss = 0.24852648377418518, acc = 0.91796875\n",
            "Batch 38: loss = 0.2614668607711792, acc = 0.90625\n",
            "Batch 39: loss = 0.2592203915119171, acc = 0.9130859375\n",
            "Batch 40: loss = 0.2690597474575043, acc = 0.9052734375\n",
            "Batch 41: loss = 0.2648051381111145, acc = 0.90625\n",
            "Batch 42: loss = 0.26455289125442505, acc = 0.9091796875\n",
            "Batch 43: loss = 0.2594854235649109, acc = 0.9169921875\n",
            "Batch 44: loss = 0.25038519501686096, acc = 0.921875\n",
            "Batch 45: loss = 0.25184062123298645, acc = 0.91015625\n",
            "Batch 46: loss = 0.22752508521080017, acc = 0.923828125\n",
            "Batch 47: loss = 0.23967863619327545, acc = 0.91796875\n",
            "Batch 48: loss = 0.26659512519836426, acc = 0.908203125\n",
            "Batch 49: loss = 0.2571106255054474, acc = 0.912109375\n",
            "Batch 50: loss = 0.2593687176704407, acc = 0.90234375\n",
            "Batch 51: loss = 0.26574039459228516, acc = 0.912109375\n",
            "Batch 52: loss = 0.2791898548603058, acc = 0.9033203125\n",
            "Batch 53: loss = 0.26113206148147583, acc = 0.908203125\n",
            "Batch 54: loss = 0.20829537510871887, acc = 0.923828125\n",
            "Batch 55: loss = 0.2611686587333679, acc = 0.921875\n",
            "Batch 56: loss = 0.2771994471549988, acc = 0.91015625\n",
            "Batch 57: loss = 0.2897157669067383, acc = 0.912109375\n",
            "Batch 58: loss = 0.2736976146697998, acc = 0.9130859375\n",
            "Batch 59: loss = 0.21906134486198425, acc = 0.9267578125\n",
            "Batch 60: loss = 0.31448209285736084, acc = 0.8955078125\n",
            "Batch 61: loss = 0.2655611038208008, acc = 0.912109375\n",
            "Batch 62: loss = 0.2774471342563629, acc = 0.9130859375\n",
            "Batch 63: loss = 0.2529289722442627, acc = 0.9150390625\n",
            "Batch 64: loss = 0.2532835006713867, acc = 0.916015625\n",
            "Batch 65: loss = 0.2841588258743286, acc = 0.9052734375\n",
            "Batch 66: loss = 0.25714945793151855, acc = 0.9169921875\n",
            "Batch 67: loss = 0.268921434879303, acc = 0.904296875\n",
            "Batch 68: loss = 0.2768198847770691, acc = 0.9169921875\n",
            "Batch 69: loss = 0.24164973199367523, acc = 0.9169921875\n",
            "Batch 70: loss = 0.26975521445274353, acc = 0.90625\n",
            "Batch 71: loss = 0.31675443053245544, acc = 0.8837890625\n",
            "Batch 72: loss = 0.2651106119155884, acc = 0.9111328125\n",
            "Batch 73: loss = 0.303654283285141, acc = 0.9013671875\n",
            "Batch 74: loss = 0.29057520627975464, acc = 0.9013671875\n",
            "Batch 75: loss = 0.33201804757118225, acc = 0.892578125\n",
            "Batch 76: loss = 0.33400091528892517, acc = 0.8876953125\n",
            "Batch 77: loss = 0.2624257504940033, acc = 0.9091796875\n",
            "Batch 78: loss = 0.26562660932540894, acc = 0.9072265625\n",
            "Batch 79: loss = 0.264726459980011, acc = 0.90625\n",
            "Batch 80: loss = 0.21942441165447235, acc = 0.9228515625\n",
            "Batch 81: loss = 0.27355536818504333, acc = 0.90625\n",
            "Batch 82: loss = 0.2792651057243347, acc = 0.904296875\n",
            "Batch 83: loss = 0.2935956120491028, acc = 0.900390625\n",
            "Batch 84: loss = 0.2751612961292267, acc = 0.9033203125\n",
            "Batch 85: loss = 0.3167324960231781, acc = 0.8837890625\n",
            "Batch 86: loss = 0.3025223910808563, acc = 0.9033203125\n",
            "Batch 87: loss = 0.283693790435791, acc = 0.912109375\n",
            "Batch 88: loss = 0.30538520216941833, acc = 0.8994140625\n",
            "Batch 89: loss = 0.23915788531303406, acc = 0.927734375\n",
            "Batch 90: loss = 0.3097822666168213, acc = 0.8974609375\n",
            "Batch 91: loss = 0.2959752678871155, acc = 0.8974609375\n",
            "Batch 92: loss = 0.2977476418018341, acc = 0.8984375\n",
            "Batch 93: loss = 0.27176180481910706, acc = 0.9091796875\n",
            "Batch 94: loss = 0.238050177693367, acc = 0.9189453125\n",
            "Batch 95: loss = 0.26322993636131287, acc = 0.9072265625\n",
            "Batch 96: loss = 0.31326380372047424, acc = 0.8857421875\n",
            "Batch 97: loss = 0.29566052556037903, acc = 0.90234375\n",
            "Batch 98: loss = 0.29508301615715027, acc = 0.8974609375\n",
            "Batch 99: loss = 0.27463552355766296, acc = 0.9150390625\n",
            "Batch 100: loss = 0.2838675081729889, acc = 0.90234375\n",
            "Batch 101: loss = 0.2450418919324875, acc = 0.919921875\n",
            "Batch 102: loss = 0.28916919231414795, acc = 0.9013671875\n",
            "Batch 103: loss = 0.28845304250717163, acc = 0.900390625\n",
            "Batch 104: loss = 0.26423364877700806, acc = 0.9150390625\n",
            "Batch 105: loss = 0.244674414396286, acc = 0.9140625\n",
            "Batch 106: loss = 0.25965574383735657, acc = 0.900390625\n",
            "Batch 107: loss = 0.2962797284126282, acc = 0.8916015625\n",
            "Batch 108: loss = 0.202914759516716, acc = 0.9326171875\n",
            "Batch 109: loss = 0.3013457953929901, acc = 0.90234375\n",
            "Batch 110: loss = 0.26366716623306274, acc = 0.912109375\n",
            "Batch 111: loss = 0.29630422592163086, acc = 0.90234375\n",
            "Batch 112: loss = 0.29465341567993164, acc = 0.9052734375\n",
            "Batch 113: loss = 0.290152907371521, acc = 0.91796875\n",
            "Batch 114: loss = 0.2827697694301605, acc = 0.9072265625\n",
            "Batch 115: loss = 0.2988961935043335, acc = 0.900390625\n",
            "Batch 116: loss = 0.29097065329551697, acc = 0.900390625\n",
            "Batch 117: loss = 0.2788689434528351, acc = 0.904296875\n",
            "Batch 118: loss = 0.24992679059505463, acc = 0.9150390625\n",
            "Batch 119: loss = 0.2508375942707062, acc = 0.9267578125\n",
            "Batch 120: loss = 0.25660470128059387, acc = 0.9140625\n",
            "Batch 121: loss = 0.28203344345092773, acc = 0.8916015625\n",
            "Batch 122: loss = 0.29201918840408325, acc = 0.8916015625\n",
            "Batch 123: loss = 0.2438911348581314, acc = 0.9169921875\n",
            "Batch 124: loss = 0.2980607748031616, acc = 0.9033203125\n",
            "Batch 125: loss = 0.30777889490127563, acc = 0.8955078125\n",
            "Batch 126: loss = 0.2702521085739136, acc = 0.9072265625\n",
            "Saved checkpoint to weights.80.h5\n",
            "\n",
            "Epoch 81/100\n",
            "Batch 1: loss = 0.3548819124698639, acc = 0.888671875\n",
            "Batch 2: loss = 0.31194689869880676, acc = 0.892578125\n",
            "Batch 3: loss = 0.26152244210243225, acc = 0.916015625\n",
            "Batch 4: loss = 0.30703628063201904, acc = 0.8935546875\n",
            "Batch 5: loss = 0.30267226696014404, acc = 0.912109375\n",
            "Batch 6: loss = 0.286549836397171, acc = 0.900390625\n",
            "Batch 7: loss = 0.2545902132987976, acc = 0.9091796875\n",
            "Batch 8: loss = 0.28686684370040894, acc = 0.91015625\n",
            "Batch 9: loss = 0.3006371557712555, acc = 0.896484375\n",
            "Batch 10: loss = 0.21964392066001892, acc = 0.9228515625\n",
            "Batch 11: loss = 0.2945651412010193, acc = 0.90625\n",
            "Batch 12: loss = 0.26424241065979004, acc = 0.9013671875\n",
            "Batch 13: loss = 0.27775222063064575, acc = 0.9013671875\n",
            "Batch 14: loss = 0.22848713397979736, acc = 0.9248046875\n",
            "Batch 15: loss = 0.24427548050880432, acc = 0.9208984375\n",
            "Batch 16: loss = 0.27463534474372864, acc = 0.91015625\n",
            "Batch 17: loss = 0.2517206072807312, acc = 0.916015625\n",
            "Batch 18: loss = 0.3221030533313751, acc = 0.888671875\n",
            "Batch 19: loss = 0.27586251497268677, acc = 0.91015625\n",
            "Batch 20: loss = 0.2777271866798401, acc = 0.904296875\n",
            "Batch 21: loss = 0.31804609298706055, acc = 0.8916015625\n",
            "Batch 22: loss = 0.2623639702796936, acc = 0.9072265625\n",
            "Batch 23: loss = 0.2791922092437744, acc = 0.9033203125\n",
            "Batch 24: loss = 0.292975515127182, acc = 0.8984375\n",
            "Batch 25: loss = 0.28445836901664734, acc = 0.91015625\n",
            "Batch 26: loss = 0.26412031054496765, acc = 0.908203125\n",
            "Batch 27: loss = 0.30655020475387573, acc = 0.9052734375\n",
            "Batch 28: loss = 0.26822078227996826, acc = 0.9130859375\n",
            "Batch 29: loss = 0.2851860523223877, acc = 0.9091796875\n",
            "Batch 30: loss = 0.2776494026184082, acc = 0.9130859375\n",
            "Batch 31: loss = 0.2954619824886322, acc = 0.904296875\n",
            "Batch 32: loss = 0.3018365800380707, acc = 0.892578125\n",
            "Batch 33: loss = 0.26164183020591736, acc = 0.9150390625\n",
            "Batch 34: loss = 0.27113965153694153, acc = 0.91015625\n",
            "Batch 35: loss = 0.27400150895118713, acc = 0.916015625\n",
            "Batch 36: loss = 0.2509322464466095, acc = 0.923828125\n",
            "Batch 37: loss = 0.2496998906135559, acc = 0.9130859375\n",
            "Batch 38: loss = 0.2645896077156067, acc = 0.9072265625\n",
            "Batch 39: loss = 0.25117120146751404, acc = 0.9169921875\n",
            "Batch 40: loss = 0.2611919045448303, acc = 0.9140625\n",
            "Batch 41: loss = 0.25199466943740845, acc = 0.9140625\n",
            "Batch 42: loss = 0.2806661128997803, acc = 0.91015625\n",
            "Batch 43: loss = 0.284982293844223, acc = 0.90234375\n",
            "Batch 44: loss = 0.23632678389549255, acc = 0.923828125\n",
            "Batch 45: loss = 0.24298971891403198, acc = 0.91796875\n",
            "Batch 46: loss = 0.2637746036052704, acc = 0.91796875\n",
            "Batch 47: loss = 0.2334081530570984, acc = 0.927734375\n",
            "Batch 48: loss = 0.24710877239704132, acc = 0.916015625\n",
            "Batch 49: loss = 0.2509031295776367, acc = 0.921875\n",
            "Batch 50: loss = 0.22955384850502014, acc = 0.923828125\n",
            "Batch 51: loss = 0.23665335774421692, acc = 0.9228515625\n",
            "Batch 52: loss = 0.25355079770088196, acc = 0.91015625\n",
            "Batch 53: loss = 0.27613937854766846, acc = 0.916015625\n",
            "Batch 54: loss = 0.250405877828598, acc = 0.9208984375\n",
            "Batch 55: loss = 0.2282579094171524, acc = 0.9228515625\n",
            "Batch 56: loss = 0.2646884620189667, acc = 0.9072265625\n",
            "Batch 57: loss = 0.28946202993392944, acc = 0.8916015625\n",
            "Batch 58: loss = 0.2894836366176605, acc = 0.900390625\n",
            "Batch 59: loss = 0.2540716528892517, acc = 0.9169921875\n",
            "Batch 60: loss = 0.2534431219100952, acc = 0.9130859375\n",
            "Batch 61: loss = 0.23020847141742706, acc = 0.92578125\n",
            "Batch 62: loss = 0.3162107765674591, acc = 0.90234375\n",
            "Batch 63: loss = 0.26150986552238464, acc = 0.9111328125\n",
            "Batch 64: loss = 0.23692604899406433, acc = 0.919921875\n",
            "Batch 65: loss = 0.2598084807395935, acc = 0.9072265625\n",
            "Batch 66: loss = 0.306388258934021, acc = 0.8984375\n",
            "Batch 67: loss = 0.2787879705429077, acc = 0.8984375\n",
            "Batch 68: loss = 0.2895905673503876, acc = 0.888671875\n",
            "Batch 69: loss = 0.23187771439552307, acc = 0.9208984375\n",
            "Batch 70: loss = 0.2973020076751709, acc = 0.8984375\n",
            "Batch 71: loss = 0.31306272745132446, acc = 0.89453125\n",
            "Batch 72: loss = 0.24533212184906006, acc = 0.9052734375\n",
            "Batch 73: loss = 0.29128748178482056, acc = 0.9013671875\n",
            "Batch 74: loss = 0.32586002349853516, acc = 0.88671875\n",
            "Batch 75: loss = 0.31135886907577515, acc = 0.892578125\n",
            "Batch 76: loss = 0.2850985825061798, acc = 0.9111328125\n",
            "Batch 77: loss = 0.291913241147995, acc = 0.9072265625\n",
            "Batch 78: loss = 0.27134454250335693, acc = 0.908203125\n",
            "Batch 79: loss = 0.2965313196182251, acc = 0.9033203125\n",
            "Batch 80: loss = 0.23283426463603973, acc = 0.91796875\n",
            "Batch 81: loss = 0.26986163854599, acc = 0.9150390625\n",
            "Batch 82: loss = 0.2650914788246155, acc = 0.90625\n",
            "Batch 83: loss = 0.30401694774627686, acc = 0.8935546875\n",
            "Batch 84: loss = 0.27883896231651306, acc = 0.900390625\n",
            "Batch 85: loss = 0.3244822025299072, acc = 0.888671875\n",
            "Batch 86: loss = 0.2961943745613098, acc = 0.90234375\n",
            "Batch 87: loss = 0.2666051983833313, acc = 0.916015625\n",
            "Batch 88: loss = 0.32189372181892395, acc = 0.89453125\n",
            "Batch 89: loss = 0.2386619746685028, acc = 0.9345703125\n",
            "Batch 90: loss = 0.3072930872440338, acc = 0.90234375\n",
            "Batch 91: loss = 0.282405823469162, acc = 0.9033203125\n",
            "Batch 92: loss = 0.3018477261066437, acc = 0.8994140625\n",
            "Batch 93: loss = 0.24935367703437805, acc = 0.9140625\n",
            "Batch 94: loss = 0.2659398913383484, acc = 0.90234375\n",
            "Batch 95: loss = 0.2954370677471161, acc = 0.900390625\n",
            "Batch 96: loss = 0.3083900511264801, acc = 0.89453125\n",
            "Batch 97: loss = 0.29531365633010864, acc = 0.90234375\n",
            "Batch 98: loss = 0.2788456082344055, acc = 0.90625\n",
            "Batch 99: loss = 0.2959170937538147, acc = 0.896484375\n",
            "Batch 100: loss = 0.27396735548973083, acc = 0.90625\n",
            "Batch 101: loss = 0.26104211807250977, acc = 0.90625\n",
            "Batch 102: loss = 0.2786516845226288, acc = 0.900390625\n",
            "Batch 103: loss = 0.278624951839447, acc = 0.912109375\n",
            "Batch 104: loss = 0.238348126411438, acc = 0.9130859375\n",
            "Batch 105: loss = 0.22693300247192383, acc = 0.9248046875\n",
            "Batch 106: loss = 0.26040416955947876, acc = 0.90625\n",
            "Batch 107: loss = 0.2760448753833771, acc = 0.8994140625\n",
            "Batch 108: loss = 0.24676711857318878, acc = 0.9169921875\n",
            "Batch 109: loss = 0.288094699382782, acc = 0.8974609375\n",
            "Batch 110: loss = 0.23962470889091492, acc = 0.9150390625\n",
            "Batch 111: loss = 0.28312674164772034, acc = 0.908203125\n",
            "Batch 112: loss = 0.2545231878757477, acc = 0.9208984375\n",
            "Batch 113: loss = 0.2525320053100586, acc = 0.912109375\n",
            "Batch 114: loss = 0.2833051085472107, acc = 0.9130859375\n",
            "Batch 115: loss = 0.2875910997390747, acc = 0.90625\n",
            "Batch 116: loss = 0.2613976299762726, acc = 0.9150390625\n",
            "Batch 117: loss = 0.29239052534103394, acc = 0.8955078125\n",
            "Batch 118: loss = 0.25472331047058105, acc = 0.91796875\n",
            "Batch 119: loss = 0.2510834336280823, acc = 0.912109375\n",
            "Batch 120: loss = 0.2720015048980713, acc = 0.90625\n",
            "Batch 121: loss = 0.24434055387973785, acc = 0.9228515625\n",
            "Batch 122: loss = 0.23733919858932495, acc = 0.9189453125\n",
            "Batch 123: loss = 0.2703920006752014, acc = 0.9130859375\n",
            "Batch 124: loss = 0.299798846244812, acc = 0.89453125\n",
            "Batch 125: loss = 0.29180485010147095, acc = 0.896484375\n",
            "Batch 126: loss = 0.2823895514011383, acc = 0.904296875\n",
            "\n",
            "Epoch 82/100\n",
            "Batch 1: loss = 0.3310890793800354, acc = 0.90234375\n",
            "Batch 2: loss = 0.28127890825271606, acc = 0.9072265625\n",
            "Batch 3: loss = 0.29391026496887207, acc = 0.9072265625\n",
            "Batch 4: loss = 0.2644643783569336, acc = 0.9091796875\n",
            "Batch 5: loss = 0.26567742228507996, acc = 0.9208984375\n",
            "Batch 6: loss = 0.28939878940582275, acc = 0.8935546875\n",
            "Batch 7: loss = 0.2679160535335541, acc = 0.9072265625\n",
            "Batch 8: loss = 0.2945330739021301, acc = 0.9052734375\n",
            "Batch 9: loss = 0.28518909215927124, acc = 0.9013671875\n",
            "Batch 10: loss = 0.22215227782726288, acc = 0.927734375\n",
            "Batch 11: loss = 0.26291555166244507, acc = 0.9130859375\n",
            "Batch 12: loss = 0.2587716579437256, acc = 0.908203125\n",
            "Batch 13: loss = 0.31024169921875, acc = 0.8955078125\n",
            "Batch 14: loss = 0.26814499497413635, acc = 0.9013671875\n",
            "Batch 15: loss = 0.26391318440437317, acc = 0.912109375\n",
            "Batch 16: loss = 0.27788758277893066, acc = 0.90234375\n",
            "Batch 17: loss = 0.29804977774620056, acc = 0.90234375\n",
            "Batch 18: loss = 0.27101582288742065, acc = 0.904296875\n",
            "Batch 19: loss = 0.2821771502494812, acc = 0.9130859375\n",
            "Batch 20: loss = 0.26920077204704285, acc = 0.9091796875\n",
            "Batch 21: loss = 0.26957982778549194, acc = 0.9091796875\n",
            "Batch 22: loss = 0.2760942578315735, acc = 0.9033203125\n",
            "Batch 23: loss = 0.29718145728111267, acc = 0.8984375\n",
            "Batch 24: loss = 0.26447391510009766, acc = 0.9169921875\n",
            "Batch 25: loss = 0.27187588810920715, acc = 0.90625\n",
            "Batch 26: loss = 0.25835081934928894, acc = 0.9140625\n",
            "Batch 27: loss = 0.30859047174453735, acc = 0.90625\n",
            "Batch 28: loss = 0.2721068263053894, acc = 0.9111328125\n",
            "Batch 29: loss = 0.2861195504665375, acc = 0.904296875\n",
            "Batch 30: loss = 0.25015535950660706, acc = 0.9140625\n",
            "Batch 31: loss = 0.29612138867378235, acc = 0.8994140625\n",
            "Batch 32: loss = 0.30605947971343994, acc = 0.9052734375\n",
            "Batch 33: loss = 0.22818851470947266, acc = 0.931640625\n",
            "Batch 34: loss = 0.265682190656662, acc = 0.9052734375\n",
            "Batch 35: loss = 0.2591727375984192, acc = 0.9150390625\n",
            "Batch 36: loss = 0.23931284248828888, acc = 0.91796875\n",
            "Batch 37: loss = 0.2609153985977173, acc = 0.9189453125\n",
            "Batch 38: loss = 0.238328754901886, acc = 0.9208984375\n",
            "Batch 39: loss = 0.2666393220424652, acc = 0.9052734375\n",
            "Batch 40: loss = 0.2568245232105255, acc = 0.9189453125\n",
            "Batch 41: loss = 0.2508271038532257, acc = 0.9111328125\n",
            "Batch 42: loss = 0.2562287449836731, acc = 0.91015625\n",
            "Batch 43: loss = 0.2738119065761566, acc = 0.90234375\n",
            "Batch 44: loss = 0.26094430685043335, acc = 0.9208984375\n",
            "Batch 45: loss = 0.2606689929962158, acc = 0.9130859375\n",
            "Batch 46: loss = 0.22379755973815918, acc = 0.9287109375\n",
            "Batch 47: loss = 0.25153207778930664, acc = 0.912109375\n",
            "Batch 48: loss = 0.2076113075017929, acc = 0.9306640625\n",
            "Batch 49: loss = 0.2341119945049286, acc = 0.9306640625\n",
            "Batch 50: loss = 0.24930791556835175, acc = 0.9091796875\n",
            "Batch 51: loss = 0.26808619499206543, acc = 0.91015625\n",
            "Batch 52: loss = 0.28807440400123596, acc = 0.9072265625\n",
            "Batch 53: loss = 0.24530234932899475, acc = 0.9248046875\n",
            "Batch 54: loss = 0.20362688601016998, acc = 0.9287109375\n",
            "Batch 55: loss = 0.2478455901145935, acc = 0.92578125\n",
            "Batch 56: loss = 0.28504306077957153, acc = 0.9013671875\n",
            "Batch 57: loss = 0.2514972686767578, acc = 0.916015625\n",
            "Batch 58: loss = 0.2766367495059967, acc = 0.904296875\n",
            "Batch 59: loss = 0.22079774737358093, acc = 0.9267578125\n",
            "Batch 60: loss = 0.24085748195648193, acc = 0.9169921875\n",
            "Batch 61: loss = 0.25353413820266724, acc = 0.9169921875\n",
            "Batch 62: loss = 0.2898077666759491, acc = 0.9052734375\n",
            "Batch 63: loss = 0.2502098083496094, acc = 0.91796875\n",
            "Batch 64: loss = 0.24279281497001648, acc = 0.9150390625\n",
            "Batch 65: loss = 0.315485417842865, acc = 0.8857421875\n",
            "Batch 66: loss = 0.257061243057251, acc = 0.9150390625\n",
            "Batch 67: loss = 0.2442299723625183, acc = 0.916015625\n",
            "Batch 68: loss = 0.25785985589027405, acc = 0.9072265625\n",
            "Batch 69: loss = 0.2493615299463272, acc = 0.9111328125\n",
            "Batch 70: loss = 0.29643285274505615, acc = 0.9033203125\n",
            "Batch 71: loss = 0.28606554865837097, acc = 0.89453125\n",
            "Batch 72: loss = 0.25198620557785034, acc = 0.9189453125\n",
            "Batch 73: loss = 0.3002242147922516, acc = 0.8916015625\n",
            "Batch 74: loss = 0.27506163716316223, acc = 0.904296875\n",
            "Batch 75: loss = 0.3196105360984802, acc = 0.8896484375\n",
            "Batch 76: loss = 0.2454890012741089, acc = 0.919921875\n",
            "Batch 77: loss = 0.2547813355922699, acc = 0.916015625\n",
            "Batch 78: loss = 0.27372312545776367, acc = 0.916015625\n",
            "Batch 79: loss = 0.24647361040115356, acc = 0.916015625\n",
            "Batch 80: loss = 0.2299647182226181, acc = 0.919921875\n",
            "Batch 81: loss = 0.28794023394584656, acc = 0.9013671875\n",
            "Batch 82: loss = 0.24916531145572662, acc = 0.9169921875\n",
            "Batch 83: loss = 0.24153965711593628, acc = 0.912109375\n",
            "Batch 84: loss = 0.26023802161216736, acc = 0.908203125\n",
            "Batch 85: loss = 0.28415679931640625, acc = 0.8994140625\n",
            "Batch 86: loss = 0.28931716084480286, acc = 0.9013671875\n",
            "Batch 87: loss = 0.26172488927841187, acc = 0.90625\n",
            "Batch 88: loss = 0.2868329882621765, acc = 0.8984375\n",
            "Batch 89: loss = 0.2374584674835205, acc = 0.9150390625\n",
            "Batch 90: loss = 0.3095042407512665, acc = 0.8984375\n",
            "Batch 91: loss = 0.2913138270378113, acc = 0.9033203125\n",
            "Batch 92: loss = 0.2553422451019287, acc = 0.9111328125\n",
            "Batch 93: loss = 0.27497896552085876, acc = 0.9052734375\n",
            "Batch 94: loss = 0.2628796696662903, acc = 0.904296875\n",
            "Batch 95: loss = 0.2915197014808655, acc = 0.9052734375\n",
            "Batch 96: loss = 0.30365079641342163, acc = 0.88671875\n",
            "Batch 97: loss = 0.3012736439704895, acc = 0.8984375\n",
            "Batch 98: loss = 0.2735893130302429, acc = 0.90234375\n",
            "Batch 99: loss = 0.2889085114002228, acc = 0.8935546875\n",
            "Batch 100: loss = 0.23633973300457, acc = 0.919921875\n",
            "Batch 101: loss = 0.27347034215927124, acc = 0.912109375\n",
            "Batch 102: loss = 0.2929207384586334, acc = 0.8994140625\n",
            "Batch 103: loss = 0.2664649784564972, acc = 0.91015625\n",
            "Batch 104: loss = 0.26204848289489746, acc = 0.9130859375\n",
            "Batch 105: loss = 0.24189573526382446, acc = 0.9140625\n",
            "Batch 106: loss = 0.24631869792938232, acc = 0.921875\n",
            "Batch 107: loss = 0.2732151746749878, acc = 0.9052734375\n",
            "Batch 108: loss = 0.2552223801612854, acc = 0.9052734375\n",
            "Batch 109: loss = 0.2535627484321594, acc = 0.92578125\n",
            "Batch 110: loss = 0.23968617618083954, acc = 0.9208984375\n",
            "Batch 111: loss = 0.2994787096977234, acc = 0.8984375\n",
            "Batch 112: loss = 0.2764689326286316, acc = 0.90625\n",
            "Batch 113: loss = 0.2634485363960266, acc = 0.896484375\n",
            "Batch 114: loss = 0.2875831127166748, acc = 0.9052734375\n",
            "Batch 115: loss = 0.3000364899635315, acc = 0.90234375\n",
            "Batch 116: loss = 0.33987271785736084, acc = 0.880859375\n",
            "Batch 117: loss = 0.2725770175457001, acc = 0.9169921875\n",
            "Batch 118: loss = 0.26075905561447144, acc = 0.90625\n",
            "Batch 119: loss = 0.25422218441963196, acc = 0.916015625\n",
            "Batch 120: loss = 0.2120358645915985, acc = 0.923828125\n",
            "Batch 121: loss = 0.2681461274623871, acc = 0.9091796875\n",
            "Batch 122: loss = 0.25865107774734497, acc = 0.9091796875\n",
            "Batch 123: loss = 0.2836446464061737, acc = 0.90234375\n",
            "Batch 124: loss = 0.28801533579826355, acc = 0.904296875\n",
            "Batch 125: loss = 0.2917535901069641, acc = 0.8935546875\n",
            "Batch 126: loss = 0.30367717146873474, acc = 0.9013671875\n",
            "\n",
            "Epoch 83/100\n",
            "Batch 1: loss = 0.34384143352508545, acc = 0.8994140625\n",
            "Batch 2: loss = 0.31048262119293213, acc = 0.8935546875\n",
            "Batch 3: loss = 0.30115318298339844, acc = 0.900390625\n",
            "Batch 4: loss = 0.23110899329185486, acc = 0.931640625\n",
            "Batch 5: loss = 0.2717237174510956, acc = 0.9033203125\n",
            "Batch 6: loss = 0.2863018810749054, acc = 0.9033203125\n",
            "Batch 7: loss = 0.24012982845306396, acc = 0.927734375\n",
            "Batch 8: loss = 0.26439669728279114, acc = 0.9150390625\n",
            "Batch 9: loss = 0.26978886127471924, acc = 0.91015625\n",
            "Batch 10: loss = 0.26619628071784973, acc = 0.9091796875\n",
            "Batch 11: loss = 0.2904546558856964, acc = 0.8935546875\n",
            "Batch 12: loss = 0.2691318988800049, acc = 0.9033203125\n",
            "Batch 13: loss = 0.2508177161216736, acc = 0.91796875\n",
            "Batch 14: loss = 0.24891209602355957, acc = 0.919921875\n",
            "Batch 15: loss = 0.261636883020401, acc = 0.91015625\n",
            "Batch 16: loss = 0.28399011492729187, acc = 0.9033203125\n",
            "Batch 17: loss = 0.2552843391895294, acc = 0.921875\n",
            "Batch 18: loss = 0.27105286717414856, acc = 0.9091796875\n",
            "Batch 19: loss = 0.2593574523925781, acc = 0.9169921875\n",
            "Batch 20: loss = 0.277523934841156, acc = 0.9111328125\n",
            "Batch 21: loss = 0.27865782380104065, acc = 0.90234375\n",
            "Batch 22: loss = 0.26628556847572327, acc = 0.9140625\n",
            "Batch 23: loss = 0.26100173592567444, acc = 0.9091796875\n",
            "Batch 24: loss = 0.28982192277908325, acc = 0.9013671875\n",
            "Batch 25: loss = 0.25983941555023193, acc = 0.912109375\n",
            "Batch 26: loss = 0.2756616771221161, acc = 0.8955078125\n",
            "Batch 27: loss = 0.27576202154159546, acc = 0.912109375\n",
            "Batch 28: loss = 0.22143183648586273, acc = 0.9267578125\n",
            "Batch 29: loss = 0.2705813944339752, acc = 0.9052734375\n",
            "Batch 30: loss = 0.24887731671333313, acc = 0.9169921875\n",
            "Batch 31: loss = 0.2982061803340912, acc = 0.8984375\n",
            "Batch 32: loss = 0.3328727185726166, acc = 0.8935546875\n",
            "Batch 33: loss = 0.23624110221862793, acc = 0.9208984375\n",
            "Batch 34: loss = 0.28318190574645996, acc = 0.9052734375\n",
            "Batch 35: loss = 0.26878201961517334, acc = 0.912109375\n",
            "Batch 36: loss = 0.24332815408706665, acc = 0.921875\n",
            "Batch 37: loss = 0.24278375506401062, acc = 0.919921875\n",
            "Batch 38: loss = 0.24625569581985474, acc = 0.91796875\n",
            "Batch 39: loss = 0.2665456533432007, acc = 0.908203125\n",
            "Batch 40: loss = 0.24736055731773376, acc = 0.919921875\n",
            "Batch 41: loss = 0.2554492950439453, acc = 0.9150390625\n",
            "Batch 42: loss = 0.2776656746864319, acc = 0.912109375\n",
            "Batch 43: loss = 0.26118332147598267, acc = 0.9140625\n",
            "Batch 44: loss = 0.23978161811828613, acc = 0.921875\n",
            "Batch 45: loss = 0.2692021131515503, acc = 0.916015625\n",
            "Batch 46: loss = 0.21592022478580475, acc = 0.9248046875\n",
            "Batch 47: loss = 0.24324952065944672, acc = 0.91796875\n",
            "Batch 48: loss = 0.23183289170265198, acc = 0.9208984375\n",
            "Batch 49: loss = 0.26695144176483154, acc = 0.9140625\n",
            "Batch 50: loss = 0.22813832759857178, acc = 0.923828125\n",
            "Batch 51: loss = 0.23885591328144073, acc = 0.923828125\n",
            "Batch 52: loss = 0.28120243549346924, acc = 0.90234375\n",
            "Batch 53: loss = 0.26209932565689087, acc = 0.90234375\n",
            "Batch 54: loss = 0.22884248197078705, acc = 0.923828125\n",
            "Batch 55: loss = 0.23596987128257751, acc = 0.9208984375\n",
            "Batch 56: loss = 0.27869677543640137, acc = 0.9013671875\n",
            "Batch 57: loss = 0.2760307490825653, acc = 0.9130859375\n",
            "Batch 58: loss = 0.27163854241371155, acc = 0.9072265625\n",
            "Batch 59: loss = 0.2297842800617218, acc = 0.9228515625\n",
            "Batch 60: loss = 0.2559335231781006, acc = 0.9150390625\n",
            "Batch 61: loss = 0.2557414174079895, acc = 0.9130859375\n",
            "Batch 62: loss = 0.28047001361846924, acc = 0.91015625\n",
            "Batch 63: loss = 0.2370697259902954, acc = 0.919921875\n",
            "Batch 64: loss = 0.2462254911661148, acc = 0.9169921875\n",
            "Batch 65: loss = 0.2648633122444153, acc = 0.90625\n",
            "Batch 66: loss = 0.2747616767883301, acc = 0.912109375\n",
            "Batch 67: loss = 0.273074209690094, acc = 0.908203125\n",
            "Batch 68: loss = 0.23507638275623322, acc = 0.921875\n",
            "Batch 69: loss = 0.25006765127182007, acc = 0.9052734375\n",
            "Batch 70: loss = 0.28116580843925476, acc = 0.9072265625\n",
            "Batch 71: loss = 0.267487108707428, acc = 0.912109375\n",
            "Batch 72: loss = 0.2362712025642395, acc = 0.9189453125\n",
            "Batch 73: loss = 0.26955646276474, acc = 0.9130859375\n",
            "Batch 74: loss = 0.2847466468811035, acc = 0.9091796875\n",
            "Batch 75: loss = 0.2945632338523865, acc = 0.8896484375\n",
            "Batch 76: loss = 0.2940371036529541, acc = 0.9072265625\n",
            "Batch 77: loss = 0.24588727951049805, acc = 0.9150390625\n",
            "Batch 78: loss = 0.28336551785469055, acc = 0.90625\n",
            "Batch 79: loss = 0.2538363039493561, acc = 0.9169921875\n",
            "Batch 80: loss = 0.22461003065109253, acc = 0.923828125\n",
            "Batch 81: loss = 0.29206591844558716, acc = 0.900390625\n",
            "Batch 82: loss = 0.28814205527305603, acc = 0.9052734375\n",
            "Batch 83: loss = 0.2754996120929718, acc = 0.904296875\n",
            "Batch 84: loss = 0.2533859610557556, acc = 0.9150390625\n",
            "Batch 85: loss = 0.299855500459671, acc = 0.8955078125\n",
            "Batch 86: loss = 0.2766591012477875, acc = 0.9013671875\n",
            "Batch 87: loss = 0.2509423494338989, acc = 0.91796875\n",
            "Batch 88: loss = 0.27540141344070435, acc = 0.9013671875\n",
            "Batch 89: loss = 0.2621307075023651, acc = 0.9111328125\n",
            "Batch 90: loss = 0.3211677670478821, acc = 0.8876953125\n",
            "Batch 91: loss = 0.28873443603515625, acc = 0.8974609375\n",
            "Batch 92: loss = 0.25180739164352417, acc = 0.9169921875\n",
            "Batch 93: loss = 0.28308528661727905, acc = 0.9140625\n",
            "Batch 94: loss = 0.2730114758014679, acc = 0.91015625\n",
            "Batch 95: loss = 0.25888288021087646, acc = 0.9072265625\n",
            "Batch 96: loss = 0.3023914694786072, acc = 0.8974609375\n",
            "Batch 97: loss = 0.27576032280921936, acc = 0.9111328125\n",
            "Batch 98: loss = 0.26702094078063965, acc = 0.90625\n",
            "Batch 99: loss = 0.27966970205307007, acc = 0.9052734375\n",
            "Batch 100: loss = 0.2813223600387573, acc = 0.904296875\n",
            "Batch 101: loss = 0.28151658177375793, acc = 0.9013671875\n",
            "Batch 102: loss = 0.27756115794181824, acc = 0.9130859375\n",
            "Batch 103: loss = 0.2631295919418335, acc = 0.908203125\n",
            "Batch 104: loss = 0.25166764855384827, acc = 0.90625\n",
            "Batch 105: loss = 0.23676615953445435, acc = 0.9228515625\n",
            "Batch 106: loss = 0.25138574838638306, acc = 0.919921875\n",
            "Batch 107: loss = 0.265180766582489, acc = 0.9072265625\n",
            "Batch 108: loss = 0.2583281397819519, acc = 0.912109375\n",
            "Batch 109: loss = 0.25390034914016724, acc = 0.91796875\n",
            "Batch 110: loss = 0.28001558780670166, acc = 0.90625\n",
            "Batch 111: loss = 0.2692318856716156, acc = 0.9189453125\n",
            "Batch 112: loss = 0.27976417541503906, acc = 0.9091796875\n",
            "Batch 113: loss = 0.25596195459365845, acc = 0.916015625\n",
            "Batch 114: loss = 0.2937130033969879, acc = 0.908203125\n",
            "Batch 115: loss = 0.26524755358695984, acc = 0.9111328125\n",
            "Batch 116: loss = 0.2709745764732361, acc = 0.9111328125\n",
            "Batch 117: loss = 0.24737192690372467, acc = 0.91796875\n",
            "Batch 118: loss = 0.25115731358528137, acc = 0.904296875\n",
            "Batch 119: loss = 0.23490558564662933, acc = 0.9169921875\n",
            "Batch 120: loss = 0.2559959292411804, acc = 0.921875\n",
            "Batch 121: loss = 0.24560734629631042, acc = 0.9189453125\n",
            "Batch 122: loss = 0.2189052551984787, acc = 0.9189453125\n",
            "Batch 123: loss = 0.2838689386844635, acc = 0.9072265625\n",
            "Batch 124: loss = 0.2602171003818512, acc = 0.9189453125\n",
            "Batch 125: loss = 0.27837127447128296, acc = 0.90234375\n",
            "Batch 126: loss = 0.2883763611316681, acc = 0.912109375\n",
            "\n",
            "Epoch 84/100\n",
            "Batch 1: loss = 0.3085564970970154, acc = 0.9072265625\n",
            "Batch 2: loss = 0.24800892174243927, acc = 0.927734375\n",
            "Batch 3: loss = 0.24585479497909546, acc = 0.919921875\n",
            "Batch 4: loss = 0.25213003158569336, acc = 0.921875\n",
            "Batch 5: loss = 0.2698443830013275, acc = 0.916015625\n",
            "Batch 6: loss = 0.27731868624687195, acc = 0.9072265625\n",
            "Batch 7: loss = 0.2519577443599701, acc = 0.91796875\n",
            "Batch 8: loss = 0.26398923993110657, acc = 0.9150390625\n",
            "Batch 9: loss = 0.27456411719322205, acc = 0.9111328125\n",
            "Batch 10: loss = 0.2390339970588684, acc = 0.921875\n",
            "Batch 11: loss = 0.24281145632266998, acc = 0.921875\n",
            "Batch 12: loss = 0.2537004351615906, acc = 0.9208984375\n",
            "Batch 13: loss = 0.22671112418174744, acc = 0.919921875\n",
            "Batch 14: loss = 0.241628035902977, acc = 0.9169921875\n",
            "Batch 15: loss = 0.22814017534255981, acc = 0.916015625\n",
            "Batch 16: loss = 0.2799015939235687, acc = 0.9140625\n",
            "Batch 17: loss = 0.2910635471343994, acc = 0.9091796875\n",
            "Batch 18: loss = 0.25346702337265015, acc = 0.9169921875\n",
            "Batch 19: loss = 0.29270070791244507, acc = 0.8974609375\n",
            "Batch 20: loss = 0.2589893341064453, acc = 0.912109375\n",
            "Batch 21: loss = 0.27271318435668945, acc = 0.916015625\n",
            "Batch 22: loss = 0.2618679702281952, acc = 0.91015625\n",
            "Batch 23: loss = 0.24182401597499847, acc = 0.9052734375\n",
            "Batch 24: loss = 0.28570201992988586, acc = 0.9150390625\n",
            "Batch 25: loss = 0.25855010747909546, acc = 0.90625\n",
            "Batch 26: loss = 0.24348315596580505, acc = 0.921875\n",
            "Batch 27: loss = 0.28521040081977844, acc = 0.8935546875\n",
            "Batch 28: loss = 0.25928327441215515, acc = 0.908203125\n",
            "Batch 29: loss = 0.3235701024532318, acc = 0.8857421875\n",
            "Batch 30: loss = 0.23293346166610718, acc = 0.93359375\n",
            "Batch 31: loss = 0.2873997092247009, acc = 0.904296875\n",
            "Batch 32: loss = 0.3250577449798584, acc = 0.884765625\n",
            "Batch 33: loss = 0.26266399025917053, acc = 0.9130859375\n",
            "Batch 34: loss = 0.26297372579574585, acc = 0.9130859375\n",
            "Batch 35: loss = 0.25999242067337036, acc = 0.9150390625\n",
            "Batch 36: loss = 0.25529780983924866, acc = 0.9140625\n",
            "Batch 37: loss = 0.23673342168331146, acc = 0.9306640625\n",
            "Batch 38: loss = 0.24105724692344666, acc = 0.92578125\n",
            "Batch 39: loss = 0.24860480427742004, acc = 0.9169921875\n",
            "Batch 40: loss = 0.2309970110654831, acc = 0.927734375\n",
            "Batch 41: loss = 0.2664698660373688, acc = 0.9130859375\n",
            "Batch 42: loss = 0.25519484281539917, acc = 0.912109375\n",
            "Batch 43: loss = 0.24632607400417328, acc = 0.919921875\n",
            "Batch 44: loss = 0.23269160091876984, acc = 0.916015625\n",
            "Batch 45: loss = 0.2397235482931137, acc = 0.9267578125\n",
            "Batch 46: loss = 0.2324857860803604, acc = 0.9140625\n",
            "Batch 47: loss = 0.23509395122528076, acc = 0.9248046875\n",
            "Batch 48: loss = 0.23499196767807007, acc = 0.9267578125\n",
            "Batch 49: loss = 0.26219481229782104, acc = 0.9228515625\n",
            "Batch 50: loss = 0.2272767424583435, acc = 0.921875\n",
            "Batch 51: loss = 0.24960683286190033, acc = 0.916015625\n",
            "Batch 52: loss = 0.25117799639701843, acc = 0.9130859375\n",
            "Batch 53: loss = 0.2393036037683487, acc = 0.921875\n",
            "Batch 54: loss = 0.23285618424415588, acc = 0.9189453125\n",
            "Batch 55: loss = 0.2453519105911255, acc = 0.9169921875\n",
            "Batch 56: loss = 0.25013822317123413, acc = 0.908203125\n",
            "Batch 57: loss = 0.24613527953624725, acc = 0.9150390625\n",
            "Batch 58: loss = 0.2842765748500824, acc = 0.9033203125\n",
            "Batch 59: loss = 0.26552847027778625, acc = 0.9130859375\n",
            "Batch 60: loss = 0.25167879462242126, acc = 0.9208984375\n",
            "Batch 61: loss = 0.23619765043258667, acc = 0.9306640625\n",
            "Batch 62: loss = 0.26582595705986023, acc = 0.9140625\n",
            "Batch 63: loss = 0.24357487261295319, acc = 0.9140625\n",
            "Batch 64: loss = 0.24926549196243286, acc = 0.91796875\n",
            "Batch 65: loss = 0.24078965187072754, acc = 0.921875\n",
            "Batch 66: loss = 0.2627226710319519, acc = 0.91796875\n",
            "Batch 67: loss = 0.27723971009254456, acc = 0.8984375\n",
            "Batch 68: loss = 0.2907027006149292, acc = 0.904296875\n",
            "Batch 69: loss = 0.23531967401504517, acc = 0.921875\n",
            "Batch 70: loss = 0.24632695317268372, acc = 0.9169921875\n",
            "Batch 71: loss = 0.26597556471824646, acc = 0.912109375\n",
            "Batch 72: loss = 0.242994025349617, acc = 0.9248046875\n",
            "Batch 73: loss = 0.26001212000846863, acc = 0.9169921875\n",
            "Batch 74: loss = 0.3046874701976776, acc = 0.900390625\n",
            "Batch 75: loss = 0.3255787491798401, acc = 0.888671875\n",
            "Batch 76: loss = 0.2955109477043152, acc = 0.896484375\n",
            "Batch 77: loss = 0.23843684792518616, acc = 0.9072265625\n",
            "Batch 78: loss = 0.2542651295661926, acc = 0.91015625\n",
            "Batch 79: loss = 0.25468766689300537, acc = 0.919921875\n",
            "Batch 80: loss = 0.24565331637859344, acc = 0.91796875\n",
            "Batch 81: loss = 0.263435035943985, acc = 0.9150390625\n",
            "Batch 82: loss = 0.28703126311302185, acc = 0.904296875\n",
            "Batch 83: loss = 0.2619485855102539, acc = 0.9169921875\n",
            "Batch 84: loss = 0.26630228757858276, acc = 0.908203125\n",
            "Batch 85: loss = 0.27311205863952637, acc = 0.9072265625\n",
            "Batch 86: loss = 0.23811735212802887, acc = 0.916015625\n",
            "Batch 87: loss = 0.2525220215320587, acc = 0.912109375\n",
            "Batch 88: loss = 0.28071874380111694, acc = 0.9033203125\n",
            "Batch 89: loss = 0.26415371894836426, acc = 0.9208984375\n",
            "Batch 90: loss = 0.27818793058395386, acc = 0.9072265625\n",
            "Batch 91: loss = 0.2525342106819153, acc = 0.91796875\n",
            "Batch 92: loss = 0.25773534178733826, acc = 0.9140625\n",
            "Batch 93: loss = 0.23966065049171448, acc = 0.923828125\n",
            "Batch 94: loss = 0.2263229638338089, acc = 0.916015625\n",
            "Batch 95: loss = 0.2314835786819458, acc = 0.9208984375\n",
            "Batch 96: loss = 0.3156379163265228, acc = 0.904296875\n",
            "Batch 97: loss = 0.3208804130554199, acc = 0.8935546875\n",
            "Batch 98: loss = 0.26544028520584106, acc = 0.912109375\n",
            "Batch 99: loss = 0.2893081605434418, acc = 0.91015625\n",
            "Batch 100: loss = 0.26137107610702515, acc = 0.90234375\n",
            "Batch 101: loss = 0.2489897459745407, acc = 0.9140625\n",
            "Batch 102: loss = 0.30812329053878784, acc = 0.89453125\n",
            "Batch 103: loss = 0.25022754073143005, acc = 0.91796875\n",
            "Batch 104: loss = 0.2595880627632141, acc = 0.900390625\n",
            "Batch 105: loss = 0.23230357468128204, acc = 0.9091796875\n",
            "Batch 106: loss = 0.25722768902778625, acc = 0.9111328125\n",
            "Batch 107: loss = 0.25522419810295105, acc = 0.9140625\n",
            "Batch 108: loss = 0.25948017835617065, acc = 0.908203125\n",
            "Batch 109: loss = 0.27761808037757874, acc = 0.904296875\n",
            "Batch 110: loss = 0.21761071681976318, acc = 0.9267578125\n",
            "Batch 111: loss = 0.24265220761299133, acc = 0.921875\n",
            "Batch 112: loss = 0.25635433197021484, acc = 0.90625\n",
            "Batch 113: loss = 0.25314581394195557, acc = 0.9130859375\n",
            "Batch 114: loss = 0.25315597653388977, acc = 0.9189453125\n",
            "Batch 115: loss = 0.24206098914146423, acc = 0.916015625\n",
            "Batch 116: loss = 0.28085675835609436, acc = 0.91015625\n",
            "Batch 117: loss = 0.26724275946617126, acc = 0.9091796875\n",
            "Batch 118: loss = 0.2512514293193817, acc = 0.9091796875\n",
            "Batch 119: loss = 0.24797898530960083, acc = 0.916015625\n",
            "Batch 120: loss = 0.22122430801391602, acc = 0.923828125\n",
            "Batch 121: loss = 0.2608909606933594, acc = 0.908203125\n",
            "Batch 122: loss = 0.23517084121704102, acc = 0.921875\n",
            "Batch 123: loss = 0.259785920381546, acc = 0.91796875\n",
            "Batch 124: loss = 0.25709477066993713, acc = 0.91015625\n",
            "Batch 125: loss = 0.2971673607826233, acc = 0.88671875\n",
            "Batch 126: loss = 0.28784072399139404, acc = 0.8974609375\n",
            "\n",
            "Epoch 85/100\n",
            "Batch 1: loss = 0.3126142621040344, acc = 0.9052734375\n",
            "Batch 2: loss = 0.2831927239894867, acc = 0.9091796875\n",
            "Batch 3: loss = 0.2746597230434418, acc = 0.900390625\n",
            "Batch 4: loss = 0.2677798271179199, acc = 0.91796875\n",
            "Batch 5: loss = 0.258535772562027, acc = 0.9111328125\n",
            "Batch 6: loss = 0.261773020029068, acc = 0.9072265625\n",
            "Batch 7: loss = 0.2715114951133728, acc = 0.9111328125\n",
            "Batch 8: loss = 0.26866042613983154, acc = 0.9140625\n",
            "Batch 9: loss = 0.268317848443985, acc = 0.9052734375\n",
            "Batch 10: loss = 0.21728073060512543, acc = 0.923828125\n",
            "Batch 11: loss = 0.26216739416122437, acc = 0.9150390625\n",
            "Batch 12: loss = 0.2207244634628296, acc = 0.92578125\n",
            "Batch 13: loss = 0.2650820314884186, acc = 0.91796875\n",
            "Batch 14: loss = 0.26141688227653503, acc = 0.9111328125\n",
            "Batch 15: loss = 0.1971575766801834, acc = 0.927734375\n",
            "Batch 16: loss = 0.26758596301078796, acc = 0.9091796875\n",
            "Batch 17: loss = 0.27831825613975525, acc = 0.9140625\n",
            "Batch 18: loss = 0.2848520874977112, acc = 0.9091796875\n",
            "Batch 19: loss = 0.27327197790145874, acc = 0.904296875\n",
            "Batch 20: loss = 0.2306130975484848, acc = 0.916015625\n",
            "Batch 21: loss = 0.24833440780639648, acc = 0.904296875\n",
            "Batch 22: loss = 0.24372856318950653, acc = 0.9150390625\n",
            "Batch 23: loss = 0.25797563791275024, acc = 0.90625\n",
            "Batch 24: loss = 0.26529863476753235, acc = 0.9208984375\n",
            "Batch 25: loss = 0.2574900984764099, acc = 0.921875\n",
            "Batch 26: loss = 0.2356271892786026, acc = 0.9150390625\n",
            "Batch 27: loss = 0.2612450122833252, acc = 0.9140625\n",
            "Batch 28: loss = 0.22622065246105194, acc = 0.919921875\n",
            "Batch 29: loss = 0.27098920941352844, acc = 0.90625\n",
            "Batch 30: loss = 0.2772826552391052, acc = 0.91015625\n",
            "Batch 31: loss = 0.2770227789878845, acc = 0.9033203125\n",
            "Batch 32: loss = 0.29126301407814026, acc = 0.904296875\n",
            "Batch 33: loss = 0.24514570832252502, acc = 0.9228515625\n",
            "Batch 34: loss = 0.2984691858291626, acc = 0.8916015625\n",
            "Batch 35: loss = 0.2583329379558563, acc = 0.9169921875\n",
            "Batch 36: loss = 0.2312185913324356, acc = 0.927734375\n",
            "Batch 37: loss = 0.21639573574066162, acc = 0.9267578125\n",
            "Batch 38: loss = 0.2292529195547104, acc = 0.9228515625\n",
            "Batch 39: loss = 0.28301650285720825, acc = 0.91015625\n",
            "Batch 40: loss = 0.267788290977478, acc = 0.9091796875\n",
            "Batch 41: loss = 0.26894259452819824, acc = 0.9052734375\n",
            "Batch 42: loss = 0.2673194110393524, acc = 0.904296875\n",
            "Batch 43: loss = 0.2613808512687683, acc = 0.9091796875\n",
            "Batch 44: loss = 0.22605958580970764, acc = 0.9326171875\n",
            "Batch 45: loss = 0.2391810417175293, acc = 0.9150390625\n",
            "Batch 46: loss = 0.21160636842250824, acc = 0.9326171875\n",
            "Batch 47: loss = 0.24376392364501953, acc = 0.912109375\n",
            "Batch 48: loss = 0.20918183028697968, acc = 0.916015625\n",
            "Batch 49: loss = 0.24756936728954315, acc = 0.919921875\n",
            "Batch 50: loss = 0.2104775607585907, acc = 0.919921875\n",
            "Batch 51: loss = 0.2383086085319519, acc = 0.9150390625\n",
            "Batch 52: loss = 0.24508671462535858, acc = 0.916015625\n",
            "Batch 53: loss = 0.24238723516464233, acc = 0.9248046875\n",
            "Batch 54: loss = 0.21649494767189026, acc = 0.9267578125\n",
            "Batch 55: loss = 0.22180715203285217, acc = 0.931640625\n",
            "Batch 56: loss = 0.25222063064575195, acc = 0.91796875\n",
            "Batch 57: loss = 0.2978362441062927, acc = 0.89453125\n",
            "Batch 58: loss = 0.27093809843063354, acc = 0.9072265625\n",
            "Batch 59: loss = 0.2286682426929474, acc = 0.91796875\n",
            "Batch 60: loss = 0.2700504958629608, acc = 0.9111328125\n",
            "Batch 61: loss = 0.2432045042514801, acc = 0.9150390625\n",
            "Batch 62: loss = 0.28573283553123474, acc = 0.900390625\n",
            "Batch 63: loss = 0.23426635563373566, acc = 0.919921875\n",
            "Batch 64: loss = 0.2324220836162567, acc = 0.9228515625\n",
            "Batch 65: loss = 0.2832047641277313, acc = 0.9013671875\n",
            "Batch 66: loss = 0.28620272874832153, acc = 0.9033203125\n",
            "Batch 67: loss = 0.2584497332572937, acc = 0.9091796875\n",
            "Batch 68: loss = 0.24634386599063873, acc = 0.9091796875\n",
            "Batch 69: loss = 0.24014809727668762, acc = 0.91796875\n",
            "Batch 70: loss = 0.2536366581916809, acc = 0.9169921875\n",
            "Batch 71: loss = 0.26438912749290466, acc = 0.9033203125\n",
            "Batch 72: loss = 0.21695950627326965, acc = 0.927734375\n",
            "Batch 73: loss = 0.28980666399002075, acc = 0.900390625\n",
            "Batch 74: loss = 0.24939575791358948, acc = 0.912109375\n",
            "Batch 75: loss = 0.3045516610145569, acc = 0.908203125\n",
            "Batch 76: loss = 0.26558196544647217, acc = 0.9208984375\n",
            "Batch 77: loss = 0.2670142948627472, acc = 0.900390625\n",
            "Batch 78: loss = 0.2714846134185791, acc = 0.90625\n",
            "Batch 79: loss = 0.20120543241500854, acc = 0.93359375\n",
            "Batch 80: loss = 0.23761144280433655, acc = 0.919921875\n",
            "Batch 81: loss = 0.2505113184452057, acc = 0.923828125\n",
            "Batch 82: loss = 0.2898600697517395, acc = 0.8984375\n",
            "Batch 83: loss = 0.2818230390548706, acc = 0.9091796875\n",
            "Batch 84: loss = 0.2604551315307617, acc = 0.90234375\n",
            "Batch 85: loss = 0.2564850151538849, acc = 0.9072265625\n",
            "Batch 86: loss = 0.27366891503334045, acc = 0.904296875\n",
            "Batch 87: loss = 0.2534562051296234, acc = 0.9140625\n",
            "Batch 88: loss = 0.28241047263145447, acc = 0.9150390625\n",
            "Batch 89: loss = 0.24415360391139984, acc = 0.9169921875\n",
            "Batch 90: loss = 0.2860269844532013, acc = 0.9052734375\n",
            "Batch 91: loss = 0.2679087221622467, acc = 0.912109375\n",
            "Batch 92: loss = 0.2655947804450989, acc = 0.916015625\n",
            "Batch 93: loss = 0.23599986732006073, acc = 0.9208984375\n",
            "Batch 94: loss = 0.23349250853061676, acc = 0.9169921875\n",
            "Batch 95: loss = 0.25045719742774963, acc = 0.9072265625\n",
            "Batch 96: loss = 0.29776841402053833, acc = 0.90234375\n",
            "Batch 97: loss = 0.2577313780784607, acc = 0.91015625\n",
            "Batch 98: loss = 0.26174625754356384, acc = 0.9169921875\n",
            "Batch 99: loss = 0.28815293312072754, acc = 0.9013671875\n",
            "Batch 100: loss = 0.24339106678962708, acc = 0.9111328125\n",
            "Batch 101: loss = 0.28645527362823486, acc = 0.888671875\n",
            "Batch 102: loss = 0.24270927906036377, acc = 0.9189453125\n",
            "Batch 103: loss = 0.28107672929763794, acc = 0.91015625\n",
            "Batch 104: loss = 0.25343865156173706, acc = 0.9111328125\n",
            "Batch 105: loss = 0.23552022874355316, acc = 0.91796875\n",
            "Batch 106: loss = 0.2594081461429596, acc = 0.9033203125\n",
            "Batch 107: loss = 0.2517240345478058, acc = 0.919921875\n",
            "Batch 108: loss = 0.24781692028045654, acc = 0.9150390625\n",
            "Batch 109: loss = 0.27609512209892273, acc = 0.912109375\n",
            "Batch 110: loss = 0.24456055462360382, acc = 0.9248046875\n",
            "Batch 111: loss = 0.2851889431476593, acc = 0.9111328125\n",
            "Batch 112: loss = 0.2685335576534271, acc = 0.908203125\n",
            "Batch 113: loss = 0.26197105646133423, acc = 0.9033203125\n",
            "Batch 114: loss = 0.24999919533729553, acc = 0.9130859375\n",
            "Batch 115: loss = 0.2715773284435272, acc = 0.90625\n",
            "Batch 116: loss = 0.2537868022918701, acc = 0.9091796875\n",
            "Batch 117: loss = 0.28937625885009766, acc = 0.908203125\n",
            "Batch 118: loss = 0.29104021191596985, acc = 0.90234375\n",
            "Batch 119: loss = 0.2135622799396515, acc = 0.921875\n",
            "Batch 120: loss = 0.257386177778244, acc = 0.91015625\n",
            "Batch 121: loss = 0.223694309592247, acc = 0.9208984375\n",
            "Batch 122: loss = 0.27726057171821594, acc = 0.912109375\n",
            "Batch 123: loss = 0.2538752257823944, acc = 0.9169921875\n",
            "Batch 124: loss = 0.308677077293396, acc = 0.9013671875\n",
            "Batch 125: loss = 0.27055811882019043, acc = 0.90234375\n",
            "Batch 126: loss = 0.2978299856185913, acc = 0.8916015625\n",
            "\n",
            "Epoch 86/100\n",
            "Batch 1: loss = 0.29244863986968994, acc = 0.90625\n",
            "Batch 2: loss = 0.27156195044517517, acc = 0.90625\n",
            "Batch 3: loss = 0.26498690247535706, acc = 0.9052734375\n",
            "Batch 4: loss = 0.22844654321670532, acc = 0.9228515625\n",
            "Batch 5: loss = 0.2513749301433563, acc = 0.9140625\n",
            "Batch 6: loss = 0.30874356627464294, acc = 0.888671875\n",
            "Batch 7: loss = 0.26824161410331726, acc = 0.912109375\n",
            "Batch 8: loss = 0.2776169776916504, acc = 0.904296875\n",
            "Batch 9: loss = 0.24773941934108734, acc = 0.91015625\n",
            "Batch 10: loss = 0.2123241424560547, acc = 0.93359375\n",
            "Batch 11: loss = 0.27028679847717285, acc = 0.90625\n",
            "Batch 12: loss = 0.23777064681053162, acc = 0.9248046875\n",
            "Batch 13: loss = 0.24890446662902832, acc = 0.9169921875\n",
            "Batch 14: loss = 0.2620547115802765, acc = 0.91015625\n",
            "Batch 15: loss = 0.19960156083106995, acc = 0.9267578125\n",
            "Batch 16: loss = 0.27587538957595825, acc = 0.908203125\n",
            "Batch 17: loss = 0.25020065903663635, acc = 0.9189453125\n",
            "Batch 18: loss = 0.2729780972003937, acc = 0.90625\n",
            "Batch 19: loss = 0.2460484355688095, acc = 0.91015625\n",
            "Batch 20: loss = 0.2756550908088684, acc = 0.8984375\n",
            "Batch 21: loss = 0.2924571931362152, acc = 0.9013671875\n",
            "Batch 22: loss = 0.2612984776496887, acc = 0.9150390625\n",
            "Batch 23: loss = 0.2514747381210327, acc = 0.9150390625\n",
            "Batch 24: loss = 0.2493208348751068, acc = 0.9130859375\n",
            "Batch 25: loss = 0.2508450150489807, acc = 0.9111328125\n",
            "Batch 26: loss = 0.24766603112220764, acc = 0.9140625\n",
            "Batch 27: loss = 0.2687269449234009, acc = 0.9072265625\n",
            "Batch 28: loss = 0.22714339196681976, acc = 0.9208984375\n",
            "Batch 29: loss = 0.2768818140029907, acc = 0.908203125\n",
            "Batch 30: loss = 0.23100410401821136, acc = 0.9140625\n",
            "Batch 31: loss = 0.2790096700191498, acc = 0.9033203125\n",
            "Batch 32: loss = 0.29200541973114014, acc = 0.9072265625\n",
            "Batch 33: loss = 0.2475372850894928, acc = 0.919921875\n",
            "Batch 34: loss = 0.24534612894058228, acc = 0.9208984375\n",
            "Batch 35: loss = 0.2683151066303253, acc = 0.91015625\n",
            "Batch 36: loss = 0.19774232804775238, acc = 0.9423828125\n",
            "Batch 37: loss = 0.211533784866333, acc = 0.9326171875\n",
            "Batch 38: loss = 0.24600626528263092, acc = 0.9248046875\n",
            "Batch 39: loss = 0.2412514090538025, acc = 0.9169921875\n",
            "Batch 40: loss = 0.2374911904335022, acc = 0.9287109375\n",
            "Batch 41: loss = 0.24355635046958923, acc = 0.9052734375\n",
            "Batch 42: loss = 0.23787353932857513, acc = 0.92578125\n",
            "Batch 43: loss = 0.26860564947128296, acc = 0.9033203125\n",
            "Batch 44: loss = 0.23723824322223663, acc = 0.923828125\n",
            "Batch 45: loss = 0.2662649154663086, acc = 0.9111328125\n",
            "Batch 46: loss = 0.2584392726421356, acc = 0.9013671875\n",
            "Batch 47: loss = 0.2617236375808716, acc = 0.9052734375\n",
            "Batch 48: loss = 0.22858662903308868, acc = 0.916015625\n",
            "Batch 49: loss = 0.2704901099205017, acc = 0.9189453125\n",
            "Batch 50: loss = 0.2138233780860901, acc = 0.9267578125\n",
            "Batch 51: loss = 0.25051647424697876, acc = 0.923828125\n",
            "Batch 52: loss = 0.25029271841049194, acc = 0.9111328125\n",
            "Batch 53: loss = 0.23716232180595398, acc = 0.9267578125\n",
            "Batch 54: loss = 0.20440061390399933, acc = 0.9287109375\n",
            "Batch 55: loss = 0.20771315693855286, acc = 0.9375\n",
            "Batch 56: loss = 0.26328253746032715, acc = 0.9111328125\n",
            "Batch 57: loss = 0.2591117024421692, acc = 0.91015625\n",
            "Batch 58: loss = 0.27765563130378723, acc = 0.9072265625\n",
            "Batch 59: loss = 0.2323024868965149, acc = 0.9169921875\n",
            "Batch 60: loss = 0.2465718537569046, acc = 0.9072265625\n",
            "Batch 61: loss = 0.2623251676559448, acc = 0.9091796875\n",
            "Batch 62: loss = 0.2800370156764984, acc = 0.9091796875\n",
            "Batch 63: loss = 0.2606673538684845, acc = 0.9189453125\n",
            "Batch 64: loss = 0.23420113325119019, acc = 0.9189453125\n",
            "Batch 65: loss = 0.2419164627790451, acc = 0.91796875\n",
            "Batch 66: loss = 0.2549924850463867, acc = 0.9130859375\n",
            "Batch 67: loss = 0.2619125247001648, acc = 0.9189453125\n",
            "Batch 68: loss = 0.24453654885292053, acc = 0.9169921875\n",
            "Batch 69: loss = 0.21562986075878143, acc = 0.9169921875\n",
            "Batch 70: loss = 0.24732652306556702, acc = 0.921875\n",
            "Batch 71: loss = 0.24947193264961243, acc = 0.919921875\n",
            "Batch 72: loss = 0.2590148448944092, acc = 0.904296875\n",
            "Batch 73: loss = 0.25898969173431396, acc = 0.9169921875\n",
            "Batch 74: loss = 0.2965433895587921, acc = 0.9052734375\n",
            "Batch 75: loss = 0.3314574360847473, acc = 0.8857421875\n",
            "Batch 76: loss = 0.28839415311813354, acc = 0.90234375\n",
            "Batch 77: loss = 0.2561533749103546, acc = 0.916015625\n",
            "Batch 78: loss = 0.2528464198112488, acc = 0.9130859375\n",
            "Batch 79: loss = 0.2700635492801666, acc = 0.8974609375\n",
            "Batch 80: loss = 0.24769766628742218, acc = 0.912109375\n",
            "Batch 81: loss = 0.27373138070106506, acc = 0.9013671875\n",
            "Batch 82: loss = 0.2829274535179138, acc = 0.9013671875\n",
            "Batch 83: loss = 0.24898529052734375, acc = 0.9189453125\n",
            "Batch 84: loss = 0.2773861289024353, acc = 0.9091796875\n",
            "Batch 85: loss = 0.292037695646286, acc = 0.8984375\n",
            "Batch 86: loss = 0.25431180000305176, acc = 0.91015625\n",
            "Batch 87: loss = 0.2779507339000702, acc = 0.9072265625\n",
            "Batch 88: loss = 0.2948783338069916, acc = 0.9013671875\n",
            "Batch 89: loss = 0.21687531471252441, acc = 0.931640625\n",
            "Batch 90: loss = 0.2637253701686859, acc = 0.912109375\n",
            "Batch 91: loss = 0.2500571608543396, acc = 0.919921875\n",
            "Batch 92: loss = 0.2876132130622864, acc = 0.900390625\n",
            "Batch 93: loss = 0.2397177815437317, acc = 0.9208984375\n",
            "Batch 94: loss = 0.24378816783428192, acc = 0.91796875\n",
            "Batch 95: loss = 0.26107057929039, acc = 0.912109375\n",
            "Batch 96: loss = 0.311996191740036, acc = 0.8955078125\n",
            "Batch 97: loss = 0.29601940512657166, acc = 0.9033203125\n",
            "Batch 98: loss = 0.2802833020687103, acc = 0.9140625\n",
            "Batch 99: loss = 0.2764275074005127, acc = 0.8974609375\n",
            "Batch 100: loss = 0.2606717646121979, acc = 0.8974609375\n",
            "Batch 101: loss = 0.280671089887619, acc = 0.912109375\n",
            "Batch 102: loss = 0.2339266687631607, acc = 0.92578125\n",
            "Batch 103: loss = 0.2482074499130249, acc = 0.9111328125\n",
            "Batch 104: loss = 0.24682527780532837, acc = 0.912109375\n",
            "Batch 105: loss = 0.2291620820760727, acc = 0.9228515625\n",
            "Batch 106: loss = 0.22349566221237183, acc = 0.921875\n",
            "Batch 107: loss = 0.24322092533111572, acc = 0.9140625\n",
            "Batch 108: loss = 0.2643575370311737, acc = 0.9130859375\n",
            "Batch 109: loss = 0.2657349109649658, acc = 0.90625\n",
            "Batch 110: loss = 0.23850378394126892, acc = 0.921875\n",
            "Batch 111: loss = 0.2620602250099182, acc = 0.919921875\n",
            "Batch 112: loss = 0.23919138312339783, acc = 0.9140625\n",
            "Batch 113: loss = 0.27694886922836304, acc = 0.8994140625\n",
            "Batch 114: loss = 0.2447899580001831, acc = 0.9267578125\n",
            "Batch 115: loss = 0.25118350982666016, acc = 0.91015625\n",
            "Batch 116: loss = 0.2610732614994049, acc = 0.91015625\n",
            "Batch 117: loss = 0.24047760665416718, acc = 0.91796875\n",
            "Batch 118: loss = 0.23979344964027405, acc = 0.9267578125\n",
            "Batch 119: loss = 0.25129881501197815, acc = 0.908203125\n",
            "Batch 120: loss = 0.23330581188201904, acc = 0.92578125\n",
            "Batch 121: loss = 0.24500900506973267, acc = 0.9248046875\n",
            "Batch 122: loss = 0.22445671260356903, acc = 0.9248046875\n",
            "Batch 123: loss = 0.23925957083702087, acc = 0.912109375\n",
            "Batch 124: loss = 0.2701414227485657, acc = 0.904296875\n",
            "Batch 125: loss = 0.29267001152038574, acc = 0.8984375\n",
            "Batch 126: loss = 0.27514567971229553, acc = 0.9052734375\n",
            "\n",
            "Epoch 87/100\n",
            "Batch 1: loss = 0.3068525195121765, acc = 0.9052734375\n",
            "Batch 2: loss = 0.28352242708206177, acc = 0.9013671875\n",
            "Batch 3: loss = 0.2666138708591461, acc = 0.9052734375\n",
            "Batch 4: loss = 0.24235738813877106, acc = 0.919921875\n",
            "Batch 5: loss = 0.24114738404750824, acc = 0.9169921875\n",
            "Batch 6: loss = 0.2824735641479492, acc = 0.90625\n",
            "Batch 7: loss = 0.23892037570476532, acc = 0.912109375\n",
            "Batch 8: loss = 0.26761728525161743, acc = 0.9111328125\n",
            "Batch 9: loss = 0.2749983072280884, acc = 0.90234375\n",
            "Batch 10: loss = 0.22519248723983765, acc = 0.923828125\n",
            "Batch 11: loss = 0.2752164304256439, acc = 0.9013671875\n",
            "Batch 12: loss = 0.2667509615421295, acc = 0.900390625\n",
            "Batch 13: loss = 0.2144092619419098, acc = 0.927734375\n",
            "Batch 14: loss = 0.22231721878051758, acc = 0.9267578125\n",
            "Batch 15: loss = 0.26926472783088684, acc = 0.9033203125\n",
            "Batch 16: loss = 0.26741355657577515, acc = 0.90625\n",
            "Batch 17: loss = 0.27780279517173767, acc = 0.912109375\n",
            "Batch 18: loss = 0.22683146595954895, acc = 0.9248046875\n",
            "Batch 19: loss = 0.24399390816688538, acc = 0.9189453125\n",
            "Batch 20: loss = 0.27626579999923706, acc = 0.90234375\n",
            "Batch 21: loss = 0.2693571448326111, acc = 0.9111328125\n",
            "Batch 22: loss = 0.2432582676410675, acc = 0.916015625\n",
            "Batch 23: loss = 0.2610625922679901, acc = 0.9130859375\n",
            "Batch 24: loss = 0.23189495503902435, acc = 0.9287109375\n",
            "Batch 25: loss = 0.28895682096481323, acc = 0.9033203125\n",
            "Batch 26: loss = 0.2640013098716736, acc = 0.908203125\n",
            "Batch 27: loss = 0.292961984872818, acc = 0.9013671875\n",
            "Batch 28: loss = 0.2554144859313965, acc = 0.9111328125\n",
            "Batch 29: loss = 0.26655206084251404, acc = 0.90625\n",
            "Batch 30: loss = 0.2384958565235138, acc = 0.921875\n",
            "Batch 31: loss = 0.2539302110671997, acc = 0.908203125\n",
            "Batch 32: loss = 0.31304967403411865, acc = 0.88671875\n",
            "Batch 33: loss = 0.23408934473991394, acc = 0.919921875\n",
            "Batch 34: loss = 0.2596021592617035, acc = 0.9091796875\n",
            "Batch 35: loss = 0.23889011144638062, acc = 0.9248046875\n",
            "Batch 36: loss = 0.20932114124298096, acc = 0.931640625\n",
            "Batch 37: loss = 0.21408744156360626, acc = 0.931640625\n",
            "Batch 38: loss = 0.21958324313163757, acc = 0.9248046875\n",
            "Batch 39: loss = 0.24379420280456543, acc = 0.91796875\n",
            "Batch 40: loss = 0.23165443539619446, acc = 0.9150390625\n",
            "Batch 41: loss = 0.23801544308662415, acc = 0.91796875\n",
            "Batch 42: loss = 0.2607433795928955, acc = 0.9208984375\n",
            "Batch 43: loss = 0.2313224971294403, acc = 0.91015625\n",
            "Batch 44: loss = 0.23639871180057526, acc = 0.927734375\n",
            "Batch 45: loss = 0.24344101548194885, acc = 0.921875\n",
            "Batch 46: loss = 0.2419113665819168, acc = 0.9189453125\n",
            "Batch 47: loss = 0.24147355556488037, acc = 0.9267578125\n",
            "Batch 48: loss = 0.20452803373336792, acc = 0.9296875\n",
            "Batch 49: loss = 0.24774634838104248, acc = 0.9169921875\n",
            "Batch 50: loss = 0.20916514098644257, acc = 0.9267578125\n",
            "Batch 51: loss = 0.2298188954591751, acc = 0.927734375\n",
            "Batch 52: loss = 0.23295530676841736, acc = 0.923828125\n",
            "Batch 53: loss = 0.2404555231332779, acc = 0.919921875\n",
            "Batch 54: loss = 0.22350367903709412, acc = 0.927734375\n",
            "Batch 55: loss = 0.23052062094211578, acc = 0.9248046875\n",
            "Batch 56: loss = 0.21617397665977478, acc = 0.9375\n",
            "Batch 57: loss = 0.26078420877456665, acc = 0.904296875\n",
            "Batch 58: loss = 0.2869422137737274, acc = 0.90234375\n",
            "Batch 59: loss = 0.23667985200881958, acc = 0.9208984375\n",
            "Batch 60: loss = 0.24156366288661957, acc = 0.923828125\n",
            "Batch 61: loss = 0.2540905475616455, acc = 0.9189453125\n",
            "Batch 62: loss = 0.25932925939559937, acc = 0.908203125\n",
            "Batch 63: loss = 0.24325890839099884, acc = 0.919921875\n",
            "Batch 64: loss = 0.23633313179016113, acc = 0.9208984375\n",
            "Batch 65: loss = 0.24730184674263, acc = 0.9111328125\n",
            "Batch 66: loss = 0.24685540795326233, acc = 0.923828125\n",
            "Batch 67: loss = 0.24700015783309937, acc = 0.916015625\n",
            "Batch 68: loss = 0.2747906744480133, acc = 0.9052734375\n",
            "Batch 69: loss = 0.2570844292640686, acc = 0.9189453125\n",
            "Batch 70: loss = 0.2582429051399231, acc = 0.9189453125\n",
            "Batch 71: loss = 0.27212363481521606, acc = 0.9052734375\n",
            "Batch 72: loss = 0.23464971780776978, acc = 0.9130859375\n",
            "Batch 73: loss = 0.25175654888153076, acc = 0.9091796875\n",
            "Batch 74: loss = 0.2588968276977539, acc = 0.9208984375\n",
            "Batch 75: loss = 0.26613572239875793, acc = 0.912109375\n",
            "Batch 76: loss = 0.2622147500514984, acc = 0.912109375\n",
            "Batch 77: loss = 0.25088560581207275, acc = 0.91796875\n",
            "Batch 78: loss = 0.25566360354423523, acc = 0.9169921875\n",
            "Batch 79: loss = 0.2700154781341553, acc = 0.9150390625\n",
            "Batch 80: loss = 0.20857739448547363, acc = 0.9296875\n",
            "Batch 81: loss = 0.2442176640033722, acc = 0.916015625\n",
            "Batch 82: loss = 0.2649562954902649, acc = 0.9130859375\n",
            "Batch 83: loss = 0.2433410882949829, acc = 0.9130859375\n",
            "Batch 84: loss = 0.2596040964126587, acc = 0.9150390625\n",
            "Batch 85: loss = 0.28259706497192383, acc = 0.9111328125\n",
            "Batch 86: loss = 0.2650603652000427, acc = 0.91015625\n",
            "Batch 87: loss = 0.24807998538017273, acc = 0.919921875\n",
            "Batch 88: loss = 0.24595953524112701, acc = 0.923828125\n",
            "Batch 89: loss = 0.2564466595649719, acc = 0.9111328125\n",
            "Batch 90: loss = 0.25659528374671936, acc = 0.90625\n",
            "Batch 91: loss = 0.29989346861839294, acc = 0.8916015625\n",
            "Batch 92: loss = 0.23717884719371796, acc = 0.92578125\n",
            "Batch 93: loss = 0.23326440155506134, acc = 0.9306640625\n",
            "Batch 94: loss = 0.24374060332775116, acc = 0.916015625\n",
            "Batch 95: loss = 0.2509571313858032, acc = 0.908203125\n",
            "Batch 96: loss = 0.27007490396499634, acc = 0.8974609375\n",
            "Batch 97: loss = 0.30013155937194824, acc = 0.900390625\n",
            "Batch 98: loss = 0.27009764313697815, acc = 0.91015625\n",
            "Batch 99: loss = 0.2607947885990143, acc = 0.912109375\n",
            "Batch 100: loss = 0.25522518157958984, acc = 0.9072265625\n",
            "Batch 101: loss = 0.24444496631622314, acc = 0.912109375\n",
            "Batch 102: loss = 0.2850742042064667, acc = 0.896484375\n",
            "Batch 103: loss = 0.280080109834671, acc = 0.908203125\n",
            "Batch 104: loss = 0.22088947892189026, acc = 0.9208984375\n",
            "Batch 105: loss = 0.21188189089298248, acc = 0.9189453125\n",
            "Batch 106: loss = 0.2384530007839203, acc = 0.9150390625\n",
            "Batch 107: loss = 0.25069451332092285, acc = 0.91796875\n",
            "Batch 108: loss = 0.23614564538002014, acc = 0.9169921875\n",
            "Batch 109: loss = 0.27620169520378113, acc = 0.9091796875\n",
            "Batch 110: loss = 0.23098310828208923, acc = 0.9208984375\n",
            "Batch 111: loss = 0.2382289171218872, acc = 0.9189453125\n",
            "Batch 112: loss = 0.27807891368865967, acc = 0.9130859375\n",
            "Batch 113: loss = 0.236028254032135, acc = 0.912109375\n",
            "Batch 114: loss = 0.2729855477809906, acc = 0.91015625\n",
            "Batch 115: loss = 0.25348174571990967, acc = 0.9169921875\n",
            "Batch 116: loss = 0.28015363216400146, acc = 0.904296875\n",
            "Batch 117: loss = 0.26875919103622437, acc = 0.9111328125\n",
            "Batch 118: loss = 0.2555716335773468, acc = 0.9130859375\n",
            "Batch 119: loss = 0.2151573896408081, acc = 0.92578125\n",
            "Batch 120: loss = 0.2322780191898346, acc = 0.9208984375\n",
            "Batch 121: loss = 0.24526166915893555, acc = 0.91796875\n",
            "Batch 122: loss = 0.23924878239631653, acc = 0.9140625\n",
            "Batch 123: loss = 0.2468668818473816, acc = 0.9208984375\n",
            "Batch 124: loss = 0.24347160756587982, acc = 0.9208984375\n",
            "Batch 125: loss = 0.2887769341468811, acc = 0.9013671875\n",
            "Batch 126: loss = 0.30758699774742126, acc = 0.8916015625\n",
            "\n",
            "Epoch 88/100\n",
            "Batch 1: loss = 0.3181491792201996, acc = 0.904296875\n",
            "Batch 2: loss = 0.28635096549987793, acc = 0.908203125\n",
            "Batch 3: loss = 0.24811163544654846, acc = 0.9169921875\n",
            "Batch 4: loss = 0.24557557702064514, acc = 0.921875\n",
            "Batch 5: loss = 0.23809559643268585, acc = 0.9140625\n",
            "Batch 6: loss = 0.2727119028568268, acc = 0.8994140625\n",
            "Batch 7: loss = 0.24591119587421417, acc = 0.9130859375\n",
            "Batch 8: loss = 0.24402785301208496, acc = 0.9306640625\n",
            "Batch 9: loss = 0.2530125081539154, acc = 0.9130859375\n",
            "Batch 10: loss = 0.24457451701164246, acc = 0.912109375\n",
            "Batch 11: loss = 0.256011426448822, acc = 0.9130859375\n",
            "Batch 12: loss = 0.23242606222629547, acc = 0.9267578125\n",
            "Batch 13: loss = 0.2423623949289322, acc = 0.9208984375\n",
            "Batch 14: loss = 0.2314961552619934, acc = 0.9296875\n",
            "Batch 15: loss = 0.2196798175573349, acc = 0.927734375\n",
            "Batch 16: loss = 0.28127044439315796, acc = 0.908203125\n",
            "Batch 17: loss = 0.24389734864234924, acc = 0.919921875\n",
            "Batch 18: loss = 0.27954912185668945, acc = 0.8955078125\n",
            "Batch 19: loss = 0.24780066311359406, acc = 0.9091796875\n",
            "Batch 20: loss = 0.2498743087053299, acc = 0.9150390625\n",
            "Batch 21: loss = 0.24396492540836334, acc = 0.923828125\n",
            "Batch 22: loss = 0.24998369812965393, acc = 0.91796875\n",
            "Batch 23: loss = 0.2591743767261505, acc = 0.9072265625\n",
            "Batch 24: loss = 0.2582952082157135, acc = 0.9130859375\n",
            "Batch 25: loss = 0.23686130344867706, acc = 0.91796875\n",
            "Batch 26: loss = 0.2643702030181885, acc = 0.90625\n",
            "Batch 27: loss = 0.2792389690876007, acc = 0.8994140625\n",
            "Batch 28: loss = 0.20714397728443146, acc = 0.9267578125\n",
            "Batch 29: loss = 0.2689809799194336, acc = 0.9033203125\n",
            "Batch 30: loss = 0.2680196762084961, acc = 0.912109375\n",
            "Batch 31: loss = 0.275265634059906, acc = 0.9091796875\n",
            "Batch 32: loss = 0.28942394256591797, acc = 0.8974609375\n",
            "Batch 33: loss = 0.2156180590391159, acc = 0.9326171875\n",
            "Batch 34: loss = 0.2775143086910248, acc = 0.904296875\n",
            "Batch 35: loss = 0.2598385214805603, acc = 0.9208984375\n",
            "Batch 36: loss = 0.2435886263847351, acc = 0.908203125\n",
            "Batch 37: loss = 0.21474003791809082, acc = 0.9326171875\n",
            "Batch 38: loss = 0.24220764636993408, acc = 0.919921875\n",
            "Batch 39: loss = 0.2316003441810608, acc = 0.9169921875\n",
            "Batch 40: loss = 0.24661476910114288, acc = 0.9111328125\n",
            "Batch 41: loss = 0.23226819932460785, acc = 0.916015625\n",
            "Batch 42: loss = 0.25802597403526306, acc = 0.9072265625\n",
            "Batch 43: loss = 0.24338673055171967, acc = 0.91015625\n",
            "Batch 44: loss = 0.22199344635009766, acc = 0.9296875\n",
            "Batch 45: loss = 0.23067115247249603, acc = 0.923828125\n",
            "Batch 46: loss = 0.203536257147789, acc = 0.9306640625\n",
            "Batch 47: loss = 0.20827104151248932, acc = 0.939453125\n",
            "Batch 48: loss = 0.20739145576953888, acc = 0.931640625\n",
            "Batch 49: loss = 0.23951944708824158, acc = 0.9111328125\n",
            "Batch 50: loss = 0.20218704640865326, acc = 0.927734375\n",
            "Batch 51: loss = 0.21514496207237244, acc = 0.919921875\n",
            "Batch 52: loss = 0.24526023864746094, acc = 0.9150390625\n",
            "Batch 53: loss = 0.26196566224098206, acc = 0.9130859375\n",
            "Batch 54: loss = 0.21515855193138123, acc = 0.9150390625\n",
            "Batch 55: loss = 0.23073801398277283, acc = 0.9169921875\n",
            "Batch 56: loss = 0.2676745057106018, acc = 0.9130859375\n",
            "Batch 57: loss = 0.27877262234687805, acc = 0.9052734375\n",
            "Batch 58: loss = 0.27362197637557983, acc = 0.9091796875\n",
            "Batch 59: loss = 0.20910458266735077, acc = 0.9326171875\n",
            "Batch 60: loss = 0.23982366919517517, acc = 0.9140625\n",
            "Batch 61: loss = 0.22986480593681335, acc = 0.927734375\n",
            "Batch 62: loss = 0.28283292055130005, acc = 0.900390625\n",
            "Batch 63: loss = 0.24037954211235046, acc = 0.921875\n",
            "Batch 64: loss = 0.21865566074848175, acc = 0.9384765625\n",
            "Batch 65: loss = 0.24145328998565674, acc = 0.9248046875\n",
            "Batch 66: loss = 0.23519061505794525, acc = 0.9228515625\n",
            "Batch 67: loss = 0.27155113220214844, acc = 0.8935546875\n",
            "Batch 68: loss = 0.26328232884407043, acc = 0.9150390625\n",
            "Batch 69: loss = 0.25348612666130066, acc = 0.9091796875\n",
            "Batch 70: loss = 0.25876373052597046, acc = 0.9052734375\n",
            "Batch 71: loss = 0.284404993057251, acc = 0.8916015625\n",
            "Batch 72: loss = 0.2166752815246582, acc = 0.919921875\n",
            "Batch 73: loss = 0.2564765214920044, acc = 0.9091796875\n",
            "Batch 74: loss = 0.29132282733917236, acc = 0.9013671875\n",
            "Batch 75: loss = 0.2967737317085266, acc = 0.8984375\n",
            "Batch 76: loss = 0.27025458216667175, acc = 0.91015625\n",
            "Batch 77: loss = 0.23525108397006989, acc = 0.9228515625\n",
            "Batch 78: loss = 0.25340041518211365, acc = 0.9111328125\n",
            "Batch 79: loss = 0.22929330170154572, acc = 0.9248046875\n",
            "Batch 80: loss = 0.22530953586101532, acc = 0.9208984375\n",
            "Batch 81: loss = 0.2454262673854828, acc = 0.9140625\n",
            "Batch 82: loss = 0.26587724685668945, acc = 0.9169921875\n",
            "Batch 83: loss = 0.2347676306962967, acc = 0.9169921875\n",
            "Batch 84: loss = 0.23686350882053375, acc = 0.916015625\n",
            "Batch 85: loss = 0.2879486680030823, acc = 0.8984375\n",
            "Batch 86: loss = 0.24476787447929382, acc = 0.919921875\n",
            "Batch 87: loss = 0.25292834639549255, acc = 0.912109375\n",
            "Batch 88: loss = 0.270333856344223, acc = 0.9052734375\n",
            "Batch 89: loss = 0.26068758964538574, acc = 0.9208984375\n",
            "Batch 90: loss = 0.24160116910934448, acc = 0.91015625\n",
            "Batch 91: loss = 0.27479618787765503, acc = 0.9033203125\n",
            "Batch 92: loss = 0.26264455914497375, acc = 0.919921875\n",
            "Batch 93: loss = 0.25731873512268066, acc = 0.9111328125\n",
            "Batch 94: loss = 0.25070542097091675, acc = 0.91015625\n",
            "Batch 95: loss = 0.23939421772956848, acc = 0.927734375\n",
            "Batch 96: loss = 0.28393444418907166, acc = 0.892578125\n",
            "Batch 97: loss = 0.29957303404808044, acc = 0.9091796875\n",
            "Batch 98: loss = 0.25516682863235474, acc = 0.9169921875\n",
            "Batch 99: loss = 0.2725084125995636, acc = 0.9013671875\n",
            "Batch 100: loss = 0.2528662085533142, acc = 0.908203125\n",
            "Batch 101: loss = 0.2478431761264801, acc = 0.9130859375\n",
            "Batch 102: loss = 0.24756240844726562, acc = 0.916015625\n",
            "Batch 103: loss = 0.2454148381948471, acc = 0.919921875\n",
            "Batch 104: loss = 0.21306096017360687, acc = 0.923828125\n",
            "Batch 105: loss = 0.22225219011306763, acc = 0.9169921875\n",
            "Batch 106: loss = 0.25347843766212463, acc = 0.9140625\n",
            "Batch 107: loss = 0.24171248078346252, acc = 0.916015625\n",
            "Batch 108: loss = 0.24754565954208374, acc = 0.9150390625\n",
            "Batch 109: loss = 0.2745981812477112, acc = 0.912109375\n",
            "Batch 110: loss = 0.24298247694969177, acc = 0.91796875\n",
            "Batch 111: loss = 0.2702137231826782, acc = 0.900390625\n",
            "Batch 112: loss = 0.2292894572019577, acc = 0.921875\n",
            "Batch 113: loss = 0.24608877301216125, acc = 0.91796875\n",
            "Batch 114: loss = 0.23168203234672546, acc = 0.9169921875\n",
            "Batch 115: loss = 0.2243635505437851, acc = 0.921875\n",
            "Batch 116: loss = 0.23595760762691498, acc = 0.923828125\n",
            "Batch 117: loss = 0.2638748586177826, acc = 0.9130859375\n",
            "Batch 118: loss = 0.2206195592880249, acc = 0.9228515625\n",
            "Batch 119: loss = 0.231834277510643, acc = 0.91796875\n",
            "Batch 120: loss = 0.22936296463012695, acc = 0.919921875\n",
            "Batch 121: loss = 0.23337014019489288, acc = 0.92578125\n",
            "Batch 122: loss = 0.22478856146335602, acc = 0.9248046875\n",
            "Batch 123: loss = 0.2705751657485962, acc = 0.90625\n",
            "Batch 124: loss = 0.27966102957725525, acc = 0.908203125\n",
            "Batch 125: loss = 0.27144235372543335, acc = 0.896484375\n",
            "Batch 126: loss = 0.26038306951522827, acc = 0.9130859375\n",
            "\n",
            "Epoch 89/100\n",
            "Batch 1: loss = 0.2762426733970642, acc = 0.921875\n",
            "Batch 2: loss = 0.24181610345840454, acc = 0.919921875\n",
            "Batch 3: loss = 0.2647601366043091, acc = 0.90234375\n",
            "Batch 4: loss = 0.23014549911022186, acc = 0.9140625\n",
            "Batch 5: loss = 0.25840169191360474, acc = 0.9140625\n",
            "Batch 6: loss = 0.24819910526275635, acc = 0.9111328125\n",
            "Batch 7: loss = 0.24442905187606812, acc = 0.919921875\n",
            "Batch 8: loss = 0.24173355102539062, acc = 0.9208984375\n",
            "Batch 9: loss = 0.2398267388343811, acc = 0.9208984375\n",
            "Batch 10: loss = 0.22190730273723602, acc = 0.931640625\n",
            "Batch 11: loss = 0.2456263303756714, acc = 0.9208984375\n",
            "Batch 12: loss = 0.24586908519268036, acc = 0.9189453125\n",
            "Batch 13: loss = 0.2246445119380951, acc = 0.912109375\n",
            "Batch 14: loss = 0.213112011551857, acc = 0.935546875\n",
            "Batch 15: loss = 0.22049681842327118, acc = 0.931640625\n",
            "Batch 16: loss = 0.28432971239089966, acc = 0.9013671875\n",
            "Batch 17: loss = 0.24087056517601013, acc = 0.91796875\n",
            "Batch 18: loss = 0.2098976969718933, acc = 0.9296875\n",
            "Batch 19: loss = 0.23516617715358734, acc = 0.9208984375\n",
            "Batch 20: loss = 0.26985347270965576, acc = 0.9111328125\n",
            "Batch 21: loss = 0.2968270182609558, acc = 0.900390625\n",
            "Batch 22: loss = 0.23692576587200165, acc = 0.91796875\n",
            "Batch 23: loss = 0.23910772800445557, acc = 0.9248046875\n",
            "Batch 24: loss = 0.2602182626724243, acc = 0.91015625\n",
            "Batch 25: loss = 0.2641170620918274, acc = 0.90234375\n",
            "Batch 26: loss = 0.23830005526542664, acc = 0.9228515625\n",
            "Batch 27: loss = 0.2553204596042633, acc = 0.9189453125\n",
            "Batch 28: loss = 0.24267955124378204, acc = 0.91015625\n",
            "Batch 29: loss = 0.25781750679016113, acc = 0.908203125\n",
            "Batch 30: loss = 0.2090211659669876, acc = 0.9248046875\n",
            "Batch 31: loss = 0.2620055675506592, acc = 0.912109375\n",
            "Batch 32: loss = 0.27006328105926514, acc = 0.9091796875\n",
            "Batch 33: loss = 0.24727648496627808, acc = 0.916015625\n",
            "Batch 34: loss = 0.26505911350250244, acc = 0.9150390625\n",
            "Batch 35: loss = 0.26443248987197876, acc = 0.9169921875\n",
            "Batch 36: loss = 0.20215976238250732, acc = 0.9296875\n",
            "Batch 37: loss = 0.20233352482318878, acc = 0.9326171875\n",
            "Batch 38: loss = 0.26475003361701965, acc = 0.9072265625\n",
            "Batch 39: loss = 0.20178620517253876, acc = 0.94140625\n",
            "Batch 40: loss = 0.21691705286502838, acc = 0.9365234375\n",
            "Batch 41: loss = 0.207472562789917, acc = 0.9208984375\n",
            "Batch 42: loss = 0.2178584635257721, acc = 0.921875\n",
            "Batch 43: loss = 0.2396991103887558, acc = 0.9150390625\n",
            "Batch 44: loss = 0.20157551765441895, acc = 0.9375\n",
            "Batch 45: loss = 0.21783512830734253, acc = 0.9248046875\n",
            "Batch 46: loss = 0.17564262449741364, acc = 0.9375\n",
            "Batch 47: loss = 0.2204788327217102, acc = 0.9248046875\n",
            "Batch 48: loss = 0.211088627576828, acc = 0.9169921875\n",
            "Batch 49: loss = 0.23448584973812103, acc = 0.9267578125\n",
            "Batch 50: loss = 0.21881923079490662, acc = 0.923828125\n",
            "Batch 51: loss = 0.22418679296970367, acc = 0.9287109375\n",
            "Batch 52: loss = 0.22223597764968872, acc = 0.921875\n",
            "Batch 53: loss = 0.2227456271648407, acc = 0.931640625\n",
            "Batch 54: loss = 0.20566508173942566, acc = 0.9287109375\n",
            "Batch 55: loss = 0.2451310008764267, acc = 0.9169921875\n",
            "Batch 56: loss = 0.25503385066986084, acc = 0.9072265625\n",
            "Batch 57: loss = 0.27512404322624207, acc = 0.908203125\n",
            "Batch 58: loss = 0.2559538781642914, acc = 0.9150390625\n",
            "Batch 59: loss = 0.21026672422885895, acc = 0.927734375\n",
            "Batch 60: loss = 0.26734232902526855, acc = 0.900390625\n",
            "Batch 61: loss = 0.23659993708133698, acc = 0.9189453125\n",
            "Batch 62: loss = 0.2780441641807556, acc = 0.9111328125\n",
            "Batch 63: loss = 0.2266189455986023, acc = 0.931640625\n",
            "Batch 64: loss = 0.24788877367973328, acc = 0.9169921875\n",
            "Batch 65: loss = 0.2425483614206314, acc = 0.919921875\n",
            "Batch 66: loss = 0.24244464933872223, acc = 0.9150390625\n",
            "Batch 67: loss = 0.24654889106750488, acc = 0.9130859375\n",
            "Batch 68: loss = 0.23942413926124573, acc = 0.9248046875\n",
            "Batch 69: loss = 0.19491448998451233, acc = 0.93359375\n",
            "Batch 70: loss = 0.2567591965198517, acc = 0.8994140625\n",
            "Batch 71: loss = 0.2628505825996399, acc = 0.91796875\n",
            "Batch 72: loss = 0.25333675742149353, acc = 0.919921875\n",
            "Batch 73: loss = 0.25093528628349304, acc = 0.9267578125\n",
            "Batch 74: loss = 0.28848519921302795, acc = 0.90234375\n",
            "Batch 75: loss = 0.30630797147750854, acc = 0.888671875\n",
            "Batch 76: loss = 0.27971988916397095, acc = 0.908203125\n",
            "Batch 77: loss = 0.23960661888122559, acc = 0.919921875\n",
            "Batch 78: loss = 0.26924315094947815, acc = 0.908203125\n",
            "Batch 79: loss = 0.24361364543437958, acc = 0.9150390625\n",
            "Batch 80: loss = 0.20700247585773468, acc = 0.919921875\n",
            "Batch 81: loss = 0.22850221395492554, acc = 0.9228515625\n",
            "Batch 82: loss = 0.2467845380306244, acc = 0.91796875\n",
            "Batch 83: loss = 0.2389279156923294, acc = 0.9150390625\n",
            "Batch 84: loss = 0.2697484493255615, acc = 0.91015625\n",
            "Batch 85: loss = 0.2727363109588623, acc = 0.9091796875\n",
            "Batch 86: loss = 0.25594282150268555, acc = 0.91796875\n",
            "Batch 87: loss = 0.2515869736671448, acc = 0.91015625\n",
            "Batch 88: loss = 0.261707067489624, acc = 0.9169921875\n",
            "Batch 89: loss = 0.23729446530342102, acc = 0.9228515625\n",
            "Batch 90: loss = 0.2549355626106262, acc = 0.921875\n",
            "Batch 91: loss = 0.24098335206508636, acc = 0.91796875\n",
            "Batch 92: loss = 0.28239405155181885, acc = 0.9072265625\n",
            "Batch 93: loss = 0.23315741121768951, acc = 0.92578125\n",
            "Batch 94: loss = 0.23387661576271057, acc = 0.921875\n",
            "Batch 95: loss = 0.252104789018631, acc = 0.916015625\n",
            "Batch 96: loss = 0.2721160054206848, acc = 0.8935546875\n",
            "Batch 97: loss = 0.2510513663291931, acc = 0.916015625\n",
            "Batch 98: loss = 0.2345946580171585, acc = 0.919921875\n",
            "Batch 99: loss = 0.2562926709651947, acc = 0.916015625\n",
            "Batch 100: loss = 0.22533336281776428, acc = 0.9248046875\n",
            "Batch 101: loss = 0.24705259501934052, acc = 0.9169921875\n",
            "Batch 102: loss = 0.25400522351264954, acc = 0.9169921875\n",
            "Batch 103: loss = 0.22745281457901, acc = 0.9296875\n",
            "Batch 104: loss = 0.24321162700653076, acc = 0.9150390625\n",
            "Batch 105: loss = 0.2057785987854004, acc = 0.9296875\n",
            "Batch 106: loss = 0.24282997846603394, acc = 0.921875\n",
            "Batch 107: loss = 0.24494881927967072, acc = 0.908203125\n",
            "Batch 108: loss = 0.22267195582389832, acc = 0.9228515625\n",
            "Batch 109: loss = 0.26325446367263794, acc = 0.912109375\n",
            "Batch 110: loss = 0.22670727968215942, acc = 0.927734375\n",
            "Batch 111: loss = 0.25638675689697266, acc = 0.9169921875\n",
            "Batch 112: loss = 0.2331857979297638, acc = 0.923828125\n",
            "Batch 113: loss = 0.24644295871257782, acc = 0.9208984375\n",
            "Batch 114: loss = 0.2441439926624298, acc = 0.912109375\n",
            "Batch 115: loss = 0.24044790863990784, acc = 0.9130859375\n",
            "Batch 116: loss = 0.2695237100124359, acc = 0.9189453125\n",
            "Batch 117: loss = 0.25168323516845703, acc = 0.9169921875\n",
            "Batch 118: loss = 0.23624758422374725, acc = 0.9150390625\n",
            "Batch 119: loss = 0.20264215767383575, acc = 0.9287109375\n",
            "Batch 120: loss = 0.2084038108587265, acc = 0.9228515625\n",
            "Batch 121: loss = 0.25586289167404175, acc = 0.919921875\n",
            "Batch 122: loss = 0.2250460386276245, acc = 0.91796875\n",
            "Batch 123: loss = 0.23460672795772552, acc = 0.921875\n",
            "Batch 124: loss = 0.25620344281196594, acc = 0.900390625\n",
            "Batch 125: loss = 0.2580285370349884, acc = 0.90625\n",
            "Batch 126: loss = 0.27501043677330017, acc = 0.9072265625\n",
            "\n",
            "Epoch 90/100\n",
            "Batch 1: loss = 0.29238471388816833, acc = 0.904296875\n",
            "Batch 2: loss = 0.2790892422199249, acc = 0.91015625\n",
            "Batch 3: loss = 0.2188294529914856, acc = 0.93359375\n",
            "Batch 4: loss = 0.23377178609371185, acc = 0.92578125\n",
            "Batch 5: loss = 0.2082688957452774, acc = 0.94140625\n",
            "Batch 6: loss = 0.25661107897758484, acc = 0.916015625\n",
            "Batch 7: loss = 0.24023963510990143, acc = 0.919921875\n",
            "Batch 8: loss = 0.23585452139377594, acc = 0.9248046875\n",
            "Batch 9: loss = 0.24635916948318481, acc = 0.91796875\n",
            "Batch 10: loss = 0.20888292789459229, acc = 0.92578125\n",
            "Batch 11: loss = 0.24359843134880066, acc = 0.916015625\n",
            "Batch 12: loss = 0.24383224546909332, acc = 0.9150390625\n",
            "Batch 13: loss = 0.23312786221504211, acc = 0.9228515625\n",
            "Batch 14: loss = 0.23829814791679382, acc = 0.9208984375\n",
            "Batch 15: loss = 0.21252888441085815, acc = 0.927734375\n",
            "Batch 16: loss = 0.2528533637523651, acc = 0.9150390625\n",
            "Batch 17: loss = 0.2170662134885788, acc = 0.923828125\n",
            "Batch 18: loss = 0.2649289071559906, acc = 0.91796875\n",
            "Batch 19: loss = 0.26007145643234253, acc = 0.91015625\n",
            "Batch 20: loss = 0.2471739649772644, acc = 0.91796875\n",
            "Batch 21: loss = 0.2704995572566986, acc = 0.908203125\n",
            "Batch 22: loss = 0.2622162103652954, acc = 0.90625\n",
            "Batch 23: loss = 0.2560712695121765, acc = 0.9208984375\n",
            "Batch 24: loss = 0.2696824371814728, acc = 0.9052734375\n",
            "Batch 25: loss = 0.23509618639945984, acc = 0.91796875\n",
            "Batch 26: loss = 0.22708633542060852, acc = 0.91796875\n",
            "Batch 27: loss = 0.2791309356689453, acc = 0.90234375\n",
            "Batch 28: loss = 0.2456899732351303, acc = 0.9189453125\n",
            "Batch 29: loss = 0.22738245129585266, acc = 0.923828125\n",
            "Batch 30: loss = 0.26716548204421997, acc = 0.9130859375\n",
            "Batch 31: loss = 0.23858657479286194, acc = 0.9228515625\n",
            "Batch 32: loss = 0.2629193365573883, acc = 0.91015625\n",
            "Batch 33: loss = 0.23670682311058044, acc = 0.91796875\n",
            "Batch 34: loss = 0.23928838968276978, acc = 0.9228515625\n",
            "Batch 35: loss = 0.24378448724746704, acc = 0.91015625\n",
            "Batch 36: loss = 0.18927548825740814, acc = 0.9326171875\n",
            "Batch 37: loss = 0.20602408051490784, acc = 0.9365234375\n",
            "Batch 38: loss = 0.23201045393943787, acc = 0.9130859375\n",
            "Batch 39: loss = 0.21277597546577454, acc = 0.939453125\n",
            "Batch 40: loss = 0.21310321986675262, acc = 0.9228515625\n",
            "Batch 41: loss = 0.2401127815246582, acc = 0.91796875\n",
            "Batch 42: loss = 0.2835351228713989, acc = 0.8984375\n",
            "Batch 43: loss = 0.26111990213394165, acc = 0.9052734375\n",
            "Batch 44: loss = 0.2342851161956787, acc = 0.9169921875\n",
            "Batch 45: loss = 0.21966475248336792, acc = 0.9287109375\n",
            "Batch 46: loss = 0.2253141701221466, acc = 0.92578125\n",
            "Batch 47: loss = 0.2397245466709137, acc = 0.919921875\n",
            "Batch 48: loss = 0.2172461450099945, acc = 0.931640625\n",
            "Batch 49: loss = 0.24455145001411438, acc = 0.9228515625\n",
            "Batch 50: loss = 0.21526938676834106, acc = 0.9384765625\n",
            "Batch 51: loss = 0.2351754754781723, acc = 0.927734375\n",
            "Batch 52: loss = 0.2719663381576538, acc = 0.9111328125\n",
            "Batch 53: loss = 0.2230423539876938, acc = 0.9248046875\n",
            "Batch 54: loss = 0.1890679895877838, acc = 0.939453125\n",
            "Batch 55: loss = 0.224561408162117, acc = 0.9287109375\n",
            "Batch 56: loss = 0.24715690314769745, acc = 0.9140625\n",
            "Batch 57: loss = 0.2628694772720337, acc = 0.90625\n",
            "Batch 58: loss = 0.27090758085250854, acc = 0.9091796875\n",
            "Batch 59: loss = 0.21232444047927856, acc = 0.931640625\n",
            "Batch 60: loss = 0.2617681920528412, acc = 0.91015625\n",
            "Batch 61: loss = 0.2400006502866745, acc = 0.92578125\n",
            "Batch 62: loss = 0.268833190202713, acc = 0.91796875\n",
            "Batch 63: loss = 0.2239103764295578, acc = 0.9287109375\n",
            "Batch 64: loss = 0.23929665982723236, acc = 0.9228515625\n",
            "Batch 65: loss = 0.22699418663978577, acc = 0.9130859375\n",
            "Batch 66: loss = 0.2394581139087677, acc = 0.9150390625\n",
            "Batch 67: loss = 0.25366246700286865, acc = 0.908203125\n",
            "Batch 68: loss = 0.24332591891288757, acc = 0.91015625\n",
            "Batch 69: loss = 0.23253706097602844, acc = 0.9208984375\n",
            "Batch 70: loss = 0.23135757446289062, acc = 0.9248046875\n",
            "Batch 71: loss = 0.22560277581214905, acc = 0.91796875\n",
            "Batch 72: loss = 0.23945459723472595, acc = 0.9287109375\n",
            "Batch 73: loss = 0.2558067739009857, acc = 0.916015625\n",
            "Batch 74: loss = 0.25298017263412476, acc = 0.9169921875\n",
            "Batch 75: loss = 0.28048476576805115, acc = 0.900390625\n",
            "Batch 76: loss = 0.23796392977237701, acc = 0.923828125\n",
            "Batch 77: loss = 0.2307514101266861, acc = 0.9150390625\n",
            "Batch 78: loss = 0.23395639657974243, acc = 0.927734375\n",
            "Batch 79: loss = 0.2320898026227951, acc = 0.9267578125\n",
            "Batch 80: loss = 0.22029279172420502, acc = 0.919921875\n",
            "Batch 81: loss = 0.27031153440475464, acc = 0.9111328125\n",
            "Batch 82: loss = 0.2503954768180847, acc = 0.8994140625\n",
            "Batch 83: loss = 0.2863551378250122, acc = 0.8955078125\n",
            "Batch 84: loss = 0.23394034802913666, acc = 0.9208984375\n",
            "Batch 85: loss = 0.24615812301635742, acc = 0.9130859375\n",
            "Batch 86: loss = 0.2549578547477722, acc = 0.9189453125\n",
            "Batch 87: loss = 0.2436380684375763, acc = 0.9140625\n",
            "Batch 88: loss = 0.26337432861328125, acc = 0.91015625\n",
            "Batch 89: loss = 0.2332073450088501, acc = 0.92578125\n",
            "Batch 90: loss = 0.2585349380970001, acc = 0.8974609375\n",
            "Batch 91: loss = 0.24719133973121643, acc = 0.91796875\n",
            "Batch 92: loss = 0.22940656542778015, acc = 0.916015625\n",
            "Batch 93: loss = 0.2390209138393402, acc = 0.923828125\n",
            "Batch 94: loss = 0.22143734991550446, acc = 0.9189453125\n",
            "Batch 95: loss = 0.24976573884487152, acc = 0.9140625\n",
            "Batch 96: loss = 0.2595495283603668, acc = 0.904296875\n",
            "Batch 97: loss = 0.23866142332553864, acc = 0.9208984375\n",
            "Batch 98: loss = 0.25161778926849365, acc = 0.9189453125\n",
            "Batch 99: loss = 0.2939850091934204, acc = 0.8994140625\n",
            "Batch 100: loss = 0.25285619497299194, acc = 0.9111328125\n",
            "Batch 101: loss = 0.24264508485794067, acc = 0.9169921875\n",
            "Batch 102: loss = 0.25055453181266785, acc = 0.916015625\n",
            "Batch 103: loss = 0.23760609328746796, acc = 0.91796875\n",
            "Batch 104: loss = 0.23968595266342163, acc = 0.91796875\n",
            "Batch 105: loss = 0.2136164903640747, acc = 0.9248046875\n",
            "Batch 106: loss = 0.2485116720199585, acc = 0.9140625\n",
            "Batch 107: loss = 0.22946015000343323, acc = 0.912109375\n",
            "Batch 108: loss = 0.22754976153373718, acc = 0.931640625\n",
            "Batch 109: loss = 0.24336113035678864, acc = 0.9189453125\n",
            "Batch 110: loss = 0.23771503567695618, acc = 0.9169921875\n",
            "Batch 111: loss = 0.23385648429393768, acc = 0.9267578125\n",
            "Batch 112: loss = 0.24617137014865875, acc = 0.916015625\n",
            "Batch 113: loss = 0.25946229696273804, acc = 0.9091796875\n",
            "Batch 114: loss = 0.22908931970596313, acc = 0.927734375\n",
            "Batch 115: loss = 0.2410477250814438, acc = 0.921875\n",
            "Batch 116: loss = 0.28257301449775696, acc = 0.9072265625\n",
            "Batch 117: loss = 0.25851985812187195, acc = 0.90625\n",
            "Batch 118: loss = 0.27600082755088806, acc = 0.9189453125\n",
            "Batch 119: loss = 0.22236214578151703, acc = 0.921875\n",
            "Batch 120: loss = 0.2097993791103363, acc = 0.9287109375\n",
            "Batch 121: loss = 0.24100717902183533, acc = 0.921875\n",
            "Batch 122: loss = 0.2478594183921814, acc = 0.9091796875\n",
            "Batch 123: loss = 0.2509496212005615, acc = 0.9111328125\n",
            "Batch 124: loss = 0.2698499858379364, acc = 0.9130859375\n",
            "Batch 125: loss = 0.2774042785167694, acc = 0.904296875\n",
            "Batch 126: loss = 0.2512768805027008, acc = 0.9189453125\n",
            "Saved checkpoint to weights.90.h5\n",
            "\n",
            "Epoch 91/100\n",
            "Batch 1: loss = 0.3134821653366089, acc = 0.8994140625\n",
            "Batch 2: loss = 0.2791760563850403, acc = 0.9130859375\n",
            "Batch 3: loss = 0.29389694333076477, acc = 0.900390625\n",
            "Batch 4: loss = 0.22828152775764465, acc = 0.9287109375\n",
            "Batch 5: loss = 0.2394411265850067, acc = 0.91796875\n",
            "Batch 6: loss = 0.25024253129959106, acc = 0.91015625\n",
            "Batch 7: loss = 0.20290198922157288, acc = 0.9375\n",
            "Batch 8: loss = 0.2536843419075012, acc = 0.9248046875\n",
            "Batch 9: loss = 0.25082889199256897, acc = 0.912109375\n",
            "Batch 10: loss = 0.21020156145095825, acc = 0.9296875\n",
            "Batch 11: loss = 0.2386443316936493, acc = 0.916015625\n",
            "Batch 12: loss = 0.24523311853408813, acc = 0.912109375\n",
            "Batch 13: loss = 0.2247731238603592, acc = 0.91796875\n",
            "Batch 14: loss = 0.21588397026062012, acc = 0.9228515625\n",
            "Batch 15: loss = 0.20686151087284088, acc = 0.9296875\n",
            "Batch 16: loss = 0.2177295684814453, acc = 0.921875\n",
            "Batch 17: loss = 0.23268678784370422, acc = 0.919921875\n",
            "Batch 18: loss = 0.22894862294197083, acc = 0.921875\n",
            "Batch 19: loss = 0.24227464199066162, acc = 0.91796875\n",
            "Batch 20: loss = 0.24467898905277252, acc = 0.9111328125\n",
            "Batch 21: loss = 0.26375606656074524, acc = 0.91796875\n",
            "Batch 22: loss = 0.2394617646932602, acc = 0.9140625\n",
            "Batch 23: loss = 0.24546964466571808, acc = 0.9140625\n",
            "Batch 24: loss = 0.24403443932533264, acc = 0.9169921875\n",
            "Batch 25: loss = 0.25179338455200195, acc = 0.91796875\n",
            "Batch 26: loss = 0.22877472639083862, acc = 0.9189453125\n",
            "Batch 27: loss = 0.26592719554901123, acc = 0.9052734375\n",
            "Batch 28: loss = 0.2169281244277954, acc = 0.9228515625\n",
            "Batch 29: loss = 0.24095816910266876, acc = 0.9130859375\n",
            "Batch 30: loss = 0.23053652048110962, acc = 0.9208984375\n",
            "Batch 31: loss = 0.2616288661956787, acc = 0.9248046875\n",
            "Batch 32: loss = 0.23869019746780396, acc = 0.9150390625\n",
            "Batch 33: loss = 0.20753630995750427, acc = 0.9248046875\n",
            "Batch 34: loss = 0.2756797969341278, acc = 0.908203125\n",
            "Batch 35: loss = 0.23504288494586945, acc = 0.9208984375\n",
            "Batch 36: loss = 0.21519355475902557, acc = 0.9248046875\n",
            "Batch 37: loss = 0.22873789072036743, acc = 0.927734375\n",
            "Batch 38: loss = 0.21397745609283447, acc = 0.935546875\n",
            "Batch 39: loss = 0.24312327802181244, acc = 0.923828125\n",
            "Batch 40: loss = 0.23738867044448853, acc = 0.9189453125\n",
            "Batch 41: loss = 0.20226678252220154, acc = 0.9326171875\n",
            "Batch 42: loss = 0.2037488967180252, acc = 0.9326171875\n",
            "Batch 43: loss = 0.24197319149971008, acc = 0.9150390625\n",
            "Batch 44: loss = 0.2231660783290863, acc = 0.9375\n",
            "Batch 45: loss = 0.21139517426490784, acc = 0.9326171875\n",
            "Batch 46: loss = 0.1964961141347885, acc = 0.92578125\n",
            "Batch 47: loss = 0.20367759466171265, acc = 0.93359375\n",
            "Batch 48: loss = 0.21268264949321747, acc = 0.9296875\n",
            "Batch 49: loss = 0.23982612788677216, acc = 0.9150390625\n",
            "Batch 50: loss = 0.21988549828529358, acc = 0.923828125\n",
            "Batch 51: loss = 0.23327915370464325, acc = 0.9169921875\n",
            "Batch 52: loss = 0.2502414584159851, acc = 0.9140625\n",
            "Batch 53: loss = 0.23479019105434418, acc = 0.923828125\n",
            "Batch 54: loss = 0.2074221670627594, acc = 0.9208984375\n",
            "Batch 55: loss = 0.2149057537317276, acc = 0.939453125\n",
            "Batch 56: loss = 0.2436532974243164, acc = 0.919921875\n",
            "Batch 57: loss = 0.28502601385116577, acc = 0.90625\n",
            "Batch 58: loss = 0.2575450539588928, acc = 0.9248046875\n",
            "Batch 59: loss = 0.21274961531162262, acc = 0.9287109375\n",
            "Batch 60: loss = 0.22556976974010468, acc = 0.9228515625\n",
            "Batch 61: loss = 0.23230385780334473, acc = 0.919921875\n",
            "Batch 62: loss = 0.24771639704704285, acc = 0.9111328125\n",
            "Batch 63: loss = 0.2102578580379486, acc = 0.9345703125\n",
            "Batch 64: loss = 0.2016204595565796, acc = 0.9326171875\n",
            "Batch 65: loss = 0.24637648463249207, acc = 0.927734375\n",
            "Batch 66: loss = 0.24502244591712952, acc = 0.9150390625\n",
            "Batch 67: loss = 0.2565346360206604, acc = 0.9130859375\n",
            "Batch 68: loss = 0.28603535890579224, acc = 0.8896484375\n",
            "Batch 69: loss = 0.22085148096084595, acc = 0.921875\n",
            "Batch 70: loss = 0.23609843850135803, acc = 0.923828125\n",
            "Batch 71: loss = 0.2559761106967926, acc = 0.91015625\n",
            "Batch 72: loss = 0.2540780007839203, acc = 0.9091796875\n",
            "Batch 73: loss = 0.27603280544281006, acc = 0.8994140625\n",
            "Batch 74: loss = 0.28093793988227844, acc = 0.900390625\n",
            "Batch 75: loss = 0.2748796045780182, acc = 0.9052734375\n",
            "Batch 76: loss = 0.25369206070899963, acc = 0.9111328125\n",
            "Batch 77: loss = 0.20132401585578918, acc = 0.9345703125\n",
            "Batch 78: loss = 0.24985893070697784, acc = 0.9189453125\n",
            "Batch 79: loss = 0.2286646068096161, acc = 0.9189453125\n",
            "Batch 80: loss = 0.21032410860061646, acc = 0.92578125\n",
            "Batch 81: loss = 0.25432443618774414, acc = 0.9111328125\n",
            "Batch 82: loss = 0.26342886686325073, acc = 0.9033203125\n",
            "Batch 83: loss = 0.2343052476644516, acc = 0.9130859375\n",
            "Batch 84: loss = 0.21862009167671204, acc = 0.9296875\n",
            "Batch 85: loss = 0.24854928255081177, acc = 0.9228515625\n",
            "Batch 86: loss = 0.24270765483379364, acc = 0.9208984375\n",
            "Batch 87: loss = 0.22919070720672607, acc = 0.9169921875\n",
            "Batch 88: loss = 0.28280138969421387, acc = 0.904296875\n",
            "Batch 89: loss = 0.24411195516586304, acc = 0.9169921875\n",
            "Batch 90: loss = 0.28441694378852844, acc = 0.90234375\n",
            "Batch 91: loss = 0.27363574504852295, acc = 0.904296875\n",
            "Batch 92: loss = 0.27645406126976013, acc = 0.9013671875\n",
            "Batch 93: loss = 0.2273375689983368, acc = 0.927734375\n",
            "Batch 94: loss = 0.2510739862918854, acc = 0.912109375\n",
            "Batch 95: loss = 0.23355266451835632, acc = 0.9267578125\n",
            "Batch 96: loss = 0.26339077949523926, acc = 0.900390625\n",
            "Batch 97: loss = 0.2638557553291321, acc = 0.90234375\n",
            "Batch 98: loss = 0.2929357588291168, acc = 0.8974609375\n",
            "Batch 99: loss = 0.28104227781295776, acc = 0.9091796875\n",
            "Batch 100: loss = 0.28421419858932495, acc = 0.8935546875\n",
            "Batch 101: loss = 0.24178370833396912, acc = 0.9091796875\n",
            "Batch 102: loss = 0.24543437361717224, acc = 0.916015625\n",
            "Batch 103: loss = 0.2531241774559021, acc = 0.919921875\n",
            "Batch 104: loss = 0.2638953924179077, acc = 0.908203125\n",
            "Batch 105: loss = 0.22302602231502533, acc = 0.9208984375\n",
            "Batch 106: loss = 0.2150874137878418, acc = 0.93359375\n",
            "Batch 107: loss = 0.26110392808914185, acc = 0.90234375\n",
            "Batch 108: loss = 0.23233814537525177, acc = 0.923828125\n",
            "Batch 109: loss = 0.2672559916973114, acc = 0.9033203125\n",
            "Batch 110: loss = 0.25146231055259705, acc = 0.923828125\n",
            "Batch 111: loss = 0.2429434061050415, acc = 0.9150390625\n",
            "Batch 112: loss = 0.25204288959503174, acc = 0.912109375\n",
            "Batch 113: loss = 0.24246536195278168, acc = 0.9150390625\n",
            "Batch 114: loss = 0.24690450727939606, acc = 0.9228515625\n",
            "Batch 115: loss = 0.24344253540039062, acc = 0.9267578125\n",
            "Batch 116: loss = 0.22647787630558014, acc = 0.9208984375\n",
            "Batch 117: loss = 0.24726903438568115, acc = 0.9228515625\n",
            "Batch 118: loss = 0.2200443148612976, acc = 0.916015625\n",
            "Batch 119: loss = 0.22865137457847595, acc = 0.9296875\n",
            "Batch 120: loss = 0.2039487361907959, acc = 0.9326171875\n",
            "Batch 121: loss = 0.22398272156715393, acc = 0.91796875\n",
            "Batch 122: loss = 0.20438334345817566, acc = 0.9345703125\n",
            "Batch 123: loss = 0.21274101734161377, acc = 0.93359375\n",
            "Batch 124: loss = 0.2608778178691864, acc = 0.904296875\n",
            "Batch 125: loss = 0.2794956862926483, acc = 0.90234375\n",
            "Batch 126: loss = 0.2796551287174225, acc = 0.90625\n",
            "\n",
            "Epoch 92/100\n",
            "Batch 1: loss = 0.2795782685279846, acc = 0.9072265625\n",
            "Batch 2: loss = 0.22959564626216888, acc = 0.921875\n",
            "Batch 3: loss = 0.25857385993003845, acc = 0.9111328125\n",
            "Batch 4: loss = 0.23875844478607178, acc = 0.9228515625\n",
            "Batch 5: loss = 0.250027060508728, acc = 0.9169921875\n",
            "Batch 6: loss = 0.27050551772117615, acc = 0.91015625\n",
            "Batch 7: loss = 0.23510484397411346, acc = 0.92578125\n",
            "Batch 8: loss = 0.2520030736923218, acc = 0.9228515625\n",
            "Batch 9: loss = 0.2210157960653305, acc = 0.9248046875\n",
            "Batch 10: loss = 0.25678902864456177, acc = 0.927734375\n",
            "Batch 11: loss = 0.23456847667694092, acc = 0.9150390625\n",
            "Batch 12: loss = 0.2010510265827179, acc = 0.9345703125\n",
            "Batch 13: loss = 0.19412203133106232, acc = 0.93359375\n",
            "Batch 14: loss = 0.23501864075660706, acc = 0.9267578125\n",
            "Batch 15: loss = 0.22255165874958038, acc = 0.9228515625\n",
            "Batch 16: loss = 0.25969502329826355, acc = 0.916015625\n",
            "Batch 17: loss = 0.2229328751564026, acc = 0.91796875\n",
            "Batch 18: loss = 0.2397792488336563, acc = 0.916015625\n",
            "Batch 19: loss = 0.25206446647644043, acc = 0.919921875\n",
            "Batch 20: loss = 0.2555864453315735, acc = 0.91015625\n",
            "Batch 21: loss = 0.25879785418510437, acc = 0.9111328125\n",
            "Batch 22: loss = 0.22708933055400848, acc = 0.9208984375\n",
            "Batch 23: loss = 0.27759799361228943, acc = 0.90625\n",
            "Batch 24: loss = 0.23800261318683624, acc = 0.92578125\n",
            "Batch 25: loss = 0.2431672215461731, acc = 0.93359375\n",
            "Batch 26: loss = 0.21608272194862366, acc = 0.9296875\n",
            "Batch 27: loss = 0.2677728235721588, acc = 0.9091796875\n",
            "Batch 28: loss = 0.22053755819797516, acc = 0.919921875\n",
            "Batch 29: loss = 0.24489980936050415, acc = 0.92578125\n",
            "Batch 30: loss = 0.23333615064620972, acc = 0.9189453125\n",
            "Batch 31: loss = 0.253560334444046, acc = 0.9140625\n",
            "Batch 32: loss = 0.2847580909729004, acc = 0.912109375\n",
            "Batch 33: loss = 0.23326687514781952, acc = 0.91796875\n",
            "Batch 34: loss = 0.2555508613586426, acc = 0.9091796875\n",
            "Batch 35: loss = 0.2056121826171875, acc = 0.927734375\n",
            "Batch 36: loss = 0.2159218043088913, acc = 0.9267578125\n",
            "Batch 37: loss = 0.22787879407405853, acc = 0.9296875\n",
            "Batch 38: loss = 0.23619374632835388, acc = 0.9169921875\n",
            "Batch 39: loss = 0.22729618847370148, acc = 0.9169921875\n",
            "Batch 40: loss = 0.2253614068031311, acc = 0.9345703125\n",
            "Batch 41: loss = 0.22367340326309204, acc = 0.9140625\n",
            "Batch 42: loss = 0.24276326596736908, acc = 0.916015625\n",
            "Batch 43: loss = 0.22110497951507568, acc = 0.9228515625\n",
            "Batch 44: loss = 0.2346721738576889, acc = 0.9248046875\n",
            "Batch 45: loss = 0.20692941546440125, acc = 0.931640625\n",
            "Batch 46: loss = 0.2143041342496872, acc = 0.935546875\n",
            "Batch 47: loss = 0.2355581820011139, acc = 0.91015625\n",
            "Batch 48: loss = 0.20739057660102844, acc = 0.9287109375\n",
            "Batch 49: loss = 0.22557541728019714, acc = 0.923828125\n",
            "Batch 50: loss = 0.2192385047674179, acc = 0.9248046875\n",
            "Batch 51: loss = 0.2406105101108551, acc = 0.90625\n",
            "Batch 52: loss = 0.2538391649723053, acc = 0.9140625\n",
            "Batch 53: loss = 0.2410125434398651, acc = 0.9150390625\n",
            "Batch 54: loss = 0.17715242505073547, acc = 0.9462890625\n",
            "Batch 55: loss = 0.2219989001750946, acc = 0.9287109375\n",
            "Batch 56: loss = 0.24862733483314514, acc = 0.9169921875\n",
            "Batch 57: loss = 0.22877910733222961, acc = 0.921875\n",
            "Batch 58: loss = 0.24443654716014862, acc = 0.919921875\n",
            "Batch 59: loss = 0.20634739100933075, acc = 0.92578125\n",
            "Batch 60: loss = 0.25242915749549866, acc = 0.9130859375\n",
            "Batch 61: loss = 0.22154636681079865, acc = 0.9296875\n",
            "Batch 62: loss = 0.2676776647567749, acc = 0.91015625\n",
            "Batch 63: loss = 0.23279090225696564, acc = 0.927734375\n",
            "Batch 64: loss = 0.22346624732017517, acc = 0.927734375\n",
            "Batch 65: loss = 0.23627665638923645, acc = 0.9189453125\n",
            "Batch 66: loss = 0.23647832870483398, acc = 0.9208984375\n",
            "Batch 67: loss = 0.2629165053367615, acc = 0.904296875\n",
            "Batch 68: loss = 0.25272053480148315, acc = 0.9150390625\n",
            "Batch 69: loss = 0.23945024609565735, acc = 0.9248046875\n",
            "Batch 70: loss = 0.2430693656206131, acc = 0.9208984375\n",
            "Batch 71: loss = 0.24107030034065247, acc = 0.9091796875\n",
            "Batch 72: loss = 0.2510540783405304, acc = 0.9140625\n",
            "Batch 73: loss = 0.2582022249698639, acc = 0.9130859375\n",
            "Batch 74: loss = 0.26462522149086, acc = 0.9091796875\n",
            "Batch 75: loss = 0.30495160818099976, acc = 0.8916015625\n",
            "Batch 76: loss = 0.268298476934433, acc = 0.9140625\n",
            "Batch 77: loss = 0.24613367021083832, acc = 0.9208984375\n",
            "Batch 78: loss = 0.2268647849559784, acc = 0.9306640625\n",
            "Batch 79: loss = 0.2309737652540207, acc = 0.9287109375\n",
            "Batch 80: loss = 0.20249465107917786, acc = 0.9296875\n",
            "Batch 81: loss = 0.24752549827098846, acc = 0.916015625\n",
            "Batch 82: loss = 0.2598690688610077, acc = 0.9140625\n",
            "Batch 83: loss = 0.2547133266925812, acc = 0.9189453125\n",
            "Batch 84: loss = 0.25039181113243103, acc = 0.912109375\n",
            "Batch 85: loss = 0.25896865129470825, acc = 0.9111328125\n",
            "Batch 86: loss = 0.24999678134918213, acc = 0.91796875\n",
            "Batch 87: loss = 0.23284263908863068, acc = 0.912109375\n",
            "Batch 88: loss = 0.2418859899044037, acc = 0.919921875\n",
            "Batch 89: loss = 0.26218611001968384, acc = 0.91015625\n",
            "Batch 90: loss = 0.2659755051136017, acc = 0.904296875\n",
            "Batch 91: loss = 0.24775570631027222, acc = 0.9091796875\n",
            "Batch 92: loss = 0.2358601689338684, acc = 0.921875\n",
            "Batch 93: loss = 0.2443471997976303, acc = 0.9150390625\n",
            "Batch 94: loss = 0.24063889682292938, acc = 0.923828125\n",
            "Batch 95: loss = 0.22281351685523987, acc = 0.9384765625\n",
            "Batch 96: loss = 0.28059643507003784, acc = 0.900390625\n",
            "Batch 97: loss = 0.22721943259239197, acc = 0.9287109375\n",
            "Batch 98: loss = 0.23964883387088776, acc = 0.916015625\n",
            "Batch 99: loss = 0.24332010746002197, acc = 0.91796875\n",
            "Batch 100: loss = 0.23830217123031616, acc = 0.927734375\n",
            "Batch 101: loss = 0.24254964292049408, acc = 0.9228515625\n",
            "Batch 102: loss = 0.255383163690567, acc = 0.912109375\n",
            "Batch 103: loss = 0.24594038724899292, acc = 0.921875\n",
            "Batch 104: loss = 0.21012374758720398, acc = 0.92578125\n",
            "Batch 105: loss = 0.210132896900177, acc = 0.9248046875\n",
            "Batch 106: loss = 0.2294531911611557, acc = 0.9111328125\n",
            "Batch 107: loss = 0.22867827117443085, acc = 0.9228515625\n",
            "Batch 108: loss = 0.23717153072357178, acc = 0.9169921875\n",
            "Batch 109: loss = 0.2510126829147339, acc = 0.912109375\n",
            "Batch 110: loss = 0.23707348108291626, acc = 0.91796875\n",
            "Batch 111: loss = 0.24739500880241394, acc = 0.908203125\n",
            "Batch 112: loss = 0.2329893708229065, acc = 0.9130859375\n",
            "Batch 113: loss = 0.23952248692512512, acc = 0.9267578125\n",
            "Batch 114: loss = 0.2556003928184509, acc = 0.923828125\n",
            "Batch 115: loss = 0.2283337414264679, acc = 0.92578125\n",
            "Batch 116: loss = 0.2122839391231537, acc = 0.9375\n",
            "Batch 117: loss = 0.26036638021469116, acc = 0.9140625\n",
            "Batch 118: loss = 0.22775006294250488, acc = 0.923828125\n",
            "Batch 119: loss = 0.21257641911506653, acc = 0.9287109375\n",
            "Batch 120: loss = 0.23530595004558563, acc = 0.9169921875\n",
            "Batch 121: loss = 0.2215588092803955, acc = 0.927734375\n",
            "Batch 122: loss = 0.22544629871845245, acc = 0.912109375\n",
            "Batch 123: loss = 0.21964874863624573, acc = 0.923828125\n",
            "Batch 124: loss = 0.24273642897605896, acc = 0.9208984375\n",
            "Batch 125: loss = 0.25058382749557495, acc = 0.91796875\n",
            "Batch 126: loss = 0.29109230637550354, acc = 0.890625\n",
            "\n",
            "Epoch 93/100\n",
            "Batch 1: loss = 0.2974824011325836, acc = 0.9052734375\n",
            "Batch 2: loss = 0.2720651626586914, acc = 0.9033203125\n",
            "Batch 3: loss = 0.24547088146209717, acc = 0.931640625\n",
            "Batch 4: loss = 0.2571609318256378, acc = 0.908203125\n",
            "Batch 5: loss = 0.22868818044662476, acc = 0.9267578125\n",
            "Batch 6: loss = 0.25463300943374634, acc = 0.908203125\n",
            "Batch 7: loss = 0.20830732583999634, acc = 0.9228515625\n",
            "Batch 8: loss = 0.25164687633514404, acc = 0.916015625\n",
            "Batch 9: loss = 0.2554144263267517, acc = 0.9208984375\n",
            "Batch 10: loss = 0.22680401802062988, acc = 0.9189453125\n",
            "Batch 11: loss = 0.23163923621177673, acc = 0.9189453125\n",
            "Batch 12: loss = 0.222332164645195, acc = 0.9267578125\n",
            "Batch 13: loss = 0.23164856433868408, acc = 0.9228515625\n",
            "Batch 14: loss = 0.20644180476665497, acc = 0.9306640625\n",
            "Batch 15: loss = 0.24328619241714478, acc = 0.9228515625\n",
            "Batch 16: loss = 0.2490599900484085, acc = 0.9248046875\n",
            "Batch 17: loss = 0.2332598865032196, acc = 0.9208984375\n",
            "Batch 18: loss = 0.23335887491703033, acc = 0.92578125\n",
            "Batch 19: loss = 0.24970488250255585, acc = 0.9091796875\n",
            "Batch 20: loss = 0.27120330929756165, acc = 0.9150390625\n",
            "Batch 21: loss = 0.23850032687187195, acc = 0.9150390625\n",
            "Batch 22: loss = 0.22950860857963562, acc = 0.919921875\n",
            "Batch 23: loss = 0.2577274441719055, acc = 0.9169921875\n",
            "Batch 24: loss = 0.24873086810112, acc = 0.921875\n",
            "Batch 25: loss = 0.2525789439678192, acc = 0.9150390625\n",
            "Batch 26: loss = 0.22731107473373413, acc = 0.919921875\n",
            "Batch 27: loss = 0.25993624329566956, acc = 0.9130859375\n",
            "Batch 28: loss = 0.22757083177566528, acc = 0.921875\n",
            "Batch 29: loss = 0.24888864159584045, acc = 0.9072265625\n",
            "Batch 30: loss = 0.25894805788993835, acc = 0.9228515625\n",
            "Batch 31: loss = 0.24732811748981476, acc = 0.9130859375\n",
            "Batch 32: loss = 0.2731267809867859, acc = 0.9150390625\n",
            "Batch 33: loss = 0.22475385665893555, acc = 0.923828125\n",
            "Batch 34: loss = 0.27848130464553833, acc = 0.8994140625\n",
            "Batch 35: loss = 0.2164740115404129, acc = 0.9287109375\n",
            "Batch 36: loss = 0.24783837795257568, acc = 0.923828125\n",
            "Batch 37: loss = 0.21941368281841278, acc = 0.9189453125\n",
            "Batch 38: loss = 0.24383917450904846, acc = 0.9130859375\n",
            "Batch 39: loss = 0.19936923682689667, acc = 0.9345703125\n",
            "Batch 40: loss = 0.2327633947134018, acc = 0.9111328125\n",
            "Batch 41: loss = 0.21574148535728455, acc = 0.9228515625\n",
            "Batch 42: loss = 0.23944413661956787, acc = 0.921875\n",
            "Batch 43: loss = 0.23186427354812622, acc = 0.9150390625\n",
            "Batch 44: loss = 0.22525542974472046, acc = 0.9228515625\n",
            "Batch 45: loss = 0.23047393560409546, acc = 0.9189453125\n",
            "Batch 46: loss = 0.18777607381343842, acc = 0.9423828125\n",
            "Batch 47: loss = 0.23694895207881927, acc = 0.9111328125\n",
            "Batch 48: loss = 0.20046541094779968, acc = 0.9228515625\n",
            "Batch 49: loss = 0.21178549528121948, acc = 0.9306640625\n",
            "Batch 50: loss = 0.2206854522228241, acc = 0.93359375\n",
            "Batch 51: loss = 0.2439403533935547, acc = 0.9228515625\n",
            "Batch 52: loss = 0.22840972244739532, acc = 0.921875\n",
            "Batch 53: loss = 0.2494763731956482, acc = 0.91015625\n",
            "Batch 54: loss = 0.1840805858373642, acc = 0.935546875\n",
            "Batch 55: loss = 0.2016846090555191, acc = 0.9345703125\n",
            "Batch 56: loss = 0.25555041432380676, acc = 0.9150390625\n",
            "Batch 57: loss = 0.24694347381591797, acc = 0.9091796875\n",
            "Batch 58: loss = 0.22624164819717407, acc = 0.92578125\n",
            "Batch 59: loss = 0.17953285574913025, acc = 0.9345703125\n",
            "Batch 60: loss = 0.226087287068367, acc = 0.92578125\n",
            "Batch 61: loss = 0.2330116629600525, acc = 0.91015625\n",
            "Batch 62: loss = 0.2428368180990219, acc = 0.9287109375\n",
            "Batch 63: loss = 0.21665732562541962, acc = 0.9306640625\n",
            "Batch 64: loss = 0.23659244179725647, acc = 0.921875\n",
            "Batch 65: loss = 0.22608064115047455, acc = 0.9208984375\n",
            "Batch 66: loss = 0.2121582180261612, acc = 0.921875\n",
            "Batch 67: loss = 0.23255711793899536, acc = 0.9140625\n",
            "Batch 68: loss = 0.2464480996131897, acc = 0.9052734375\n",
            "Batch 69: loss = 0.24157966673374176, acc = 0.9150390625\n",
            "Batch 70: loss = 0.2635941505432129, acc = 0.91015625\n",
            "Batch 71: loss = 0.21638590097427368, acc = 0.9296875\n",
            "Batch 72: loss = 0.2413036823272705, acc = 0.91796875\n",
            "Batch 73: loss = 0.26554086804389954, acc = 0.9052734375\n",
            "Batch 74: loss = 0.29053017497062683, acc = 0.896484375\n",
            "Batch 75: loss = 0.2715742886066437, acc = 0.896484375\n",
            "Batch 76: loss = 0.22446760535240173, acc = 0.9208984375\n",
            "Batch 77: loss = 0.17607462406158447, acc = 0.9423828125\n",
            "Batch 78: loss = 0.23944979906082153, acc = 0.921875\n",
            "Batch 79: loss = 0.24721094965934753, acc = 0.91796875\n",
            "Batch 80: loss = 0.21735705435276031, acc = 0.923828125\n",
            "Batch 81: loss = 0.2291812300682068, acc = 0.923828125\n",
            "Batch 82: loss = 0.24136091768741608, acc = 0.9287109375\n",
            "Batch 83: loss = 0.23218974471092224, acc = 0.91796875\n",
            "Batch 84: loss = 0.21548399329185486, acc = 0.93359375\n",
            "Batch 85: loss = 0.2477271407842636, acc = 0.916015625\n",
            "Batch 86: loss = 0.2584403157234192, acc = 0.9130859375\n",
            "Batch 87: loss = 0.23220393061637878, acc = 0.9208984375\n",
            "Batch 88: loss = 0.3007604479789734, acc = 0.9033203125\n",
            "Batch 89: loss = 0.23046012222766876, acc = 0.9287109375\n",
            "Batch 90: loss = 0.27463629841804504, acc = 0.8994140625\n",
            "Batch 91: loss = 0.2542586922645569, acc = 0.91015625\n",
            "Batch 92: loss = 0.24866634607315063, acc = 0.9130859375\n",
            "Batch 93: loss = 0.20606331527233124, acc = 0.923828125\n",
            "Batch 94: loss = 0.2340748906135559, acc = 0.9228515625\n",
            "Batch 95: loss = 0.24386081099510193, acc = 0.9140625\n",
            "Batch 96: loss = 0.26255083084106445, acc = 0.9072265625\n",
            "Batch 97: loss = 0.25189408659935, acc = 0.919921875\n",
            "Batch 98: loss = 0.21745499968528748, acc = 0.921875\n",
            "Batch 99: loss = 0.2598934471607208, acc = 0.9208984375\n",
            "Batch 100: loss = 0.2533438205718994, acc = 0.9033203125\n",
            "Batch 101: loss = 0.241859570145607, acc = 0.9150390625\n",
            "Batch 102: loss = 0.26886317133903503, acc = 0.916015625\n",
            "Batch 103: loss = 0.23222704231739044, acc = 0.927734375\n",
            "Batch 104: loss = 0.2038075476884842, acc = 0.9287109375\n",
            "Batch 105: loss = 0.22702693939208984, acc = 0.9287109375\n",
            "Batch 106: loss = 0.24304133653640747, acc = 0.9150390625\n",
            "Batch 107: loss = 0.22384226322174072, acc = 0.921875\n",
            "Batch 108: loss = 0.20929020643234253, acc = 0.9208984375\n",
            "Batch 109: loss = 0.23960205912590027, acc = 0.9150390625\n",
            "Batch 110: loss = 0.2214111089706421, acc = 0.921875\n",
            "Batch 111: loss = 0.24091579020023346, acc = 0.919921875\n",
            "Batch 112: loss = 0.2068285346031189, acc = 0.9345703125\n",
            "Batch 113: loss = 0.23503746092319489, acc = 0.9150390625\n",
            "Batch 114: loss = 0.28613561391830444, acc = 0.91015625\n",
            "Batch 115: loss = 0.23830896615982056, acc = 0.916015625\n",
            "Batch 116: loss = 0.24949496984481812, acc = 0.9150390625\n",
            "Batch 117: loss = 0.2461230456829071, acc = 0.9169921875\n",
            "Batch 118: loss = 0.23303963243961334, acc = 0.9296875\n",
            "Batch 119: loss = 0.21649953722953796, acc = 0.9267578125\n",
            "Batch 120: loss = 0.22989098727703094, acc = 0.923828125\n",
            "Batch 121: loss = 0.2367030829191208, acc = 0.9248046875\n",
            "Batch 122: loss = 0.21809671819210052, acc = 0.9189453125\n",
            "Batch 123: loss = 0.2145266830921173, acc = 0.9326171875\n",
            "Batch 124: loss = 0.2508505582809448, acc = 0.9150390625\n",
            "Batch 125: loss = 0.27148449420928955, acc = 0.8974609375\n",
            "Batch 126: loss = 0.22357270121574402, acc = 0.9296875\n",
            "\n",
            "Epoch 94/100\n",
            "Batch 1: loss = 0.29226434230804443, acc = 0.91015625\n",
            "Batch 2: loss = 0.2544657588005066, acc = 0.9140625\n",
            "Batch 3: loss = 0.2589587867259979, acc = 0.9150390625\n",
            "Batch 4: loss = 0.22972404956817627, acc = 0.919921875\n",
            "Batch 5: loss = 0.19932448863983154, acc = 0.9345703125\n",
            "Batch 6: loss = 0.250887393951416, acc = 0.91796875\n",
            "Batch 7: loss = 0.22270512580871582, acc = 0.9130859375\n",
            "Batch 8: loss = 0.23211687803268433, acc = 0.9306640625\n",
            "Batch 9: loss = 0.2389274686574936, acc = 0.9228515625\n",
            "Batch 10: loss = 0.21139667928218842, acc = 0.9306640625\n",
            "Batch 11: loss = 0.22306224703788757, acc = 0.921875\n",
            "Batch 12: loss = 0.2436300665140152, acc = 0.92578125\n",
            "Batch 13: loss = 0.22459109127521515, acc = 0.921875\n",
            "Batch 14: loss = 0.21808913350105286, acc = 0.9287109375\n",
            "Batch 15: loss = 0.19648288190364838, acc = 0.9267578125\n",
            "Batch 16: loss = 0.27281731367111206, acc = 0.90625\n",
            "Batch 17: loss = 0.19725599884986877, acc = 0.9384765625\n",
            "Batch 18: loss = 0.2332073152065277, acc = 0.927734375\n",
            "Batch 19: loss = 0.24661029875278473, acc = 0.9130859375\n",
            "Batch 20: loss = 0.23526330292224884, acc = 0.9189453125\n",
            "Batch 21: loss = 0.27926769852638245, acc = 0.90625\n",
            "Batch 22: loss = 0.2556780278682709, acc = 0.912109375\n",
            "Batch 23: loss = 0.2344346046447754, acc = 0.916015625\n",
            "Batch 24: loss = 0.24931181967258453, acc = 0.9091796875\n",
            "Batch 25: loss = 0.23172304034233093, acc = 0.923828125\n",
            "Batch 26: loss = 0.23578329384326935, acc = 0.91796875\n",
            "Batch 27: loss = 0.23595240712165833, acc = 0.919921875\n",
            "Batch 28: loss = 0.22615984082221985, acc = 0.919921875\n",
            "Batch 29: loss = 0.21845325827598572, acc = 0.9248046875\n",
            "Batch 30: loss = 0.2645247280597687, acc = 0.9111328125\n",
            "Batch 31: loss = 0.23955081403255463, acc = 0.9189453125\n",
            "Batch 32: loss = 0.27406030893325806, acc = 0.9033203125\n",
            "Batch 33: loss = 0.20934024453163147, acc = 0.9248046875\n",
            "Batch 34: loss = 0.23104101419448853, acc = 0.923828125\n",
            "Batch 35: loss = 0.21525903046131134, acc = 0.93359375\n",
            "Batch 36: loss = 0.2196098268032074, acc = 0.9287109375\n",
            "Batch 37: loss = 0.2187977433204651, acc = 0.921875\n",
            "Batch 38: loss = 0.23508687317371368, acc = 0.9169921875\n",
            "Batch 39: loss = 0.21761637926101685, acc = 0.9296875\n",
            "Batch 40: loss = 0.21956588327884674, acc = 0.9296875\n",
            "Batch 41: loss = 0.21047835052013397, acc = 0.92578125\n",
            "Batch 42: loss = 0.20180827379226685, acc = 0.9326171875\n",
            "Batch 43: loss = 0.2484816014766693, acc = 0.9150390625\n",
            "Batch 44: loss = 0.21615156531333923, acc = 0.9345703125\n",
            "Batch 45: loss = 0.19707366824150085, acc = 0.9375\n",
            "Batch 46: loss = 0.17573444545269012, acc = 0.9423828125\n",
            "Batch 47: loss = 0.22259467840194702, acc = 0.9296875\n",
            "Batch 48: loss = 0.21912294626235962, acc = 0.923828125\n",
            "Batch 49: loss = 0.2322680652141571, acc = 0.9326171875\n",
            "Batch 50: loss = 0.21727563440799713, acc = 0.9208984375\n",
            "Batch 51: loss = 0.26097649335861206, acc = 0.9140625\n",
            "Batch 52: loss = 0.2365865558385849, acc = 0.9013671875\n",
            "Batch 53: loss = 0.2557292580604553, acc = 0.91015625\n",
            "Batch 54: loss = 0.2082485556602478, acc = 0.931640625\n",
            "Batch 55: loss = 0.22832494974136353, acc = 0.9267578125\n",
            "Batch 56: loss = 0.26086974143981934, acc = 0.908203125\n",
            "Batch 57: loss = 0.2589832842350006, acc = 0.908203125\n",
            "Batch 58: loss = 0.24329088628292084, acc = 0.9169921875\n",
            "Batch 59: loss = 0.1907951682806015, acc = 0.9404296875\n",
            "Batch 60: loss = 0.21828387677669525, acc = 0.9189453125\n",
            "Batch 61: loss = 0.2230479121208191, acc = 0.9248046875\n",
            "Batch 62: loss = 0.23494331538677216, acc = 0.9150390625\n",
            "Batch 63: loss = 0.1889176368713379, acc = 0.9423828125\n",
            "Batch 64: loss = 0.2117224931716919, acc = 0.9306640625\n",
            "Batch 65: loss = 0.22685399651527405, acc = 0.92578125\n",
            "Batch 66: loss = 0.2369297444820404, acc = 0.9228515625\n",
            "Batch 67: loss = 0.22963647544384003, acc = 0.9189453125\n",
            "Batch 68: loss = 0.2418426126241684, acc = 0.908203125\n",
            "Batch 69: loss = 0.2141498327255249, acc = 0.9306640625\n",
            "Batch 70: loss = 0.24309907853603363, acc = 0.9140625\n",
            "Batch 71: loss = 0.2512330114841461, acc = 0.921875\n",
            "Batch 72: loss = 0.22646832466125488, acc = 0.9267578125\n",
            "Batch 73: loss = 0.23943260312080383, acc = 0.9248046875\n",
            "Batch 74: loss = 0.2265356332063675, acc = 0.9189453125\n",
            "Batch 75: loss = 0.2516050338745117, acc = 0.912109375\n",
            "Batch 76: loss = 0.24536465108394623, acc = 0.91796875\n",
            "Batch 77: loss = 0.20885077118873596, acc = 0.93359375\n",
            "Batch 78: loss = 0.24903851747512817, acc = 0.9111328125\n",
            "Batch 79: loss = 0.21903203427791595, acc = 0.927734375\n",
            "Batch 80: loss = 0.22720961272716522, acc = 0.927734375\n",
            "Batch 81: loss = 0.27660584449768066, acc = 0.9111328125\n",
            "Batch 82: loss = 0.2790233790874481, acc = 0.91015625\n",
            "Batch 83: loss = 0.22465981543064117, acc = 0.9248046875\n",
            "Batch 84: loss = 0.23245099186897278, acc = 0.9169921875\n",
            "Batch 85: loss = 0.2762488126754761, acc = 0.908203125\n",
            "Batch 86: loss = 0.24900640547275543, acc = 0.9140625\n",
            "Batch 87: loss = 0.23121672868728638, acc = 0.9169921875\n",
            "Batch 88: loss = 0.2676399350166321, acc = 0.8955078125\n",
            "Batch 89: loss = 0.21478697657585144, acc = 0.93359375\n",
            "Batch 90: loss = 0.26011425256729126, acc = 0.908203125\n",
            "Batch 91: loss = 0.2736245095729828, acc = 0.89453125\n",
            "Batch 92: loss = 0.2379213124513626, acc = 0.9267578125\n",
            "Batch 93: loss = 0.24546006321907043, acc = 0.91796875\n",
            "Batch 94: loss = 0.23543670773506165, acc = 0.916015625\n",
            "Batch 95: loss = 0.20817163586616516, acc = 0.9208984375\n",
            "Batch 96: loss = 0.24940747022628784, acc = 0.916015625\n",
            "Batch 97: loss = 0.2717863917350769, acc = 0.9033203125\n",
            "Batch 98: loss = 0.2406514286994934, acc = 0.919921875\n",
            "Batch 99: loss = 0.262788325548172, acc = 0.9169921875\n",
            "Batch 100: loss = 0.25812023878097534, acc = 0.912109375\n",
            "Batch 101: loss = 0.22356081008911133, acc = 0.9296875\n",
            "Batch 102: loss = 0.27393853664398193, acc = 0.90625\n",
            "Batch 103: loss = 0.24735945463180542, acc = 0.9189453125\n",
            "Batch 104: loss = 0.23223888874053955, acc = 0.9189453125\n",
            "Batch 105: loss = 0.21198198199272156, acc = 0.9326171875\n",
            "Batch 106: loss = 0.23522840440273285, acc = 0.9248046875\n",
            "Batch 107: loss = 0.24363505840301514, acc = 0.9150390625\n",
            "Batch 108: loss = 0.24766288697719574, acc = 0.91015625\n",
            "Batch 109: loss = 0.24078549444675446, acc = 0.9091796875\n",
            "Batch 110: loss = 0.20503966510295868, acc = 0.93359375\n",
            "Batch 111: loss = 0.2556155323982239, acc = 0.916015625\n",
            "Batch 112: loss = 0.19747871160507202, acc = 0.9365234375\n",
            "Batch 113: loss = 0.22793713212013245, acc = 0.9248046875\n",
            "Batch 114: loss = 0.2578362226486206, acc = 0.9111328125\n",
            "Batch 115: loss = 0.21131940186023712, acc = 0.935546875\n",
            "Batch 116: loss = 0.22313222289085388, acc = 0.9228515625\n",
            "Batch 117: loss = 0.21773824095726013, acc = 0.921875\n",
            "Batch 118: loss = 0.23288750648498535, acc = 0.9208984375\n",
            "Batch 119: loss = 0.204878568649292, acc = 0.92578125\n",
            "Batch 120: loss = 0.18737733364105225, acc = 0.9384765625\n",
            "Batch 121: loss = 0.23470497131347656, acc = 0.9189453125\n",
            "Batch 122: loss = 0.1958533227443695, acc = 0.93359375\n",
            "Batch 123: loss = 0.22137227654457092, acc = 0.92578125\n",
            "Batch 124: loss = 0.26894187927246094, acc = 0.908203125\n",
            "Batch 125: loss = 0.24076861143112183, acc = 0.921875\n",
            "Batch 126: loss = 0.25944778323173523, acc = 0.912109375\n",
            "\n",
            "Epoch 95/100\n",
            "Batch 1: loss = 0.3281947076320648, acc = 0.89453125\n",
            "Batch 2: loss = 0.24200376868247986, acc = 0.916015625\n",
            "Batch 3: loss = 0.23175667226314545, acc = 0.9248046875\n",
            "Batch 4: loss = 0.21337890625, acc = 0.923828125\n",
            "Batch 5: loss = 0.24177393317222595, acc = 0.921875\n",
            "Batch 6: loss = 0.25003501772880554, acc = 0.9150390625\n",
            "Batch 7: loss = 0.24419498443603516, acc = 0.9208984375\n",
            "Batch 8: loss = 0.2533128559589386, acc = 0.9140625\n",
            "Batch 9: loss = 0.20817384123802185, acc = 0.931640625\n",
            "Batch 10: loss = 0.19401505589485168, acc = 0.9345703125\n",
            "Batch 11: loss = 0.243395134806633, acc = 0.921875\n",
            "Batch 12: loss = 0.23003050684928894, acc = 0.9169921875\n",
            "Batch 13: loss = 0.22124554216861725, acc = 0.9248046875\n",
            "Batch 14: loss = 0.22453585267066956, acc = 0.923828125\n",
            "Batch 15: loss = 0.21956641972064972, acc = 0.92578125\n",
            "Batch 16: loss = 0.23399239778518677, acc = 0.9287109375\n",
            "Batch 17: loss = 0.22218336164951324, acc = 0.9228515625\n",
            "Batch 18: loss = 0.23614495992660522, acc = 0.91796875\n",
            "Batch 19: loss = 0.27017438411712646, acc = 0.908203125\n",
            "Batch 20: loss = 0.21280735731124878, acc = 0.9169921875\n",
            "Batch 21: loss = 0.2342800348997116, acc = 0.919921875\n",
            "Batch 22: loss = 0.23740769922733307, acc = 0.912109375\n",
            "Batch 23: loss = 0.2233375608921051, acc = 0.9248046875\n",
            "Batch 24: loss = 0.24864010512828827, acc = 0.9130859375\n",
            "Batch 25: loss = 0.2586471736431122, acc = 0.912109375\n",
            "Batch 26: loss = 0.24932114779949188, acc = 0.9130859375\n",
            "Batch 27: loss = 0.24715226888656616, acc = 0.921875\n",
            "Batch 28: loss = 0.19922474026679993, acc = 0.9375\n",
            "Batch 29: loss = 0.24235424399375916, acc = 0.9208984375\n",
            "Batch 30: loss = 0.2504503130912781, acc = 0.9150390625\n",
            "Batch 31: loss = 0.2526770234107971, acc = 0.912109375\n",
            "Batch 32: loss = 0.2450636327266693, acc = 0.91796875\n",
            "Batch 33: loss = 0.22071775794029236, acc = 0.93359375\n",
            "Batch 34: loss = 0.27184852957725525, acc = 0.9033203125\n",
            "Batch 35: loss = 0.24016433954238892, acc = 0.92578125\n",
            "Batch 36: loss = 0.2194826304912567, acc = 0.923828125\n",
            "Batch 37: loss = 0.20828776061534882, acc = 0.9345703125\n",
            "Batch 38: loss = 0.23739634454250336, acc = 0.9189453125\n",
            "Batch 39: loss = 0.21176967024803162, acc = 0.92578125\n",
            "Batch 40: loss = 0.21329550445079803, acc = 0.9189453125\n",
            "Batch 41: loss = 0.221099853515625, acc = 0.9208984375\n",
            "Batch 42: loss = 0.20364297926425934, acc = 0.9365234375\n",
            "Batch 43: loss = 0.23504425585269928, acc = 0.921875\n",
            "Batch 44: loss = 0.23546189069747925, acc = 0.9365234375\n",
            "Batch 45: loss = 0.21252839267253876, acc = 0.9248046875\n",
            "Batch 46: loss = 0.2115960568189621, acc = 0.9326171875\n",
            "Batch 47: loss = 0.21620826423168182, acc = 0.9287109375\n",
            "Batch 48: loss = 0.19650426506996155, acc = 0.9384765625\n",
            "Batch 49: loss = 0.2443208247423172, acc = 0.92578125\n",
            "Batch 50: loss = 0.21175123751163483, acc = 0.9345703125\n",
            "Batch 51: loss = 0.20429036021232605, acc = 0.9384765625\n",
            "Batch 52: loss = 0.20640268921852112, acc = 0.9306640625\n",
            "Batch 53: loss = 0.20452280342578888, acc = 0.9345703125\n",
            "Batch 54: loss = 0.20075730979442596, acc = 0.9296875\n",
            "Batch 55: loss = 0.2114798128604889, acc = 0.9326171875\n",
            "Batch 56: loss = 0.23292666673660278, acc = 0.921875\n",
            "Batch 57: loss = 0.23090305924415588, acc = 0.9189453125\n",
            "Batch 58: loss = 0.27387160062789917, acc = 0.90625\n",
            "Batch 59: loss = 0.19815535843372345, acc = 0.92578125\n",
            "Batch 60: loss = 0.21619826555252075, acc = 0.939453125\n",
            "Batch 61: loss = 0.21178245544433594, acc = 0.9345703125\n",
            "Batch 62: loss = 0.23583735525608063, acc = 0.916015625\n",
            "Batch 63: loss = 0.20982439815998077, acc = 0.935546875\n",
            "Batch 64: loss = 0.19669413566589355, acc = 0.9345703125\n",
            "Batch 65: loss = 0.21963214874267578, acc = 0.9287109375\n",
            "Batch 66: loss = 0.24452424049377441, acc = 0.916015625\n",
            "Batch 67: loss = 0.25109437108039856, acc = 0.9150390625\n",
            "Batch 68: loss = 0.22362478077411652, acc = 0.9228515625\n",
            "Batch 69: loss = 0.19810883700847626, acc = 0.927734375\n",
            "Batch 70: loss = 0.23081180453300476, acc = 0.92578125\n",
            "Batch 71: loss = 0.22708934545516968, acc = 0.9189453125\n",
            "Batch 72: loss = 0.19571606814861298, acc = 0.9248046875\n",
            "Batch 73: loss = 0.23575173318386078, acc = 0.9248046875\n",
            "Batch 74: loss = 0.2270628809928894, acc = 0.923828125\n",
            "Batch 75: loss = 0.28024229407310486, acc = 0.900390625\n",
            "Batch 76: loss = 0.24474966526031494, acc = 0.9169921875\n",
            "Batch 77: loss = 0.217620387673378, acc = 0.9248046875\n",
            "Batch 78: loss = 0.2505272328853607, acc = 0.9091796875\n",
            "Batch 79: loss = 0.20607659220695496, acc = 0.9345703125\n",
            "Batch 80: loss = 0.18510951101779938, acc = 0.9345703125\n",
            "Batch 81: loss = 0.22976718842983246, acc = 0.9228515625\n",
            "Batch 82: loss = 0.23303616046905518, acc = 0.9287109375\n",
            "Batch 83: loss = 0.21743781864643097, acc = 0.9267578125\n",
            "Batch 84: loss = 0.25421738624572754, acc = 0.9150390625\n",
            "Batch 85: loss = 0.23381739854812622, acc = 0.9140625\n",
            "Batch 86: loss = 0.21744364500045776, acc = 0.9267578125\n",
            "Batch 87: loss = 0.2211925983428955, acc = 0.916015625\n",
            "Batch 88: loss = 0.23677363991737366, acc = 0.923828125\n",
            "Batch 89: loss = 0.24237307906150818, acc = 0.9267578125\n",
            "Batch 90: loss = 0.22926652431488037, acc = 0.921875\n",
            "Batch 91: loss = 0.22312337160110474, acc = 0.9248046875\n",
            "Batch 92: loss = 0.23543837666511536, acc = 0.927734375\n",
            "Batch 93: loss = 0.2078639268875122, acc = 0.927734375\n",
            "Batch 94: loss = 0.2255590856075287, acc = 0.91796875\n",
            "Batch 95: loss = 0.22486910223960876, acc = 0.921875\n",
            "Batch 96: loss = 0.2912617623806, acc = 0.896484375\n",
            "Batch 97: loss = 0.24558837711811066, acc = 0.923828125\n",
            "Batch 98: loss = 0.2383270263671875, acc = 0.9130859375\n",
            "Batch 99: loss = 0.23939694464206696, acc = 0.9228515625\n",
            "Batch 100: loss = 0.2440149188041687, acc = 0.91796875\n",
            "Batch 101: loss = 0.23250983655452728, acc = 0.9306640625\n",
            "Batch 102: loss = 0.21178050339221954, acc = 0.9228515625\n",
            "Batch 103: loss = 0.2532486915588379, acc = 0.923828125\n",
            "Batch 104: loss = 0.2301647663116455, acc = 0.9130859375\n",
            "Batch 105: loss = 0.23571471869945526, acc = 0.9208984375\n",
            "Batch 106: loss = 0.23920409381389618, acc = 0.9228515625\n",
            "Batch 107: loss = 0.21738435328006744, acc = 0.9228515625\n",
            "Batch 108: loss = 0.2286553978919983, acc = 0.921875\n",
            "Batch 109: loss = 0.240254744887352, acc = 0.916015625\n",
            "Batch 110: loss = 0.22748050093650818, acc = 0.92578125\n",
            "Batch 111: loss = 0.23950718343257904, acc = 0.921875\n",
            "Batch 112: loss = 0.2180548459291458, acc = 0.9248046875\n",
            "Batch 113: loss = 0.23357492685317993, acc = 0.92578125\n",
            "Batch 114: loss = 0.24988678097724915, acc = 0.9169921875\n",
            "Batch 115: loss = 0.21315699815750122, acc = 0.931640625\n",
            "Batch 116: loss = 0.23651036620140076, acc = 0.9150390625\n",
            "Batch 117: loss = 0.23349541425704956, acc = 0.9169921875\n",
            "Batch 118: loss = 0.2274119108915329, acc = 0.91796875\n",
            "Batch 119: loss = 0.2238001823425293, acc = 0.9130859375\n",
            "Batch 120: loss = 0.19713889062404633, acc = 0.93359375\n",
            "Batch 121: loss = 0.2389247715473175, acc = 0.9140625\n",
            "Batch 122: loss = 0.21676945686340332, acc = 0.931640625\n",
            "Batch 123: loss = 0.2382986843585968, acc = 0.9169921875\n",
            "Batch 124: loss = 0.2405470311641693, acc = 0.9228515625\n",
            "Batch 125: loss = 0.24019421637058258, acc = 0.91796875\n",
            "Batch 126: loss = 0.23512780666351318, acc = 0.9228515625\n",
            "\n",
            "Epoch 96/100\n",
            "Batch 1: loss = 0.31096920371055603, acc = 0.908203125\n",
            "Batch 2: loss = 0.25649359822273254, acc = 0.9130859375\n",
            "Batch 3: loss = 0.2817070186138153, acc = 0.9072265625\n",
            "Batch 4: loss = 0.2541409730911255, acc = 0.919921875\n",
            "Batch 5: loss = 0.2298356145620346, acc = 0.9287109375\n",
            "Batch 6: loss = 0.23108196258544922, acc = 0.9306640625\n",
            "Batch 7: loss = 0.25348421931266785, acc = 0.9072265625\n",
            "Batch 8: loss = 0.24031026661396027, acc = 0.921875\n",
            "Batch 9: loss = 0.24278336763381958, acc = 0.916015625\n",
            "Batch 10: loss = 0.22052285075187683, acc = 0.9248046875\n",
            "Batch 11: loss = 0.22128932178020477, acc = 0.9228515625\n",
            "Batch 12: loss = 0.2090844213962555, acc = 0.9365234375\n",
            "Batch 13: loss = 0.21022826433181763, acc = 0.9267578125\n",
            "Batch 14: loss = 0.21913491189479828, acc = 0.92578125\n",
            "Batch 15: loss = 0.2503110468387604, acc = 0.9111328125\n",
            "Batch 16: loss = 0.2209886908531189, acc = 0.923828125\n",
            "Batch 17: loss = 0.2291855663061142, acc = 0.919921875\n",
            "Batch 18: loss = 0.22914260625839233, acc = 0.908203125\n",
            "Batch 19: loss = 0.254887193441391, acc = 0.9111328125\n",
            "Batch 20: loss = 0.24026302993297577, acc = 0.9140625\n",
            "Batch 21: loss = 0.25159603357315063, acc = 0.919921875\n",
            "Batch 22: loss = 0.20504212379455566, acc = 0.91796875\n",
            "Batch 23: loss = 0.23813462257385254, acc = 0.9267578125\n",
            "Batch 24: loss = 0.2444761097431183, acc = 0.9228515625\n",
            "Batch 25: loss = 0.2298758327960968, acc = 0.92578125\n",
            "Batch 26: loss = 0.2124241590499878, acc = 0.93359375\n",
            "Batch 27: loss = 0.21335764229297638, acc = 0.9296875\n",
            "Batch 28: loss = 0.22223830223083496, acc = 0.9267578125\n",
            "Batch 29: loss = 0.2678626775741577, acc = 0.9091796875\n",
            "Batch 30: loss = 0.22558970749378204, acc = 0.927734375\n",
            "Batch 31: loss = 0.2250642031431198, acc = 0.919921875\n",
            "Batch 32: loss = 0.27904748916625977, acc = 0.9111328125\n",
            "Batch 33: loss = 0.24133680760860443, acc = 0.9248046875\n",
            "Batch 34: loss = 0.229842871427536, acc = 0.9326171875\n",
            "Batch 35: loss = 0.22221964597702026, acc = 0.9267578125\n",
            "Batch 36: loss = 0.19320005178451538, acc = 0.947265625\n",
            "Batch 37: loss = 0.22079487144947052, acc = 0.9287109375\n",
            "Batch 38: loss = 0.23720458149909973, acc = 0.9091796875\n",
            "Batch 39: loss = 0.20210105180740356, acc = 0.9345703125\n",
            "Batch 40: loss = 0.2149849534034729, acc = 0.9267578125\n",
            "Batch 41: loss = 0.2444462925195694, acc = 0.91015625\n",
            "Batch 42: loss = 0.2591710686683655, acc = 0.9169921875\n",
            "Batch 43: loss = 0.22296400368213654, acc = 0.9326171875\n",
            "Batch 44: loss = 0.21040493249893188, acc = 0.93359375\n",
            "Batch 45: loss = 0.22836945950984955, acc = 0.921875\n",
            "Batch 46: loss = 0.20591150224208832, acc = 0.9326171875\n",
            "Batch 47: loss = 0.20185443758964539, acc = 0.931640625\n",
            "Batch 48: loss = 0.1984921544790268, acc = 0.927734375\n",
            "Batch 49: loss = 0.2232401967048645, acc = 0.9248046875\n",
            "Batch 50: loss = 0.19705261290073395, acc = 0.9345703125\n",
            "Batch 51: loss = 0.21035093069076538, acc = 0.9365234375\n",
            "Batch 52: loss = 0.24472136795520782, acc = 0.9169921875\n",
            "Batch 53: loss = 0.19543097913265228, acc = 0.93359375\n",
            "Batch 54: loss = 0.1839079111814499, acc = 0.9287109375\n",
            "Batch 55: loss = 0.22580863535404205, acc = 0.93359375\n",
            "Batch 56: loss = 0.25146326422691345, acc = 0.916015625\n",
            "Batch 57: loss = 0.21445584297180176, acc = 0.9208984375\n",
            "Batch 58: loss = 0.25651752948760986, acc = 0.9228515625\n",
            "Batch 59: loss = 0.19651442766189575, acc = 0.931640625\n",
            "Batch 60: loss = 0.25414445996284485, acc = 0.919921875\n",
            "Batch 61: loss = 0.22156178951263428, acc = 0.9228515625\n",
            "Batch 62: loss = 0.2375124990940094, acc = 0.919921875\n",
            "Batch 63: loss = 0.21391834318637848, acc = 0.927734375\n",
            "Batch 64: loss = 0.210475355386734, acc = 0.927734375\n",
            "Batch 65: loss = 0.2283431887626648, acc = 0.9228515625\n",
            "Batch 66: loss = 0.2542901933193207, acc = 0.9140625\n",
            "Batch 67: loss = 0.20485597848892212, acc = 0.9306640625\n",
            "Batch 68: loss = 0.2463725209236145, acc = 0.91796875\n",
            "Batch 69: loss = 0.2108660787343979, acc = 0.931640625\n",
            "Batch 70: loss = 0.20319069921970367, acc = 0.9267578125\n",
            "Batch 71: loss = 0.24747440218925476, acc = 0.9130859375\n",
            "Batch 72: loss = 0.24876773357391357, acc = 0.9267578125\n",
            "Batch 73: loss = 0.21484413743019104, acc = 0.9248046875\n",
            "Batch 74: loss = 0.2541412115097046, acc = 0.9111328125\n",
            "Batch 75: loss = 0.24794432520866394, acc = 0.912109375\n",
            "Batch 76: loss = 0.2768619656562805, acc = 0.9130859375\n",
            "Batch 77: loss = 0.2096022069454193, acc = 0.927734375\n",
            "Batch 78: loss = 0.20992808043956757, acc = 0.919921875\n",
            "Batch 79: loss = 0.22023847699165344, acc = 0.9267578125\n",
            "Batch 80: loss = 0.17472225427627563, acc = 0.9462890625\n",
            "Batch 81: loss = 0.2603526711463928, acc = 0.912109375\n",
            "Batch 82: loss = 0.22828173637390137, acc = 0.9326171875\n",
            "Batch 83: loss = 0.21760258078575134, acc = 0.921875\n",
            "Batch 84: loss = 0.21572820842266083, acc = 0.9267578125\n",
            "Batch 85: loss = 0.27092769742012024, acc = 0.9091796875\n",
            "Batch 86: loss = 0.2347199022769928, acc = 0.927734375\n",
            "Batch 87: loss = 0.23284900188446045, acc = 0.921875\n",
            "Batch 88: loss = 0.2464841902256012, acc = 0.9091796875\n",
            "Batch 89: loss = 0.2326696515083313, acc = 0.927734375\n",
            "Batch 90: loss = 0.24939081072807312, acc = 0.90625\n",
            "Batch 91: loss = 0.20821785926818848, acc = 0.9267578125\n",
            "Batch 92: loss = 0.24294859170913696, acc = 0.9140625\n",
            "Batch 93: loss = 0.22643724083900452, acc = 0.927734375\n",
            "Batch 94: loss = 0.21814082562923431, acc = 0.923828125\n",
            "Batch 95: loss = 0.2191244661808014, acc = 0.92578125\n",
            "Batch 96: loss = 0.2551366686820984, acc = 0.912109375\n",
            "Batch 97: loss = 0.2539198398590088, acc = 0.912109375\n",
            "Batch 98: loss = 0.2550300359725952, acc = 0.916015625\n",
            "Batch 99: loss = 0.247737318277359, acc = 0.9140625\n",
            "Batch 100: loss = 0.20393845438957214, acc = 0.92578125\n",
            "Batch 101: loss = 0.25226065516471863, acc = 0.91015625\n",
            "Batch 102: loss = 0.24289052188396454, acc = 0.91796875\n",
            "Batch 103: loss = 0.20931561291217804, acc = 0.9306640625\n",
            "Batch 104: loss = 0.21947020292282104, acc = 0.921875\n",
            "Batch 105: loss = 0.20568373799324036, acc = 0.9296875\n",
            "Batch 106: loss = 0.2321939468383789, acc = 0.921875\n",
            "Batch 107: loss = 0.24058686196804047, acc = 0.9169921875\n",
            "Batch 108: loss = 0.2139599472284317, acc = 0.921875\n",
            "Batch 109: loss = 0.23595030605793, acc = 0.9189453125\n",
            "Batch 110: loss = 0.2241765558719635, acc = 0.9287109375\n",
            "Batch 111: loss = 0.23961514234542847, acc = 0.916015625\n",
            "Batch 112: loss = 0.23266947269439697, acc = 0.921875\n",
            "Batch 113: loss = 0.26913806796073914, acc = 0.892578125\n",
            "Batch 114: loss = 0.2409113645553589, acc = 0.9228515625\n",
            "Batch 115: loss = 0.23469537496566772, acc = 0.927734375\n",
            "Batch 116: loss = 0.2446909099817276, acc = 0.9130859375\n",
            "Batch 117: loss = 0.22966259717941284, acc = 0.919921875\n",
            "Batch 118: loss = 0.22002357244491577, acc = 0.91796875\n",
            "Batch 119: loss = 0.24012348055839539, acc = 0.9169921875\n",
            "Batch 120: loss = 0.21023917198181152, acc = 0.9248046875\n",
            "Batch 121: loss = 0.22054561972618103, acc = 0.927734375\n",
            "Batch 122: loss = 0.2247922718524933, acc = 0.921875\n",
            "Batch 123: loss = 0.2375258207321167, acc = 0.9287109375\n",
            "Batch 124: loss = 0.2624388039112091, acc = 0.9033203125\n",
            "Batch 125: loss = 0.2725255489349365, acc = 0.9013671875\n",
            "Batch 126: loss = 0.25624164938926697, acc = 0.9091796875\n",
            "\n",
            "Epoch 97/100\n",
            "Batch 1: loss = 0.2881227433681488, acc = 0.90625\n",
            "Batch 2: loss = 0.2393743395805359, acc = 0.9189453125\n",
            "Batch 3: loss = 0.2347535341978073, acc = 0.9208984375\n",
            "Batch 4: loss = 0.24088257551193237, acc = 0.935546875\n",
            "Batch 5: loss = 0.22048279643058777, acc = 0.9228515625\n",
            "Batch 6: loss = 0.2562173008918762, acc = 0.90625\n",
            "Batch 7: loss = 0.25331416726112366, acc = 0.9052734375\n",
            "Batch 8: loss = 0.23616516590118408, acc = 0.92578125\n",
            "Batch 9: loss = 0.21993938088417053, acc = 0.9267578125\n",
            "Batch 10: loss = 0.20214344561100006, acc = 0.9384765625\n",
            "Batch 11: loss = 0.23930419981479645, acc = 0.91015625\n",
            "Batch 12: loss = 0.22151561081409454, acc = 0.927734375\n",
            "Batch 13: loss = 0.21746578812599182, acc = 0.921875\n",
            "Batch 14: loss = 0.20209653675556183, acc = 0.931640625\n",
            "Batch 15: loss = 0.20446059107780457, acc = 0.931640625\n",
            "Batch 16: loss = 0.23805031180381775, acc = 0.9228515625\n",
            "Batch 17: loss = 0.24484464526176453, acc = 0.9208984375\n",
            "Batch 18: loss = 0.23840144276618958, acc = 0.91796875\n",
            "Batch 19: loss = 0.2365407794713974, acc = 0.92578125\n",
            "Batch 20: loss = 0.22172871232032776, acc = 0.919921875\n",
            "Batch 21: loss = 0.23994745314121246, acc = 0.912109375\n",
            "Batch 22: loss = 0.2194068431854248, acc = 0.9248046875\n",
            "Batch 23: loss = 0.23845309019088745, acc = 0.9189453125\n",
            "Batch 24: loss = 0.2164117395877838, acc = 0.9228515625\n",
            "Batch 25: loss = 0.21640576422214508, acc = 0.923828125\n",
            "Batch 26: loss = 0.23817749321460724, acc = 0.9228515625\n",
            "Batch 27: loss = 0.2543967366218567, acc = 0.912109375\n",
            "Batch 28: loss = 0.20592302083969116, acc = 0.923828125\n",
            "Batch 29: loss = 0.2125079184770584, acc = 0.9345703125\n",
            "Batch 30: loss = 0.2471708357334137, acc = 0.9169921875\n",
            "Batch 31: loss = 0.2317139357328415, acc = 0.9189453125\n",
            "Batch 32: loss = 0.23335546255111694, acc = 0.91796875\n",
            "Batch 33: loss = 0.2284730225801468, acc = 0.9326171875\n",
            "Batch 34: loss = 0.24597850441932678, acc = 0.91015625\n",
            "Batch 35: loss = 0.20730075240135193, acc = 0.931640625\n",
            "Batch 36: loss = 0.19654816389083862, acc = 0.9306640625\n",
            "Batch 37: loss = 0.19860200583934784, acc = 0.9326171875\n",
            "Batch 38: loss = 0.21269671618938446, acc = 0.9345703125\n",
            "Batch 39: loss = 0.202090322971344, acc = 0.939453125\n",
            "Batch 40: loss = 0.21923686563968658, acc = 0.9228515625\n",
            "Batch 41: loss = 0.20824576914310455, acc = 0.9306640625\n",
            "Batch 42: loss = 0.23132649064064026, acc = 0.923828125\n",
            "Batch 43: loss = 0.23951023817062378, acc = 0.9228515625\n",
            "Batch 44: loss = 0.2169039249420166, acc = 0.9296875\n",
            "Batch 45: loss = 0.2067139446735382, acc = 0.9248046875\n",
            "Batch 46: loss = 0.1751205176115036, acc = 0.9345703125\n",
            "Batch 47: loss = 0.22736628353595734, acc = 0.9248046875\n",
            "Batch 48: loss = 0.18529623746871948, acc = 0.931640625\n",
            "Batch 49: loss = 0.2308541089296341, acc = 0.93359375\n",
            "Batch 50: loss = 0.2115509808063507, acc = 0.927734375\n",
            "Batch 51: loss = 0.22080263495445251, acc = 0.9287109375\n",
            "Batch 52: loss = 0.21917502582073212, acc = 0.92578125\n",
            "Batch 53: loss = 0.2393903136253357, acc = 0.9169921875\n",
            "Batch 54: loss = 0.19973784685134888, acc = 0.9326171875\n",
            "Batch 55: loss = 0.19900090992450714, acc = 0.931640625\n",
            "Batch 56: loss = 0.19922570884227753, acc = 0.9345703125\n",
            "Batch 57: loss = 0.24701249599456787, acc = 0.912109375\n",
            "Batch 58: loss = 0.25441354513168335, acc = 0.9052734375\n",
            "Batch 59: loss = 0.20185452699661255, acc = 0.9267578125\n",
            "Batch 60: loss = 0.21161781251430511, acc = 0.92578125\n",
            "Batch 61: loss = 0.21291427314281464, acc = 0.9306640625\n",
            "Batch 62: loss = 0.2609308660030365, acc = 0.9091796875\n",
            "Batch 63: loss = 0.21886110305786133, acc = 0.9345703125\n",
            "Batch 64: loss = 0.2208559364080429, acc = 0.931640625\n",
            "Batch 65: loss = 0.2171441912651062, acc = 0.9208984375\n",
            "Batch 66: loss = 0.22052086889743805, acc = 0.927734375\n",
            "Batch 67: loss = 0.23399917781352997, acc = 0.921875\n",
            "Batch 68: loss = 0.21215301752090454, acc = 0.931640625\n",
            "Batch 69: loss = 0.20506010949611664, acc = 0.939453125\n",
            "Batch 70: loss = 0.24675725400447845, acc = 0.9208984375\n",
            "Batch 71: loss = 0.22272269427776337, acc = 0.921875\n",
            "Batch 72: loss = 0.2185695469379425, acc = 0.9267578125\n",
            "Batch 73: loss = 0.25228551030158997, acc = 0.9150390625\n",
            "Batch 74: loss = 0.26256170868873596, acc = 0.91015625\n",
            "Batch 75: loss = 0.23703119158744812, acc = 0.9150390625\n",
            "Batch 76: loss = 0.23921728134155273, acc = 0.9208984375\n",
            "Batch 77: loss = 0.2219683676958084, acc = 0.9091796875\n",
            "Batch 78: loss = 0.21849896013736725, acc = 0.92578125\n",
            "Batch 79: loss = 0.2190248668193817, acc = 0.931640625\n",
            "Batch 80: loss = 0.19753600656986237, acc = 0.935546875\n",
            "Batch 81: loss = 0.2375035583972931, acc = 0.9208984375\n",
            "Batch 82: loss = 0.253311425447464, acc = 0.9140625\n",
            "Batch 83: loss = 0.20704510807991028, acc = 0.9375\n",
            "Batch 84: loss = 0.22423946857452393, acc = 0.921875\n",
            "Batch 85: loss = 0.23887944221496582, acc = 0.9052734375\n",
            "Batch 86: loss = 0.25471600890159607, acc = 0.91015625\n",
            "Batch 87: loss = 0.19833074510097504, acc = 0.9326171875\n",
            "Batch 88: loss = 0.25221318006515503, acc = 0.9111328125\n",
            "Batch 89: loss = 0.22305670380592346, acc = 0.931640625\n",
            "Batch 90: loss = 0.24776284396648407, acc = 0.9140625\n",
            "Batch 91: loss = 0.23288120329380035, acc = 0.912109375\n",
            "Batch 92: loss = 0.2637549638748169, acc = 0.91015625\n",
            "Batch 93: loss = 0.22691896557807922, acc = 0.919921875\n",
            "Batch 94: loss = 0.19538699090480804, acc = 0.9375\n",
            "Batch 95: loss = 0.24201810359954834, acc = 0.919921875\n",
            "Batch 96: loss = 0.26907435059547424, acc = 0.9033203125\n",
            "Batch 97: loss = 0.2508026659488678, acc = 0.9130859375\n",
            "Batch 98: loss = 0.22759580612182617, acc = 0.9189453125\n",
            "Batch 99: loss = 0.24407266080379486, acc = 0.9169921875\n",
            "Batch 100: loss = 0.21752378344535828, acc = 0.9248046875\n",
            "Batch 101: loss = 0.22161626815795898, acc = 0.916015625\n",
            "Batch 102: loss = 0.2853413224220276, acc = 0.8974609375\n",
            "Batch 103: loss = 0.22260159254074097, acc = 0.927734375\n",
            "Batch 104: loss = 0.21902388334274292, acc = 0.9189453125\n",
            "Batch 105: loss = 0.18847960233688354, acc = 0.935546875\n",
            "Batch 106: loss = 0.2513013780117035, acc = 0.91015625\n",
            "Batch 107: loss = 0.24269142746925354, acc = 0.9140625\n",
            "Batch 108: loss = 0.21971987187862396, acc = 0.927734375\n",
            "Batch 109: loss = 0.25339946150779724, acc = 0.912109375\n",
            "Batch 110: loss = 0.2325197160243988, acc = 0.9287109375\n",
            "Batch 111: loss = 0.21246656775474548, acc = 0.9208984375\n",
            "Batch 112: loss = 0.21351759135723114, acc = 0.92578125\n",
            "Batch 113: loss = 0.22287222743034363, acc = 0.923828125\n",
            "Batch 114: loss = 0.2513495981693268, acc = 0.9208984375\n",
            "Batch 115: loss = 0.21882207691669464, acc = 0.92578125\n",
            "Batch 116: loss = 0.23174695670604706, acc = 0.919921875\n",
            "Batch 117: loss = 0.250211626291275, acc = 0.9130859375\n",
            "Batch 118: loss = 0.2058272808790207, acc = 0.9296875\n",
            "Batch 119: loss = 0.2026996910572052, acc = 0.923828125\n",
            "Batch 120: loss = 0.21014852821826935, acc = 0.92578125\n",
            "Batch 121: loss = 0.21002408862113953, acc = 0.9306640625\n",
            "Batch 122: loss = 0.21209299564361572, acc = 0.921875\n",
            "Batch 123: loss = 0.2335953712463379, acc = 0.927734375\n",
            "Batch 124: loss = 0.25703907012939453, acc = 0.9091796875\n",
            "Batch 125: loss = 0.22890706360340118, acc = 0.921875\n",
            "Batch 126: loss = 0.21616053581237793, acc = 0.923828125\n",
            "\n",
            "Epoch 98/100\n",
            "Batch 1: loss = 0.2754884958267212, acc = 0.9033203125\n",
            "Batch 2: loss = 0.25087133049964905, acc = 0.912109375\n",
            "Batch 3: loss = 0.20603293180465698, acc = 0.9306640625\n",
            "Batch 4: loss = 0.2259940356016159, acc = 0.927734375\n",
            "Batch 5: loss = 0.25541219115257263, acc = 0.9072265625\n",
            "Batch 6: loss = 0.2391875982284546, acc = 0.919921875\n",
            "Batch 7: loss = 0.21476411819458008, acc = 0.9228515625\n",
            "Batch 8: loss = 0.2249051183462143, acc = 0.92578125\n",
            "Batch 9: loss = 0.23374375700950623, acc = 0.91796875\n",
            "Batch 10: loss = 0.20886851847171783, acc = 0.9287109375\n",
            "Batch 11: loss = 0.23687037825584412, acc = 0.9140625\n",
            "Batch 12: loss = 0.22166037559509277, acc = 0.9189453125\n",
            "Batch 13: loss = 0.22216886281967163, acc = 0.9169921875\n",
            "Batch 14: loss = 0.20334173738956451, acc = 0.9384765625\n",
            "Batch 15: loss = 0.2003132402896881, acc = 0.9287109375\n",
            "Batch 16: loss = 0.22900338470935822, acc = 0.919921875\n",
            "Batch 17: loss = 0.2028326541185379, acc = 0.935546875\n",
            "Batch 18: loss = 0.21447573602199554, acc = 0.921875\n",
            "Batch 19: loss = 0.22925586998462677, acc = 0.9169921875\n",
            "Batch 20: loss = 0.2255871295928955, acc = 0.9208984375\n",
            "Batch 21: loss = 0.2717827260494232, acc = 0.91015625\n",
            "Batch 22: loss = 0.22019034624099731, acc = 0.927734375\n",
            "Batch 23: loss = 0.2609027326107025, acc = 0.9033203125\n",
            "Batch 24: loss = 0.2380245476961136, acc = 0.9169921875\n",
            "Batch 25: loss = 0.22257140278816223, acc = 0.9267578125\n",
            "Batch 26: loss = 0.2132028192281723, acc = 0.923828125\n",
            "Batch 27: loss = 0.24853268265724182, acc = 0.9140625\n",
            "Batch 28: loss = 0.21562223136425018, acc = 0.9296875\n",
            "Batch 29: loss = 0.23931455612182617, acc = 0.9169921875\n",
            "Batch 30: loss = 0.2579612731933594, acc = 0.9189453125\n",
            "Batch 31: loss = 0.19459247589111328, acc = 0.9384765625\n",
            "Batch 32: loss = 0.2678060531616211, acc = 0.9013671875\n",
            "Batch 33: loss = 0.2395429164171219, acc = 0.9208984375\n",
            "Batch 34: loss = 0.21041300892829895, acc = 0.927734375\n",
            "Batch 35: loss = 0.21031248569488525, acc = 0.93359375\n",
            "Batch 36: loss = 0.22230836749076843, acc = 0.9208984375\n",
            "Batch 37: loss = 0.21972715854644775, acc = 0.9296875\n",
            "Batch 38: loss = 0.21630917489528656, acc = 0.931640625\n",
            "Batch 39: loss = 0.20927371084690094, acc = 0.9384765625\n",
            "Batch 40: loss = 0.19792839884757996, acc = 0.9287109375\n",
            "Batch 41: loss = 0.23160883784294128, acc = 0.9208984375\n",
            "Batch 42: loss = 0.19978132843971252, acc = 0.9423828125\n",
            "Batch 43: loss = 0.2525256872177124, acc = 0.91015625\n",
            "Batch 44: loss = 0.22871312499046326, acc = 0.9287109375\n",
            "Batch 45: loss = 0.2147739678621292, acc = 0.9296875\n",
            "Batch 46: loss = 0.1782040297985077, acc = 0.931640625\n",
            "Batch 47: loss = 0.2322172075510025, acc = 0.919921875\n",
            "Batch 48: loss = 0.20128685235977173, acc = 0.9267578125\n",
            "Batch 49: loss = 0.22922557592391968, acc = 0.919921875\n",
            "Batch 50: loss = 0.22766858339309692, acc = 0.921875\n",
            "Batch 51: loss = 0.19864793121814728, acc = 0.9375\n",
            "Batch 52: loss = 0.20022322237491608, acc = 0.9345703125\n",
            "Batch 53: loss = 0.22651641070842743, acc = 0.9306640625\n",
            "Batch 54: loss = 0.18800312280654907, acc = 0.9345703125\n",
            "Batch 55: loss = 0.23285073041915894, acc = 0.9267578125\n",
            "Batch 56: loss = 0.23606032133102417, acc = 0.9248046875\n",
            "Batch 57: loss = 0.24530071020126343, acc = 0.9072265625\n",
            "Batch 58: loss = 0.25029027462005615, acc = 0.9189453125\n",
            "Batch 59: loss = 0.2036055028438568, acc = 0.9287109375\n",
            "Batch 60: loss = 0.23792165517807007, acc = 0.923828125\n",
            "Batch 61: loss = 0.22158506512641907, acc = 0.9326171875\n",
            "Batch 62: loss = 0.2667357623577118, acc = 0.9130859375\n",
            "Batch 63: loss = 0.2108091562986374, acc = 0.9267578125\n",
            "Batch 64: loss = 0.2102668732404709, acc = 0.935546875\n",
            "Batch 65: loss = 0.24555878341197968, acc = 0.9189453125\n",
            "Batch 66: loss = 0.22491732239723206, acc = 0.921875\n",
            "Batch 67: loss = 0.23127570748329163, acc = 0.9208984375\n",
            "Batch 68: loss = 0.233156219124794, acc = 0.9140625\n",
            "Batch 69: loss = 0.19459915161132812, acc = 0.93359375\n",
            "Batch 70: loss = 0.23671425879001617, acc = 0.9150390625\n",
            "Batch 71: loss = 0.2257508933544159, acc = 0.9130859375\n",
            "Batch 72: loss = 0.21620681881904602, acc = 0.921875\n",
            "Batch 73: loss = 0.23815028369426727, acc = 0.9208984375\n",
            "Batch 74: loss = 0.2398933321237564, acc = 0.916015625\n",
            "Batch 75: loss = 0.30397143959999084, acc = 0.90234375\n",
            "Batch 76: loss = 0.236677348613739, acc = 0.921875\n",
            "Batch 77: loss = 0.215928316116333, acc = 0.9228515625\n",
            "Batch 78: loss = 0.230070561170578, acc = 0.9228515625\n",
            "Batch 79: loss = 0.23558935523033142, acc = 0.9248046875\n",
            "Batch 80: loss = 0.22195196151733398, acc = 0.9189453125\n",
            "Batch 81: loss = 0.25853362679481506, acc = 0.9033203125\n",
            "Batch 82: loss = 0.2289198637008667, acc = 0.9169921875\n",
            "Batch 83: loss = 0.22877603769302368, acc = 0.923828125\n",
            "Batch 84: loss = 0.2380555272102356, acc = 0.9208984375\n",
            "Batch 85: loss = 0.24239805340766907, acc = 0.9208984375\n",
            "Batch 86: loss = 0.23076653480529785, acc = 0.9140625\n",
            "Batch 87: loss = 0.2568453848361969, acc = 0.9072265625\n",
            "Batch 88: loss = 0.25300586223602295, acc = 0.9130859375\n",
            "Batch 89: loss = 0.22971515357494354, acc = 0.9189453125\n",
            "Batch 90: loss = 0.23197022080421448, acc = 0.916015625\n",
            "Batch 91: loss = 0.26113027334213257, acc = 0.9091796875\n",
            "Batch 92: loss = 0.2487233281135559, acc = 0.9111328125\n",
            "Batch 93: loss = 0.22868062555789948, acc = 0.9267578125\n",
            "Batch 94: loss = 0.20200031995773315, acc = 0.9267578125\n",
            "Batch 95: loss = 0.22758036851882935, acc = 0.9296875\n",
            "Batch 96: loss = 0.2716255187988281, acc = 0.9072265625\n",
            "Batch 97: loss = 0.2722143530845642, acc = 0.900390625\n",
            "Batch 98: loss = 0.27377012372016907, acc = 0.9033203125\n",
            "Batch 99: loss = 0.22717581689357758, acc = 0.9326171875\n",
            "Batch 100: loss = 0.23989583551883698, acc = 0.916015625\n",
            "Batch 101: loss = 0.2242632955312729, acc = 0.9169921875\n",
            "Batch 102: loss = 0.23383454978466034, acc = 0.9169921875\n",
            "Batch 103: loss = 0.23810027539730072, acc = 0.9169921875\n",
            "Batch 104: loss = 0.21715430915355682, acc = 0.9267578125\n",
            "Batch 105: loss = 0.2066870778799057, acc = 0.9375\n",
            "Batch 106: loss = 0.24572542309761047, acc = 0.912109375\n",
            "Batch 107: loss = 0.25228172540664673, acc = 0.9130859375\n",
            "Batch 108: loss = 0.20557744801044464, acc = 0.923828125\n",
            "Batch 109: loss = 0.2425566464662552, acc = 0.919921875\n",
            "Batch 110: loss = 0.1991228610277176, acc = 0.93359375\n",
            "Batch 111: loss = 0.24408161640167236, acc = 0.9130859375\n",
            "Batch 112: loss = 0.22052600979804993, acc = 0.931640625\n",
            "Batch 113: loss = 0.20204497873783112, acc = 0.9287109375\n",
            "Batch 114: loss = 0.25373801589012146, acc = 0.9189453125\n",
            "Batch 115: loss = 0.25850746035575867, acc = 0.9130859375\n",
            "Batch 116: loss = 0.24119049310684204, acc = 0.916015625\n",
            "Batch 117: loss = 0.23788335919380188, acc = 0.9248046875\n",
            "Batch 118: loss = 0.21999453008174896, acc = 0.9248046875\n",
            "Batch 119: loss = 0.1780584454536438, acc = 0.94140625\n",
            "Batch 120: loss = 0.19258993864059448, acc = 0.9287109375\n",
            "Batch 121: loss = 0.2378436028957367, acc = 0.9189453125\n",
            "Batch 122: loss = 0.20939099788665771, acc = 0.9296875\n",
            "Batch 123: loss = 0.2320965975522995, acc = 0.9130859375\n",
            "Batch 124: loss = 0.2470088005065918, acc = 0.912109375\n",
            "Batch 125: loss = 0.23069389164447784, acc = 0.9228515625\n",
            "Batch 126: loss = 0.23878440260887146, acc = 0.92578125\n",
            "\n",
            "Epoch 99/100\n",
            "Batch 1: loss = 0.2743580639362335, acc = 0.9130859375\n",
            "Batch 2: loss = 0.22022366523742676, acc = 0.93359375\n",
            "Batch 3: loss = 0.25948411226272583, acc = 0.9169921875\n",
            "Batch 4: loss = 0.21738334000110626, acc = 0.9345703125\n",
            "Batch 5: loss = 0.19937926530838013, acc = 0.9296875\n",
            "Batch 6: loss = 0.2546386420726776, acc = 0.9072265625\n",
            "Batch 7: loss = 0.21491394937038422, acc = 0.9248046875\n",
            "Batch 8: loss = 0.25097787380218506, acc = 0.9208984375\n",
            "Batch 9: loss = 0.24471838772296906, acc = 0.9130859375\n",
            "Batch 10: loss = 0.2157047688961029, acc = 0.923828125\n",
            "Batch 11: loss = 0.22044040262699127, acc = 0.9287109375\n",
            "Batch 12: loss = 0.22255533933639526, acc = 0.927734375\n",
            "Batch 13: loss = 0.22325533628463745, acc = 0.9248046875\n",
            "Batch 14: loss = 0.24118490517139435, acc = 0.9248046875\n",
            "Batch 15: loss = 0.20110222697257996, acc = 0.935546875\n",
            "Batch 16: loss = 0.23012466728687286, acc = 0.9248046875\n",
            "Batch 17: loss = 0.20600570738315582, acc = 0.931640625\n",
            "Batch 18: loss = 0.19957074522972107, acc = 0.9384765625\n",
            "Batch 19: loss = 0.25181588530540466, acc = 0.921875\n",
            "Batch 20: loss = 0.2249605655670166, acc = 0.921875\n",
            "Batch 21: loss = 0.2306993156671524, acc = 0.9306640625\n",
            "Batch 22: loss = 0.2265506237745285, acc = 0.919921875\n",
            "Batch 23: loss = 0.23893354833126068, acc = 0.921875\n",
            "Batch 24: loss = 0.22664715349674225, acc = 0.923828125\n",
            "Batch 25: loss = 0.22960467636585236, acc = 0.9296875\n",
            "Batch 26: loss = 0.23219195008277893, acc = 0.9208984375\n",
            "Batch 27: loss = 0.25159308314323425, acc = 0.9111328125\n",
            "Batch 28: loss = 0.19241060316562653, acc = 0.9296875\n",
            "Batch 29: loss = 0.21607333421707153, acc = 0.9296875\n",
            "Batch 30: loss = 0.211872398853302, acc = 0.9296875\n",
            "Batch 31: loss = 0.24205990135669708, acc = 0.916015625\n",
            "Batch 32: loss = 0.22635777294635773, acc = 0.9189453125\n",
            "Batch 33: loss = 0.19702322781085968, acc = 0.9306640625\n",
            "Batch 34: loss = 0.20372828841209412, acc = 0.931640625\n",
            "Batch 35: loss = 0.21951130032539368, acc = 0.9287109375\n",
            "Batch 36: loss = 0.1981060951948166, acc = 0.927734375\n",
            "Batch 37: loss = 0.19540594518184662, acc = 0.939453125\n",
            "Batch 38: loss = 0.21427680552005768, acc = 0.9208984375\n",
            "Batch 39: loss = 0.21441105008125305, acc = 0.927734375\n",
            "Batch 40: loss = 0.20364651083946228, acc = 0.931640625\n",
            "Batch 41: loss = 0.20945505797863007, acc = 0.9287109375\n",
            "Batch 42: loss = 0.21177442371845245, acc = 0.9306640625\n",
            "Batch 43: loss = 0.24229884147644043, acc = 0.9130859375\n",
            "Batch 44: loss = 0.17305316030979156, acc = 0.9453125\n",
            "Batch 45: loss = 0.2078181505203247, acc = 0.9228515625\n",
            "Batch 46: loss = 0.19434475898742676, acc = 0.935546875\n",
            "Batch 47: loss = 0.19828131794929504, acc = 0.931640625\n",
            "Batch 48: loss = 0.18197792768478394, acc = 0.9365234375\n",
            "Batch 49: loss = 0.21648305654525757, acc = 0.92578125\n",
            "Batch 50: loss = 0.19653138518333435, acc = 0.931640625\n",
            "Batch 51: loss = 0.20862653851509094, acc = 0.9228515625\n",
            "Batch 52: loss = 0.21782340109348297, acc = 0.9169921875\n",
            "Batch 53: loss = 0.22979962825775146, acc = 0.9208984375\n",
            "Batch 54: loss = 0.16638334095478058, acc = 0.9423828125\n",
            "Batch 55: loss = 0.18346565961837769, acc = 0.9453125\n",
            "Batch 56: loss = 0.20184437930583954, acc = 0.9365234375\n",
            "Batch 57: loss = 0.24242334067821503, acc = 0.919921875\n",
            "Batch 58: loss = 0.2862139344215393, acc = 0.904296875\n",
            "Batch 59: loss = 0.21597829461097717, acc = 0.9228515625\n",
            "Batch 60: loss = 0.27726662158966064, acc = 0.912109375\n",
            "Batch 61: loss = 0.19001197814941406, acc = 0.9462890625\n",
            "Batch 62: loss = 0.2565045654773712, acc = 0.9150390625\n",
            "Batch 63: loss = 0.20599834620952606, acc = 0.9296875\n",
            "Batch 64: loss = 0.20166601240634918, acc = 0.9248046875\n",
            "Batch 65: loss = 0.21991366147994995, acc = 0.921875\n",
            "Batch 66: loss = 0.246461421251297, acc = 0.912109375\n",
            "Batch 67: loss = 0.20115242898464203, acc = 0.9326171875\n",
            "Batch 68: loss = 0.2541792392730713, acc = 0.9140625\n",
            "Batch 69: loss = 0.2146444469690323, acc = 0.9375\n",
            "Batch 70: loss = 0.21705222129821777, acc = 0.9208984375\n",
            "Batch 71: loss = 0.2152252346277237, acc = 0.92578125\n",
            "Batch 72: loss = 0.2666475474834442, acc = 0.91796875\n",
            "Batch 73: loss = 0.2344391644001007, acc = 0.9140625\n",
            "Batch 74: loss = 0.23816142976284027, acc = 0.9248046875\n",
            "Batch 75: loss = 0.25080013275146484, acc = 0.9169921875\n",
            "Batch 76: loss = 0.21882115304470062, acc = 0.9306640625\n",
            "Batch 77: loss = 0.2291836440563202, acc = 0.9208984375\n",
            "Batch 78: loss = 0.24845385551452637, acc = 0.9150390625\n",
            "Batch 79: loss = 0.22124013304710388, acc = 0.9296875\n",
            "Batch 80: loss = 0.2046799212694168, acc = 0.9248046875\n",
            "Batch 81: loss = 0.24120566248893738, acc = 0.916015625\n",
            "Batch 82: loss = 0.2256893515586853, acc = 0.921875\n",
            "Batch 83: loss = 0.21559514105319977, acc = 0.9296875\n",
            "Batch 84: loss = 0.2171366959810257, acc = 0.91796875\n",
            "Batch 85: loss = 0.22244501113891602, acc = 0.921875\n",
            "Batch 86: loss = 0.21926793456077576, acc = 0.9326171875\n",
            "Batch 87: loss = 0.23944680392742157, acc = 0.9169921875\n",
            "Batch 88: loss = 0.23310528695583344, acc = 0.9189453125\n",
            "Batch 89: loss = 0.21524067223072052, acc = 0.9296875\n",
            "Batch 90: loss = 0.2477474808692932, acc = 0.912109375\n",
            "Batch 91: loss = 0.24140189588069916, acc = 0.9091796875\n",
            "Batch 92: loss = 0.22146981954574585, acc = 0.9248046875\n",
            "Batch 93: loss = 0.2249484807252884, acc = 0.9287109375\n",
            "Batch 94: loss = 0.22726960480213165, acc = 0.923828125\n",
            "Batch 95: loss = 0.2481590062379837, acc = 0.921875\n",
            "Batch 96: loss = 0.2743418216705322, acc = 0.9013671875\n",
            "Batch 97: loss = 0.2539023160934448, acc = 0.919921875\n",
            "Batch 98: loss = 0.2397173047065735, acc = 0.91015625\n",
            "Batch 99: loss = 0.25452888011932373, acc = 0.9091796875\n",
            "Batch 100: loss = 0.2510265111923218, acc = 0.9150390625\n",
            "Batch 101: loss = 0.221844881772995, acc = 0.9296875\n",
            "Batch 102: loss = 0.26432955265045166, acc = 0.896484375\n",
            "Batch 103: loss = 0.23015426099300385, acc = 0.927734375\n",
            "Batch 104: loss = 0.23798491060733795, acc = 0.912109375\n",
            "Batch 105: loss = 0.21788299083709717, acc = 0.9267578125\n",
            "Batch 106: loss = 0.2136807143688202, acc = 0.9296875\n",
            "Batch 107: loss = 0.2145015448331833, acc = 0.9296875\n",
            "Batch 108: loss = 0.23089678585529327, acc = 0.9150390625\n",
            "Batch 109: loss = 0.2231968194246292, acc = 0.9228515625\n",
            "Batch 110: loss = 0.22565822303295135, acc = 0.9248046875\n",
            "Batch 111: loss = 0.2113415151834488, acc = 0.94140625\n",
            "Batch 112: loss = 0.1938948780298233, acc = 0.9384765625\n",
            "Batch 113: loss = 0.23017051815986633, acc = 0.921875\n",
            "Batch 114: loss = 0.21180205047130585, acc = 0.9326171875\n",
            "Batch 115: loss = 0.2324228286743164, acc = 0.9248046875\n",
            "Batch 116: loss = 0.2091379314661026, acc = 0.9208984375\n",
            "Batch 117: loss = 0.21541041135787964, acc = 0.9296875\n",
            "Batch 118: loss = 0.19888004660606384, acc = 0.92578125\n",
            "Batch 119: loss = 0.17280007898807526, acc = 0.951171875\n",
            "Batch 120: loss = 0.19153934717178345, acc = 0.927734375\n",
            "Batch 121: loss = 0.186149463057518, acc = 0.9404296875\n",
            "Batch 122: loss = 0.2137906551361084, acc = 0.9208984375\n",
            "Batch 123: loss = 0.22564677894115448, acc = 0.9248046875\n",
            "Batch 124: loss = 0.2353266328573227, acc = 0.9150390625\n",
            "Batch 125: loss = 0.2259545624256134, acc = 0.921875\n",
            "Batch 126: loss = 0.21682478487491608, acc = 0.9189453125\n",
            "\n",
            "Epoch 100/100\n",
            "Batch 1: loss = 0.24791279435157776, acc = 0.9228515625\n",
            "Batch 2: loss = 0.23604224622249603, acc = 0.916015625\n",
            "Batch 3: loss = 0.24654249846935272, acc = 0.9140625\n",
            "Batch 4: loss = 0.20638126134872437, acc = 0.9296875\n",
            "Batch 5: loss = 0.22970321774482727, acc = 0.9267578125\n",
            "Batch 6: loss = 0.2514934539794922, acc = 0.912109375\n",
            "Batch 7: loss = 0.20297543704509735, acc = 0.9306640625\n",
            "Batch 8: loss = 0.23066969215869904, acc = 0.921875\n",
            "Batch 9: loss = 0.20332354307174683, acc = 0.9326171875\n",
            "Batch 10: loss = 0.21651175618171692, acc = 0.931640625\n",
            "Batch 11: loss = 0.22155646979808807, acc = 0.9248046875\n",
            "Batch 12: loss = 0.22419850528240204, acc = 0.93359375\n",
            "Batch 13: loss = 0.22090813517570496, acc = 0.9248046875\n",
            "Batch 14: loss = 0.22165748476982117, acc = 0.9306640625\n",
            "Batch 15: loss = 0.21783126890659332, acc = 0.9296875\n",
            "Batch 16: loss = 0.2027842402458191, acc = 0.927734375\n",
            "Batch 17: loss = 0.21454192698001862, acc = 0.9296875\n",
            "Batch 18: loss = 0.21335701644420624, acc = 0.931640625\n",
            "Batch 19: loss = 0.2108752280473709, acc = 0.93359375\n",
            "Batch 20: loss = 0.2206527441740036, acc = 0.92578125\n",
            "Batch 21: loss = 0.26850175857543945, acc = 0.9189453125\n",
            "Batch 22: loss = 0.23834504187107086, acc = 0.9169921875\n",
            "Batch 23: loss = 0.22332771122455597, acc = 0.91796875\n",
            "Batch 24: loss = 0.23418334126472473, acc = 0.9150390625\n",
            "Batch 25: loss = 0.24579280614852905, acc = 0.9169921875\n",
            "Batch 26: loss = 0.2352665364742279, acc = 0.9189453125\n",
            "Batch 27: loss = 0.25930702686309814, acc = 0.9150390625\n",
            "Batch 28: loss = 0.21111176908016205, acc = 0.9296875\n",
            "Batch 29: loss = 0.22989442944526672, acc = 0.9248046875\n",
            "Batch 30: loss = 0.2316073179244995, acc = 0.9208984375\n",
            "Batch 31: loss = 0.23415425419807434, acc = 0.9228515625\n",
            "Batch 32: loss = 0.23391327261924744, acc = 0.9287109375\n",
            "Batch 33: loss = 0.21754296123981476, acc = 0.921875\n",
            "Batch 34: loss = 0.21301177144050598, acc = 0.9228515625\n",
            "Batch 35: loss = 0.1865982711315155, acc = 0.9345703125\n",
            "Batch 36: loss = 0.21893826127052307, acc = 0.92578125\n",
            "Batch 37: loss = 0.20342497527599335, acc = 0.9384765625\n",
            "Batch 38: loss = 0.19876335561275482, acc = 0.9404296875\n",
            "Batch 39: loss = 0.19917497038841248, acc = 0.9365234375\n",
            "Batch 40: loss = 0.210209459066391, acc = 0.9248046875\n",
            "Batch 41: loss = 0.21717506647109985, acc = 0.9326171875\n",
            "Batch 42: loss = 0.2108863890171051, acc = 0.92578125\n",
            "Batch 43: loss = 0.25009846687316895, acc = 0.9052734375\n",
            "Batch 44: loss = 0.2098766565322876, acc = 0.92578125\n",
            "Batch 45: loss = 0.2131185233592987, acc = 0.927734375\n",
            "Batch 46: loss = 0.18800047039985657, acc = 0.923828125\n",
            "Batch 47: loss = 0.22157219052314758, acc = 0.9306640625\n",
            "Batch 48: loss = 0.2072499394416809, acc = 0.9306640625\n",
            "Batch 49: loss = 0.23216617107391357, acc = 0.931640625\n",
            "Batch 50: loss = 0.18837177753448486, acc = 0.94140625\n",
            "Batch 51: loss = 0.21576672792434692, acc = 0.923828125\n",
            "Batch 52: loss = 0.24198085069656372, acc = 0.9228515625\n",
            "Batch 53: loss = 0.22255076467990875, acc = 0.91796875\n",
            "Batch 54: loss = 0.2251974642276764, acc = 0.92578125\n",
            "Batch 55: loss = 0.23507088422775269, acc = 0.923828125\n",
            "Batch 56: loss = 0.20304517447948456, acc = 0.9345703125\n",
            "Batch 57: loss = 0.19943572580814362, acc = 0.93359375\n",
            "Batch 58: loss = 0.25713682174682617, acc = 0.916015625\n",
            "Batch 59: loss = 0.1898977905511856, acc = 0.94140625\n",
            "Batch 60: loss = 0.21745723485946655, acc = 0.9267578125\n",
            "Batch 61: loss = 0.24008004367351532, acc = 0.923828125\n",
            "Batch 62: loss = 0.2074539065361023, acc = 0.927734375\n",
            "Batch 63: loss = 0.21139803528785706, acc = 0.93359375\n",
            "Batch 64: loss = 0.2101912498474121, acc = 0.9248046875\n",
            "Batch 65: loss = 0.2258748710155487, acc = 0.9267578125\n",
            "Batch 66: loss = 0.23748257756233215, acc = 0.9091796875\n",
            "Batch 67: loss = 0.24304215610027313, acc = 0.9169921875\n",
            "Batch 68: loss = 0.21569956839084625, acc = 0.9306640625\n",
            "Batch 69: loss = 0.20255278050899506, acc = 0.9365234375\n",
            "Batch 70: loss = 0.2451881617307663, acc = 0.9208984375\n",
            "Batch 71: loss = 0.20901313424110413, acc = 0.92578125\n",
            "Batch 72: loss = 0.21263110637664795, acc = 0.9296875\n",
            "Batch 73: loss = 0.2151937633752823, acc = 0.923828125\n",
            "Batch 74: loss = 0.2671261429786682, acc = 0.9140625\n",
            "Batch 75: loss = 0.2601214051246643, acc = 0.90625\n",
            "Batch 76: loss = 0.23782528936862946, acc = 0.9140625\n",
            "Batch 77: loss = 0.19009995460510254, acc = 0.9384765625\n",
            "Batch 78: loss = 0.222572460770607, acc = 0.92578125\n",
            "Batch 79: loss = 0.23488374054431915, acc = 0.9228515625\n",
            "Batch 80: loss = 0.195586696267128, acc = 0.9326171875\n",
            "Batch 81: loss = 0.26467597484588623, acc = 0.9091796875\n",
            "Batch 82: loss = 0.25525015592575073, acc = 0.9169921875\n",
            "Batch 83: loss = 0.20000822842121124, acc = 0.9365234375\n",
            "Batch 84: loss = 0.2241787612438202, acc = 0.9208984375\n",
            "Batch 85: loss = 0.23182646930217743, acc = 0.927734375\n",
            "Batch 86: loss = 0.21057307720184326, acc = 0.9404296875\n",
            "Batch 87: loss = 0.25334927439689636, acc = 0.90625\n",
            "Batch 88: loss = 0.2551487386226654, acc = 0.9248046875\n",
            "Batch 89: loss = 0.23535846173763275, acc = 0.9228515625\n",
            "Batch 90: loss = 0.23511341214179993, acc = 0.9208984375\n",
            "Batch 91: loss = 0.21655304729938507, acc = 0.9267578125\n",
            "Batch 92: loss = 0.23602518439292908, acc = 0.91796875\n",
            "Batch 93: loss = 0.2041279822587967, acc = 0.9306640625\n",
            "Batch 94: loss = 0.2045731246471405, acc = 0.9326171875\n",
            "Batch 95: loss = 0.22977329790592194, acc = 0.9169921875\n",
            "Batch 96: loss = 0.2491982877254486, acc = 0.9169921875\n",
            "Batch 97: loss = 0.2482185810804367, acc = 0.927734375\n",
            "Batch 98: loss = 0.20977726578712463, acc = 0.9306640625\n",
            "Batch 99: loss = 0.2236192226409912, acc = 0.91796875\n",
            "Batch 100: loss = 0.23962074518203735, acc = 0.91015625\n",
            "Batch 101: loss = 0.2689090073108673, acc = 0.90625\n",
            "Batch 102: loss = 0.24350157380104065, acc = 0.916015625\n",
            "Batch 103: loss = 0.26062750816345215, acc = 0.9052734375\n",
            "Batch 104: loss = 0.2138616293668747, acc = 0.9296875\n",
            "Batch 105: loss = 0.20296426117420197, acc = 0.9326171875\n",
            "Batch 106: loss = 0.2114555835723877, acc = 0.9228515625\n",
            "Batch 107: loss = 0.23981669545173645, acc = 0.904296875\n",
            "Batch 108: loss = 0.19392050802707672, acc = 0.9326171875\n",
            "Batch 109: loss = 0.22253219783306122, acc = 0.921875\n",
            "Batch 110: loss = 0.19983118772506714, acc = 0.935546875\n",
            "Batch 111: loss = 0.24016743898391724, acc = 0.9248046875\n",
            "Batch 112: loss = 0.1895814836025238, acc = 0.9375\n",
            "Batch 113: loss = 0.2303670197725296, acc = 0.916015625\n",
            "Batch 114: loss = 0.22187498211860657, acc = 0.931640625\n",
            "Batch 115: loss = 0.252873033285141, acc = 0.9091796875\n",
            "Batch 116: loss = 0.23932704329490662, acc = 0.921875\n",
            "Batch 117: loss = 0.2295735627412796, acc = 0.9228515625\n",
            "Batch 118: loss = 0.19234023988246918, acc = 0.9345703125\n",
            "Batch 119: loss = 0.17494508624076843, acc = 0.9365234375\n",
            "Batch 120: loss = 0.18214601278305054, acc = 0.9404296875\n",
            "Batch 121: loss = 0.22062240540981293, acc = 0.921875\n",
            "Batch 122: loss = 0.2158079594373703, acc = 0.9287109375\n",
            "Batch 123: loss = 0.2045358121395111, acc = 0.9287109375\n",
            "Batch 124: loss = 0.2514726221561432, acc = 0.9150390625\n",
            "Batch 125: loss = 0.2636283338069916, acc = 0.9169921875\n",
            "Batch 126: loss = 0.22596287727355957, acc = 0.9189453125\n",
            "Saved checkpoint to weights.100.h5\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (16, 64, 512)             44032     \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (16, 64, 256)             787456    \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (16, 64, 256)             0         \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (16, 64, 256)             525312    \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (16, 64, 256)             0         \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (16, 64, 256)             525312    \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (16, 64, 256)             0         \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (16, 64, 86)              22102     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (16, 64, 86)              0         \n",
            "=================================================================\n",
            "Total params: 1,904,214\n",
            "Trainable params: 1,904,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8rU-6iKSpBQ"
      },
      "source": [
        "## Training Loss & Accuracy\n",
        "### We can find the epoch number from these graphs to choose the optimal weights to give as an input for sample sequence generator model"
      ],
      "id": "N8rU-6iKSpBQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "ahead-candy",
        "outputId": "99bac652-819d-44df-af6d-67a11b0c55c9"
      },
      "source": [
        "plt.plot(Train_epoch_loss_acc['Losses'])\n",
        "plt.title('LSTM Model train')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train_loss'], loc='upper right')"
      ],
      "id": "ahead-candy",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f061c0aa8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "insured-plumbing",
        "outputId": "dbc77ed0-263c-4b07-fe23-7f54545c3cc0"
      },
      "source": [
        "plt.plot(Train_epoch_loss_acc['Accuracy'])\n",
        "plt.title('LSTM Model train')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train_accuracy'], loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "id": "insured-plumbing",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR70FJsHS7U6"
      },
      "source": [
        "## Model Output\n",
        "### Please enter input values for epoch number, starting character index, sequence length"
      ],
      "id": "bR70FJsHS7U6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "similar-interference",
        "outputId": "32784d1b-c3e4-4613-b19d-24ad26467bb7"
      },
      "source": [
        "epoch_number = int(input(\"Enter epoch number between 1 to 100 only multiples of 10: \"))\n",
        "character_index = int(input(\"Enter any number between 0 to 86 to sequence generation: \"))\n",
        "Sequence_length = int(input(\"Number of characters to generate: \"))\n",
        "\n",
        "Generated_music_sequence = sample_seq_generator(epoch_number, character_index, Sequence_length)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(Generated_music_sequence)"
      ],
      "id": "similar-interference",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter epoch number between 1 to 100 only multiples of 10: 100\n",
            "Enter any number between 0 to 86 to sequence generation: 1\n",
            "Number of characters to generate: 600\n",
            "\n",
            "\n",
            " Dogs) Wide\n",
            "% Nottingham Music Database\n",
            "S:Trad, via EF\n",
            "M:6/8\n",
            "K:D\n",
            "\"D\"F3 \"A7/e\"FED|\"D/f+\"FAA A2d|\"G\"BdB \"D/f+\"AFD|\"G\"DFG \"A7\"EFG|\"D\"FDD D2|\n",
            "\n",
            "\n",
            "X: 277\n",
            "T:Scramble Pilcring\n",
            "% Nottingham Music Database\n",
            "S:Trad, arr Phil Rowe\n",
            "M:6/8\n",
            "K:D\n",
            "A|\"D\"d2d \"A\"edc|\"D\"d2A F2A|\"G\"B2c dBG|\"D\"A2A \"A7\"GFE|\n",
            "\"D\"DEF \"A\"EFG|\"D\"F2D D2d|\"A7\"cdc cBA|\"D\"d2A F2A|\"G\"B2c dcB|\"A7\"ABG \"D\"F2E|\n",
            "\"D\"DFA d2A|\"G\"B2A Bcd|\"C\"e2G \"G\"d2:|\n",
            "P:B\n",
            "d|\"D\"AFD DFA|\"D\"d^cd \"A\"efe|\"D\"edc \"G\"B2A|\"D\"A3 -AFA|\"G\"B2B BAG|\n",
            "\"D\"A2A AFA|\"G\"G2A Bdc|\"D\"A3 -AFA|\"G\"B2B Bcd|\n",
            "\"D\"A2d f2a|\"A\"f2e c2e|\"D\"f2d \"G\"gfe|\"D\"f2d \"A7\"edc|\"D\"d3 -d2:|\n",
            "A|\"D\"f2d faf|\"D\"d2f afd|\"G\"d2e \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78MOWEjzTGPu"
      },
      "source": [
        "## Model Architecure visualization"
      ],
      "id": "78MOWEjzTGPu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "01BEjgQuoqkW",
        "outputId": "fc212dd0-c89e-4c28-a30f-4cfc018ee8b8"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(86, 512, batch_input_shape=(16, 64)))\n",
        "for i in range(3):\n",
        "  model.add(LSTM(256, return_sequences=True, stateful=True))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "model.add(TimeDistributed(Dense(86))) \n",
        "model.add(Activation('softmax'))\n",
        "plot_model(model, to_file='LSTM_model.png',show_shapes=True, show_layer_names=True)"
      ],
      "id": "01BEjgQuoqkW",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxuIDCZyaomH"
      },
      "source": [
        "**--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "id": "gxuIDCZyaomH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adapted-modeling"
      },
      "source": [
        "# Additional work - Expermineting with various architectures"
      ],
      "id": "adapted-modeling"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhYXfHBQKDxG"
      },
      "source": [
        "# GRU - Multi stack model"
      ],
      "id": "xhYXfHBQKDxG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSSOKUqgJHeA"
      },
      "source": [
        "## Model architecture"
      ],
      "id": "SSSOKUqgJHeA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsuLqSp2pcRe"
      },
      "source": [
        "MODEL_DIR = '/content/drive/My Drive/Deep_Learning_Project/model/GRU_iteration1'"
      ],
      "id": "EsuLqSp2pcRe",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "internal-annotation"
      },
      "source": [
        "def save_weights(epoch, model):\n",
        "    if not os.path.exists(MODEL_DIR):\n",
        "        os.makedirs(MODEL_DIR)\n",
        "    model.save_weights(os.path.join(MODEL_DIR, 'gru_weights.{}.h5'.format(epoch)))\n",
        "\n",
        "def load_weights(epoch, model):\n",
        "    model.load_weights(os.path.join(MODEL_DIR, 'gru_weights.{}.h5'.format(epoch)))\n",
        "\n",
        "def build_model_gru(batch_size, seq_len, vocab_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 512, batch_input_shape=(batch_size, seq_len)))\n",
        "    for i in range(3):\n",
        "        model.add(GRU(256, return_sequences=True, stateful=True))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(TimeDistributed(Dense(vocab_size))) \n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ],
      "id": "internal-annotation",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "educational-lemon"
      },
      "source": [
        "## GRU model training function"
      ],
      "id": "educational-lemon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liked-inflation"
      },
      "source": [
        "#### text has the data pressent in input.txt file, # of epochs is 100, after every 10 epochs we are saving the model\n",
        "def train_gru(text, epochs=100, save_freq=10):\n",
        "\n",
        "    # Sorting the characters in the text file and assigning index numbers to each character. \n",
        "    # char_to_idx will be a dictionary of unique characters as key and index number is the value pair of dictionary\n",
        "    # To summarize we are comverting each character to a numerical index\n",
        "    \n",
        "    char_to_idx = { ch: i for (i, ch) in enumerate(sorted(list(set(text)))) }\n",
        "    print(\"Number of unique characters: \" + str(len(char_to_idx))) #86\n",
        "\n",
        "    ## Saving the char_to_idx to a json file\n",
        "    with open(os.path.join(DATA_DIR, 'char_to_idx.json'), 'w') as f:\n",
        "        json.dump(char_to_idx, f)\n",
        "\n",
        "    #3 Here we are creating index to character mapping, i.e. given an index we want to get the character for that index\n",
        "    idx_to_char = { i: ch for (ch, i) in char_to_idx.items() }\n",
        "    vocab_size = len(char_to_idx)\n",
        "\n",
        "    ######################################\n",
        "    ######### Model architecture #########\n",
        "    ######################################\n",
        "    \n",
        "    model = build_model_gru(BATCH_SIZE, SEQ_LENGTH, vocab_size)\n",
        "    model.summary()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    ###################################\n",
        "    ###### Train data generation ######\n",
        "    ###################################\n",
        "    \n",
        "    #convert complete text into numerical indices\n",
        "    T = np.asarray([char_to_idx[c] for c in text], dtype=np.int64)\n",
        "\n",
        "    print(\"Length of text:\" + str(T.size)) #129,665\n",
        "\n",
        "    steps_per_epoch = (len(text) / BATCH_SIZE - 1) / SEQ_LENGTH  \n",
        "    \n",
        "    epc,losses, accs = [], [], []\n",
        "    ### This for loop will run for 100 epochs\n",
        "    for epoch in range(epochs):\n",
        "        print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
        "        \n",
        "        \n",
        "\n",
        "        # For each epoch it will generate a batch of X , Y values. For each batch we will train the model. \n",
        "        for i, (X, Y) in enumerate(read_batches(T, vocab_size)):\n",
        "            \n",
        "            #print(X);\n",
        "            \n",
        "            ## Details about train_on_batch here: https://keras.io/models/sequential/\n",
        "            loss, acc = model.train_on_batch(X, Y)\n",
        "            print('Batch {}: loss = {}, acc = {}'.format(i + 1, loss, acc))\n",
        "            epc.append(epoch + 1)\n",
        "            losses.append(loss)\n",
        "            accs.append(acc)\n",
        "        \n",
        "      \n",
        "        # Saving the model after every 10 epochs\n",
        "        if (epoch + 1) % save_freq == 0:\n",
        "            save_weights(epoch + 1, model)\n",
        "            print('Saved checkpoint to', 'gru_weights.{}.h5'.format(epoch + 1))\n",
        "    \n",
        "    df1 = pd.DataFrame(epc)\n",
        "    df2 = pd.DataFrame(losses)\n",
        "    df3 = pd.DataFrame(accs)\n",
        "    frames = [df1,df2, df3]\n",
        "\n",
        "    result = pd.concat(frames, axis =1)\n",
        "    result.columns=['Epoch','Losses','Accuracy']\n",
        "\n",
        "    grouped_multiple = result.groupby(['Epoch'], as_index=False).agg({'Losses': 'mean'\n",
        "                                              ,'Accuracy':'mean'})\n",
        "    \n",
        "    return grouped_multiple\n"
      ],
      "id": "liked-inflation",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "applied-burst"
      },
      "source": [
        "## Code for building a GRU model to generate sample sequence of characters"
      ],
      "id": "applied-burst"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "closed-jacket"
      },
      "source": [
        "def build_model_seq_gen_gru(unique_chars):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(unique_chars, 512, batch_input_shape=(1, 1)))\n",
        "    for i in range(3):\n",
        "        model.add(GRU(256, return_sequences=(i != 2), stateful=True))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(unique_chars))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ],
      "id": "closed-jacket",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1YTaSr4KUEs"
      },
      "source": [
        "## Sample sequence generator"
      ],
      "id": "C1YTaSr4KUEs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "professional-eating"
      },
      "source": [
        "def sample_seq_generator_gru(epoch_num, character_index, seq_length):\n",
        "    with open(os.path.join(DATA_DIR, 'char_to_idx.json')) as f:\n",
        "        char_to_index = json.load(f)\n",
        "    index_to_char = {i:ch for ch, i in char_to_index.items()}\n",
        "    vocab_size = len(index_to_char)\n",
        "\n",
        "    ##########################################################\n",
        "    ######### Sequence generator model architecture  #########\n",
        "    ##########################################################\n",
        "    \n",
        "    model = build_model_seq_gen_gru(vocab_size)\n",
        "    model.load_weights(MODEL_DIR + '/gru_weights.{}.h5'.format(epoch_num))\n",
        "     \n",
        "    sequence_index = [character_index]\n",
        "    \n",
        "    for _ in range(seq_length):\n",
        "        batch = np.zeros((1, 1))\n",
        "        batch[0, 0] = sequence_index[-1]\n",
        "        \n",
        "        predicted_probs = model.predict(batch).reshape(-1)\n",
        "        #print(predicted_probs)\n",
        "        sample = np.random.choice(range(vocab_size), size = 1, p = predicted_probs)\n",
        "        \n",
        "        sequence_index.append(sample[0])\n",
        "    \n",
        "    seq = ''.join(index_to_char[c] for c in sequence_index)\n",
        "    return seq"
      ],
      "id": "professional-eating",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjRrvka0MVX8"
      },
      "source": [
        "## Calling Train function"
      ],
      "id": "AjRrvka0MVX8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fatty-front",
        "outputId": "ce277a9e-8c8a-4ed2-97e1-783a0de1ea77"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    epochs = 100\n",
        "    freq = 10\n",
        "    \n",
        "    ### Calling the train function to read the data from input.txt file \n",
        "    Train_epoch_loss_acc_gru = train_gru(open(os.path.join(DATA_DIR, filename)).read(), epochs, freq)"
      ],
      "id": "fatty-front",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch 123: loss = 0.4293166399002075, acc = 0.8603515625\n",
            "Batch 124: loss = 0.46637892723083496, acc = 0.8447265625\n",
            "Batch 125: loss = 0.42210620641708374, acc = 0.859375\n",
            "Batch 126: loss = 0.4782896935939789, acc = 0.849609375\n",
            "\n",
            "Epoch 62/100\n",
            "Batch 1: loss = 0.49956852197647095, acc = 0.8408203125\n",
            "Batch 2: loss = 0.463256299495697, acc = 0.845703125\n",
            "Batch 3: loss = 0.4390028119087219, acc = 0.8642578125\n",
            "Batch 4: loss = 0.40432220697402954, acc = 0.8740234375\n",
            "Batch 5: loss = 0.4261327385902405, acc = 0.8564453125\n",
            "Batch 6: loss = 0.47348666191101074, acc = 0.8427734375\n",
            "Batch 7: loss = 0.4268985986709595, acc = 0.8486328125\n",
            "Batch 8: loss = 0.44006621837615967, acc = 0.8466796875\n",
            "Batch 9: loss = 0.41902056336402893, acc = 0.8671875\n",
            "Batch 10: loss = 0.3559955656528473, acc = 0.8818359375\n",
            "Batch 11: loss = 0.3810996413230896, acc = 0.8671875\n",
            "Batch 12: loss = 0.42374372482299805, acc = 0.865234375\n",
            "Batch 13: loss = 0.42782360315322876, acc = 0.85546875\n",
            "Batch 14: loss = 0.4038945436477661, acc = 0.8671875\n",
            "Batch 15: loss = 0.3886522352695465, acc = 0.8623046875\n",
            "Batch 16: loss = 0.43698441982269287, acc = 0.861328125\n",
            "Batch 17: loss = 0.4354693293571472, acc = 0.857421875\n",
            "Batch 18: loss = 0.4485478103160858, acc = 0.84765625\n",
            "Batch 19: loss = 0.3978043794631958, acc = 0.87109375\n",
            "Batch 20: loss = 0.40887945890426636, acc = 0.8623046875\n",
            "Batch 21: loss = 0.45651108026504517, acc = 0.849609375\n",
            "Batch 22: loss = 0.4302482604980469, acc = 0.8447265625\n",
            "Batch 23: loss = 0.43546539545059204, acc = 0.8564453125\n",
            "Batch 24: loss = 0.4457184672355652, acc = 0.837890625\n",
            "Batch 25: loss = 0.4063642919063568, acc = 0.8662109375\n",
            "Batch 26: loss = 0.39961323142051697, acc = 0.8525390625\n",
            "Batch 27: loss = 0.45185965299606323, acc = 0.8486328125\n",
            "Batch 28: loss = 0.4773191809654236, acc = 0.828125\n",
            "Batch 29: loss = 0.43005505204200745, acc = 0.845703125\n",
            "Batch 30: loss = 0.4048027992248535, acc = 0.8642578125\n",
            "Batch 31: loss = 0.43486279249191284, acc = 0.84375\n",
            "Batch 32: loss = 0.4693271219730377, acc = 0.84765625\n",
            "Batch 33: loss = 0.4292597770690918, acc = 0.8603515625\n",
            "Batch 34: loss = 0.4354923367500305, acc = 0.8564453125\n",
            "Batch 35: loss = 0.453864723443985, acc = 0.84765625\n",
            "Batch 36: loss = 0.3997218608856201, acc = 0.8740234375\n",
            "Batch 37: loss = 0.392197847366333, acc = 0.8798828125\n",
            "Batch 38: loss = 0.41877931356430054, acc = 0.8486328125\n",
            "Batch 39: loss = 0.4075336754322052, acc = 0.8681640625\n",
            "Batch 40: loss = 0.44789162278175354, acc = 0.849609375\n",
            "Batch 41: loss = 0.3707658350467682, acc = 0.873046875\n",
            "Batch 42: loss = 0.40049493312835693, acc = 0.87890625\n",
            "Batch 43: loss = 0.47190916538238525, acc = 0.841796875\n",
            "Batch 44: loss = 0.42860856652259827, acc = 0.859375\n",
            "Batch 45: loss = 0.3821345567703247, acc = 0.8798828125\n",
            "Batch 46: loss = 0.3828011453151703, acc = 0.865234375\n",
            "Batch 47: loss = 0.3837444484233856, acc = 0.876953125\n",
            "Batch 48: loss = 0.40505895018577576, acc = 0.87109375\n",
            "Batch 49: loss = 0.3866976499557495, acc = 0.8720703125\n",
            "Batch 50: loss = 0.3759230375289917, acc = 0.876953125\n",
            "Batch 51: loss = 0.44213348627090454, acc = 0.859375\n",
            "Batch 52: loss = 0.39136803150177, acc = 0.8603515625\n",
            "Batch 53: loss = 0.4185261130332947, acc = 0.8583984375\n",
            "Batch 54: loss = 0.3250610828399658, acc = 0.8876953125\n",
            "Batch 55: loss = 0.3928031921386719, acc = 0.865234375\n",
            "Batch 56: loss = 0.42951786518096924, acc = 0.84375\n",
            "Batch 57: loss = 0.47915688157081604, acc = 0.8466796875\n",
            "Batch 58: loss = 0.43577510118484497, acc = 0.8466796875\n",
            "Batch 59: loss = 0.3548922836780548, acc = 0.8896484375\n",
            "Batch 60: loss = 0.4536346197128296, acc = 0.8408203125\n",
            "Batch 61: loss = 0.3972747325897217, acc = 0.8759765625\n",
            "Batch 62: loss = 0.4677085876464844, acc = 0.8349609375\n",
            "Batch 63: loss = 0.41550305485725403, acc = 0.87109375\n",
            "Batch 64: loss = 0.3832850754261017, acc = 0.875\n",
            "Batch 65: loss = 0.3938581049442291, acc = 0.8662109375\n",
            "Batch 66: loss = 0.41896677017211914, acc = 0.8603515625\n",
            "Batch 67: loss = 0.42901232838630676, acc = 0.85546875\n",
            "Batch 68: loss = 0.396597295999527, acc = 0.861328125\n",
            "Batch 69: loss = 0.3519572615623474, acc = 0.8896484375\n",
            "Batch 70: loss = 0.4628698229789734, acc = 0.84375\n",
            "Batch 71: loss = 0.4477524757385254, acc = 0.845703125\n",
            "Batch 72: loss = 0.40857642889022827, acc = 0.8701171875\n",
            "Batch 73: loss = 0.4220803380012512, acc = 0.8720703125\n",
            "Batch 74: loss = 0.5001325607299805, acc = 0.8359375\n",
            "Batch 75: loss = 0.481406033039093, acc = 0.84765625\n",
            "Batch 76: loss = 0.4213380217552185, acc = 0.8544921875\n",
            "Batch 77: loss = 0.4053089916706085, acc = 0.8662109375\n",
            "Batch 78: loss = 0.45172929763793945, acc = 0.8515625\n",
            "Batch 79: loss = 0.3989466428756714, acc = 0.8515625\n",
            "Batch 80: loss = 0.3831248879432678, acc = 0.8662109375\n",
            "Batch 81: loss = 0.4200879633426666, acc = 0.8515625\n",
            "Batch 82: loss = 0.3820262551307678, acc = 0.8701171875\n",
            "Batch 83: loss = 0.4141901731491089, acc = 0.853515625\n",
            "Batch 84: loss = 0.4305277466773987, acc = 0.8427734375\n",
            "Batch 85: loss = 0.4652555286884308, acc = 0.8330078125\n",
            "Batch 86: loss = 0.42564913630485535, acc = 0.859375\n",
            "Batch 87: loss = 0.3989449143409729, acc = 0.859375\n",
            "Batch 88: loss = 0.48763325810432434, acc = 0.8291015625\n",
            "Batch 89: loss = 0.42226484417915344, acc = 0.8564453125\n",
            "Batch 90: loss = 0.432960569858551, acc = 0.849609375\n",
            "Batch 91: loss = 0.472462922334671, acc = 0.833984375\n",
            "Batch 92: loss = 0.4738716781139374, acc = 0.841796875\n",
            "Batch 93: loss = 0.4170061945915222, acc = 0.857421875\n",
            "Batch 94: loss = 0.408917099237442, acc = 0.8681640625\n",
            "Batch 95: loss = 0.41157951951026917, acc = 0.865234375\n",
            "Batch 96: loss = 0.47325843572616577, acc = 0.8349609375\n",
            "Batch 97: loss = 0.45149216055870056, acc = 0.8505859375\n",
            "Batch 98: loss = 0.4324336051940918, acc = 0.8564453125\n",
            "Batch 99: loss = 0.44148969650268555, acc = 0.8505859375\n",
            "Batch 100: loss = 0.4105219841003418, acc = 0.8525390625\n",
            "Batch 101: loss = 0.41271597146987915, acc = 0.857421875\n",
            "Batch 102: loss = 0.49295204877853394, acc = 0.833984375\n",
            "Batch 103: loss = 0.4375287890434265, acc = 0.8642578125\n",
            "Batch 104: loss = 0.3871347904205322, acc = 0.85546875\n",
            "Batch 105: loss = 0.352067232131958, acc = 0.880859375\n",
            "Batch 106: loss = 0.39656832814216614, acc = 0.8681640625\n",
            "Batch 107: loss = 0.42431071400642395, acc = 0.857421875\n",
            "Batch 108: loss = 0.4398016333580017, acc = 0.8564453125\n",
            "Batch 109: loss = 0.4220358729362488, acc = 0.8505859375\n",
            "Batch 110: loss = 0.39107874035835266, acc = 0.8798828125\n",
            "Batch 111: loss = 0.44047269225120544, acc = 0.8447265625\n",
            "Batch 112: loss = 0.4359663426876068, acc = 0.8564453125\n",
            "Batch 113: loss = 0.42768844962120056, acc = 0.8564453125\n",
            "Batch 114: loss = 0.43872418999671936, acc = 0.8505859375\n",
            "Batch 115: loss = 0.44776758551597595, acc = 0.8486328125\n",
            "Batch 116: loss = 0.45868992805480957, acc = 0.849609375\n",
            "Batch 117: loss = 0.4047919511795044, acc = 0.86328125\n",
            "Batch 118: loss = 0.3764246702194214, acc = 0.8681640625\n",
            "Batch 119: loss = 0.4321396052837372, acc = 0.8515625\n",
            "Batch 120: loss = 0.3999096155166626, acc = 0.8642578125\n",
            "Batch 121: loss = 0.39708438515663147, acc = 0.8642578125\n",
            "Batch 122: loss = 0.41759735345840454, acc = 0.8701171875\n",
            "Batch 123: loss = 0.4094398021697998, acc = 0.8681640625\n",
            "Batch 124: loss = 0.44576242566108704, acc = 0.8486328125\n",
            "Batch 125: loss = 0.4143531322479248, acc = 0.85546875\n",
            "Batch 126: loss = 0.4538067877292633, acc = 0.849609375\n",
            "\n",
            "Epoch 63/100\n",
            "Batch 1: loss = 0.5763956904411316, acc = 0.822265625\n",
            "Batch 2: loss = 0.45523664355278015, acc = 0.8408203125\n",
            "Batch 3: loss = 0.43673330545425415, acc = 0.8564453125\n",
            "Batch 4: loss = 0.36801865696907043, acc = 0.884765625\n",
            "Batch 5: loss = 0.4303830564022064, acc = 0.8466796875\n",
            "Batch 6: loss = 0.48095884919166565, acc = 0.8544921875\n",
            "Batch 7: loss = 0.4286288321018219, acc = 0.861328125\n",
            "Batch 8: loss = 0.4515988230705261, acc = 0.853515625\n",
            "Batch 9: loss = 0.43697845935821533, acc = 0.857421875\n",
            "Batch 10: loss = 0.34383153915405273, acc = 0.8818359375\n",
            "Batch 11: loss = 0.4169062674045563, acc = 0.8515625\n",
            "Batch 12: loss = 0.44455283880233765, acc = 0.8447265625\n",
            "Batch 13: loss = 0.4013254940509796, acc = 0.8681640625\n",
            "Batch 14: loss = 0.3821236789226532, acc = 0.8662109375\n",
            "Batch 15: loss = 0.3755553066730499, acc = 0.8701171875\n",
            "Batch 16: loss = 0.4457017779350281, acc = 0.859375\n",
            "Batch 17: loss = 0.3765231966972351, acc = 0.8798828125\n",
            "Batch 18: loss = 0.4360862672328949, acc = 0.849609375\n",
            "Batch 19: loss = 0.43929678201675415, acc = 0.849609375\n",
            "Batch 20: loss = 0.43627190589904785, acc = 0.8623046875\n",
            "Batch 21: loss = 0.42524605989456177, acc = 0.8515625\n",
            "Batch 22: loss = 0.40884384512901306, acc = 0.853515625\n",
            "Batch 23: loss = 0.4027932584285736, acc = 0.85546875\n",
            "Batch 24: loss = 0.4301607012748718, acc = 0.8505859375\n",
            "Batch 25: loss = 0.4343186020851135, acc = 0.869140625\n",
            "Batch 26: loss = 0.3817340135574341, acc = 0.8837890625\n",
            "Batch 27: loss = 0.48056602478027344, acc = 0.8349609375\n",
            "Batch 28: loss = 0.47391656041145325, acc = 0.8388671875\n",
            "Batch 29: loss = 0.41002127528190613, acc = 0.859375\n",
            "Batch 30: loss = 0.4073246717453003, acc = 0.873046875\n",
            "Batch 31: loss = 0.4447847902774811, acc = 0.841796875\n",
            "Batch 32: loss = 0.45243561267852783, acc = 0.8544921875\n",
            "Batch 33: loss = 0.4476650655269623, acc = 0.845703125\n",
            "Batch 34: loss = 0.42125678062438965, acc = 0.857421875\n",
            "Batch 35: loss = 0.3857671618461609, acc = 0.8662109375\n",
            "Batch 36: loss = 0.39589953422546387, acc = 0.8681640625\n",
            "Batch 37: loss = 0.41203877329826355, acc = 0.87109375\n",
            "Batch 38: loss = 0.3956059217453003, acc = 0.861328125\n",
            "Batch 39: loss = 0.377221018075943, acc = 0.875\n",
            "Batch 40: loss = 0.41602107882499695, acc = 0.8642578125\n",
            "Batch 41: loss = 0.41082531213760376, acc = 0.84375\n",
            "Batch 42: loss = 0.4113912582397461, acc = 0.857421875\n",
            "Batch 43: loss = 0.46133360266685486, acc = 0.8427734375\n",
            "Batch 44: loss = 0.4006043076515198, acc = 0.873046875\n",
            "Batch 45: loss = 0.37450870871543884, acc = 0.8798828125\n",
            "Batch 46: loss = 0.3920363187789917, acc = 0.86328125\n",
            "Batch 47: loss = 0.4259788990020752, acc = 0.845703125\n",
            "Batch 48: loss = 0.38078176975250244, acc = 0.8779296875\n",
            "Batch 49: loss = 0.37543928623199463, acc = 0.8779296875\n",
            "Batch 50: loss = 0.3698699176311493, acc = 0.8779296875\n",
            "Batch 51: loss = 0.4436532258987427, acc = 0.853515625\n",
            "Batch 52: loss = 0.4405529797077179, acc = 0.845703125\n",
            "Batch 53: loss = 0.42635032534599304, acc = 0.8662109375\n",
            "Batch 54: loss = 0.35938841104507446, acc = 0.888671875\n",
            "Batch 55: loss = 0.3780932128429413, acc = 0.8662109375\n",
            "Batch 56: loss = 0.41406747698783875, acc = 0.849609375\n",
            "Batch 57: loss = 0.45819687843322754, acc = 0.841796875\n",
            "Batch 58: loss = 0.43943920731544495, acc = 0.85546875\n",
            "Batch 59: loss = 0.36768588423728943, acc = 0.8828125\n",
            "Batch 60: loss = 0.4225255250930786, acc = 0.861328125\n",
            "Batch 61: loss = 0.41100579500198364, acc = 0.8662109375\n",
            "Batch 62: loss = 0.44961559772491455, acc = 0.8525390625\n",
            "Batch 63: loss = 0.36266764998435974, acc = 0.8828125\n",
            "Batch 64: loss = 0.35298240184783936, acc = 0.890625\n",
            "Batch 65: loss = 0.40897318720817566, acc = 0.859375\n",
            "Batch 66: loss = 0.42748668789863586, acc = 0.8525390625\n",
            "Batch 67: loss = 0.4139733910560608, acc = 0.86328125\n",
            "Batch 68: loss = 0.4442400336265564, acc = 0.8505859375\n",
            "Batch 69: loss = 0.3825405240058899, acc = 0.8701171875\n",
            "Batch 70: loss = 0.42107245326042175, acc = 0.8544921875\n",
            "Batch 71: loss = 0.4400491416454315, acc = 0.84765625\n",
            "Batch 72: loss = 0.43162310123443604, acc = 0.8623046875\n",
            "Batch 73: loss = 0.474520742893219, acc = 0.865234375\n",
            "Batch 74: loss = 0.4492928981781006, acc = 0.837890625\n",
            "Batch 75: loss = 0.494157075881958, acc = 0.830078125\n",
            "Batch 76: loss = 0.4668242931365967, acc = 0.8310546875\n",
            "Batch 77: loss = 0.38825589418411255, acc = 0.8544921875\n",
            "Batch 78: loss = 0.4264652132987976, acc = 0.857421875\n",
            "Batch 79: loss = 0.4257313013076782, acc = 0.8505859375\n",
            "Batch 80: loss = 0.4039945602416992, acc = 0.86328125\n",
            "Batch 81: loss = 0.4096716046333313, acc = 0.8544921875\n",
            "Batch 82: loss = 0.4143820106983185, acc = 0.8623046875\n",
            "Batch 83: loss = 0.40189406275749207, acc = 0.8720703125\n",
            "Batch 84: loss = 0.4111822545528412, acc = 0.8515625\n",
            "Batch 85: loss = 0.4683743119239807, acc = 0.8359375\n",
            "Batch 86: loss = 0.4134499132633209, acc = 0.8623046875\n",
            "Batch 87: loss = 0.41466501355171204, acc = 0.8671875\n",
            "Batch 88: loss = 0.4773094654083252, acc = 0.8505859375\n",
            "Batch 89: loss = 0.40523281693458557, acc = 0.857421875\n",
            "Batch 90: loss = 0.4582541584968567, acc = 0.8486328125\n",
            "Batch 91: loss = 0.42960599064826965, acc = 0.8525390625\n",
            "Batch 92: loss = 0.48888635635375977, acc = 0.8505859375\n",
            "Batch 93: loss = 0.39052632451057434, acc = 0.8759765625\n",
            "Batch 94: loss = 0.417359858751297, acc = 0.8662109375\n",
            "Batch 95: loss = 0.4255700409412384, acc = 0.8486328125\n",
            "Batch 96: loss = 0.4319021701812744, acc = 0.8583984375\n",
            "Batch 97: loss = 0.4337177872657776, acc = 0.8505859375\n",
            "Batch 98: loss = 0.4310135245323181, acc = 0.8642578125\n",
            "Batch 99: loss = 0.4364343583583832, acc = 0.86328125\n",
            "Batch 100: loss = 0.4195720851421356, acc = 0.869140625\n",
            "Batch 101: loss = 0.41206198930740356, acc = 0.859375\n",
            "Batch 102: loss = 0.4488808810710907, acc = 0.8515625\n",
            "Batch 103: loss = 0.44535407423973083, acc = 0.8408203125\n",
            "Batch 104: loss = 0.3922092318534851, acc = 0.865234375\n",
            "Batch 105: loss = 0.41369569301605225, acc = 0.8681640625\n",
            "Batch 106: loss = 0.43272772431373596, acc = 0.841796875\n",
            "Batch 107: loss = 0.4179859459400177, acc = 0.8642578125\n",
            "Batch 108: loss = 0.40607118606567383, acc = 0.86328125\n",
            "Batch 109: loss = 0.4055614173412323, acc = 0.86328125\n",
            "Batch 110: loss = 0.3899359703063965, acc = 0.869140625\n",
            "Batch 111: loss = 0.41359594464302063, acc = 0.8583984375\n",
            "Batch 112: loss = 0.3962373435497284, acc = 0.8642578125\n",
            "Batch 113: loss = 0.44282248616218567, acc = 0.8564453125\n",
            "Batch 114: loss = 0.43881627917289734, acc = 0.8486328125\n",
            "Batch 115: loss = 0.39430874586105347, acc = 0.869140625\n",
            "Batch 116: loss = 0.42202234268188477, acc = 0.8583984375\n",
            "Batch 117: loss = 0.38523155450820923, acc = 0.875\n",
            "Batch 118: loss = 0.4015965461730957, acc = 0.8671875\n",
            "Batch 119: loss = 0.42143604159355164, acc = 0.865234375\n",
            "Batch 120: loss = 0.3601469397544861, acc = 0.8720703125\n",
            "Batch 121: loss = 0.44038116931915283, acc = 0.8662109375\n",
            "Batch 122: loss = 0.4195213317871094, acc = 0.8642578125\n",
            "Batch 123: loss = 0.41152137517929077, acc = 0.8662109375\n",
            "Batch 124: loss = 0.4550817012786865, acc = 0.8583984375\n",
            "Batch 125: loss = 0.44415366649627686, acc = 0.84375\n",
            "Batch 126: loss = 0.42910876870155334, acc = 0.8623046875\n",
            "\n",
            "Epoch 64/100\n",
            "Batch 1: loss = 0.5557912588119507, acc = 0.828125\n",
            "Batch 2: loss = 0.4299945831298828, acc = 0.857421875\n",
            "Batch 3: loss = 0.44507771730422974, acc = 0.8486328125\n",
            "Batch 4: loss = 0.4072536528110504, acc = 0.8671875\n",
            "Batch 5: loss = 0.42573603987693787, acc = 0.85546875\n",
            "Batch 6: loss = 0.44087615609169006, acc = 0.8583984375\n",
            "Batch 7: loss = 0.38449031114578247, acc = 0.87890625\n",
            "Batch 8: loss = 0.4335255026817322, acc = 0.8564453125\n",
            "Batch 9: loss = 0.4219718277454376, acc = 0.8583984375\n",
            "Batch 10: loss = 0.32005903124809265, acc = 0.8857421875\n",
            "Batch 11: loss = 0.4265575408935547, acc = 0.8603515625\n",
            "Batch 12: loss = 0.40617477893829346, acc = 0.8544921875\n",
            "Batch 13: loss = 0.3887494206428528, acc = 0.8740234375\n",
            "Batch 14: loss = 0.35653722286224365, acc = 0.8837890625\n",
            "Batch 15: loss = 0.44144874811172485, acc = 0.8544921875\n",
            "Batch 16: loss = 0.4066830575466156, acc = 0.8701171875\n",
            "Batch 17: loss = 0.4024972915649414, acc = 0.8544921875\n",
            "Batch 18: loss = 0.46217167377471924, acc = 0.8486328125\n",
            "Batch 19: loss = 0.38738083839416504, acc = 0.8701171875\n",
            "Batch 20: loss = 0.39194318652153015, acc = 0.8583984375\n",
            "Batch 21: loss = 0.42634114623069763, acc = 0.8564453125\n",
            "Batch 22: loss = 0.3905178904533386, acc = 0.8701171875\n",
            "Batch 23: loss = 0.4318656921386719, acc = 0.8544921875\n",
            "Batch 24: loss = 0.47319966554641724, acc = 0.84375\n",
            "Batch 25: loss = 0.38784196972846985, acc = 0.8720703125\n",
            "Batch 26: loss = 0.3978820741176605, acc = 0.875\n",
            "Batch 27: loss = 0.47306177020072937, acc = 0.8427734375\n",
            "Batch 28: loss = 0.44233086705207825, acc = 0.84375\n",
            "Batch 29: loss = 0.44109973311424255, acc = 0.8564453125\n",
            "Batch 30: loss = 0.3912147879600525, acc = 0.8642578125\n",
            "Batch 31: loss = 0.4139781594276428, acc = 0.8623046875\n",
            "Batch 32: loss = 0.47635817527770996, acc = 0.8486328125\n",
            "Batch 33: loss = 0.41110605001449585, acc = 0.86328125\n",
            "Batch 34: loss = 0.46499428153038025, acc = 0.837890625\n",
            "Batch 35: loss = 0.3724786341190338, acc = 0.8818359375\n",
            "Batch 36: loss = 0.4202035069465637, acc = 0.8564453125\n",
            "Batch 37: loss = 0.3758130967617035, acc = 0.873046875\n",
            "Batch 38: loss = 0.41578590869903564, acc = 0.8583984375\n",
            "Batch 39: loss = 0.4159494638442993, acc = 0.857421875\n",
            "Batch 40: loss = 0.4004964232444763, acc = 0.8603515625\n",
            "Batch 41: loss = 0.40237897634506226, acc = 0.859375\n",
            "Batch 42: loss = 0.39360472559928894, acc = 0.873046875\n",
            "Batch 43: loss = 0.48882347345352173, acc = 0.841796875\n",
            "Batch 44: loss = 0.3746209144592285, acc = 0.876953125\n",
            "Batch 45: loss = 0.36019155383110046, acc = 0.8779296875\n",
            "Batch 46: loss = 0.3858256936073303, acc = 0.86328125\n",
            "Batch 47: loss = 0.40505847334861755, acc = 0.86328125\n",
            "Batch 48: loss = 0.396791934967041, acc = 0.87109375\n",
            "Batch 49: loss = 0.39917629957199097, acc = 0.8681640625\n",
            "Batch 50: loss = 0.3955024778842926, acc = 0.8671875\n",
            "Batch 51: loss = 0.4287245571613312, acc = 0.85546875\n",
            "Batch 52: loss = 0.4124477505683899, acc = 0.8603515625\n",
            "Batch 53: loss = 0.451640784740448, acc = 0.8603515625\n",
            "Batch 54: loss = 0.3247605264186859, acc = 0.8916015625\n",
            "Batch 55: loss = 0.3712345063686371, acc = 0.87109375\n",
            "Batch 56: loss = 0.43389391899108887, acc = 0.8486328125\n",
            "Batch 57: loss = 0.45658209919929504, acc = 0.849609375\n",
            "Batch 58: loss = 0.46696120500564575, acc = 0.8359375\n",
            "Batch 59: loss = 0.3627096712589264, acc = 0.8818359375\n",
            "Batch 60: loss = 0.41485950350761414, acc = 0.861328125\n",
            "Batch 61: loss = 0.3569210171699524, acc = 0.8857421875\n",
            "Batch 62: loss = 0.46439915895462036, acc = 0.841796875\n",
            "Batch 63: loss = 0.36516156792640686, acc = 0.8779296875\n",
            "Batch 64: loss = 0.38691312074661255, acc = 0.8662109375\n",
            "Batch 65: loss = 0.4565950334072113, acc = 0.853515625\n",
            "Batch 66: loss = 0.3895091712474823, acc = 0.8701171875\n",
            "Batch 67: loss = 0.40454918146133423, acc = 0.8623046875\n",
            "Batch 68: loss = 0.45585897564888, acc = 0.841796875\n",
            "Batch 69: loss = 0.3516298234462738, acc = 0.87890625\n",
            "Batch 70: loss = 0.38958120346069336, acc = 0.8779296875\n",
            "Batch 71: loss = 0.43542253971099854, acc = 0.8466796875\n",
            "Batch 72: loss = 0.42431625723838806, acc = 0.8623046875\n",
            "Batch 73: loss = 0.4585185647010803, acc = 0.857421875\n",
            "Batch 74: loss = 0.4211369752883911, acc = 0.857421875\n",
            "Batch 75: loss = 0.48548823595046997, acc = 0.837890625\n",
            "Batch 76: loss = 0.4087426960468292, acc = 0.8525390625\n",
            "Batch 77: loss = 0.39810842275619507, acc = 0.8583984375\n",
            "Batch 78: loss = 0.4133354127407074, acc = 0.849609375\n",
            "Batch 79: loss = 0.39208799600601196, acc = 0.869140625\n",
            "Batch 80: loss = 0.3691539466381073, acc = 0.869140625\n",
            "Batch 81: loss = 0.4405202865600586, acc = 0.8525390625\n",
            "Batch 82: loss = 0.38558876514434814, acc = 0.8681640625\n",
            "Batch 83: loss = 0.4207695424556732, acc = 0.8583984375\n",
            "Batch 84: loss = 0.4134962260723114, acc = 0.8623046875\n",
            "Batch 85: loss = 0.4434037506580353, acc = 0.8505859375\n",
            "Batch 86: loss = 0.41224581003189087, acc = 0.8603515625\n",
            "Batch 87: loss = 0.4002060890197754, acc = 0.876953125\n",
            "Batch 88: loss = 0.49800726771354675, acc = 0.845703125\n",
            "Batch 89: loss = 0.40001651644706726, acc = 0.8671875\n",
            "Batch 90: loss = 0.43950513005256653, acc = 0.84765625\n",
            "Batch 91: loss = 0.46855735778808594, acc = 0.8369140625\n",
            "Batch 92: loss = 0.4841304123401642, acc = 0.83203125\n",
            "Batch 93: loss = 0.4035869240760803, acc = 0.865234375\n",
            "Batch 94: loss = 0.4267856478691101, acc = 0.861328125\n",
            "Batch 95: loss = 0.39983195066452026, acc = 0.865234375\n",
            "Batch 96: loss = 0.43425482511520386, acc = 0.84765625\n",
            "Batch 97: loss = 0.45999598503112793, acc = 0.857421875\n",
            "Batch 98: loss = 0.4367641806602478, acc = 0.857421875\n",
            "Batch 99: loss = 0.43360793590545654, acc = 0.8408203125\n",
            "Batch 100: loss = 0.42315053939819336, acc = 0.84375\n",
            "Batch 101: loss = 0.4060465097427368, acc = 0.8642578125\n",
            "Batch 102: loss = 0.4645390212535858, acc = 0.849609375\n",
            "Batch 103: loss = 0.46619805693626404, acc = 0.8466796875\n",
            "Batch 104: loss = 0.3817386031150818, acc = 0.8720703125\n",
            "Batch 105: loss = 0.36591458320617676, acc = 0.876953125\n",
            "Batch 106: loss = 0.43703389167785645, acc = 0.8603515625\n",
            "Batch 107: loss = 0.4087279736995697, acc = 0.8740234375\n",
            "Batch 108: loss = 0.4163587689399719, acc = 0.8583984375\n",
            "Batch 109: loss = 0.4139363467693329, acc = 0.857421875\n",
            "Batch 110: loss = 0.40063971281051636, acc = 0.8701171875\n",
            "Batch 111: loss = 0.42969852685928345, acc = 0.8466796875\n",
            "Batch 112: loss = 0.4128039479255676, acc = 0.861328125\n",
            "Batch 113: loss = 0.4245139956474304, acc = 0.8515625\n",
            "Batch 114: loss = 0.45377475023269653, acc = 0.8525390625\n",
            "Batch 115: loss = 0.4374905228614807, acc = 0.8544921875\n",
            "Batch 116: loss = 0.40624427795410156, acc = 0.8642578125\n",
            "Batch 117: loss = 0.41091036796569824, acc = 0.8662109375\n",
            "Batch 118: loss = 0.36378538608551025, acc = 0.875\n",
            "Batch 119: loss = 0.4141244888305664, acc = 0.8603515625\n",
            "Batch 120: loss = 0.36530831456184387, acc = 0.865234375\n",
            "Batch 121: loss = 0.41036850214004517, acc = 0.8662109375\n",
            "Batch 122: loss = 0.394972562789917, acc = 0.8701171875\n",
            "Batch 123: loss = 0.42002543807029724, acc = 0.8525390625\n",
            "Batch 124: loss = 0.44231724739074707, acc = 0.8349609375\n",
            "Batch 125: loss = 0.44230014085769653, acc = 0.8505859375\n",
            "Batch 126: loss = 0.4369342625141144, acc = 0.85546875\n",
            "\n",
            "Epoch 65/100\n",
            "Batch 1: loss = 0.5482147932052612, acc = 0.8291015625\n",
            "Batch 2: loss = 0.47150036692619324, acc = 0.8388671875\n",
            "Batch 3: loss = 0.4439859986305237, acc = 0.8466796875\n",
            "Batch 4: loss = 0.38988247513771057, acc = 0.8740234375\n",
            "Batch 5: loss = 0.4079710841178894, acc = 0.86328125\n",
            "Batch 6: loss = 0.4689120352268219, acc = 0.8330078125\n",
            "Batch 7: loss = 0.3725900948047638, acc = 0.876953125\n",
            "Batch 8: loss = 0.4973987936973572, acc = 0.8447265625\n",
            "Batch 9: loss = 0.423642098903656, acc = 0.861328125\n",
            "Batch 10: loss = 0.4008115231990814, acc = 0.8720703125\n",
            "Batch 11: loss = 0.38700228929519653, acc = 0.869140625\n",
            "Batch 12: loss = 0.4383428394794464, acc = 0.8515625\n",
            "Batch 13: loss = 0.40098392963409424, acc = 0.8623046875\n",
            "Batch 14: loss = 0.35462290048599243, acc = 0.875\n",
            "Batch 15: loss = 0.43133676052093506, acc = 0.859375\n",
            "Batch 16: loss = 0.41605326533317566, acc = 0.8505859375\n",
            "Batch 17: loss = 0.4271269142627716, acc = 0.853515625\n",
            "Batch 18: loss = 0.452109694480896, acc = 0.8349609375\n",
            "Batch 19: loss = 0.4514250159263611, acc = 0.8486328125\n",
            "Batch 20: loss = 0.3948197364807129, acc = 0.8623046875\n",
            "Batch 21: loss = 0.42585691809654236, acc = 0.853515625\n",
            "Batch 22: loss = 0.4169696569442749, acc = 0.865234375\n",
            "Batch 23: loss = 0.4605829417705536, acc = 0.8525390625\n",
            "Batch 24: loss = 0.3733219504356384, acc = 0.876953125\n",
            "Batch 25: loss = 0.4273086190223694, acc = 0.8408203125\n",
            "Batch 26: loss = 0.38907843828201294, acc = 0.8603515625\n",
            "Batch 27: loss = 0.455294668674469, acc = 0.84375\n",
            "Batch 28: loss = 0.39007246494293213, acc = 0.8701171875\n",
            "Batch 29: loss = 0.40967243909835815, acc = 0.8583984375\n",
            "Batch 30: loss = 0.39315560460090637, acc = 0.875\n",
            "Batch 31: loss = 0.40839239954948425, acc = 0.861328125\n",
            "Batch 32: loss = 0.4765446186065674, acc = 0.845703125\n",
            "Batch 33: loss = 0.3769286572933197, acc = 0.8779296875\n",
            "Batch 34: loss = 0.37612706422805786, acc = 0.87109375\n",
            "Batch 35: loss = 0.40581873059272766, acc = 0.86328125\n",
            "Batch 36: loss = 0.3817197382450104, acc = 0.8671875\n",
            "Batch 37: loss = 0.39026546478271484, acc = 0.865234375\n",
            "Batch 38: loss = 0.4038393199443817, acc = 0.8642578125\n",
            "Batch 39: loss = 0.3754982054233551, acc = 0.865234375\n",
            "Batch 40: loss = 0.40340545773506165, acc = 0.861328125\n",
            "Batch 41: loss = 0.3616732358932495, acc = 0.87890625\n",
            "Batch 42: loss = 0.38483166694641113, acc = 0.869140625\n",
            "Batch 43: loss = 0.44721201062202454, acc = 0.8466796875\n",
            "Batch 44: loss = 0.404801607131958, acc = 0.861328125\n",
            "Batch 45: loss = 0.37365829944610596, acc = 0.876953125\n",
            "Batch 46: loss = 0.37351250648498535, acc = 0.8681640625\n",
            "Batch 47: loss = 0.3981110453605652, acc = 0.869140625\n",
            "Batch 48: loss = 0.37357962131500244, acc = 0.8740234375\n",
            "Batch 49: loss = 0.3623540699481964, acc = 0.8779296875\n",
            "Batch 50: loss = 0.3750896453857422, acc = 0.8779296875\n",
            "Batch 51: loss = 0.41941574215888977, acc = 0.8623046875\n",
            "Batch 52: loss = 0.4042980372905731, acc = 0.857421875\n",
            "Batch 53: loss = 0.422364741563797, acc = 0.85546875\n",
            "Batch 54: loss = 0.3187221586704254, acc = 0.884765625\n",
            "Batch 55: loss = 0.343005895614624, acc = 0.8837890625\n",
            "Batch 56: loss = 0.43343597650527954, acc = 0.845703125\n",
            "Batch 57: loss = 0.427385151386261, acc = 0.85546875\n",
            "Batch 58: loss = 0.44510573148727417, acc = 0.84765625\n",
            "Batch 59: loss = 0.3442525267601013, acc = 0.8798828125\n",
            "Batch 60: loss = 0.3988345265388489, acc = 0.865234375\n",
            "Batch 61: loss = 0.36497023701667786, acc = 0.8779296875\n",
            "Batch 62: loss = 0.5013814568519592, acc = 0.8232421875\n",
            "Batch 63: loss = 0.42824921011924744, acc = 0.8623046875\n",
            "Batch 64: loss = 0.35897719860076904, acc = 0.8720703125\n",
            "Batch 65: loss = 0.440042108297348, acc = 0.8505859375\n",
            "Batch 66: loss = 0.4183523654937744, acc = 0.853515625\n",
            "Batch 67: loss = 0.3863731026649475, acc = 0.869140625\n",
            "Batch 68: loss = 0.4307657778263092, acc = 0.8515625\n",
            "Batch 69: loss = 0.3490375876426697, acc = 0.873046875\n",
            "Batch 70: loss = 0.4257373809814453, acc = 0.84765625\n",
            "Batch 71: loss = 0.4438629746437073, acc = 0.8505859375\n",
            "Batch 72: loss = 0.39737793803215027, acc = 0.87109375\n",
            "Batch 73: loss = 0.4680831730365753, acc = 0.8466796875\n",
            "Batch 74: loss = 0.4272114932537079, acc = 0.8525390625\n",
            "Batch 75: loss = 0.4869726002216339, acc = 0.8486328125\n",
            "Batch 76: loss = 0.44509580731391907, acc = 0.857421875\n",
            "Batch 77: loss = 0.38686251640319824, acc = 0.876953125\n",
            "Batch 78: loss = 0.4151979386806488, acc = 0.8642578125\n",
            "Batch 79: loss = 0.39369285106658936, acc = 0.8759765625\n",
            "Batch 80: loss = 0.36128437519073486, acc = 0.865234375\n",
            "Batch 81: loss = 0.4035266041755676, acc = 0.8623046875\n",
            "Batch 82: loss = 0.35885411500930786, acc = 0.896484375\n",
            "Batch 83: loss = 0.40620702505111694, acc = 0.8740234375\n",
            "Batch 84: loss = 0.37636125087738037, acc = 0.8642578125\n",
            "Batch 85: loss = 0.4177965521812439, acc = 0.8564453125\n",
            "Batch 86: loss = 0.41012123227119446, acc = 0.853515625\n",
            "Batch 87: loss = 0.4060603678226471, acc = 0.869140625\n",
            "Batch 88: loss = 0.49925753474235535, acc = 0.8408203125\n",
            "Batch 89: loss = 0.40965843200683594, acc = 0.869140625\n",
            "Batch 90: loss = 0.43673089146614075, acc = 0.857421875\n",
            "Batch 91: loss = 0.48276546597480774, acc = 0.8349609375\n",
            "Batch 92: loss = 0.47374773025512695, acc = 0.83984375\n",
            "Batch 93: loss = 0.3691374957561493, acc = 0.880859375\n",
            "Batch 94: loss = 0.4263404905796051, acc = 0.8623046875\n",
            "Batch 95: loss = 0.39853668212890625, acc = 0.859375\n",
            "Batch 96: loss = 0.45207974314689636, acc = 0.841796875\n",
            "Batch 97: loss = 0.41524720191955566, acc = 0.8671875\n",
            "Batch 98: loss = 0.3960842192173004, acc = 0.8623046875\n",
            "Batch 99: loss = 0.4876788556575775, acc = 0.83984375\n",
            "Batch 100: loss = 0.4248353838920593, acc = 0.861328125\n",
            "Batch 101: loss = 0.41269809007644653, acc = 0.8486328125\n",
            "Batch 102: loss = 0.44745367765426636, acc = 0.8466796875\n",
            "Batch 103: loss = 0.4250340163707733, acc = 0.8564453125\n",
            "Batch 104: loss = 0.3607582151889801, acc = 0.8818359375\n",
            "Batch 105: loss = 0.3807625472545624, acc = 0.87109375\n",
            "Batch 106: loss = 0.38963791728019714, acc = 0.8681640625\n",
            "Batch 107: loss = 0.4067760407924652, acc = 0.8671875\n",
            "Batch 108: loss = 0.3888070285320282, acc = 0.8642578125\n",
            "Batch 109: loss = 0.43690362572669983, acc = 0.8583984375\n",
            "Batch 110: loss = 0.37454214692115784, acc = 0.8857421875\n",
            "Batch 111: loss = 0.43457770347595215, acc = 0.8544921875\n",
            "Batch 112: loss = 0.4035989046096802, acc = 0.857421875\n",
            "Batch 113: loss = 0.4382982552051544, acc = 0.8564453125\n",
            "Batch 114: loss = 0.4605172276496887, acc = 0.8544921875\n",
            "Batch 115: loss = 0.43953853845596313, acc = 0.845703125\n",
            "Batch 116: loss = 0.4264139235019684, acc = 0.857421875\n",
            "Batch 117: loss = 0.38264739513397217, acc = 0.873046875\n",
            "Batch 118: loss = 0.4038945734500885, acc = 0.873046875\n",
            "Batch 119: loss = 0.41884151101112366, acc = 0.8642578125\n",
            "Batch 120: loss = 0.4044760763645172, acc = 0.85546875\n",
            "Batch 121: loss = 0.42894694209098816, acc = 0.841796875\n",
            "Batch 122: loss = 0.4201977252960205, acc = 0.8671875\n",
            "Batch 123: loss = 0.4331485629081726, acc = 0.849609375\n",
            "Batch 124: loss = 0.4517710208892822, acc = 0.857421875\n",
            "Batch 125: loss = 0.4461332857608795, acc = 0.84765625\n",
            "Batch 126: loss = 0.4311930537223816, acc = 0.853515625\n",
            "\n",
            "Epoch 66/100\n",
            "Batch 1: loss = 0.532720685005188, acc = 0.83203125\n",
            "Batch 2: loss = 0.42177480459213257, acc = 0.8583984375\n",
            "Batch 3: loss = 0.43491309881210327, acc = 0.8603515625\n",
            "Batch 4: loss = 0.38570475578308105, acc = 0.869140625\n",
            "Batch 5: loss = 0.43233948945999146, acc = 0.859375\n",
            "Batch 6: loss = 0.447393000125885, acc = 0.8505859375\n",
            "Batch 7: loss = 0.4330916702747345, acc = 0.865234375\n",
            "Batch 8: loss = 0.44375425577163696, acc = 0.859375\n",
            "Batch 9: loss = 0.42574140429496765, acc = 0.8525390625\n",
            "Batch 10: loss = 0.3547931909561157, acc = 0.8798828125\n",
            "Batch 11: loss = 0.38908299803733826, acc = 0.876953125\n",
            "Batch 12: loss = 0.423715204000473, acc = 0.8623046875\n",
            "Batch 13: loss = 0.38950079679489136, acc = 0.8720703125\n",
            "Batch 14: loss = 0.33921197056770325, acc = 0.888671875\n",
            "Batch 15: loss = 0.4008408486843109, acc = 0.86328125\n",
            "Batch 16: loss = 0.4551765024662018, acc = 0.8447265625\n",
            "Batch 17: loss = 0.3959064185619354, acc = 0.85546875\n",
            "Batch 18: loss = 0.418317586183548, acc = 0.8642578125\n",
            "Batch 19: loss = 0.3729066848754883, acc = 0.8759765625\n",
            "Batch 20: loss = 0.43379876017570496, acc = 0.8525390625\n",
            "Batch 21: loss = 0.40210679173469543, acc = 0.8662109375\n",
            "Batch 22: loss = 0.42203086614608765, acc = 0.8544921875\n",
            "Batch 23: loss = 0.4165530502796173, acc = 0.859375\n",
            "Batch 24: loss = 0.4147520661354065, acc = 0.84765625\n",
            "Batch 25: loss = 0.36974477767944336, acc = 0.8701171875\n",
            "Batch 26: loss = 0.42014384269714355, acc = 0.8603515625\n",
            "Batch 27: loss = 0.43978434801101685, acc = 0.8564453125\n",
            "Batch 28: loss = 0.4302336573600769, acc = 0.8583984375\n",
            "Batch 29: loss = 0.4121929705142975, acc = 0.859375\n",
            "Batch 30: loss = 0.4198320806026459, acc = 0.8623046875\n",
            "Batch 31: loss = 0.45876801013946533, acc = 0.845703125\n",
            "Batch 32: loss = 0.43335750699043274, acc = 0.8466796875\n",
            "Batch 33: loss = 0.3925383687019348, acc = 0.87890625\n",
            "Batch 34: loss = 0.4304763674736023, acc = 0.8505859375\n",
            "Batch 35: loss = 0.41414985060691833, acc = 0.8583984375\n",
            "Batch 36: loss = 0.37384599447250366, acc = 0.8701171875\n",
            "Batch 37: loss = 0.3718637228012085, acc = 0.88671875\n",
            "Batch 38: loss = 0.3910973370075226, acc = 0.8681640625\n",
            "Batch 39: loss = 0.3766791820526123, acc = 0.87890625\n",
            "Batch 40: loss = 0.440956711769104, acc = 0.84765625\n",
            "Batch 41: loss = 0.4057329595088959, acc = 0.8720703125\n",
            "Batch 42: loss = 0.38260138034820557, acc = 0.87109375\n",
            "Batch 43: loss = 0.47273415327072144, acc = 0.8349609375\n",
            "Batch 44: loss = 0.43259674310684204, acc = 0.8544921875\n",
            "Batch 45: loss = 0.38210490345954895, acc = 0.8720703125\n",
            "Batch 46: loss = 0.3949962854385376, acc = 0.869140625\n",
            "Batch 47: loss = 0.37377333641052246, acc = 0.8701171875\n",
            "Batch 48: loss = 0.37409186363220215, acc = 0.876953125\n",
            "Batch 49: loss = 0.3909824788570404, acc = 0.86328125\n",
            "Batch 50: loss = 0.38329893350601196, acc = 0.8701171875\n",
            "Batch 51: loss = 0.4444340467453003, acc = 0.84765625\n",
            "Batch 52: loss = 0.3864094614982605, acc = 0.8701171875\n",
            "Batch 53: loss = 0.4422479271888733, acc = 0.853515625\n",
            "Batch 54: loss = 0.3361838459968567, acc = 0.8916015625\n",
            "Batch 55: loss = 0.37570878863334656, acc = 0.8662109375\n",
            "Batch 56: loss = 0.3731587529182434, acc = 0.8681640625\n",
            "Batch 57: loss = 0.4437994658946991, acc = 0.8505859375\n",
            "Batch 58: loss = 0.45321324467658997, acc = 0.8603515625\n",
            "Batch 59: loss = 0.3418760895729065, acc = 0.8837890625\n",
            "Batch 60: loss = 0.37238171696662903, acc = 0.87890625\n",
            "Batch 61: loss = 0.4021075963973999, acc = 0.8603515625\n",
            "Batch 62: loss = 0.39173707365989685, acc = 0.8662109375\n",
            "Batch 63: loss = 0.38847973942756653, acc = 0.87109375\n",
            "Batch 64: loss = 0.3409384489059448, acc = 0.8876953125\n",
            "Batch 65: loss = 0.44037461280822754, acc = 0.859375\n",
            "Batch 66: loss = 0.4067878723144531, acc = 0.861328125\n",
            "Batch 67: loss = 0.422851026058197, acc = 0.8466796875\n",
            "Batch 68: loss = 0.4431305527687073, acc = 0.857421875\n",
            "Batch 69: loss = 0.3903811275959015, acc = 0.865234375\n",
            "Batch 70: loss = 0.43002206087112427, acc = 0.8515625\n",
            "Batch 71: loss = 0.4259893000125885, acc = 0.859375\n",
            "Batch 72: loss = 0.37247031927108765, acc = 0.876953125\n",
            "Batch 73: loss = 0.4688751697540283, acc = 0.8427734375\n",
            "Batch 74: loss = 0.42676031589508057, acc = 0.849609375\n",
            "Batch 75: loss = 0.4840591847896576, acc = 0.8330078125\n",
            "Batch 76: loss = 0.3973671793937683, acc = 0.85546875\n",
            "Batch 77: loss = 0.38730692863464355, acc = 0.87109375\n",
            "Batch 78: loss = 0.4247567355632782, acc = 0.8544921875\n",
            "Batch 79: loss = 0.3793315887451172, acc = 0.87109375\n",
            "Batch 80: loss = 0.39421579241752625, acc = 0.8662109375\n",
            "Batch 81: loss = 0.43694785237312317, acc = 0.8505859375\n",
            "Batch 82: loss = 0.4068525433540344, acc = 0.861328125\n",
            "Batch 83: loss = 0.39280179142951965, acc = 0.873046875\n",
            "Batch 84: loss = 0.4303455650806427, acc = 0.8515625\n",
            "Batch 85: loss = 0.45801684260368347, acc = 0.841796875\n",
            "Batch 86: loss = 0.45509907603263855, acc = 0.8427734375\n",
            "Batch 87: loss = 0.4293791353702545, acc = 0.87109375\n",
            "Batch 88: loss = 0.4769604206085205, acc = 0.8330078125\n",
            "Batch 89: loss = 0.4063950777053833, acc = 0.859375\n",
            "Batch 90: loss = 0.42597174644470215, acc = 0.8564453125\n",
            "Batch 91: loss = 0.45107322931289673, acc = 0.837890625\n",
            "Batch 92: loss = 0.4730587601661682, acc = 0.845703125\n",
            "Batch 93: loss = 0.39676862955093384, acc = 0.8701171875\n",
            "Batch 94: loss = 0.42730826139450073, acc = 0.86328125\n",
            "Batch 95: loss = 0.41893309354782104, acc = 0.86328125\n",
            "Batch 96: loss = 0.42436936497688293, acc = 0.845703125\n",
            "Batch 97: loss = 0.4071345329284668, acc = 0.8662109375\n",
            "Batch 98: loss = 0.40101179480552673, acc = 0.86328125\n",
            "Batch 99: loss = 0.4430283308029175, acc = 0.8466796875\n",
            "Batch 100: loss = 0.4454437792301178, acc = 0.8388671875\n",
            "Batch 101: loss = 0.4062079191207886, acc = 0.8623046875\n",
            "Batch 102: loss = 0.4523448348045349, acc = 0.8486328125\n",
            "Batch 103: loss = 0.40149742364883423, acc = 0.8701171875\n",
            "Batch 104: loss = 0.3407350182533264, acc = 0.8759765625\n",
            "Batch 105: loss = 0.3735928237438202, acc = 0.8701171875\n",
            "Batch 106: loss = 0.4212339520454407, acc = 0.865234375\n",
            "Batch 107: loss = 0.39600878953933716, acc = 0.869140625\n",
            "Batch 108: loss = 0.40812158584594727, acc = 0.861328125\n",
            "Batch 109: loss = 0.39709436893463135, acc = 0.875\n",
            "Batch 110: loss = 0.36122778058052063, acc = 0.8837890625\n",
            "Batch 111: loss = 0.4015110731124878, acc = 0.873046875\n",
            "Batch 112: loss = 0.402435302734375, acc = 0.86328125\n",
            "Batch 113: loss = 0.460653692483902, acc = 0.8466796875\n",
            "Batch 114: loss = 0.44654202461242676, acc = 0.845703125\n",
            "Batch 115: loss = 0.404647558927536, acc = 0.865234375\n",
            "Batch 116: loss = 0.46927112340927124, acc = 0.8369140625\n",
            "Batch 117: loss = 0.38830262422561646, acc = 0.8681640625\n",
            "Batch 118: loss = 0.3722449541091919, acc = 0.8779296875\n",
            "Batch 119: loss = 0.4586937427520752, acc = 0.8505859375\n",
            "Batch 120: loss = 0.3709995150566101, acc = 0.8740234375\n",
            "Batch 121: loss = 0.3743521571159363, acc = 0.8701171875\n",
            "Batch 122: loss = 0.39061665534973145, acc = 0.8720703125\n",
            "Batch 123: loss = 0.37178322672843933, acc = 0.8701171875\n",
            "Batch 124: loss = 0.4281714856624603, acc = 0.8603515625\n",
            "Batch 125: loss = 0.4460199475288391, acc = 0.8447265625\n",
            "Batch 126: loss = 0.46947336196899414, acc = 0.853515625\n",
            "\n",
            "Epoch 67/100\n",
            "Batch 1: loss = 0.5445606708526611, acc = 0.8369140625\n",
            "Batch 2: loss = 0.43081343173980713, acc = 0.8505859375\n",
            "Batch 3: loss = 0.4522310495376587, acc = 0.8583984375\n",
            "Batch 4: loss = 0.39117419719696045, acc = 0.8671875\n",
            "Batch 5: loss = 0.43533259630203247, acc = 0.8564453125\n",
            "Batch 6: loss = 0.4263913333415985, acc = 0.85546875\n",
            "Batch 7: loss = 0.41392573714256287, acc = 0.853515625\n",
            "Batch 8: loss = 0.3953450918197632, acc = 0.8740234375\n",
            "Batch 9: loss = 0.38950464129447937, acc = 0.869140625\n",
            "Batch 10: loss = 0.33304068446159363, acc = 0.8837890625\n",
            "Batch 11: loss = 0.38544416427612305, acc = 0.8720703125\n",
            "Batch 12: loss = 0.3890271782875061, acc = 0.8701171875\n",
            "Batch 13: loss = 0.3892589807510376, acc = 0.87109375\n",
            "Batch 14: loss = 0.3722113370895386, acc = 0.8798828125\n",
            "Batch 15: loss = 0.3972892165184021, acc = 0.869140625\n",
            "Batch 16: loss = 0.451728880405426, acc = 0.8486328125\n",
            "Batch 17: loss = 0.37188440561294556, acc = 0.8759765625\n",
            "Batch 18: loss = 0.4506751298904419, acc = 0.8427734375\n",
            "Batch 19: loss = 0.4010004699230194, acc = 0.8583984375\n",
            "Batch 20: loss = 0.4082065522670746, acc = 0.861328125\n",
            "Batch 21: loss = 0.47838401794433594, acc = 0.8486328125\n",
            "Batch 22: loss = 0.41128775477409363, acc = 0.865234375\n",
            "Batch 23: loss = 0.41182079911231995, acc = 0.8681640625\n",
            "Batch 24: loss = 0.4081493020057678, acc = 0.859375\n",
            "Batch 25: loss = 0.4064701199531555, acc = 0.87109375\n",
            "Batch 26: loss = 0.39034250378608704, acc = 0.875\n",
            "Batch 27: loss = 0.4323734641075134, acc = 0.8466796875\n",
            "Batch 28: loss = 0.4081573188304901, acc = 0.869140625\n",
            "Batch 29: loss = 0.44493791460990906, acc = 0.8525390625\n",
            "Batch 30: loss = 0.38585418462753296, acc = 0.869140625\n",
            "Batch 31: loss = 0.41231346130371094, acc = 0.8720703125\n",
            "Batch 32: loss = 0.46880918741226196, acc = 0.84375\n",
            "Batch 33: loss = 0.4144725799560547, acc = 0.8603515625\n",
            "Batch 34: loss = 0.44182053208351135, acc = 0.84765625\n",
            "Batch 35: loss = 0.4528528153896332, acc = 0.8544921875\n",
            "Batch 36: loss = 0.3816106617450714, acc = 0.87109375\n",
            "Batch 37: loss = 0.37748557329177856, acc = 0.8671875\n",
            "Batch 38: loss = 0.4086649715900421, acc = 0.861328125\n",
            "Batch 39: loss = 0.3601411283016205, acc = 0.8798828125\n",
            "Batch 40: loss = 0.3817027807235718, acc = 0.8681640625\n",
            "Batch 41: loss = 0.35198649764060974, acc = 0.8759765625\n",
            "Batch 42: loss = 0.3854995369911194, acc = 0.8701171875\n",
            "Batch 43: loss = 0.4666587710380554, acc = 0.845703125\n",
            "Batch 44: loss = 0.4086303114891052, acc = 0.8662109375\n",
            "Batch 45: loss = 0.3887961804866791, acc = 0.869140625\n",
            "Batch 46: loss = 0.41053712368011475, acc = 0.8623046875\n",
            "Batch 47: loss = 0.3913794755935669, acc = 0.865234375\n",
            "Batch 48: loss = 0.3748989701271057, acc = 0.87890625\n",
            "Batch 49: loss = 0.36340296268463135, acc = 0.8818359375\n",
            "Batch 50: loss = 0.4100288450717926, acc = 0.86328125\n",
            "Batch 51: loss = 0.42856231331825256, acc = 0.8525390625\n",
            "Batch 52: loss = 0.40374818444252014, acc = 0.861328125\n",
            "Batch 53: loss = 0.4258839190006256, acc = 0.8486328125\n",
            "Batch 54: loss = 0.3031368851661682, acc = 0.8974609375\n",
            "Batch 55: loss = 0.36047351360321045, acc = 0.880859375\n",
            "Batch 56: loss = 0.39199721813201904, acc = 0.86328125\n",
            "Batch 57: loss = 0.4685927927494049, acc = 0.8447265625\n",
            "Batch 58: loss = 0.4195907711982727, acc = 0.8720703125\n",
            "Batch 59: loss = 0.35597583651542664, acc = 0.88671875\n",
            "Batch 60: loss = 0.4069654643535614, acc = 0.865234375\n",
            "Batch 61: loss = 0.37642747163772583, acc = 0.8720703125\n",
            "Batch 62: loss = 0.4661720395088196, acc = 0.853515625\n",
            "Batch 63: loss = 0.4139121472835541, acc = 0.8515625\n",
            "Batch 64: loss = 0.3319072127342224, acc = 0.8857421875\n",
            "Batch 65: loss = 0.39767158031463623, acc = 0.87109375\n",
            "Batch 66: loss = 0.39457079768180847, acc = 0.8662109375\n",
            "Batch 67: loss = 0.38840150833129883, acc = 0.875\n",
            "Batch 68: loss = 0.40155088901519775, acc = 0.865234375\n",
            "Batch 69: loss = 0.3653251528739929, acc = 0.8837890625\n",
            "Batch 70: loss = 0.4672885239124298, acc = 0.8447265625\n",
            "Batch 71: loss = 0.40454956889152527, acc = 0.8544921875\n",
            "Batch 72: loss = 0.38380539417266846, acc = 0.880859375\n",
            "Batch 73: loss = 0.4387708902359009, acc = 0.8603515625\n",
            "Batch 74: loss = 0.3923930823802948, acc = 0.8759765625\n",
            "Batch 75: loss = 0.49218738079071045, acc = 0.833984375\n",
            "Batch 76: loss = 0.43393829464912415, acc = 0.84765625\n",
            "Batch 77: loss = 0.36051714420318604, acc = 0.884765625\n",
            "Batch 78: loss = 0.4156785309314728, acc = 0.861328125\n",
            "Batch 79: loss = 0.4178377091884613, acc = 0.85546875\n",
            "Batch 80: loss = 0.38166123628616333, acc = 0.8828125\n",
            "Batch 81: loss = 0.4528045356273651, acc = 0.8486328125\n",
            "Batch 82: loss = 0.360156774520874, acc = 0.8837890625\n",
            "Batch 83: loss = 0.36717474460601807, acc = 0.8681640625\n",
            "Batch 84: loss = 0.4146307110786438, acc = 0.859375\n",
            "Batch 85: loss = 0.43221038579940796, acc = 0.8515625\n",
            "Batch 86: loss = 0.4321483075618744, acc = 0.8466796875\n",
            "Batch 87: loss = 0.40806782245635986, acc = 0.869140625\n",
            "Batch 88: loss = 0.49746787548065186, acc = 0.8251953125\n",
            "Batch 89: loss = 0.4251040518283844, acc = 0.869140625\n",
            "Batch 90: loss = 0.44941553473472595, acc = 0.84765625\n",
            "Batch 91: loss = 0.44710344076156616, acc = 0.84765625\n",
            "Batch 92: loss = 0.49226003885269165, acc = 0.837890625\n",
            "Batch 93: loss = 0.37433677911758423, acc = 0.8818359375\n",
            "Batch 94: loss = 0.4123346209526062, acc = 0.8603515625\n",
            "Batch 95: loss = 0.3793325126171112, acc = 0.8681640625\n",
            "Batch 96: loss = 0.4336061477661133, acc = 0.8505859375\n",
            "Batch 97: loss = 0.4199422597885132, acc = 0.8564453125\n",
            "Batch 98: loss = 0.39059722423553467, acc = 0.8720703125\n",
            "Batch 99: loss = 0.4241960942745209, acc = 0.8544921875\n",
            "Batch 100: loss = 0.4114609956741333, acc = 0.8544921875\n",
            "Batch 101: loss = 0.4497247636318207, acc = 0.837890625\n",
            "Batch 102: loss = 0.458931565284729, acc = 0.8388671875\n",
            "Batch 103: loss = 0.4516863226890564, acc = 0.8671875\n",
            "Batch 104: loss = 0.34960848093032837, acc = 0.8837890625\n",
            "Batch 105: loss = 0.3783130645751953, acc = 0.8759765625\n",
            "Batch 106: loss = 0.40111514925956726, acc = 0.8466796875\n",
            "Batch 107: loss = 0.37688755989074707, acc = 0.876953125\n",
            "Batch 108: loss = 0.4349236488342285, acc = 0.8603515625\n",
            "Batch 109: loss = 0.3988746702671051, acc = 0.861328125\n",
            "Batch 110: loss = 0.3745884597301483, acc = 0.8681640625\n",
            "Batch 111: loss = 0.41543877124786377, acc = 0.8525390625\n",
            "Batch 112: loss = 0.3858146071434021, acc = 0.8740234375\n",
            "Batch 113: loss = 0.4153972864151001, acc = 0.8681640625\n",
            "Batch 114: loss = 0.45379114151000977, acc = 0.8505859375\n",
            "Batch 115: loss = 0.41173920035362244, acc = 0.8759765625\n",
            "Batch 116: loss = 0.42229652404785156, acc = 0.8486328125\n",
            "Batch 117: loss = 0.37518906593322754, acc = 0.8720703125\n",
            "Batch 118: loss = 0.3571939766407013, acc = 0.875\n",
            "Batch 119: loss = 0.3553165793418884, acc = 0.884765625\n",
            "Batch 120: loss = 0.38276341557502747, acc = 0.873046875\n",
            "Batch 121: loss = 0.3817642331123352, acc = 0.8681640625\n",
            "Batch 122: loss = 0.4190567433834076, acc = 0.8564453125\n",
            "Batch 123: loss = 0.3844733238220215, acc = 0.8740234375\n",
            "Batch 124: loss = 0.47884875535964966, acc = 0.8291015625\n",
            "Batch 125: loss = 0.46188196539878845, acc = 0.8525390625\n",
            "Batch 126: loss = 0.43385380506515503, acc = 0.8544921875\n",
            "\n",
            "Epoch 68/100\n",
            "Batch 1: loss = 0.555053174495697, acc = 0.806640625\n",
            "Batch 2: loss = 0.5125598907470703, acc = 0.8291015625\n",
            "Batch 3: loss = 0.38906288146972656, acc = 0.873046875\n",
            "Batch 4: loss = 0.3918788433074951, acc = 0.8544921875\n",
            "Batch 5: loss = 0.39649486541748047, acc = 0.8662109375\n",
            "Batch 6: loss = 0.4618079662322998, acc = 0.8359375\n",
            "Batch 7: loss = 0.42885249853134155, acc = 0.8583984375\n",
            "Batch 8: loss = 0.4275333881378174, acc = 0.84765625\n",
            "Batch 9: loss = 0.3946416974067688, acc = 0.86328125\n",
            "Batch 10: loss = 0.3261493146419525, acc = 0.8798828125\n",
            "Batch 11: loss = 0.371145635843277, acc = 0.87890625\n",
            "Batch 12: loss = 0.4184037744998932, acc = 0.8603515625\n",
            "Batch 13: loss = 0.38274210691452026, acc = 0.8759765625\n",
            "Batch 14: loss = 0.4057345688343048, acc = 0.87109375\n",
            "Batch 15: loss = 0.4051641523838043, acc = 0.8525390625\n",
            "Batch 16: loss = 0.4604054093360901, acc = 0.849609375\n",
            "Batch 17: loss = 0.4143429696559906, acc = 0.86328125\n",
            "Batch 18: loss = 0.4507470428943634, acc = 0.849609375\n",
            "Batch 19: loss = 0.40985459089279175, acc = 0.865234375\n",
            "Batch 20: loss = 0.4161969721317291, acc = 0.8671875\n",
            "Batch 21: loss = 0.4280546307563782, acc = 0.8486328125\n",
            "Batch 22: loss = 0.42834383249282837, acc = 0.857421875\n",
            "Batch 23: loss = 0.38295218348503113, acc = 0.861328125\n",
            "Batch 24: loss = 0.4048220217227936, acc = 0.8623046875\n",
            "Batch 25: loss = 0.38141974806785583, acc = 0.86328125\n",
            "Batch 26: loss = 0.3769344985485077, acc = 0.8798828125\n",
            "Batch 27: loss = 0.4755295217037201, acc = 0.8447265625\n",
            "Batch 28: loss = 0.397940993309021, acc = 0.857421875\n",
            "Batch 29: loss = 0.38605937361717224, acc = 0.87109375\n",
            "Batch 30: loss = 0.393964946269989, acc = 0.86328125\n",
            "Batch 31: loss = 0.43129879236221313, acc = 0.845703125\n",
            "Batch 32: loss = 0.4747571051120758, acc = 0.833984375\n",
            "Batch 33: loss = 0.40739139914512634, acc = 0.8564453125\n",
            "Batch 34: loss = 0.4032522141933441, acc = 0.8623046875\n",
            "Batch 35: loss = 0.44289228320121765, acc = 0.841796875\n",
            "Batch 36: loss = 0.3792319595813751, acc = 0.8740234375\n",
            "Batch 37: loss = 0.3779383897781372, acc = 0.8876953125\n",
            "Batch 38: loss = 0.37866446375846863, acc = 0.875\n",
            "Batch 39: loss = 0.33248642086982727, acc = 0.8896484375\n",
            "Batch 40: loss = 0.3987694978713989, acc = 0.8701171875\n",
            "Batch 41: loss = 0.38476598262786865, acc = 0.880859375\n",
            "Batch 42: loss = 0.4040255546569824, acc = 0.8623046875\n",
            "Batch 43: loss = 0.40922725200653076, acc = 0.8720703125\n",
            "Batch 44: loss = 0.39841228723526, acc = 0.8759765625\n",
            "Batch 45: loss = 0.37766018509864807, acc = 0.875\n",
            "Batch 46: loss = 0.40870586037635803, acc = 0.8623046875\n",
            "Batch 47: loss = 0.3650769591331482, acc = 0.8828125\n",
            "Batch 48: loss = 0.3961560130119324, acc = 0.8603515625\n",
            "Batch 49: loss = 0.38318556547164917, acc = 0.873046875\n",
            "Batch 50: loss = 0.39153632521629333, acc = 0.87109375\n",
            "Batch 51: loss = 0.41558125615119934, acc = 0.857421875\n",
            "Batch 52: loss = 0.42943131923675537, acc = 0.845703125\n",
            "Batch 53: loss = 0.42202481627464294, acc = 0.8583984375\n",
            "Batch 54: loss = 0.3431646525859833, acc = 0.8876953125\n",
            "Batch 55: loss = 0.3519790470600128, acc = 0.8779296875\n",
            "Batch 56: loss = 0.3839763104915619, acc = 0.86328125\n",
            "Batch 57: loss = 0.40659889578819275, acc = 0.8642578125\n",
            "Batch 58: loss = 0.445721834897995, acc = 0.8544921875\n",
            "Batch 59: loss = 0.35490816831588745, acc = 0.8740234375\n",
            "Batch 60: loss = 0.40635520219802856, acc = 0.859375\n",
            "Batch 61: loss = 0.3828709125518799, acc = 0.8759765625\n",
            "Batch 62: loss = 0.46461498737335205, acc = 0.8486328125\n",
            "Batch 63: loss = 0.3997021019458771, acc = 0.873046875\n",
            "Batch 64: loss = 0.379364013671875, acc = 0.865234375\n",
            "Batch 65: loss = 0.4092259109020233, acc = 0.8603515625\n",
            "Batch 66: loss = 0.3718913197517395, acc = 0.8642578125\n",
            "Batch 67: loss = 0.3796099126338959, acc = 0.86328125\n",
            "Batch 68: loss = 0.4100917875766754, acc = 0.865234375\n",
            "Batch 69: loss = 0.3791760206222534, acc = 0.8798828125\n",
            "Batch 70: loss = 0.4319348931312561, acc = 0.8466796875\n",
            "Batch 71: loss = 0.4443986117839813, acc = 0.83984375\n",
            "Batch 72: loss = 0.37388667464256287, acc = 0.869140625\n",
            "Batch 73: loss = 0.4272071123123169, acc = 0.869140625\n",
            "Batch 74: loss = 0.44876232743263245, acc = 0.8466796875\n",
            "Batch 75: loss = 0.46815598011016846, acc = 0.8486328125\n",
            "Batch 76: loss = 0.4673944115638733, acc = 0.8271484375\n",
            "Batch 77: loss = 0.3680633306503296, acc = 0.8720703125\n",
            "Batch 78: loss = 0.42558982968330383, acc = 0.849609375\n",
            "Batch 79: loss = 0.39282721281051636, acc = 0.869140625\n",
            "Batch 80: loss = 0.34775444865226746, acc = 0.884765625\n",
            "Batch 81: loss = 0.42918238043785095, acc = 0.857421875\n",
            "Batch 82: loss = 0.36470720171928406, acc = 0.884765625\n",
            "Batch 83: loss = 0.3648950159549713, acc = 0.880859375\n",
            "Batch 84: loss = 0.3907720744609833, acc = 0.8623046875\n",
            "Batch 85: loss = 0.4575227200984955, acc = 0.8408203125\n",
            "Batch 86: loss = 0.43643730878829956, acc = 0.85546875\n",
            "Batch 87: loss = 0.4201068580150604, acc = 0.8544921875\n",
            "Batch 88: loss = 0.4645814299583435, acc = 0.849609375\n",
            "Batch 89: loss = 0.40639346837997437, acc = 0.8623046875\n",
            "Batch 90: loss = 0.4404694736003876, acc = 0.84765625\n",
            "Batch 91: loss = 0.44143491983413696, acc = 0.8564453125\n",
            "Batch 92: loss = 0.4461255669593811, acc = 0.859375\n",
            "Batch 93: loss = 0.3741077780723572, acc = 0.875\n",
            "Batch 94: loss = 0.4133046269416809, acc = 0.853515625\n",
            "Batch 95: loss = 0.3753984272480011, acc = 0.86328125\n",
            "Batch 96: loss = 0.4413553774356842, acc = 0.853515625\n",
            "Batch 97: loss = 0.40718263387680054, acc = 0.87109375\n",
            "Batch 98: loss = 0.3966437578201294, acc = 0.8662109375\n",
            "Batch 99: loss = 0.3993830680847168, acc = 0.857421875\n",
            "Batch 100: loss = 0.37952351570129395, acc = 0.8759765625\n",
            "Batch 101: loss = 0.38465020060539246, acc = 0.869140625\n",
            "Batch 102: loss = 0.4152180254459381, acc = 0.859375\n",
            "Batch 103: loss = 0.41748422384262085, acc = 0.86328125\n",
            "Batch 104: loss = 0.37174421548843384, acc = 0.8662109375\n",
            "Batch 105: loss = 0.3930838406085968, acc = 0.8740234375\n",
            "Batch 106: loss = 0.4007774293422699, acc = 0.8623046875\n",
            "Batch 107: loss = 0.43648892641067505, acc = 0.8564453125\n",
            "Batch 108: loss = 0.3727416396141052, acc = 0.87109375\n",
            "Batch 109: loss = 0.4154821038246155, acc = 0.8447265625\n",
            "Batch 110: loss = 0.35735762119293213, acc = 0.8857421875\n",
            "Batch 111: loss = 0.4407971501350403, acc = 0.8544921875\n",
            "Batch 112: loss = 0.37510016560554504, acc = 0.880859375\n",
            "Batch 113: loss = 0.39305949211120605, acc = 0.857421875\n",
            "Batch 114: loss = 0.4558700919151306, acc = 0.8486328125\n",
            "Batch 115: loss = 0.4301748275756836, acc = 0.857421875\n",
            "Batch 116: loss = 0.47774916887283325, acc = 0.8349609375\n",
            "Batch 117: loss = 0.42953288555145264, acc = 0.8662109375\n",
            "Batch 118: loss = 0.38688796758651733, acc = 0.8681640625\n",
            "Batch 119: loss = 0.4112273156642914, acc = 0.8720703125\n",
            "Batch 120: loss = 0.36114487051963806, acc = 0.873046875\n",
            "Batch 121: loss = 0.4249095618724823, acc = 0.8642578125\n",
            "Batch 122: loss = 0.3466501533985138, acc = 0.8828125\n",
            "Batch 123: loss = 0.39514681696891785, acc = 0.87109375\n",
            "Batch 124: loss = 0.4255429208278656, acc = 0.8603515625\n",
            "Batch 125: loss = 0.40515294671058655, acc = 0.875\n",
            "Batch 126: loss = 0.4112098813056946, acc = 0.8564453125\n",
            "\n",
            "Epoch 69/100\n",
            "Batch 1: loss = 0.5374951362609863, acc = 0.830078125\n",
            "Batch 2: loss = 0.4412289261817932, acc = 0.8525390625\n",
            "Batch 3: loss = 0.411728173494339, acc = 0.8671875\n",
            "Batch 4: loss = 0.3931087255477905, acc = 0.8701171875\n",
            "Batch 5: loss = 0.4097161889076233, acc = 0.869140625\n",
            "Batch 6: loss = 0.45042237639427185, acc = 0.857421875\n",
            "Batch 7: loss = 0.38893944025039673, acc = 0.8681640625\n",
            "Batch 8: loss = 0.3841424584388733, acc = 0.8681640625\n",
            "Batch 9: loss = 0.38454654812812805, acc = 0.8623046875\n",
            "Batch 10: loss = 0.3801654577255249, acc = 0.8720703125\n",
            "Batch 11: loss = 0.41163957118988037, acc = 0.8662109375\n",
            "Batch 12: loss = 0.3939216732978821, acc = 0.8544921875\n",
            "Batch 13: loss = 0.4353952407836914, acc = 0.8515625\n",
            "Batch 14: loss = 0.37682393193244934, acc = 0.8876953125\n",
            "Batch 15: loss = 0.4145268201828003, acc = 0.857421875\n",
            "Batch 16: loss = 0.42235514521598816, acc = 0.8603515625\n",
            "Batch 17: loss = 0.38238000869750977, acc = 0.875\n",
            "Batch 18: loss = 0.446611225605011, acc = 0.8505859375\n",
            "Batch 19: loss = 0.3800166845321655, acc = 0.869140625\n",
            "Batch 20: loss = 0.3910796642303467, acc = 0.8583984375\n",
            "Batch 21: loss = 0.4091789126396179, acc = 0.853515625\n",
            "Batch 22: loss = 0.4238634705543518, acc = 0.8515625\n",
            "Batch 23: loss = 0.3997328579425812, acc = 0.8564453125\n",
            "Batch 24: loss = 0.4447494447231293, acc = 0.8486328125\n",
            "Batch 25: loss = 0.42626553773880005, acc = 0.8486328125\n",
            "Batch 26: loss = 0.3981068730354309, acc = 0.8662109375\n",
            "Batch 27: loss = 0.43297553062438965, acc = 0.8603515625\n",
            "Batch 28: loss = 0.3978649973869324, acc = 0.8583984375\n",
            "Batch 29: loss = 0.3900596797466278, acc = 0.8662109375\n",
            "Batch 30: loss = 0.4168074131011963, acc = 0.8701171875\n",
            "Batch 31: loss = 0.4325868785381317, acc = 0.85546875\n",
            "Batch 32: loss = 0.4205373227596283, acc = 0.8515625\n",
            "Batch 33: loss = 0.39598822593688965, acc = 0.8779296875\n",
            "Batch 34: loss = 0.4191629886627197, acc = 0.869140625\n",
            "Batch 35: loss = 0.3795352578163147, acc = 0.869140625\n",
            "Batch 36: loss = 0.3832683265209198, acc = 0.86328125\n",
            "Batch 37: loss = 0.3874804973602295, acc = 0.8701171875\n",
            "Batch 38: loss = 0.38411396741867065, acc = 0.859375\n",
            "Batch 39: loss = 0.3665100932121277, acc = 0.873046875\n",
            "Batch 40: loss = 0.3869969844818115, acc = 0.8720703125\n",
            "Batch 41: loss = 0.3867388367652893, acc = 0.873046875\n",
            "Batch 42: loss = 0.363441526889801, acc = 0.8818359375\n",
            "Batch 43: loss = 0.43351051211357117, acc = 0.859375\n",
            "Batch 44: loss = 0.4068375825881958, acc = 0.8662109375\n",
            "Batch 45: loss = 0.4029536545276642, acc = 0.8583984375\n",
            "Batch 46: loss = 0.3716985583305359, acc = 0.869140625\n",
            "Batch 47: loss = 0.3792973756790161, acc = 0.869140625\n",
            "Batch 48: loss = 0.34481313824653625, acc = 0.880859375\n",
            "Batch 49: loss = 0.35796698927879333, acc = 0.88671875\n",
            "Batch 50: loss = 0.4027007222175598, acc = 0.8583984375\n",
            "Batch 51: loss = 0.37352365255355835, acc = 0.8779296875\n",
            "Batch 52: loss = 0.38744056224823, acc = 0.876953125\n",
            "Batch 53: loss = 0.4304547905921936, acc = 0.865234375\n",
            "Batch 54: loss = 0.3193278908729553, acc = 0.900390625\n",
            "Batch 55: loss = 0.35454413294792175, acc = 0.8779296875\n",
            "Batch 56: loss = 0.4242292642593384, acc = 0.859375\n",
            "Batch 57: loss = 0.460223913192749, acc = 0.84765625\n",
            "Batch 58: loss = 0.4045625329017639, acc = 0.8642578125\n",
            "Batch 59: loss = 0.3332567811012268, acc = 0.8798828125\n",
            "Batch 60: loss = 0.4401703476905823, acc = 0.8583984375\n",
            "Batch 61: loss = 0.3792085647583008, acc = 0.8671875\n",
            "Batch 62: loss = 0.4825472831726074, acc = 0.841796875\n",
            "Batch 63: loss = 0.4020503759384155, acc = 0.8642578125\n",
            "Batch 64: loss = 0.37798023223876953, acc = 0.8740234375\n",
            "Batch 65: loss = 0.43064334988594055, acc = 0.8466796875\n",
            "Batch 66: loss = 0.3951493799686432, acc = 0.873046875\n",
            "Batch 67: loss = 0.39800527691841125, acc = 0.8642578125\n",
            "Batch 68: loss = 0.455915242433548, acc = 0.8515625\n",
            "Batch 69: loss = 0.3921152949333191, acc = 0.875\n",
            "Batch 70: loss = 0.3952662944793701, acc = 0.8583984375\n",
            "Batch 71: loss = 0.46145355701446533, acc = 0.83984375\n",
            "Batch 72: loss = 0.4220997393131256, acc = 0.8544921875\n",
            "Batch 73: loss = 0.44771090149879456, acc = 0.861328125\n",
            "Batch 74: loss = 0.4147876799106598, acc = 0.861328125\n",
            "Batch 75: loss = 0.46176090836524963, acc = 0.845703125\n",
            "Batch 76: loss = 0.4637802839279175, acc = 0.841796875\n",
            "Batch 77: loss = 0.39598530530929565, acc = 0.8603515625\n",
            "Batch 78: loss = 0.39598730206489563, acc = 0.853515625\n",
            "Batch 79: loss = 0.3905922472476959, acc = 0.873046875\n",
            "Batch 80: loss = 0.36641407012939453, acc = 0.8701171875\n",
            "Batch 81: loss = 0.46222013235092163, acc = 0.84765625\n",
            "Batch 82: loss = 0.36062583327293396, acc = 0.876953125\n",
            "Batch 83: loss = 0.3702058792114258, acc = 0.876953125\n",
            "Batch 84: loss = 0.4026585817337036, acc = 0.8623046875\n",
            "Batch 85: loss = 0.42330223321914673, acc = 0.8564453125\n",
            "Batch 86: loss = 0.41295745968818665, acc = 0.849609375\n",
            "Batch 87: loss = 0.38638827204704285, acc = 0.876953125\n",
            "Batch 88: loss = 0.46232104301452637, acc = 0.837890625\n",
            "Batch 89: loss = 0.41477325558662415, acc = 0.8662109375\n",
            "Batch 90: loss = 0.40279433131217957, acc = 0.8583984375\n",
            "Batch 91: loss = 0.43911439180374146, acc = 0.8525390625\n",
            "Batch 92: loss = 0.44390323758125305, acc = 0.8603515625\n",
            "Batch 93: loss = 0.3736993074417114, acc = 0.8828125\n",
            "Batch 94: loss = 0.3979109227657318, acc = 0.8701171875\n",
            "Batch 95: loss = 0.359688401222229, acc = 0.888671875\n",
            "Batch 96: loss = 0.45543918013572693, acc = 0.8525390625\n",
            "Batch 97: loss = 0.4224518835544586, acc = 0.859375\n",
            "Batch 98: loss = 0.3860429525375366, acc = 0.8623046875\n",
            "Batch 99: loss = 0.4260804355144501, acc = 0.857421875\n",
            "Batch 100: loss = 0.39998406171798706, acc = 0.8525390625\n",
            "Batch 101: loss = 0.3829936683177948, acc = 0.865234375\n",
            "Batch 102: loss = 0.4291500449180603, acc = 0.8623046875\n",
            "Batch 103: loss = 0.4384029507637024, acc = 0.8662109375\n",
            "Batch 104: loss = 0.3756466209888458, acc = 0.861328125\n",
            "Batch 105: loss = 0.3951687514781952, acc = 0.875\n",
            "Batch 106: loss = 0.3980509638786316, acc = 0.8720703125\n",
            "Batch 107: loss = 0.39389339089393616, acc = 0.86328125\n",
            "Batch 108: loss = 0.39725732803344727, acc = 0.8779296875\n",
            "Batch 109: loss = 0.4336007833480835, acc = 0.853515625\n",
            "Batch 110: loss = 0.3606671690940857, acc = 0.8828125\n",
            "Batch 111: loss = 0.4320148825645447, acc = 0.8466796875\n",
            "Batch 112: loss = 0.39082661271095276, acc = 0.8740234375\n",
            "Batch 113: loss = 0.39331597089767456, acc = 0.8583984375\n",
            "Batch 114: loss = 0.4096642732620239, acc = 0.8623046875\n",
            "Batch 115: loss = 0.42627090215682983, acc = 0.8564453125\n",
            "Batch 116: loss = 0.4471328854560852, acc = 0.8427734375\n",
            "Batch 117: loss = 0.3781147599220276, acc = 0.8759765625\n",
            "Batch 118: loss = 0.36322465538978577, acc = 0.8857421875\n",
            "Batch 119: loss = 0.41781914234161377, acc = 0.861328125\n",
            "Batch 120: loss = 0.39328286051750183, acc = 0.8720703125\n",
            "Batch 121: loss = 0.4103183150291443, acc = 0.859375\n",
            "Batch 122: loss = 0.4158620536327362, acc = 0.8603515625\n",
            "Batch 123: loss = 0.4000205397605896, acc = 0.8623046875\n",
            "Batch 124: loss = 0.3933155834674835, acc = 0.8681640625\n",
            "Batch 125: loss = 0.38803932070732117, acc = 0.861328125\n",
            "Batch 126: loss = 0.4318831264972687, acc = 0.8544921875\n",
            "\n",
            "Epoch 70/100\n",
            "Batch 1: loss = 0.521192193031311, acc = 0.83984375\n",
            "Batch 2: loss = 0.4729471206665039, acc = 0.8427734375\n",
            "Batch 3: loss = 0.4620220959186554, acc = 0.841796875\n",
            "Batch 4: loss = 0.38561689853668213, acc = 0.8740234375\n",
            "Batch 5: loss = 0.4274550974369049, acc = 0.8544921875\n",
            "Batch 6: loss = 0.4421643912792206, acc = 0.853515625\n",
            "Batch 7: loss = 0.3946482539176941, acc = 0.8681640625\n",
            "Batch 8: loss = 0.4014125466346741, acc = 0.8681640625\n",
            "Batch 9: loss = 0.3735681474208832, acc = 0.8740234375\n",
            "Batch 10: loss = 0.3387620747089386, acc = 0.880859375\n",
            "Batch 11: loss = 0.3656657636165619, acc = 0.880859375\n",
            "Batch 12: loss = 0.4229934513568878, acc = 0.853515625\n",
            "Batch 13: loss = 0.3988263010978699, acc = 0.87109375\n",
            "Batch 14: loss = 0.3731115758419037, acc = 0.8642578125\n",
            "Batch 15: loss = 0.3874233365058899, acc = 0.87109375\n",
            "Batch 16: loss = 0.4325142204761505, acc = 0.8466796875\n",
            "Batch 17: loss = 0.3796628713607788, acc = 0.8876953125\n",
            "Batch 18: loss = 0.4231085181236267, acc = 0.861328125\n",
            "Batch 19: loss = 0.4092904329299927, acc = 0.86328125\n",
            "Batch 20: loss = 0.4190930724143982, acc = 0.8681640625\n",
            "Batch 21: loss = 0.42869627475738525, acc = 0.853515625\n",
            "Batch 22: loss = 0.384535551071167, acc = 0.8740234375\n",
            "Batch 23: loss = 0.39371755719184875, acc = 0.86328125\n",
            "Batch 24: loss = 0.4427456259727478, acc = 0.8515625\n",
            "Batch 25: loss = 0.3996693193912506, acc = 0.8720703125\n",
            "Batch 26: loss = 0.35682329535484314, acc = 0.8779296875\n",
            "Batch 27: loss = 0.42292073369026184, acc = 0.8583984375\n",
            "Batch 28: loss = 0.3825129270553589, acc = 0.86328125\n",
            "Batch 29: loss = 0.399985134601593, acc = 0.849609375\n",
            "Batch 30: loss = 0.416165828704834, acc = 0.8486328125\n",
            "Batch 31: loss = 0.4020259976387024, acc = 0.8681640625\n",
            "Batch 32: loss = 0.4178435206413269, acc = 0.8583984375\n",
            "Batch 33: loss = 0.38964250683784485, acc = 0.8701171875\n",
            "Batch 34: loss = 0.4020816385746002, acc = 0.859375\n",
            "Batch 35: loss = 0.3572455048561096, acc = 0.873046875\n",
            "Batch 36: loss = 0.39625126123428345, acc = 0.861328125\n",
            "Batch 37: loss = 0.38949719071388245, acc = 0.876953125\n",
            "Batch 38: loss = 0.3677317500114441, acc = 0.890625\n",
            "Batch 39: loss = 0.39338698983192444, acc = 0.8662109375\n",
            "Batch 40: loss = 0.38276407122612, acc = 0.8671875\n",
            "Batch 41: loss = 0.3770945370197296, acc = 0.8759765625\n",
            "Batch 42: loss = 0.39695560932159424, acc = 0.8701171875\n",
            "Batch 43: loss = 0.41506731510162354, acc = 0.845703125\n",
            "Batch 44: loss = 0.3806157410144806, acc = 0.8740234375\n",
            "Batch 45: loss = 0.3374032974243164, acc = 0.8857421875\n",
            "Batch 46: loss = 0.35955503582954407, acc = 0.875\n",
            "Batch 47: loss = 0.36476510763168335, acc = 0.8779296875\n",
            "Batch 48: loss = 0.3565017879009247, acc = 0.8779296875\n",
            "Batch 49: loss = 0.36353474855422974, acc = 0.8857421875\n",
            "Batch 50: loss = 0.3808027505874634, acc = 0.87109375\n",
            "Batch 51: loss = 0.3868851363658905, acc = 0.8681640625\n",
            "Batch 52: loss = 0.39940810203552246, acc = 0.8525390625\n",
            "Batch 53: loss = 0.4147171676158905, acc = 0.8671875\n",
            "Batch 54: loss = 0.3061360716819763, acc = 0.900390625\n",
            "Batch 55: loss = 0.34138697385787964, acc = 0.888671875\n",
            "Batch 56: loss = 0.3872964382171631, acc = 0.853515625\n",
            "Batch 57: loss = 0.4087594747543335, acc = 0.8603515625\n",
            "Batch 58: loss = 0.4374374449253082, acc = 0.8583984375\n",
            "Batch 59: loss = 0.3161778151988983, acc = 0.9013671875\n",
            "Batch 60: loss = 0.399720698595047, acc = 0.85546875\n",
            "Batch 61: loss = 0.3545142710208893, acc = 0.8857421875\n",
            "Batch 62: loss = 0.4823782742023468, acc = 0.8388671875\n",
            "Batch 63: loss = 0.41226544976234436, acc = 0.869140625\n",
            "Batch 64: loss = 0.32428663969039917, acc = 0.8935546875\n",
            "Batch 65: loss = 0.41855588555336, acc = 0.8662109375\n",
            "Batch 66: loss = 0.4335232675075531, acc = 0.8466796875\n",
            "Batch 67: loss = 0.36862608790397644, acc = 0.8818359375\n",
            "Batch 68: loss = 0.43082454800605774, acc = 0.84765625\n",
            "Batch 69: loss = 0.3538908362388611, acc = 0.8779296875\n",
            "Batch 70: loss = 0.4123073220252991, acc = 0.8671875\n",
            "Batch 71: loss = 0.4402959942817688, acc = 0.8525390625\n",
            "Batch 72: loss = 0.41086235642433167, acc = 0.8642578125\n",
            "Batch 73: loss = 0.44171422719955444, acc = 0.8583984375\n",
            "Batch 74: loss = 0.4090092182159424, acc = 0.865234375\n",
            "Batch 75: loss = 0.46012407541275024, acc = 0.833984375\n",
            "Batch 76: loss = 0.44481346011161804, acc = 0.8486328125\n",
            "Batch 77: loss = 0.3733200132846832, acc = 0.8779296875\n",
            "Batch 78: loss = 0.4042755961418152, acc = 0.87109375\n",
            "Batch 79: loss = 0.4215155839920044, acc = 0.869140625\n",
            "Batch 80: loss = 0.37766754627227783, acc = 0.861328125\n",
            "Batch 81: loss = 0.43640926480293274, acc = 0.8447265625\n",
            "Batch 82: loss = 0.3830099105834961, acc = 0.869140625\n",
            "Batch 83: loss = 0.38720715045928955, acc = 0.8681640625\n",
            "Batch 84: loss = 0.3768436312675476, acc = 0.869140625\n",
            "Batch 85: loss = 0.3972601294517517, acc = 0.8623046875\n",
            "Batch 86: loss = 0.4164164960384369, acc = 0.8583984375\n",
            "Batch 87: loss = 0.41549748182296753, acc = 0.8740234375\n",
            "Batch 88: loss = 0.4582321047782898, acc = 0.8466796875\n",
            "Batch 89: loss = 0.38260215520858765, acc = 0.861328125\n",
            "Batch 90: loss = 0.4335044026374817, acc = 0.8583984375\n",
            "Batch 91: loss = 0.4362258315086365, acc = 0.8408203125\n",
            "Batch 92: loss = 0.41898098587989807, acc = 0.86328125\n",
            "Batch 93: loss = 0.3728841245174408, acc = 0.8798828125\n",
            "Batch 94: loss = 0.3725750148296356, acc = 0.8798828125\n",
            "Batch 95: loss = 0.3537430465221405, acc = 0.8779296875\n",
            "Batch 96: loss = 0.4634884297847748, acc = 0.8466796875\n",
            "Batch 97: loss = 0.38706889748573303, acc = 0.869140625\n",
            "Batch 98: loss = 0.41121935844421387, acc = 0.85546875\n",
            "Batch 99: loss = 0.3994503915309906, acc = 0.875\n",
            "Batch 100: loss = 0.4012085199356079, acc = 0.85546875\n",
            "Batch 101: loss = 0.4035507142543793, acc = 0.8720703125\n",
            "Batch 102: loss = 0.47261306643486023, acc = 0.8486328125\n",
            "Batch 103: loss = 0.37871718406677246, acc = 0.87890625\n",
            "Batch 104: loss = 0.3749068081378937, acc = 0.873046875\n",
            "Batch 105: loss = 0.3942965865135193, acc = 0.8701171875\n",
            "Batch 106: loss = 0.387367844581604, acc = 0.873046875\n",
            "Batch 107: loss = 0.4209442734718323, acc = 0.8515625\n",
            "Batch 108: loss = 0.40085548162460327, acc = 0.859375\n",
            "Batch 109: loss = 0.40808579325675964, acc = 0.8544921875\n",
            "Batch 110: loss = 0.3529421389102936, acc = 0.873046875\n",
            "Batch 111: loss = 0.39186277985572815, acc = 0.865234375\n",
            "Batch 112: loss = 0.4053449332714081, acc = 0.857421875\n",
            "Batch 113: loss = 0.4245167374610901, acc = 0.86328125\n",
            "Batch 114: loss = 0.43717291951179504, acc = 0.8583984375\n",
            "Batch 115: loss = 0.44250497221946716, acc = 0.8603515625\n",
            "Batch 116: loss = 0.4159190058708191, acc = 0.861328125\n",
            "Batch 117: loss = 0.3954881727695465, acc = 0.880859375\n",
            "Batch 118: loss = 0.35842785239219666, acc = 0.88671875\n",
            "Batch 119: loss = 0.3814432621002197, acc = 0.88671875\n",
            "Batch 120: loss = 0.3218920826911926, acc = 0.8896484375\n",
            "Batch 121: loss = 0.42119336128234863, acc = 0.8583984375\n",
            "Batch 122: loss = 0.35558366775512695, acc = 0.8720703125\n",
            "Batch 123: loss = 0.4165487289428711, acc = 0.859375\n",
            "Batch 124: loss = 0.4190608859062195, acc = 0.8603515625\n",
            "Batch 125: loss = 0.37600818276405334, acc = 0.8564453125\n",
            "Batch 126: loss = 0.4114200472831726, acc = 0.859375\n",
            "Saved checkpoint to gru_weights.70.h5\n",
            "\n",
            "Epoch 71/100\n",
            "Batch 1: loss = 0.5356903672218323, acc = 0.8349609375\n",
            "Batch 2: loss = 0.45644569396972656, acc = 0.8447265625\n",
            "Batch 3: loss = 0.4386328458786011, acc = 0.8623046875\n",
            "Batch 4: loss = 0.3758165240287781, acc = 0.86328125\n",
            "Batch 5: loss = 0.41893720626831055, acc = 0.8623046875\n",
            "Batch 6: loss = 0.44745373725891113, acc = 0.861328125\n",
            "Batch 7: loss = 0.3635108172893524, acc = 0.8759765625\n",
            "Batch 8: loss = 0.40388739109039307, acc = 0.8701171875\n",
            "Batch 9: loss = 0.3929547071456909, acc = 0.8720703125\n",
            "Batch 10: loss = 0.3591301441192627, acc = 0.8671875\n",
            "Batch 11: loss = 0.3744248151779175, acc = 0.876953125\n",
            "Batch 12: loss = 0.4001621901988983, acc = 0.8603515625\n",
            "Batch 13: loss = 0.39090976119041443, acc = 0.865234375\n",
            "Batch 14: loss = 0.3535026013851166, acc = 0.8876953125\n",
            "Batch 15: loss = 0.32377246022224426, acc = 0.896484375\n",
            "Batch 16: loss = 0.41901594400405884, acc = 0.8544921875\n",
            "Batch 17: loss = 0.3715883195400238, acc = 0.8779296875\n",
            "Batch 18: loss = 0.40851399302482605, acc = 0.8701171875\n",
            "Batch 19: loss = 0.39498451352119446, acc = 0.8701171875\n",
            "Batch 20: loss = 0.3772240877151489, acc = 0.87109375\n",
            "Batch 21: loss = 0.40863656997680664, acc = 0.8544921875\n",
            "Batch 22: loss = 0.385842502117157, acc = 0.8701171875\n",
            "Batch 23: loss = 0.36079442501068115, acc = 0.8759765625\n",
            "Batch 24: loss = 0.38061392307281494, acc = 0.880859375\n",
            "Batch 25: loss = 0.3923024833202362, acc = 0.8671875\n",
            "Batch 26: loss = 0.3968903422355652, acc = 0.86328125\n",
            "Batch 27: loss = 0.4395610988140106, acc = 0.853515625\n",
            "Batch 28: loss = 0.4132227599620819, acc = 0.857421875\n",
            "Batch 29: loss = 0.3548145294189453, acc = 0.876953125\n",
            "Batch 30: loss = 0.3840256929397583, acc = 0.869140625\n",
            "Batch 31: loss = 0.39464306831359863, acc = 0.8623046875\n",
            "Batch 32: loss = 0.45743900537490845, acc = 0.8427734375\n",
            "Batch 33: loss = 0.4288763105869293, acc = 0.86328125\n",
            "Batch 34: loss = 0.40470612049102783, acc = 0.875\n",
            "Batch 35: loss = 0.40157538652420044, acc = 0.8662109375\n",
            "Batch 36: loss = 0.38838762044906616, acc = 0.8701171875\n",
            "Batch 37: loss = 0.365984708070755, acc = 0.876953125\n",
            "Batch 38: loss = 0.35126397013664246, acc = 0.8837890625\n",
            "Batch 39: loss = 0.3722347617149353, acc = 0.8779296875\n",
            "Batch 40: loss = 0.3926408886909485, acc = 0.8740234375\n",
            "Batch 41: loss = 0.346182644367218, acc = 0.8876953125\n",
            "Batch 42: loss = 0.38624483346939087, acc = 0.87109375\n",
            "Batch 43: loss = 0.41072192788124084, acc = 0.85546875\n",
            "Batch 44: loss = 0.368917852640152, acc = 0.869140625\n",
            "Batch 45: loss = 0.3661457300186157, acc = 0.8662109375\n",
            "Batch 46: loss = 0.3950023055076599, acc = 0.8642578125\n",
            "Batch 47: loss = 0.35019156336784363, acc = 0.8818359375\n",
            "Batch 48: loss = 0.3217034935951233, acc = 0.8984375\n",
            "Batch 49: loss = 0.3737691044807434, acc = 0.8876953125\n",
            "Batch 50: loss = 0.4267621338367462, acc = 0.8564453125\n",
            "Batch 51: loss = 0.40161043405532837, acc = 0.8642578125\n",
            "Batch 52: loss = 0.3862762153148651, acc = 0.8681640625\n",
            "Batch 53: loss = 0.40851908922195435, acc = 0.8623046875\n",
            "Batch 54: loss = 0.3077053725719452, acc = 0.896484375\n",
            "Batch 55: loss = 0.3669254779815674, acc = 0.8798828125\n",
            "Batch 56: loss = 0.3807159662246704, acc = 0.8779296875\n",
            "Batch 57: loss = 0.4489002823829651, acc = 0.83984375\n",
            "Batch 58: loss = 0.3928534686565399, acc = 0.8681640625\n",
            "Batch 59: loss = 0.3509964644908905, acc = 0.8857421875\n",
            "Batch 60: loss = 0.3994824290275574, acc = 0.869140625\n",
            "Batch 61: loss = 0.36575743556022644, acc = 0.880859375\n",
            "Batch 62: loss = 0.4326894283294678, acc = 0.8603515625\n",
            "Batch 63: loss = 0.36783450841903687, acc = 0.8671875\n",
            "Batch 64: loss = 0.343651682138443, acc = 0.8818359375\n",
            "Batch 65: loss = 0.42112067341804504, acc = 0.85546875\n",
            "Batch 66: loss = 0.40253207087516785, acc = 0.8701171875\n",
            "Batch 67: loss = 0.41506338119506836, acc = 0.861328125\n",
            "Batch 68: loss = 0.41710376739501953, acc = 0.8642578125\n",
            "Batch 69: loss = 0.37326520681381226, acc = 0.8720703125\n",
            "Batch 70: loss = 0.43200138211250305, acc = 0.8564453125\n",
            "Batch 71: loss = 0.43525978922843933, acc = 0.8447265625\n",
            "Batch 72: loss = 0.364836186170578, acc = 0.8837890625\n",
            "Batch 73: loss = 0.4320808947086334, acc = 0.8525390625\n",
            "Batch 74: loss = 0.4134100377559662, acc = 0.8486328125\n",
            "Batch 75: loss = 0.4750511646270752, acc = 0.84765625\n",
            "Batch 76: loss = 0.3921588957309723, acc = 0.8701171875\n",
            "Batch 77: loss = 0.3868740499019623, acc = 0.8779296875\n",
            "Batch 78: loss = 0.39511069655418396, acc = 0.8662109375\n",
            "Batch 79: loss = 0.40878212451934814, acc = 0.8681640625\n",
            "Batch 80: loss = 0.35264888405799866, acc = 0.8818359375\n",
            "Batch 81: loss = 0.4200766086578369, acc = 0.8515625\n",
            "Batch 82: loss = 0.38027817010879517, acc = 0.8720703125\n",
            "Batch 83: loss = 0.3996395468711853, acc = 0.87109375\n",
            "Batch 84: loss = 0.3855145275592804, acc = 0.869140625\n",
            "Batch 85: loss = 0.40662699937820435, acc = 0.859375\n",
            "Batch 86: loss = 0.40878796577453613, acc = 0.8603515625\n",
            "Batch 87: loss = 0.39550578594207764, acc = 0.8603515625\n",
            "Batch 88: loss = 0.4690110981464386, acc = 0.8427734375\n",
            "Batch 89: loss = 0.3670823574066162, acc = 0.892578125\n",
            "Batch 90: loss = 0.4073428511619568, acc = 0.8583984375\n",
            "Batch 91: loss = 0.4435111880302429, acc = 0.8525390625\n",
            "Batch 92: loss = 0.4264974892139435, acc = 0.8642578125\n",
            "Batch 93: loss = 0.3677293658256531, acc = 0.8916015625\n",
            "Batch 94: loss = 0.39785507321357727, acc = 0.869140625\n",
            "Batch 95: loss = 0.3820207118988037, acc = 0.869140625\n",
            "Batch 96: loss = 0.44514167308807373, acc = 0.85546875\n",
            "Batch 97: loss = 0.4328802227973938, acc = 0.857421875\n",
            "Batch 98: loss = 0.4125165343284607, acc = 0.8564453125\n",
            "Batch 99: loss = 0.3691805899143219, acc = 0.8720703125\n",
            "Batch 100: loss = 0.4532787799835205, acc = 0.8447265625\n",
            "Batch 101: loss = 0.4082562029361725, acc = 0.8564453125\n",
            "Batch 102: loss = 0.431843101978302, acc = 0.8544921875\n",
            "Batch 103: loss = 0.3822457790374756, acc = 0.8740234375\n",
            "Batch 104: loss = 0.3095819354057312, acc = 0.888671875\n",
            "Batch 105: loss = 0.33873820304870605, acc = 0.888671875\n",
            "Batch 106: loss = 0.4259723424911499, acc = 0.8583984375\n",
            "Batch 107: loss = 0.4045667052268982, acc = 0.8720703125\n",
            "Batch 108: loss = 0.38454994559288025, acc = 0.8662109375\n",
            "Batch 109: loss = 0.39341026544570923, acc = 0.876953125\n",
            "Batch 110: loss = 0.36635133624076843, acc = 0.87109375\n",
            "Batch 111: loss = 0.3934101164340973, acc = 0.8662109375\n",
            "Batch 112: loss = 0.3842795491218567, acc = 0.8642578125\n",
            "Batch 113: loss = 0.40317073464393616, acc = 0.8642578125\n",
            "Batch 114: loss = 0.40893667936325073, acc = 0.8642578125\n",
            "Batch 115: loss = 0.4130920171737671, acc = 0.8681640625\n",
            "Batch 116: loss = 0.45493990182876587, acc = 0.8349609375\n",
            "Batch 117: loss = 0.4078444242477417, acc = 0.8603515625\n",
            "Batch 118: loss = 0.3766802251338959, acc = 0.8759765625\n",
            "Batch 119: loss = 0.36905455589294434, acc = 0.8916015625\n",
            "Batch 120: loss = 0.377718061208725, acc = 0.8779296875\n",
            "Batch 121: loss = 0.3684489130973816, acc = 0.8759765625\n",
            "Batch 122: loss = 0.39054393768310547, acc = 0.8642578125\n",
            "Batch 123: loss = 0.4122632145881653, acc = 0.8583984375\n",
            "Batch 124: loss = 0.4577823579311371, acc = 0.84765625\n",
            "Batch 125: loss = 0.40377023816108704, acc = 0.85546875\n",
            "Batch 126: loss = 0.4183196425437927, acc = 0.8603515625\n",
            "\n",
            "Epoch 72/100\n",
            "Batch 1: loss = 0.5238889455795288, acc = 0.8388671875\n",
            "Batch 2: loss = 0.46016424894332886, acc = 0.8515625\n",
            "Batch 3: loss = 0.4078376293182373, acc = 0.853515625\n",
            "Batch 4: loss = 0.3610993027687073, acc = 0.8837890625\n",
            "Batch 5: loss = 0.38873589038848877, acc = 0.8798828125\n",
            "Batch 6: loss = 0.4907034635543823, acc = 0.8388671875\n",
            "Batch 7: loss = 0.3744395971298218, acc = 0.87890625\n",
            "Batch 8: loss = 0.39428675174713135, acc = 0.865234375\n",
            "Batch 9: loss = 0.3759140074253082, acc = 0.8779296875\n",
            "Batch 10: loss = 0.33483415842056274, acc = 0.8837890625\n",
            "Batch 11: loss = 0.3827360272407532, acc = 0.869140625\n",
            "Batch 12: loss = 0.386402428150177, acc = 0.875\n",
            "Batch 13: loss = 0.3605941832065582, acc = 0.880859375\n",
            "Batch 14: loss = 0.37498268485069275, acc = 0.873046875\n",
            "Batch 15: loss = 0.37305551767349243, acc = 0.8720703125\n",
            "Batch 16: loss = 0.40621036291122437, acc = 0.86328125\n",
            "Batch 17: loss = 0.37219592928886414, acc = 0.87890625\n",
            "Batch 18: loss = 0.425601989030838, acc = 0.8515625\n",
            "Batch 19: loss = 0.3892366886138916, acc = 0.8720703125\n",
            "Batch 20: loss = 0.41156959533691406, acc = 0.8642578125\n",
            "Batch 21: loss = 0.4226510524749756, acc = 0.8466796875\n",
            "Batch 22: loss = 0.36883699893951416, acc = 0.869140625\n",
            "Batch 23: loss = 0.40609174966812134, acc = 0.8642578125\n",
            "Batch 24: loss = 0.4128134846687317, acc = 0.853515625\n",
            "Batch 25: loss = 0.3834517002105713, acc = 0.8701171875\n",
            "Batch 26: loss = 0.4195398688316345, acc = 0.857421875\n",
            "Batch 27: loss = 0.41586336493492126, acc = 0.8564453125\n",
            "Batch 28: loss = 0.41470059752464294, acc = 0.869140625\n",
            "Batch 29: loss = 0.3708358407020569, acc = 0.8740234375\n",
            "Batch 30: loss = 0.4147688150405884, acc = 0.8603515625\n",
            "Batch 31: loss = 0.40957674384117126, acc = 0.869140625\n",
            "Batch 32: loss = 0.4321899116039276, acc = 0.853515625\n",
            "Batch 33: loss = 0.38893893361091614, acc = 0.8779296875\n",
            "Batch 34: loss = 0.4347570538520813, acc = 0.8427734375\n",
            "Batch 35: loss = 0.39234352111816406, acc = 0.8759765625\n",
            "Batch 36: loss = 0.3735135793685913, acc = 0.876953125\n",
            "Batch 37: loss = 0.3577074408531189, acc = 0.8798828125\n",
            "Batch 38: loss = 0.3998032808303833, acc = 0.8671875\n",
            "Batch 39: loss = 0.3617526590824127, acc = 0.880859375\n",
            "Batch 40: loss = 0.40417754650115967, acc = 0.8701171875\n",
            "Batch 41: loss = 0.3952358365058899, acc = 0.8701171875\n",
            "Batch 42: loss = 0.35913920402526855, acc = 0.8818359375\n",
            "Batch 43: loss = 0.38474833965301514, acc = 0.8623046875\n",
            "Batch 44: loss = 0.3862602114677429, acc = 0.8720703125\n",
            "Batch 45: loss = 0.34199294447898865, acc = 0.880859375\n",
            "Batch 46: loss = 0.38230279088020325, acc = 0.869140625\n",
            "Batch 47: loss = 0.4019009470939636, acc = 0.865234375\n",
            "Batch 48: loss = 0.3537258505821228, acc = 0.8779296875\n",
            "Batch 49: loss = 0.347176730632782, acc = 0.8828125\n",
            "Batch 50: loss = 0.40724071860313416, acc = 0.85546875\n",
            "Batch 51: loss = 0.4250025153160095, acc = 0.857421875\n",
            "Batch 52: loss = 0.361880898475647, acc = 0.8740234375\n",
            "Batch 53: loss = 0.3783034086227417, acc = 0.8779296875\n",
            "Batch 54: loss = 0.3100728988647461, acc = 0.9033203125\n",
            "Batch 55: loss = 0.359188973903656, acc = 0.8720703125\n",
            "Batch 56: loss = 0.3876437544822693, acc = 0.8642578125\n",
            "Batch 57: loss = 0.45428791642189026, acc = 0.8447265625\n",
            "Batch 58: loss = 0.3906730115413666, acc = 0.8759765625\n",
            "Batch 59: loss = 0.3438737392425537, acc = 0.888671875\n",
            "Batch 60: loss = 0.40213456749916077, acc = 0.8642578125\n",
            "Batch 61: loss = 0.4302489757537842, acc = 0.8564453125\n",
            "Batch 62: loss = 0.43657341599464417, acc = 0.85546875\n",
            "Batch 63: loss = 0.38353651762008667, acc = 0.875\n",
            "Batch 64: loss = 0.32478004693984985, acc = 0.900390625\n",
            "Batch 65: loss = 0.3868306577205658, acc = 0.87109375\n",
            "Batch 66: loss = 0.4029364585876465, acc = 0.8525390625\n",
            "Batch 67: loss = 0.38101711869239807, acc = 0.87109375\n",
            "Batch 68: loss = 0.40780776739120483, acc = 0.859375\n",
            "Batch 69: loss = 0.35254836082458496, acc = 0.8837890625\n",
            "Batch 70: loss = 0.4488707184791565, acc = 0.8544921875\n",
            "Batch 71: loss = 0.4515800476074219, acc = 0.8505859375\n",
            "Batch 72: loss = 0.3729361891746521, acc = 0.8720703125\n",
            "Batch 73: loss = 0.45337745547294617, acc = 0.861328125\n",
            "Batch 74: loss = 0.4276055693626404, acc = 0.8583984375\n",
            "Batch 75: loss = 0.45623913407325745, acc = 0.845703125\n",
            "Batch 76: loss = 0.3931795656681061, acc = 0.86328125\n",
            "Batch 77: loss = 0.38756462931632996, acc = 0.859375\n",
            "Batch 78: loss = 0.38903266191482544, acc = 0.8818359375\n",
            "Batch 79: loss = 0.371089369058609, acc = 0.87890625\n",
            "Batch 80: loss = 0.39095813035964966, acc = 0.8701171875\n",
            "Batch 81: loss = 0.42699307203292847, acc = 0.8623046875\n",
            "Batch 82: loss = 0.38114434480667114, acc = 0.8623046875\n",
            "Batch 83: loss = 0.42873623967170715, acc = 0.859375\n",
            "Batch 84: loss = 0.3797631561756134, acc = 0.8642578125\n",
            "Batch 85: loss = 0.4166656732559204, acc = 0.8583984375\n",
            "Batch 86: loss = 0.41303879022598267, acc = 0.859375\n",
            "Batch 87: loss = 0.3943909704685211, acc = 0.8720703125\n",
            "Batch 88: loss = 0.4721640944480896, acc = 0.837890625\n",
            "Batch 89: loss = 0.37360879778862, acc = 0.8740234375\n",
            "Batch 90: loss = 0.40490010380744934, acc = 0.8671875\n",
            "Batch 91: loss = 0.46153271198272705, acc = 0.833984375\n",
            "Batch 92: loss = 0.4748271107673645, acc = 0.84765625\n",
            "Batch 93: loss = 0.36811643838882446, acc = 0.8779296875\n",
            "Batch 94: loss = 0.4044482111930847, acc = 0.8662109375\n",
            "Batch 95: loss = 0.33552223443984985, acc = 0.8828125\n",
            "Batch 96: loss = 0.4272313416004181, acc = 0.853515625\n",
            "Batch 97: loss = 0.40876656770706177, acc = 0.869140625\n",
            "Batch 98: loss = 0.397522509098053, acc = 0.8662109375\n",
            "Batch 99: loss = 0.4466779828071594, acc = 0.857421875\n",
            "Batch 100: loss = 0.41335853934288025, acc = 0.85546875\n",
            "Batch 101: loss = 0.4132169485092163, acc = 0.8583984375\n",
            "Batch 102: loss = 0.4388289749622345, acc = 0.83984375\n",
            "Batch 103: loss = 0.3911322355270386, acc = 0.859375\n",
            "Batch 104: loss = 0.3242813050746918, acc = 0.88671875\n",
            "Batch 105: loss = 0.34060144424438477, acc = 0.884765625\n",
            "Batch 106: loss = 0.38587597012519836, acc = 0.8671875\n",
            "Batch 107: loss = 0.40292802453041077, acc = 0.869140625\n",
            "Batch 108: loss = 0.3952329158782959, acc = 0.869140625\n",
            "Batch 109: loss = 0.38504159450531006, acc = 0.8662109375\n",
            "Batch 110: loss = 0.36442622542381287, acc = 0.888671875\n",
            "Batch 111: loss = 0.4191768169403076, acc = 0.859375\n",
            "Batch 112: loss = 0.41183924674987793, acc = 0.8759765625\n",
            "Batch 113: loss = 0.4330921769142151, acc = 0.8583984375\n",
            "Batch 114: loss = 0.44890281558036804, acc = 0.8525390625\n",
            "Batch 115: loss = 0.40370017290115356, acc = 0.869140625\n",
            "Batch 116: loss = 0.4189821481704712, acc = 0.8720703125\n",
            "Batch 117: loss = 0.3587763011455536, acc = 0.8740234375\n",
            "Batch 118: loss = 0.3883606195449829, acc = 0.87109375\n",
            "Batch 119: loss = 0.3995971083641052, acc = 0.865234375\n",
            "Batch 120: loss = 0.3776339888572693, acc = 0.8671875\n",
            "Batch 121: loss = 0.3861233592033386, acc = 0.86328125\n",
            "Batch 122: loss = 0.42338240146636963, acc = 0.8564453125\n",
            "Batch 123: loss = 0.39451977610588074, acc = 0.8740234375\n",
            "Batch 124: loss = 0.38914668560028076, acc = 0.8759765625\n",
            "Batch 125: loss = 0.40081334114074707, acc = 0.8681640625\n",
            "Batch 126: loss = 0.40878942608833313, acc = 0.865234375\n",
            "\n",
            "Epoch 73/100\n",
            "Batch 1: loss = 0.5141838788986206, acc = 0.837890625\n",
            "Batch 2: loss = 0.3999214172363281, acc = 0.8583984375\n",
            "Batch 3: loss = 0.40846359729766846, acc = 0.87109375\n",
            "Batch 4: loss = 0.3452777862548828, acc = 0.8857421875\n",
            "Batch 5: loss = 0.41388246417045593, acc = 0.8642578125\n",
            "Batch 6: loss = 0.42912760376930237, acc = 0.8583984375\n",
            "Batch 7: loss = 0.37624937295913696, acc = 0.873046875\n",
            "Batch 8: loss = 0.4293005168437958, acc = 0.8505859375\n",
            "Batch 9: loss = 0.3784043788909912, acc = 0.8623046875\n",
            "Batch 10: loss = 0.33701637387275696, acc = 0.8828125\n",
            "Batch 11: loss = 0.3627023994922638, acc = 0.876953125\n",
            "Batch 12: loss = 0.37680184841156006, acc = 0.87109375\n",
            "Batch 13: loss = 0.36045968532562256, acc = 0.880859375\n",
            "Batch 14: loss = 0.35856911540031433, acc = 0.87890625\n",
            "Batch 15: loss = 0.3401066064834595, acc = 0.888671875\n",
            "Batch 16: loss = 0.3918585479259491, acc = 0.873046875\n",
            "Batch 17: loss = 0.36948034167289734, acc = 0.884765625\n",
            "Batch 18: loss = 0.41569000482559204, acc = 0.859375\n",
            "Batch 19: loss = 0.3671436011791229, acc = 0.8740234375\n",
            "Batch 20: loss = 0.3870992362499237, acc = 0.8740234375\n",
            "Batch 21: loss = 0.4359644055366516, acc = 0.8544921875\n",
            "Batch 22: loss = 0.36196786165237427, acc = 0.8798828125\n",
            "Batch 23: loss = 0.3785809576511383, acc = 0.8671875\n",
            "Batch 24: loss = 0.4096950590610504, acc = 0.85546875\n",
            "Batch 25: loss = 0.3727342188358307, acc = 0.8779296875\n",
            "Batch 26: loss = 0.35981321334838867, acc = 0.8828125\n",
            "Batch 27: loss = 0.4462345540523529, acc = 0.84765625\n",
            "Batch 28: loss = 0.38648003339767456, acc = 0.8623046875\n",
            "Batch 29: loss = 0.3989127278327942, acc = 0.857421875\n",
            "Batch 30: loss = 0.36936262249946594, acc = 0.8671875\n",
            "Batch 31: loss = 0.4115707278251648, acc = 0.8603515625\n",
            "Batch 32: loss = 0.43328967690467834, acc = 0.8525390625\n",
            "Batch 33: loss = 0.3994724452495575, acc = 0.8681640625\n",
            "Batch 34: loss = 0.38936102390289307, acc = 0.875\n",
            "Batch 35: loss = 0.380307674407959, acc = 0.8720703125\n",
            "Batch 36: loss = 0.3658193349838257, acc = 0.873046875\n",
            "Batch 37: loss = 0.351784348487854, acc = 0.8876953125\n",
            "Batch 38: loss = 0.3309745788574219, acc = 0.88671875\n",
            "Batch 39: loss = 0.34117597341537476, acc = 0.8828125\n",
            "Batch 40: loss = 0.3772244453430176, acc = 0.8662109375\n",
            "Batch 41: loss = 0.37219080328941345, acc = 0.8759765625\n",
            "Batch 42: loss = 0.352891206741333, acc = 0.8876953125\n",
            "Batch 43: loss = 0.4429510831832886, acc = 0.8427734375\n",
            "Batch 44: loss = 0.4113295078277588, acc = 0.8642578125\n",
            "Batch 45: loss = 0.3477405905723572, acc = 0.884765625\n",
            "Batch 46: loss = 0.3943060636520386, acc = 0.869140625\n",
            "Batch 47: loss = 0.3845018148422241, acc = 0.869140625\n",
            "Batch 48: loss = 0.3413834571838379, acc = 0.8876953125\n",
            "Batch 49: loss = 0.3569105863571167, acc = 0.8837890625\n",
            "Batch 50: loss = 0.3668164610862732, acc = 0.875\n",
            "Batch 51: loss = 0.404614120721817, acc = 0.857421875\n",
            "Batch 52: loss = 0.4013001620769501, acc = 0.8720703125\n",
            "Batch 53: loss = 0.4043765962123871, acc = 0.8623046875\n",
            "Batch 54: loss = 0.3381651043891907, acc = 0.8837890625\n",
            "Batch 55: loss = 0.35828065872192383, acc = 0.8857421875\n",
            "Batch 56: loss = 0.3672080934047699, acc = 0.8662109375\n",
            "Batch 57: loss = 0.42475274205207825, acc = 0.8544921875\n",
            "Batch 58: loss = 0.3997284471988678, acc = 0.8623046875\n",
            "Batch 59: loss = 0.3544038236141205, acc = 0.8857421875\n",
            "Batch 60: loss = 0.410610556602478, acc = 0.8662109375\n",
            "Batch 61: loss = 0.36330148577690125, acc = 0.8798828125\n",
            "Batch 62: loss = 0.4707675576210022, acc = 0.8349609375\n",
            "Batch 63: loss = 0.3781563341617584, acc = 0.87109375\n",
            "Batch 64: loss = 0.3126518130302429, acc = 0.8955078125\n",
            "Batch 65: loss = 0.3955405354499817, acc = 0.8623046875\n",
            "Batch 66: loss = 0.3819630742073059, acc = 0.8779296875\n",
            "Batch 67: loss = 0.36641985177993774, acc = 0.876953125\n",
            "Batch 68: loss = 0.38677775859832764, acc = 0.853515625\n",
            "Batch 69: loss = 0.3512965142726898, acc = 0.8896484375\n",
            "Batch 70: loss = 0.40833038091659546, acc = 0.845703125\n",
            "Batch 71: loss = 0.411923885345459, acc = 0.857421875\n",
            "Batch 72: loss = 0.39451864361763, acc = 0.869140625\n",
            "Batch 73: loss = 0.40788349509239197, acc = 0.8681640625\n",
            "Batch 74: loss = 0.39257657527923584, acc = 0.8642578125\n",
            "Batch 75: loss = 0.47060441970825195, acc = 0.8369140625\n",
            "Batch 76: loss = 0.40486815571784973, acc = 0.86328125\n",
            "Batch 77: loss = 0.34898650646209717, acc = 0.8798828125\n",
            "Batch 78: loss = 0.3763728737831116, acc = 0.8623046875\n",
            "Batch 79: loss = 0.3716118037700653, acc = 0.87890625\n",
            "Batch 80: loss = 0.37478142976760864, acc = 0.8779296875\n",
            "Batch 81: loss = 0.4191768765449524, acc = 0.8623046875\n",
            "Batch 82: loss = 0.37689194083213806, acc = 0.873046875\n",
            "Batch 83: loss = 0.41536062955856323, acc = 0.861328125\n",
            "Batch 84: loss = 0.3366764485836029, acc = 0.87890625\n",
            "Batch 85: loss = 0.38310620188713074, acc = 0.86328125\n",
            "Batch 86: loss = 0.3981850743293762, acc = 0.8662109375\n",
            "Batch 87: loss = 0.40968164801597595, acc = 0.8740234375\n",
            "Batch 88: loss = 0.440155953168869, acc = 0.8388671875\n",
            "Batch 89: loss = 0.3915419280529022, acc = 0.8779296875\n",
            "Batch 90: loss = 0.4002162218093872, acc = 0.8681640625\n",
            "Batch 91: loss = 0.412437379360199, acc = 0.8662109375\n",
            "Batch 92: loss = 0.43884629011154175, acc = 0.849609375\n",
            "Batch 93: loss = 0.3532535433769226, acc = 0.884765625\n",
            "Batch 94: loss = 0.3669964671134949, acc = 0.8828125\n",
            "Batch 95: loss = 0.35576382279396057, acc = 0.8828125\n",
            "Batch 96: loss = 0.4109291136264801, acc = 0.8603515625\n",
            "Batch 97: loss = 0.4121371805667877, acc = 0.8642578125\n",
            "Batch 98: loss = 0.4314543604850769, acc = 0.853515625\n",
            "Batch 99: loss = 0.3864542841911316, acc = 0.8681640625\n",
            "Batch 100: loss = 0.36538752913475037, acc = 0.8671875\n",
            "Batch 101: loss = 0.39093858003616333, acc = 0.865234375\n",
            "Batch 102: loss = 0.4868815243244171, acc = 0.8388671875\n",
            "Batch 103: loss = 0.42879244685173035, acc = 0.857421875\n",
            "Batch 104: loss = 0.3466446101665497, acc = 0.875\n",
            "Batch 105: loss = 0.33805152773857117, acc = 0.9013671875\n",
            "Batch 106: loss = 0.39311254024505615, acc = 0.853515625\n",
            "Batch 107: loss = 0.3817554712295532, acc = 0.86328125\n",
            "Batch 108: loss = 0.37565112113952637, acc = 0.8701171875\n",
            "Batch 109: loss = 0.43573784828186035, acc = 0.84375\n",
            "Batch 110: loss = 0.3844526410102844, acc = 0.869140625\n",
            "Batch 111: loss = 0.3931705951690674, acc = 0.861328125\n",
            "Batch 112: loss = 0.39772742986679077, acc = 0.8671875\n",
            "Batch 113: loss = 0.3991169333457947, acc = 0.8603515625\n",
            "Batch 114: loss = 0.46389061212539673, acc = 0.8564453125\n",
            "Batch 115: loss = 0.4114300310611725, acc = 0.8544921875\n",
            "Batch 116: loss = 0.4463232159614563, acc = 0.84375\n",
            "Batch 117: loss = 0.39699119329452515, acc = 0.861328125\n",
            "Batch 118: loss = 0.34524115920066833, acc = 0.8857421875\n",
            "Batch 119: loss = 0.3997712731361389, acc = 0.8623046875\n",
            "Batch 120: loss = 0.34734058380126953, acc = 0.8720703125\n",
            "Batch 121: loss = 0.3979533314704895, acc = 0.859375\n",
            "Batch 122: loss = 0.36702245473861694, acc = 0.8701171875\n",
            "Batch 123: loss = 0.416188508272171, acc = 0.8720703125\n",
            "Batch 124: loss = 0.45674625039100647, acc = 0.8408203125\n",
            "Batch 125: loss = 0.4229482412338257, acc = 0.8564453125\n",
            "Batch 126: loss = 0.39595746994018555, acc = 0.8564453125\n",
            "\n",
            "Epoch 74/100\n",
            "Batch 1: loss = 0.4903582036495209, acc = 0.8369140625\n",
            "Batch 2: loss = 0.4630126357078552, acc = 0.84375\n",
            "Batch 3: loss = 0.44420498609542847, acc = 0.865234375\n",
            "Batch 4: loss = 0.4043274223804474, acc = 0.8671875\n",
            "Batch 5: loss = 0.4045030474662781, acc = 0.8681640625\n",
            "Batch 6: loss = 0.4647286534309387, acc = 0.8544921875\n",
            "Batch 7: loss = 0.3911024332046509, acc = 0.8642578125\n",
            "Batch 8: loss = 0.38782691955566406, acc = 0.8759765625\n",
            "Batch 9: loss = 0.39184466004371643, acc = 0.87109375\n",
            "Batch 10: loss = 0.37551915645599365, acc = 0.87109375\n",
            "Batch 11: loss = 0.4192104637622833, acc = 0.85546875\n",
            "Batch 12: loss = 0.4002037048339844, acc = 0.8759765625\n",
            "Batch 13: loss = 0.34808579087257385, acc = 0.884765625\n",
            "Batch 14: loss = 0.348259299993515, acc = 0.875\n",
            "Batch 15: loss = 0.3922025263309479, acc = 0.8701171875\n",
            "Batch 16: loss = 0.4088543653488159, acc = 0.86328125\n",
            "Batch 17: loss = 0.36867669224739075, acc = 0.8837890625\n",
            "Batch 18: loss = 0.40040719509124756, acc = 0.8701171875\n",
            "Batch 19: loss = 0.374806672334671, acc = 0.875\n",
            "Batch 20: loss = 0.3775866627693176, acc = 0.8740234375\n",
            "Batch 21: loss = 0.3968885540962219, acc = 0.87109375\n",
            "Batch 22: loss = 0.3797954320907593, acc = 0.8671875\n",
            "Batch 23: loss = 0.37748393416404724, acc = 0.8583984375\n",
            "Batch 24: loss = 0.3791702389717102, acc = 0.87890625\n",
            "Batch 25: loss = 0.38766172528266907, acc = 0.8798828125\n",
            "Batch 26: loss = 0.37348461151123047, acc = 0.875\n",
            "Batch 27: loss = 0.4252755343914032, acc = 0.859375\n",
            "Batch 28: loss = 0.4135826826095581, acc = 0.8583984375\n",
            "Batch 29: loss = 0.4040645658969879, acc = 0.859375\n",
            "Batch 30: loss = 0.34694963693618774, acc = 0.880859375\n",
            "Batch 31: loss = 0.4266400933265686, acc = 0.857421875\n",
            "Batch 32: loss = 0.42704883217811584, acc = 0.85546875\n",
            "Batch 33: loss = 0.38375598192214966, acc = 0.873046875\n",
            "Batch 34: loss = 0.38145747780799866, acc = 0.8720703125\n",
            "Batch 35: loss = 0.3777546286582947, acc = 0.880859375\n",
            "Batch 36: loss = 0.35712093114852905, acc = 0.8759765625\n",
            "Batch 37: loss = 0.39628830552101135, acc = 0.8740234375\n",
            "Batch 38: loss = 0.40363189578056335, acc = 0.8671875\n",
            "Batch 39: loss = 0.3514830768108368, acc = 0.880859375\n",
            "Batch 40: loss = 0.39269325137138367, acc = 0.8603515625\n",
            "Batch 41: loss = 0.36851590871810913, acc = 0.8740234375\n",
            "Batch 42: loss = 0.36357519030570984, acc = 0.873046875\n",
            "Batch 43: loss = 0.44060567021369934, acc = 0.8603515625\n",
            "Batch 44: loss = 0.3781139850616455, acc = 0.875\n",
            "Batch 45: loss = 0.36463603377342224, acc = 0.875\n",
            "Batch 46: loss = 0.34351590275764465, acc = 0.8896484375\n",
            "Batch 47: loss = 0.3807758092880249, acc = 0.8818359375\n",
            "Batch 48: loss = 0.36930304765701294, acc = 0.8662109375\n",
            "Batch 49: loss = 0.3532872200012207, acc = 0.8798828125\n",
            "Batch 50: loss = 0.3765765428543091, acc = 0.8740234375\n",
            "Batch 51: loss = 0.39183226227760315, acc = 0.8759765625\n",
            "Batch 52: loss = 0.3692779839038849, acc = 0.8701171875\n",
            "Batch 53: loss = 0.4399925470352173, acc = 0.8486328125\n",
            "Batch 54: loss = 0.32735636830329895, acc = 0.8896484375\n",
            "Batch 55: loss = 0.36523011326789856, acc = 0.8759765625\n",
            "Batch 56: loss = 0.3945806324481964, acc = 0.8603515625\n",
            "Batch 57: loss = 0.45101267099380493, acc = 0.828125\n",
            "Batch 58: loss = 0.4042472243309021, acc = 0.8681640625\n",
            "Batch 59: loss = 0.3638073205947876, acc = 0.8876953125\n",
            "Batch 60: loss = 0.42433273792266846, acc = 0.8583984375\n",
            "Batch 61: loss = 0.3930942118167877, acc = 0.86328125\n",
            "Batch 62: loss = 0.42584526538848877, acc = 0.84765625\n",
            "Batch 63: loss = 0.3761941194534302, acc = 0.8798828125\n",
            "Batch 64: loss = 0.3525868058204651, acc = 0.8916015625\n",
            "Batch 65: loss = 0.41097477078437805, acc = 0.859375\n",
            "Batch 66: loss = 0.37303221225738525, acc = 0.8681640625\n",
            "Batch 67: loss = 0.3774179220199585, acc = 0.8642578125\n",
            "Batch 68: loss = 0.4084198474884033, acc = 0.857421875\n",
            "Batch 69: loss = 0.3746803402900696, acc = 0.873046875\n",
            "Batch 70: loss = 0.3978895843029022, acc = 0.8662109375\n",
            "Batch 71: loss = 0.4218125343322754, acc = 0.8466796875\n",
            "Batch 72: loss = 0.3684271275997162, acc = 0.875\n",
            "Batch 73: loss = 0.4402562379837036, acc = 0.865234375\n",
            "Batch 74: loss = 0.3888401389122009, acc = 0.859375\n",
            "Batch 75: loss = 0.4798768162727356, acc = 0.8359375\n",
            "Batch 76: loss = 0.4083935022354126, acc = 0.859375\n",
            "Batch 77: loss = 0.36161771416664124, acc = 0.8740234375\n",
            "Batch 78: loss = 0.39405515789985657, acc = 0.86328125\n",
            "Batch 79: loss = 0.3738686144351959, acc = 0.869140625\n",
            "Batch 80: loss = 0.36442601680755615, acc = 0.8720703125\n",
            "Batch 81: loss = 0.4395190477371216, acc = 0.8447265625\n",
            "Batch 82: loss = 0.3944324254989624, acc = 0.86328125\n",
            "Batch 83: loss = 0.3924349546432495, acc = 0.8623046875\n",
            "Batch 84: loss = 0.37424591183662415, acc = 0.87109375\n",
            "Batch 85: loss = 0.44759660959243774, acc = 0.8515625\n",
            "Batch 86: loss = 0.4213526248931885, acc = 0.8466796875\n",
            "Batch 87: loss = 0.381252259016037, acc = 0.8701171875\n",
            "Batch 88: loss = 0.48456892371177673, acc = 0.83203125\n",
            "Batch 89: loss = 0.3854355812072754, acc = 0.8828125\n",
            "Batch 90: loss = 0.4381728172302246, acc = 0.8544921875\n",
            "Batch 91: loss = 0.39244329929351807, acc = 0.8740234375\n",
            "Batch 92: loss = 0.45788371562957764, acc = 0.8427734375\n",
            "Batch 93: loss = 0.3326778709888458, acc = 0.8935546875\n",
            "Batch 94: loss = 0.4101097881793976, acc = 0.8681640625\n",
            "Batch 95: loss = 0.3796132802963257, acc = 0.87109375\n",
            "Batch 96: loss = 0.4589536190032959, acc = 0.83203125\n",
            "Batch 97: loss = 0.4007032513618469, acc = 0.865234375\n",
            "Batch 98: loss = 0.4270752966403961, acc = 0.84765625\n",
            "Batch 99: loss = 0.39367082715034485, acc = 0.8701171875\n",
            "Batch 100: loss = 0.38226449489593506, acc = 0.86328125\n",
            "Batch 101: loss = 0.3923121392726898, acc = 0.861328125\n",
            "Batch 102: loss = 0.4571196734905243, acc = 0.84765625\n",
            "Batch 103: loss = 0.40471893548965454, acc = 0.8681640625\n",
            "Batch 104: loss = 0.3860880732536316, acc = 0.8701171875\n",
            "Batch 105: loss = 0.3467162549495697, acc = 0.8818359375\n",
            "Batch 106: loss = 0.3600579500198364, acc = 0.8740234375\n",
            "Batch 107: loss = 0.42126181721687317, acc = 0.8701171875\n",
            "Batch 108: loss = 0.3686697781085968, acc = 0.87890625\n",
            "Batch 109: loss = 0.39415186643600464, acc = 0.8623046875\n",
            "Batch 110: loss = 0.34178492426872253, acc = 0.884765625\n",
            "Batch 111: loss = 0.38720086216926575, acc = 0.8759765625\n",
            "Batch 112: loss = 0.4261495769023895, acc = 0.853515625\n",
            "Batch 113: loss = 0.42851898074150085, acc = 0.84765625\n",
            "Batch 114: loss = 0.43142542243003845, acc = 0.8505859375\n",
            "Batch 115: loss = 0.39551255106925964, acc = 0.873046875\n",
            "Batch 116: loss = 0.4202486276626587, acc = 0.8583984375\n",
            "Batch 117: loss = 0.3872843086719513, acc = 0.873046875\n",
            "Batch 118: loss = 0.38697531819343567, acc = 0.8662109375\n",
            "Batch 119: loss = 0.37983447313308716, acc = 0.875\n",
            "Batch 120: loss = 0.37667137384414673, acc = 0.8857421875\n",
            "Batch 121: loss = 0.39428627490997314, acc = 0.8671875\n",
            "Batch 122: loss = 0.33905306458473206, acc = 0.87109375\n",
            "Batch 123: loss = 0.42110344767570496, acc = 0.8642578125\n",
            "Batch 124: loss = 0.38968679308891296, acc = 0.8642578125\n",
            "Batch 125: loss = 0.3711548447608948, acc = 0.875\n",
            "Batch 126: loss = 0.3961646556854248, acc = 0.869140625\n",
            "\n",
            "Epoch 75/100\n",
            "Batch 1: loss = 0.49942874908447266, acc = 0.841796875\n",
            "Batch 2: loss = 0.4259871244430542, acc = 0.857421875\n",
            "Batch 3: loss = 0.4297609031200409, acc = 0.869140625\n",
            "Batch 4: loss = 0.3771192729473114, acc = 0.8837890625\n",
            "Batch 5: loss = 0.3925716280937195, acc = 0.865234375\n",
            "Batch 6: loss = 0.42079198360443115, acc = 0.8603515625\n",
            "Batch 7: loss = 0.3364434838294983, acc = 0.8779296875\n",
            "Batch 8: loss = 0.38734668493270874, acc = 0.873046875\n",
            "Batch 9: loss = 0.42043745517730713, acc = 0.8623046875\n",
            "Batch 10: loss = 0.3263503313064575, acc = 0.8876953125\n",
            "Batch 11: loss = 0.3649819791316986, acc = 0.8642578125\n",
            "Batch 12: loss = 0.3934064209461212, acc = 0.8740234375\n",
            "Batch 13: loss = 0.3757461607456207, acc = 0.8740234375\n",
            "Batch 14: loss = 0.38340362906455994, acc = 0.869140625\n",
            "Batch 15: loss = 0.3472806215286255, acc = 0.8818359375\n",
            "Batch 16: loss = 0.37070226669311523, acc = 0.865234375\n",
            "Batch 17: loss = 0.3291962146759033, acc = 0.8916015625\n",
            "Batch 18: loss = 0.43980005383491516, acc = 0.8564453125\n",
            "Batch 19: loss = 0.40070244669914246, acc = 0.8623046875\n",
            "Batch 20: loss = 0.4094967246055603, acc = 0.8603515625\n",
            "Batch 21: loss = 0.4181821942329407, acc = 0.8515625\n",
            "Batch 22: loss = 0.3745535612106323, acc = 0.8701171875\n",
            "Batch 23: loss = 0.39373648166656494, acc = 0.8642578125\n",
            "Batch 24: loss = 0.40273892879486084, acc = 0.8583984375\n",
            "Batch 25: loss = 0.3485937714576721, acc = 0.87890625\n",
            "Batch 26: loss = 0.3470168113708496, acc = 0.8818359375\n",
            "Batch 27: loss = 0.4141427278518677, acc = 0.865234375\n",
            "Batch 28: loss = 0.4132860004901886, acc = 0.859375\n",
            "Batch 29: loss = 0.39308011531829834, acc = 0.8642578125\n",
            "Batch 30: loss = 0.39273422956466675, acc = 0.8779296875\n",
            "Batch 31: loss = 0.3819716274738312, acc = 0.8759765625\n",
            "Batch 32: loss = 0.44761866331100464, acc = 0.85546875\n",
            "Batch 33: loss = 0.3847861886024475, acc = 0.8642578125\n",
            "Batch 34: loss = 0.37296822667121887, acc = 0.87109375\n",
            "Batch 35: loss = 0.38771840929985046, acc = 0.8828125\n",
            "Batch 36: loss = 0.3790516257286072, acc = 0.8681640625\n",
            "Batch 37: loss = 0.3440012037754059, acc = 0.892578125\n",
            "Batch 38: loss = 0.3735610842704773, acc = 0.8681640625\n",
            "Batch 39: loss = 0.33531761169433594, acc = 0.8896484375\n",
            "Batch 40: loss = 0.3708629012107849, acc = 0.87109375\n",
            "Batch 41: loss = 0.35853758454322815, acc = 0.8759765625\n",
            "Batch 42: loss = 0.3797837495803833, acc = 0.87109375\n",
            "Batch 43: loss = 0.4081857204437256, acc = 0.8671875\n",
            "Batch 44: loss = 0.3583117127418518, acc = 0.8828125\n",
            "Batch 45: loss = 0.36093375086784363, acc = 0.8720703125\n",
            "Batch 46: loss = 0.39585012197494507, acc = 0.85546875\n",
            "Batch 47: loss = 0.3452538847923279, acc = 0.88671875\n",
            "Batch 48: loss = 0.33998483419418335, acc = 0.8974609375\n",
            "Batch 49: loss = 0.3519110381603241, acc = 0.875\n",
            "Batch 50: loss = 0.3921288251876831, acc = 0.873046875\n",
            "Batch 51: loss = 0.39593952894210815, acc = 0.8681640625\n",
            "Batch 52: loss = 0.40144336223602295, acc = 0.861328125\n",
            "Batch 53: loss = 0.40824732184410095, acc = 0.8740234375\n",
            "Batch 54: loss = 0.3280191421508789, acc = 0.8896484375\n",
            "Batch 55: loss = 0.3597745895385742, acc = 0.873046875\n",
            "Batch 56: loss = 0.3772669732570648, acc = 0.8720703125\n",
            "Batch 57: loss = 0.40598008036613464, acc = 0.8681640625\n",
            "Batch 58: loss = 0.38136646151542664, acc = 0.861328125\n",
            "Batch 59: loss = 0.3488627076148987, acc = 0.8740234375\n",
            "Batch 60: loss = 0.39015039801597595, acc = 0.8603515625\n",
            "Batch 61: loss = 0.35060611367225647, acc = 0.8798828125\n",
            "Batch 62: loss = 0.4146048128604889, acc = 0.86328125\n",
            "Batch 63: loss = 0.39600276947021484, acc = 0.8662109375\n",
            "Batch 64: loss = 0.3301406800746918, acc = 0.890625\n",
            "Batch 65: loss = 0.4470451772212982, acc = 0.8486328125\n",
            "Batch 66: loss = 0.3910779654979706, acc = 0.8759765625\n",
            "Batch 67: loss = 0.3824552595615387, acc = 0.8662109375\n",
            "Batch 68: loss = 0.37173810601234436, acc = 0.875\n",
            "Batch 69: loss = 0.36335527896881104, acc = 0.8603515625\n",
            "Batch 70: loss = 0.4091726243495941, acc = 0.8662109375\n",
            "Batch 71: loss = 0.3876085579395294, acc = 0.8564453125\n",
            "Batch 72: loss = 0.3413397967815399, acc = 0.8828125\n",
            "Batch 73: loss = 0.40234777331352234, acc = 0.8642578125\n",
            "Batch 74: loss = 0.41786766052246094, acc = 0.8564453125\n",
            "Batch 75: loss = 0.4433925151824951, acc = 0.8583984375\n",
            "Batch 76: loss = 0.419318825006485, acc = 0.85546875\n",
            "Batch 77: loss = 0.3642277121543884, acc = 0.875\n",
            "Batch 78: loss = 0.3951054513454437, acc = 0.8681640625\n",
            "Batch 79: loss = 0.3769611120223999, acc = 0.865234375\n",
            "Batch 80: loss = 0.35514208674430847, acc = 0.8798828125\n",
            "Batch 81: loss = 0.36490145325660706, acc = 0.87890625\n",
            "Batch 82: loss = 0.3928741216659546, acc = 0.8681640625\n",
            "Batch 83: loss = 0.3583562970161438, acc = 0.8857421875\n",
            "Batch 84: loss = 0.3926953077316284, acc = 0.861328125\n",
            "Batch 85: loss = 0.4155961871147156, acc = 0.8564453125\n",
            "Batch 86: loss = 0.4000853896141052, acc = 0.8642578125\n",
            "Batch 87: loss = 0.3831626772880554, acc = 0.8759765625\n",
            "Batch 88: loss = 0.44523343443870544, acc = 0.8515625\n",
            "Batch 89: loss = 0.39254552125930786, acc = 0.8720703125\n",
            "Batch 90: loss = 0.4035108983516693, acc = 0.8525390625\n",
            "Batch 91: loss = 0.3978281617164612, acc = 0.861328125\n",
            "Batch 92: loss = 0.4566386044025421, acc = 0.849609375\n",
            "Batch 93: loss = 0.3820652663707733, acc = 0.87890625\n",
            "Batch 94: loss = 0.41552379727363586, acc = 0.8583984375\n",
            "Batch 95: loss = 0.3691045939922333, acc = 0.8837890625\n",
            "Batch 96: loss = 0.4423730969429016, acc = 0.8544921875\n",
            "Batch 97: loss = 0.39314162731170654, acc = 0.8603515625\n",
            "Batch 98: loss = 0.36952653527259827, acc = 0.8759765625\n",
            "Batch 99: loss = 0.4320991635322571, acc = 0.861328125\n",
            "Batch 100: loss = 0.3308493494987488, acc = 0.884765625\n",
            "Batch 101: loss = 0.40979278087615967, acc = 0.8486328125\n",
            "Batch 102: loss = 0.39652684330940247, acc = 0.8662109375\n",
            "Batch 103: loss = 0.40721550583839417, acc = 0.8720703125\n",
            "Batch 104: loss = 0.32499080896377563, acc = 0.890625\n",
            "Batch 105: loss = 0.3839336633682251, acc = 0.8671875\n",
            "Batch 106: loss = 0.3805307447910309, acc = 0.8662109375\n",
            "Batch 107: loss = 0.37822508811950684, acc = 0.880859375\n",
            "Batch 108: loss = 0.39931032061576843, acc = 0.8701171875\n",
            "Batch 109: loss = 0.3955351710319519, acc = 0.859375\n",
            "Batch 110: loss = 0.3545598089694977, acc = 0.8740234375\n",
            "Batch 111: loss = 0.40746283531188965, acc = 0.8642578125\n",
            "Batch 112: loss = 0.37338143587112427, acc = 0.87890625\n",
            "Batch 113: loss = 0.3937188386917114, acc = 0.857421875\n",
            "Batch 114: loss = 0.4379099905490875, acc = 0.8505859375\n",
            "Batch 115: loss = 0.38447481393814087, acc = 0.875\n",
            "Batch 116: loss = 0.4281367361545563, acc = 0.857421875\n",
            "Batch 117: loss = 0.4195658266544342, acc = 0.857421875\n",
            "Batch 118: loss = 0.39345791935920715, acc = 0.8662109375\n",
            "Batch 119: loss = 0.3930233418941498, acc = 0.8701171875\n",
            "Batch 120: loss = 0.37286627292633057, acc = 0.861328125\n",
            "Batch 121: loss = 0.3963060975074768, acc = 0.8720703125\n",
            "Batch 122: loss = 0.39519253373146057, acc = 0.869140625\n",
            "Batch 123: loss = 0.4002884030342102, acc = 0.857421875\n",
            "Batch 124: loss = 0.4337623417377472, acc = 0.8505859375\n",
            "Batch 125: loss = 0.3986983299255371, acc = 0.8671875\n",
            "Batch 126: loss = 0.44265392422676086, acc = 0.845703125\n",
            "\n",
            "Epoch 76/100\n",
            "Batch 1: loss = 0.4783460795879364, acc = 0.8515625\n",
            "Batch 2: loss = 0.4134289026260376, acc = 0.8671875\n",
            "Batch 3: loss = 0.38354915380477905, acc = 0.869140625\n",
            "Batch 4: loss = 0.36100903153419495, acc = 0.8759765625\n",
            "Batch 5: loss = 0.40152063965797424, acc = 0.865234375\n",
            "Batch 6: loss = 0.4107644855976105, acc = 0.85546875\n",
            "Batch 7: loss = 0.3677256405353546, acc = 0.8701171875\n",
            "Batch 8: loss = 0.3919786810874939, acc = 0.8701171875\n",
            "Batch 9: loss = 0.4071211516857147, acc = 0.8720703125\n",
            "Batch 10: loss = 0.3332449197769165, acc = 0.8857421875\n",
            "Batch 11: loss = 0.3530101478099823, acc = 0.880859375\n",
            "Batch 12: loss = 0.3958548605442047, acc = 0.87109375\n",
            "Batch 13: loss = 0.38758620619773865, acc = 0.87109375\n",
            "Batch 14: loss = 0.3783087730407715, acc = 0.8828125\n",
            "Batch 15: loss = 0.32740527391433716, acc = 0.88671875\n",
            "Batch 16: loss = 0.4160500764846802, acc = 0.8671875\n",
            "Batch 17: loss = 0.3515733480453491, acc = 0.876953125\n",
            "Batch 18: loss = 0.3988903760910034, acc = 0.8603515625\n",
            "Batch 19: loss = 0.397545725107193, acc = 0.875\n",
            "Batch 20: loss = 0.3805360198020935, acc = 0.8671875\n",
            "Batch 21: loss = 0.3882129490375519, acc = 0.8818359375\n",
            "Batch 22: loss = 0.40460890531539917, acc = 0.8583984375\n",
            "Batch 23: loss = 0.4146425127983093, acc = 0.861328125\n",
            "Batch 24: loss = 0.4245977997779846, acc = 0.8564453125\n",
            "Batch 25: loss = 0.38907212018966675, acc = 0.8681640625\n",
            "Batch 26: loss = 0.36268988251686096, acc = 0.8759765625\n",
            "Batch 27: loss = 0.4167758524417877, acc = 0.8671875\n",
            "Batch 28: loss = 0.42913877964019775, acc = 0.857421875\n",
            "Batch 29: loss = 0.4125680923461914, acc = 0.8623046875\n",
            "Batch 30: loss = 0.354086309671402, acc = 0.8828125\n",
            "Batch 31: loss = 0.3932596445083618, acc = 0.8740234375\n",
            "Batch 32: loss = 0.4031140208244324, acc = 0.869140625\n",
            "Batch 33: loss = 0.39687812328338623, acc = 0.873046875\n",
            "Batch 34: loss = 0.3704332709312439, acc = 0.8720703125\n",
            "Batch 35: loss = 0.3764842450618744, acc = 0.8779296875\n",
            "Batch 36: loss = 0.3940044939517975, acc = 0.8583984375\n",
            "Batch 37: loss = 0.3681710660457611, acc = 0.875\n",
            "Batch 38: loss = 0.3786521852016449, acc = 0.87890625\n",
            "Batch 39: loss = 0.3466432988643646, acc = 0.88671875\n",
            "Batch 40: loss = 0.3575727045536041, acc = 0.87890625\n",
            "Batch 41: loss = 0.38400062918663025, acc = 0.8642578125\n",
            "Batch 42: loss = 0.3702075481414795, acc = 0.8671875\n",
            "Batch 43: loss = 0.4166278839111328, acc = 0.8447265625\n",
            "Batch 44: loss = 0.35157284140586853, acc = 0.888671875\n",
            "Batch 45: loss = 0.3258247971534729, acc = 0.8857421875\n",
            "Batch 46: loss = 0.37347522377967834, acc = 0.8759765625\n",
            "Batch 47: loss = 0.3555676341056824, acc = 0.87890625\n",
            "Batch 48: loss = 0.32937267422676086, acc = 0.890625\n",
            "Batch 49: loss = 0.3507663905620575, acc = 0.8916015625\n",
            "Batch 50: loss = 0.3510148525238037, acc = 0.8759765625\n",
            "Batch 51: loss = 0.38274896144866943, acc = 0.8740234375\n",
            "Batch 52: loss = 0.41590824723243713, acc = 0.8486328125\n",
            "Batch 53: loss = 0.4252912998199463, acc = 0.8662109375\n",
            "Batch 54: loss = 0.29495081305503845, acc = 0.9091796875\n",
            "Batch 55: loss = 0.35820117592811584, acc = 0.8720703125\n",
            "Batch 56: loss = 0.37859493494033813, acc = 0.8701171875\n",
            "Batch 57: loss = 0.4449479877948761, acc = 0.8486328125\n",
            "Batch 58: loss = 0.39805471897125244, acc = 0.857421875\n",
            "Batch 59: loss = 0.33341193199157715, acc = 0.890625\n",
            "Batch 60: loss = 0.39265698194503784, acc = 0.8603515625\n",
            "Batch 61: loss = 0.3599593937397003, acc = 0.87890625\n",
            "Batch 62: loss = 0.42825666069984436, acc = 0.8505859375\n",
            "Batch 63: loss = 0.40712907910346985, acc = 0.85546875\n",
            "Batch 64: loss = 0.34112367033958435, acc = 0.8935546875\n",
            "Batch 65: loss = 0.39591386914253235, acc = 0.8603515625\n",
            "Batch 66: loss = 0.4118894934654236, acc = 0.8662109375\n",
            "Batch 67: loss = 0.3785095512866974, acc = 0.869140625\n",
            "Batch 68: loss = 0.3656066060066223, acc = 0.869140625\n",
            "Batch 69: loss = 0.33336901664733887, acc = 0.8876953125\n",
            "Batch 70: loss = 0.41319945454597473, acc = 0.8662109375\n",
            "Batch 71: loss = 0.3983798921108246, acc = 0.86328125\n",
            "Batch 72: loss = 0.39934319257736206, acc = 0.87109375\n",
            "Batch 73: loss = 0.4169088304042816, acc = 0.8681640625\n",
            "Batch 74: loss = 0.3966217339038849, acc = 0.8740234375\n",
            "Batch 75: loss = 0.4114933907985687, acc = 0.8505859375\n",
            "Batch 76: loss = 0.41709351539611816, acc = 0.8564453125\n",
            "Batch 77: loss = 0.3703819811344147, acc = 0.8642578125\n",
            "Batch 78: loss = 0.3757169842720032, acc = 0.8837890625\n",
            "Batch 79: loss = 0.375393807888031, acc = 0.8779296875\n",
            "Batch 80: loss = 0.349987268447876, acc = 0.87890625\n",
            "Batch 81: loss = 0.39876896142959595, acc = 0.8720703125\n",
            "Batch 82: loss = 0.37552252411842346, acc = 0.8818359375\n",
            "Batch 83: loss = 0.37780269980430603, acc = 0.8759765625\n",
            "Batch 84: loss = 0.3922986090183258, acc = 0.8740234375\n",
            "Batch 85: loss = 0.41424623131752014, acc = 0.849609375\n",
            "Batch 86: loss = 0.40483957529067993, acc = 0.865234375\n",
            "Batch 87: loss = 0.3720782399177551, acc = 0.876953125\n",
            "Batch 88: loss = 0.4713743329048157, acc = 0.8486328125\n",
            "Batch 89: loss = 0.38557204604148865, acc = 0.8740234375\n",
            "Batch 90: loss = 0.4235079288482666, acc = 0.8505859375\n",
            "Batch 91: loss = 0.4396756887435913, acc = 0.8603515625\n",
            "Batch 92: loss = 0.43979260325431824, acc = 0.859375\n",
            "Batch 93: loss = 0.35781553387641907, acc = 0.8828125\n",
            "Batch 94: loss = 0.35504913330078125, acc = 0.8857421875\n",
            "Batch 95: loss = 0.3511713743209839, acc = 0.873046875\n",
            "Batch 96: loss = 0.43712592124938965, acc = 0.84375\n",
            "Batch 97: loss = 0.4114079475402832, acc = 0.875\n",
            "Batch 98: loss = 0.37092089653015137, acc = 0.880859375\n",
            "Batch 99: loss = 0.40121912956237793, acc = 0.8623046875\n",
            "Batch 100: loss = 0.3648070693016052, acc = 0.869140625\n",
            "Batch 101: loss = 0.34666407108306885, acc = 0.8720703125\n",
            "Batch 102: loss = 0.41598671674728394, acc = 0.8564453125\n",
            "Batch 103: loss = 0.419350266456604, acc = 0.869140625\n",
            "Batch 104: loss = 0.3516021966934204, acc = 0.873046875\n",
            "Batch 105: loss = 0.3367393910884857, acc = 0.8837890625\n",
            "Batch 106: loss = 0.4148660898208618, acc = 0.85546875\n",
            "Batch 107: loss = 0.3899376392364502, acc = 0.8671875\n",
            "Batch 108: loss = 0.3734886348247528, acc = 0.8828125\n",
            "Batch 109: loss = 0.4026714265346527, acc = 0.8671875\n",
            "Batch 110: loss = 0.35790449380874634, acc = 0.884765625\n",
            "Batch 111: loss = 0.38650068640708923, acc = 0.876953125\n",
            "Batch 112: loss = 0.4293874204158783, acc = 0.8544921875\n",
            "Batch 113: loss = 0.40115782618522644, acc = 0.861328125\n",
            "Batch 114: loss = 0.4379783272743225, acc = 0.86328125\n",
            "Batch 115: loss = 0.39468130469322205, acc = 0.8681640625\n",
            "Batch 116: loss = 0.42472711205482483, acc = 0.85546875\n",
            "Batch 117: loss = 0.3866565227508545, acc = 0.8779296875\n",
            "Batch 118: loss = 0.37563860416412354, acc = 0.875\n",
            "Batch 119: loss = 0.3776921331882477, acc = 0.87890625\n",
            "Batch 120: loss = 0.36081427335739136, acc = 0.8759765625\n",
            "Batch 121: loss = 0.3833017349243164, acc = 0.87890625\n",
            "Batch 122: loss = 0.3586987257003784, acc = 0.8818359375\n",
            "Batch 123: loss = 0.4216744899749756, acc = 0.8603515625\n",
            "Batch 124: loss = 0.41948944330215454, acc = 0.8623046875\n",
            "Batch 125: loss = 0.39860689640045166, acc = 0.853515625\n",
            "Batch 126: loss = 0.3863171339035034, acc = 0.8740234375\n",
            "\n",
            "Epoch 77/100\n",
            "Batch 1: loss = 0.49784761667251587, acc = 0.84375\n",
            "Batch 2: loss = 0.4000641107559204, acc = 0.8623046875\n",
            "Batch 3: loss = 0.37517961859703064, acc = 0.880859375\n",
            "Batch 4: loss = 0.3568919599056244, acc = 0.880859375\n",
            "Batch 5: loss = 0.3915776014328003, acc = 0.8623046875\n",
            "Batch 6: loss = 0.4054867923259735, acc = 0.8701171875\n",
            "Batch 7: loss = 0.41636085510253906, acc = 0.8583984375\n",
            "Batch 8: loss = 0.42204490303993225, acc = 0.8720703125\n",
            "Batch 9: loss = 0.3678779900074005, acc = 0.8798828125\n",
            "Batch 10: loss = 0.35876938700675964, acc = 0.873046875\n",
            "Batch 11: loss = 0.38333016633987427, acc = 0.873046875\n",
            "Batch 12: loss = 0.4065324366092682, acc = 0.8583984375\n",
            "Batch 13: loss = 0.37368467450141907, acc = 0.8720703125\n",
            "Batch 14: loss = 0.32702523469924927, acc = 0.8955078125\n",
            "Batch 15: loss = 0.36350852251052856, acc = 0.884765625\n",
            "Batch 16: loss = 0.38263562321662903, acc = 0.869140625\n",
            "Batch 17: loss = 0.3590053915977478, acc = 0.888671875\n",
            "Batch 18: loss = 0.4199341833591461, acc = 0.859375\n",
            "Batch 19: loss = 0.38818320631980896, acc = 0.865234375\n",
            "Batch 20: loss = 0.3879128694534302, acc = 0.8779296875\n",
            "Batch 21: loss = 0.3823337256908417, acc = 0.865234375\n",
            "Batch 22: loss = 0.4050024747848511, acc = 0.865234375\n",
            "Batch 23: loss = 0.40295568108558655, acc = 0.8623046875\n",
            "Batch 24: loss = 0.3587481677532196, acc = 0.87109375\n",
            "Batch 25: loss = 0.3704417049884796, acc = 0.8720703125\n",
            "Batch 26: loss = 0.36924120783805847, acc = 0.869140625\n",
            "Batch 27: loss = 0.3964512050151825, acc = 0.865234375\n",
            "Batch 28: loss = 0.39862704277038574, acc = 0.8603515625\n",
            "Batch 29: loss = 0.37704747915267944, acc = 0.853515625\n",
            "Batch 30: loss = 0.3885143995285034, acc = 0.876953125\n",
            "Batch 31: loss = 0.43915796279907227, acc = 0.8681640625\n",
            "Batch 32: loss = 0.446442574262619, acc = 0.8486328125\n",
            "Batch 33: loss = 0.4151211082935333, acc = 0.8515625\n",
            "Batch 34: loss = 0.3866535723209381, acc = 0.87109375\n",
            "Batch 35: loss = 0.37734201550483704, acc = 0.8720703125\n",
            "Batch 36: loss = 0.37103208899497986, acc = 0.865234375\n",
            "Batch 37: loss = 0.36319348216056824, acc = 0.875\n",
            "Batch 38: loss = 0.3602825403213501, acc = 0.8828125\n",
            "Batch 39: loss = 0.3342131972312927, acc = 0.8828125\n",
            "Batch 40: loss = 0.3617316782474518, acc = 0.8720703125\n",
            "Batch 41: loss = 0.3306664228439331, acc = 0.8837890625\n",
            "Batch 42: loss = 0.39554154872894287, acc = 0.865234375\n",
            "Batch 43: loss = 0.43460893630981445, acc = 0.85546875\n",
            "Batch 44: loss = 0.3671506643295288, acc = 0.8828125\n",
            "Batch 45: loss = 0.3540506958961487, acc = 0.8779296875\n",
            "Batch 46: loss = 0.39369574189186096, acc = 0.8681640625\n",
            "Batch 47: loss = 0.31138473749160767, acc = 0.8984375\n",
            "Batch 48: loss = 0.3531694710254669, acc = 0.8818359375\n",
            "Batch 49: loss = 0.3760223686695099, acc = 0.8818359375\n",
            "Batch 50: loss = 0.37106966972351074, acc = 0.8740234375\n",
            "Batch 51: loss = 0.40949246287345886, acc = 0.861328125\n",
            "Batch 52: loss = 0.3950941264629364, acc = 0.8642578125\n",
            "Batch 53: loss = 0.36914801597595215, acc = 0.869140625\n",
            "Batch 54: loss = 0.32780948281288147, acc = 0.8896484375\n",
            "Batch 55: loss = 0.3337137997150421, acc = 0.8896484375\n",
            "Batch 56: loss = 0.3685298562049866, acc = 0.8779296875\n",
            "Batch 57: loss = 0.42409154772758484, acc = 0.857421875\n",
            "Batch 58: loss = 0.41457629203796387, acc = 0.8623046875\n",
            "Batch 59: loss = 0.3077141344547272, acc = 0.900390625\n",
            "Batch 60: loss = 0.36662647128105164, acc = 0.8671875\n",
            "Batch 61: loss = 0.352206289768219, acc = 0.8837890625\n",
            "Batch 62: loss = 0.4678839147090912, acc = 0.849609375\n",
            "Batch 63: loss = 0.4070424437522888, acc = 0.8564453125\n",
            "Batch 64: loss = 0.3257386088371277, acc = 0.8955078125\n",
            "Batch 65: loss = 0.3948154151439667, acc = 0.8671875\n",
            "Batch 66: loss = 0.40938153862953186, acc = 0.8623046875\n",
            "Batch 67: loss = 0.335063099861145, acc = 0.88671875\n",
            "Batch 68: loss = 0.38787227869033813, acc = 0.8740234375\n",
            "Batch 69: loss = 0.35609397292137146, acc = 0.88671875\n",
            "Batch 70: loss = 0.40617796778678894, acc = 0.8583984375\n",
            "Batch 71: loss = 0.4046265780925751, acc = 0.8720703125\n",
            "Batch 72: loss = 0.368630975484848, acc = 0.8779296875\n",
            "Batch 73: loss = 0.4180431663990021, acc = 0.869140625\n",
            "Batch 74: loss = 0.43316569924354553, acc = 0.853515625\n",
            "Batch 75: loss = 0.43847155570983887, acc = 0.84375\n",
            "Batch 76: loss = 0.3851029872894287, acc = 0.8740234375\n",
            "Batch 77: loss = 0.357671856880188, acc = 0.8720703125\n",
            "Batch 78: loss = 0.4155576229095459, acc = 0.8603515625\n",
            "Batch 79: loss = 0.3684353828430176, acc = 0.8876953125\n",
            "Batch 80: loss = 0.3598179519176483, acc = 0.8740234375\n",
            "Batch 81: loss = 0.3994412422180176, acc = 0.8681640625\n",
            "Batch 82: loss = 0.3875223994255066, acc = 0.87890625\n",
            "Batch 83: loss = 0.3793126046657562, acc = 0.875\n",
            "Batch 84: loss = 0.35098835825920105, acc = 0.8818359375\n",
            "Batch 85: loss = 0.4061836898326874, acc = 0.8564453125\n",
            "Batch 86: loss = 0.37679892778396606, acc = 0.869140625\n",
            "Batch 87: loss = 0.400571346282959, acc = 0.8701171875\n",
            "Batch 88: loss = 0.4577193558216095, acc = 0.8408203125\n",
            "Batch 89: loss = 0.378060519695282, acc = 0.8720703125\n",
            "Batch 90: loss = 0.41965532302856445, acc = 0.8447265625\n",
            "Batch 91: loss = 0.42047807574272156, acc = 0.8564453125\n",
            "Batch 92: loss = 0.41316062211990356, acc = 0.8623046875\n",
            "Batch 93: loss = 0.35550424456596375, acc = 0.880859375\n",
            "Batch 94: loss = 0.3712840974330902, acc = 0.875\n",
            "Batch 95: loss = 0.3360442519187927, acc = 0.8818359375\n",
            "Batch 96: loss = 0.4051471948623657, acc = 0.8525390625\n",
            "Batch 97: loss = 0.4015876054763794, acc = 0.8701171875\n",
            "Batch 98: loss = 0.3707091212272644, acc = 0.875\n",
            "Batch 99: loss = 0.3804526925086975, acc = 0.8740234375\n",
            "Batch 100: loss = 0.3956131339073181, acc = 0.8662109375\n",
            "Batch 101: loss = 0.3783400356769562, acc = 0.8603515625\n",
            "Batch 102: loss = 0.40457290410995483, acc = 0.8564453125\n",
            "Batch 103: loss = 0.3631894886493683, acc = 0.8759765625\n",
            "Batch 104: loss = 0.32845833897590637, acc = 0.8876953125\n",
            "Batch 105: loss = 0.3774928152561188, acc = 0.884765625\n",
            "Batch 106: loss = 0.3872644603252411, acc = 0.859375\n",
            "Batch 107: loss = 0.3903464674949646, acc = 0.8701171875\n",
            "Batch 108: loss = 0.38486528396606445, acc = 0.869140625\n",
            "Batch 109: loss = 0.41177958250045776, acc = 0.865234375\n",
            "Batch 110: loss = 0.3140738606452942, acc = 0.896484375\n",
            "Batch 111: loss = 0.4157710373401642, acc = 0.8486328125\n",
            "Batch 112: loss = 0.3827139139175415, acc = 0.86328125\n",
            "Batch 113: loss = 0.3888450860977173, acc = 0.8662109375\n",
            "Batch 114: loss = 0.41497066617012024, acc = 0.8515625\n",
            "Batch 115: loss = 0.4151964783668518, acc = 0.8603515625\n",
            "Batch 116: loss = 0.400254487991333, acc = 0.85546875\n",
            "Batch 117: loss = 0.35371410846710205, acc = 0.8876953125\n",
            "Batch 118: loss = 0.3453812897205353, acc = 0.8798828125\n",
            "Batch 119: loss = 0.3740743398666382, acc = 0.873046875\n",
            "Batch 120: loss = 0.3681277632713318, acc = 0.8779296875\n",
            "Batch 121: loss = 0.41840022802352905, acc = 0.8583984375\n",
            "Batch 122: loss = 0.3928752541542053, acc = 0.861328125\n",
            "Batch 123: loss = 0.4089670479297638, acc = 0.8564453125\n",
            "Batch 124: loss = 0.41163963079452515, acc = 0.8603515625\n",
            "Batch 125: loss = 0.40896061062812805, acc = 0.84765625\n",
            "Batch 126: loss = 0.3793167173862457, acc = 0.869140625\n",
            "\n",
            "Epoch 78/100\n",
            "Batch 1: loss = 0.46976786851882935, acc = 0.853515625\n",
            "Batch 2: loss = 0.41047585010528564, acc = 0.859375\n",
            "Batch 3: loss = 0.4199221134185791, acc = 0.8642578125\n",
            "Batch 4: loss = 0.3613834083080292, acc = 0.8857421875\n",
            "Batch 5: loss = 0.4030199348926544, acc = 0.8662109375\n",
            "Batch 6: loss = 0.44694212079048157, acc = 0.84765625\n",
            "Batch 7: loss = 0.3551231622695923, acc = 0.87890625\n",
            "Batch 8: loss = 0.3964146077632904, acc = 0.8623046875\n",
            "Batch 9: loss = 0.39654791355133057, acc = 0.861328125\n",
            "Batch 10: loss = 0.3402980864048004, acc = 0.892578125\n",
            "Batch 11: loss = 0.3802337646484375, acc = 0.8701171875\n",
            "Batch 12: loss = 0.3875521719455719, acc = 0.8681640625\n",
            "Batch 13: loss = 0.39368292689323425, acc = 0.865234375\n",
            "Batch 14: loss = 0.3791680932044983, acc = 0.8720703125\n",
            "Batch 15: loss = 0.3618919253349304, acc = 0.87890625\n",
            "Batch 16: loss = 0.42262065410614014, acc = 0.8564453125\n",
            "Batch 17: loss = 0.35163429379463196, acc = 0.8857421875\n",
            "Batch 18: loss = 0.39493685960769653, acc = 0.8642578125\n",
            "Batch 19: loss = 0.3927386403083801, acc = 0.8671875\n",
            "Batch 20: loss = 0.3678452968597412, acc = 0.8818359375\n",
            "Batch 21: loss = 0.4370296895503998, acc = 0.85546875\n",
            "Batch 22: loss = 0.38997504115104675, acc = 0.8623046875\n",
            "Batch 23: loss = 0.40007179975509644, acc = 0.8671875\n",
            "Batch 24: loss = 0.3741295337677002, acc = 0.865234375\n",
            "Batch 25: loss = 0.4096326231956482, acc = 0.8583984375\n",
            "Batch 26: loss = 0.3829955756664276, acc = 0.873046875\n",
            "Batch 27: loss = 0.40296751260757446, acc = 0.8759765625\n",
            "Batch 28: loss = 0.42507439851760864, acc = 0.8505859375\n",
            "Batch 29: loss = 0.3856680393218994, acc = 0.876953125\n",
            "Batch 30: loss = 0.4059656858444214, acc = 0.857421875\n",
            "Batch 31: loss = 0.4154866933822632, acc = 0.8681640625\n",
            "Batch 32: loss = 0.4116594195365906, acc = 0.8583984375\n",
            "Batch 33: loss = 0.4314586818218231, acc = 0.861328125\n",
            "Batch 34: loss = 0.3692071735858917, acc = 0.873046875\n",
            "Batch 35: loss = 0.3586558699607849, acc = 0.8759765625\n",
            "Batch 36: loss = 0.3363877236843109, acc = 0.880859375\n",
            "Batch 37: loss = 0.3325692117214203, acc = 0.884765625\n",
            "Batch 38: loss = 0.3729850649833679, acc = 0.87109375\n",
            "Batch 39: loss = 0.3250029981136322, acc = 0.8837890625\n",
            "Batch 40: loss = 0.38902193307876587, acc = 0.8642578125\n",
            "Batch 41: loss = 0.37128010392189026, acc = 0.873046875\n",
            "Batch 42: loss = 0.36455899477005005, acc = 0.876953125\n",
            "Batch 43: loss = 0.4040035903453827, acc = 0.8623046875\n",
            "Batch 44: loss = 0.3646860122680664, acc = 0.875\n",
            "Batch 45: loss = 0.34294742345809937, acc = 0.8720703125\n",
            "Batch 46: loss = 0.3658762574195862, acc = 0.875\n",
            "Batch 47: loss = 0.38028138875961304, acc = 0.8837890625\n",
            "Batch 48: loss = 0.32537081837654114, acc = 0.8798828125\n",
            "Batch 49: loss = 0.34086883068084717, acc = 0.8935546875\n",
            "Batch 50: loss = 0.3664216995239258, acc = 0.8818359375\n",
            "Batch 51: loss = 0.3619368374347687, acc = 0.8671875\n",
            "Batch 52: loss = 0.3984156847000122, acc = 0.857421875\n",
            "Batch 53: loss = 0.37724193930625916, acc = 0.8857421875\n",
            "Batch 54: loss = 0.344687283039093, acc = 0.8837890625\n",
            "Batch 55: loss = 0.3694750666618347, acc = 0.876953125\n",
            "Batch 56: loss = 0.3795453906059265, acc = 0.869140625\n",
            "Batch 57: loss = 0.4544682502746582, acc = 0.8525390625\n",
            "Batch 58: loss = 0.4165324568748474, acc = 0.85546875\n",
            "Batch 59: loss = 0.3289443552494049, acc = 0.8896484375\n",
            "Batch 60: loss = 0.3746441602706909, acc = 0.87890625\n",
            "Batch 61: loss = 0.3249620497226715, acc = 0.8896484375\n",
            "Batch 62: loss = 0.41332340240478516, acc = 0.85546875\n",
            "Batch 63: loss = 0.3417697548866272, acc = 0.896484375\n",
            "Batch 64: loss = 0.32203036546707153, acc = 0.8916015625\n",
            "Batch 65: loss = 0.3768045902252197, acc = 0.8701171875\n",
            "Batch 66: loss = 0.3649912476539612, acc = 0.869140625\n",
            "Batch 67: loss = 0.3832441568374634, acc = 0.857421875\n",
            "Batch 68: loss = 0.42728495597839355, acc = 0.8583984375\n",
            "Batch 69: loss = 0.3387574553489685, acc = 0.8857421875\n",
            "Batch 70: loss = 0.4272744655609131, acc = 0.8505859375\n",
            "Batch 71: loss = 0.4340175986289978, acc = 0.85546875\n",
            "Batch 72: loss = 0.3796827793121338, acc = 0.8779296875\n",
            "Batch 73: loss = 0.46295416355133057, acc = 0.8447265625\n",
            "Batch 74: loss = 0.4009252190589905, acc = 0.8603515625\n",
            "Batch 75: loss = 0.4554121792316437, acc = 0.8505859375\n",
            "Batch 76: loss = 0.43685656785964966, acc = 0.8564453125\n",
            "Batch 77: loss = 0.3722498416900635, acc = 0.875\n",
            "Batch 78: loss = 0.3821665942668915, acc = 0.8740234375\n",
            "Batch 79: loss = 0.3313266932964325, acc = 0.890625\n",
            "Batch 80: loss = 0.31353265047073364, acc = 0.888671875\n",
            "Batch 81: loss = 0.4072123169898987, acc = 0.86328125\n",
            "Batch 82: loss = 0.3697277009487152, acc = 0.8759765625\n",
            "Batch 83: loss = 0.3392012119293213, acc = 0.88671875\n",
            "Batch 84: loss = 0.36158207058906555, acc = 0.8740234375\n",
            "Batch 85: loss = 0.4061289429664612, acc = 0.86328125\n",
            "Batch 86: loss = 0.3914143443107605, acc = 0.8681640625\n",
            "Batch 87: loss = 0.4158434271812439, acc = 0.87890625\n",
            "Batch 88: loss = 0.38875699043273926, acc = 0.8671875\n",
            "Batch 89: loss = 0.3878806233406067, acc = 0.8759765625\n",
            "Batch 90: loss = 0.40686148405075073, acc = 0.861328125\n",
            "Batch 91: loss = 0.43575555086135864, acc = 0.8603515625\n",
            "Batch 92: loss = 0.4225269556045532, acc = 0.8623046875\n",
            "Batch 93: loss = 0.38109689950942993, acc = 0.884765625\n",
            "Batch 94: loss = 0.37720057368278503, acc = 0.873046875\n",
            "Batch 95: loss = 0.35820233821868896, acc = 0.8759765625\n",
            "Batch 96: loss = 0.42887091636657715, acc = 0.859375\n",
            "Batch 97: loss = 0.398925244808197, acc = 0.876953125\n",
            "Batch 98: loss = 0.3807699680328369, acc = 0.869140625\n",
            "Batch 99: loss = 0.38539963960647583, acc = 0.876953125\n",
            "Batch 100: loss = 0.38929274678230286, acc = 0.8701171875\n",
            "Batch 101: loss = 0.38014429807662964, acc = 0.8681640625\n",
            "Batch 102: loss = 0.4207724630832672, acc = 0.8544921875\n",
            "Batch 103: loss = 0.3733717203140259, acc = 0.8837890625\n",
            "Batch 104: loss = 0.3486255407333374, acc = 0.8818359375\n",
            "Batch 105: loss = 0.33170801401138306, acc = 0.890625\n",
            "Batch 106: loss = 0.36875563859939575, acc = 0.8857421875\n",
            "Batch 107: loss = 0.4545706510543823, acc = 0.849609375\n",
            "Batch 108: loss = 0.36663585901260376, acc = 0.87890625\n",
            "Batch 109: loss = 0.39086928963661194, acc = 0.861328125\n",
            "Batch 110: loss = 0.3384154438972473, acc = 0.8837890625\n",
            "Batch 111: loss = 0.3918391764163971, acc = 0.865234375\n",
            "Batch 112: loss = 0.34361547231674194, acc = 0.8818359375\n",
            "Batch 113: loss = 0.3827853500843048, acc = 0.873046875\n",
            "Batch 114: loss = 0.42015165090560913, acc = 0.8525390625\n",
            "Batch 115: loss = 0.3933830261230469, acc = 0.8671875\n",
            "Batch 116: loss = 0.4390953481197357, acc = 0.8447265625\n",
            "Batch 117: loss = 0.3837534487247467, acc = 0.8701171875\n",
            "Batch 118: loss = 0.3399170935153961, acc = 0.88671875\n",
            "Batch 119: loss = 0.38795095682144165, acc = 0.8642578125\n",
            "Batch 120: loss = 0.3247508108615875, acc = 0.8828125\n",
            "Batch 121: loss = 0.40485790371894836, acc = 0.8662109375\n",
            "Batch 122: loss = 0.36083489656448364, acc = 0.875\n",
            "Batch 123: loss = 0.3803878128528595, acc = 0.8671875\n",
            "Batch 124: loss = 0.43271732330322266, acc = 0.8544921875\n",
            "Batch 125: loss = 0.4191755950450897, acc = 0.8671875\n",
            "Batch 126: loss = 0.4113435745239258, acc = 0.87109375\n",
            "\n",
            "Epoch 79/100\n",
            "Batch 1: loss = 0.48674890398979187, acc = 0.8544921875\n",
            "Batch 2: loss = 0.37949877977371216, acc = 0.8740234375\n",
            "Batch 3: loss = 0.40432578325271606, acc = 0.8740234375\n",
            "Batch 4: loss = 0.3747478425502777, acc = 0.876953125\n",
            "Batch 5: loss = 0.3972634971141815, acc = 0.87890625\n",
            "Batch 6: loss = 0.4569612741470337, acc = 0.8408203125\n",
            "Batch 7: loss = 0.39757615327835083, acc = 0.8671875\n",
            "Batch 8: loss = 0.38376742601394653, acc = 0.876953125\n",
            "Batch 9: loss = 0.37295517325401306, acc = 0.873046875\n",
            "Batch 10: loss = 0.3557676672935486, acc = 0.8798828125\n",
            "Batch 11: loss = 0.3783732056617737, acc = 0.875\n",
            "Batch 12: loss = 0.37608328461647034, acc = 0.8720703125\n",
            "Batch 13: loss = 0.3581705689430237, acc = 0.875\n",
            "Batch 14: loss = 0.3777120113372803, acc = 0.873046875\n",
            "Batch 15: loss = 0.37406668066978455, acc = 0.873046875\n",
            "Batch 16: loss = 0.424215167760849, acc = 0.86328125\n",
            "Batch 17: loss = 0.34840652346611023, acc = 0.8828125\n",
            "Batch 18: loss = 0.4096633791923523, acc = 0.8671875\n",
            "Batch 19: loss = 0.3911947011947632, acc = 0.873046875\n",
            "Batch 20: loss = 0.3514064848423004, acc = 0.8837890625\n",
            "Batch 21: loss = 0.3687014877796173, acc = 0.8681640625\n",
            "Batch 22: loss = 0.3871379494667053, acc = 0.8701171875\n",
            "Batch 23: loss = 0.38539326190948486, acc = 0.8681640625\n",
            "Batch 24: loss = 0.38424113392829895, acc = 0.8681640625\n",
            "Batch 25: loss = 0.3766508996486664, acc = 0.876953125\n",
            "Batch 26: loss = 0.3742034137248993, acc = 0.873046875\n",
            "Batch 27: loss = 0.40224817395210266, acc = 0.8681640625\n",
            "Batch 28: loss = 0.3936702013015747, acc = 0.8583984375\n",
            "Batch 29: loss = 0.360588937997818, acc = 0.873046875\n",
            "Batch 30: loss = 0.36009377241134644, acc = 0.875\n",
            "Batch 31: loss = 0.3981940746307373, acc = 0.8681640625\n",
            "Batch 32: loss = 0.45995134115219116, acc = 0.8447265625\n",
            "Batch 33: loss = 0.4084295630455017, acc = 0.86328125\n",
            "Batch 34: loss = 0.39426863193511963, acc = 0.86328125\n",
            "Batch 35: loss = 0.3946163058280945, acc = 0.87109375\n",
            "Batch 36: loss = 0.38116389513015747, acc = 0.8779296875\n",
            "Batch 37: loss = 0.3581860065460205, acc = 0.876953125\n",
            "Batch 38: loss = 0.379707396030426, acc = 0.8662109375\n",
            "Batch 39: loss = 0.32695212960243225, acc = 0.8828125\n",
            "Batch 40: loss = 0.3908274173736572, acc = 0.8701171875\n",
            "Batch 41: loss = 0.3566347360610962, acc = 0.88671875\n",
            "Batch 42: loss = 0.36381030082702637, acc = 0.8876953125\n",
            "Batch 43: loss = 0.4165761470794678, acc = 0.861328125\n",
            "Batch 44: loss = 0.37655606865882874, acc = 0.8818359375\n",
            "Batch 45: loss = 0.3536766469478607, acc = 0.8779296875\n",
            "Batch 46: loss = 0.392711341381073, acc = 0.8720703125\n",
            "Batch 47: loss = 0.3719612956047058, acc = 0.880859375\n",
            "Batch 48: loss = 0.3183947205543518, acc = 0.890625\n",
            "Batch 49: loss = 0.31952983140945435, acc = 0.904296875\n",
            "Batch 50: loss = 0.40022629499435425, acc = 0.865234375\n",
            "Batch 51: loss = 0.3807814419269562, acc = 0.8681640625\n",
            "Batch 52: loss = 0.3973202705383301, acc = 0.86328125\n",
            "Batch 53: loss = 0.3539051115512848, acc = 0.8828125\n",
            "Batch 54: loss = 0.2983860671520233, acc = 0.8984375\n",
            "Batch 55: loss = 0.34634748101234436, acc = 0.896484375\n",
            "Batch 56: loss = 0.35683321952819824, acc = 0.873046875\n",
            "Batch 57: loss = 0.407157838344574, acc = 0.857421875\n",
            "Batch 58: loss = 0.39134082198143005, acc = 0.8740234375\n",
            "Batch 59: loss = 0.3181876540184021, acc = 0.8935546875\n",
            "Batch 60: loss = 0.37436050176620483, acc = 0.87890625\n",
            "Batch 61: loss = 0.3307349681854248, acc = 0.890625\n",
            "Batch 62: loss = 0.4384717345237732, acc = 0.84765625\n",
            "Batch 63: loss = 0.3518902361392975, acc = 0.8818359375\n",
            "Batch 64: loss = 0.3561207354068756, acc = 0.880859375\n",
            "Batch 65: loss = 0.3994860351085663, acc = 0.8505859375\n",
            "Batch 66: loss = 0.3745998442173004, acc = 0.87890625\n",
            "Batch 67: loss = 0.3658495545387268, acc = 0.865234375\n",
            "Batch 68: loss = 0.3845544159412384, acc = 0.869140625\n",
            "Batch 69: loss = 0.33069372177124023, acc = 0.88671875\n",
            "Batch 70: loss = 0.36545202136039734, acc = 0.873046875\n",
            "Batch 71: loss = 0.40070098638534546, acc = 0.8671875\n",
            "Batch 72: loss = 0.33329784870147705, acc = 0.8876953125\n",
            "Batch 73: loss = 0.45172107219696045, acc = 0.8427734375\n",
            "Batch 74: loss = 0.39678290486335754, acc = 0.8642578125\n",
            "Batch 75: loss = 0.47836852073669434, acc = 0.845703125\n",
            "Batch 76: loss = 0.4085468053817749, acc = 0.86328125\n",
            "Batch 77: loss = 0.3462880551815033, acc = 0.875\n",
            "Batch 78: loss = 0.39427870512008667, acc = 0.8642578125\n",
            "Batch 79: loss = 0.38165026903152466, acc = 0.87890625\n",
            "Batch 80: loss = 0.32976794242858887, acc = 0.888671875\n",
            "Batch 81: loss = 0.3886207342147827, acc = 0.8603515625\n",
            "Batch 82: loss = 0.3755180835723877, acc = 0.8798828125\n",
            "Batch 83: loss = 0.3648855686187744, acc = 0.8837890625\n",
            "Batch 84: loss = 0.34205788373947144, acc = 0.880859375\n",
            "Batch 85: loss = 0.4157637059688568, acc = 0.857421875\n",
            "Batch 86: loss = 0.38036566972732544, acc = 0.8720703125\n",
            "Batch 87: loss = 0.3997608423233032, acc = 0.875\n",
            "Batch 88: loss = 0.4405410885810852, acc = 0.8486328125\n",
            "Batch 89: loss = 0.39567333459854126, acc = 0.876953125\n",
            "Batch 90: loss = 0.3806164562702179, acc = 0.8671875\n",
            "Batch 91: loss = 0.4244092106819153, acc = 0.86328125\n",
            "Batch 92: loss = 0.42695558071136475, acc = 0.861328125\n",
            "Batch 93: loss = 0.35408103466033936, acc = 0.896484375\n",
            "Batch 94: loss = 0.3871261477470398, acc = 0.865234375\n",
            "Batch 95: loss = 0.3618758022785187, acc = 0.8740234375\n",
            "Batch 96: loss = 0.421306312084198, acc = 0.8564453125\n",
            "Batch 97: loss = 0.3872987926006317, acc = 0.8720703125\n",
            "Batch 98: loss = 0.3682834208011627, acc = 0.880859375\n",
            "Batch 99: loss = 0.39529162645339966, acc = 0.8779296875\n",
            "Batch 100: loss = 0.374641478061676, acc = 0.8701171875\n",
            "Batch 101: loss = 0.39408865571022034, acc = 0.8564453125\n",
            "Batch 102: loss = 0.445742666721344, acc = 0.8525390625\n",
            "Batch 103: loss = 0.41396069526672363, acc = 0.8623046875\n",
            "Batch 104: loss = 0.35920730233192444, acc = 0.8759765625\n",
            "Batch 105: loss = 0.34672650694847107, acc = 0.8779296875\n",
            "Batch 106: loss = 0.3889874815940857, acc = 0.869140625\n",
            "Batch 107: loss = 0.39910244941711426, acc = 0.8642578125\n",
            "Batch 108: loss = 0.35394084453582764, acc = 0.880859375\n",
            "Batch 109: loss = 0.35840892791748047, acc = 0.8857421875\n",
            "Batch 110: loss = 0.32711324095726013, acc = 0.890625\n",
            "Batch 111: loss = 0.3791443407535553, acc = 0.8642578125\n",
            "Batch 112: loss = 0.35856014490127563, acc = 0.8779296875\n",
            "Batch 113: loss = 0.41364774107933044, acc = 0.8583984375\n",
            "Batch 114: loss = 0.4090368449687958, acc = 0.8583984375\n",
            "Batch 115: loss = 0.4232368469238281, acc = 0.8583984375\n",
            "Batch 116: loss = 0.4155183434486389, acc = 0.859375\n",
            "Batch 117: loss = 0.3631393313407898, acc = 0.8740234375\n",
            "Batch 118: loss = 0.3640556037425995, acc = 0.865234375\n",
            "Batch 119: loss = 0.3868221044540405, acc = 0.87109375\n",
            "Batch 120: loss = 0.35111358761787415, acc = 0.8818359375\n",
            "Batch 121: loss = 0.37791115045547485, acc = 0.8740234375\n",
            "Batch 122: loss = 0.3748377859592438, acc = 0.8642578125\n",
            "Batch 123: loss = 0.38834214210510254, acc = 0.8837890625\n",
            "Batch 124: loss = 0.43829530477523804, acc = 0.84375\n",
            "Batch 125: loss = 0.4324069023132324, acc = 0.845703125\n",
            "Batch 126: loss = 0.40763041377067566, acc = 0.8544921875\n",
            "\n",
            "Epoch 80/100\n",
            "Batch 1: loss = 0.4983380436897278, acc = 0.84375\n",
            "Batch 2: loss = 0.4291883111000061, acc = 0.841796875\n",
            "Batch 3: loss = 0.3792458772659302, acc = 0.87109375\n",
            "Batch 4: loss = 0.3276180028915405, acc = 0.88671875\n",
            "Batch 5: loss = 0.3520733118057251, acc = 0.8876953125\n",
            "Batch 6: loss = 0.40743371844291687, acc = 0.8642578125\n",
            "Batch 7: loss = 0.3770568370819092, acc = 0.865234375\n",
            "Batch 8: loss = 0.4153212904930115, acc = 0.8505859375\n",
            "Batch 9: loss = 0.3820306658744812, acc = 0.873046875\n",
            "Batch 10: loss = 0.34483304619789124, acc = 0.880859375\n",
            "Batch 11: loss = 0.3661855459213257, acc = 0.873046875\n",
            "Batch 12: loss = 0.3666764497756958, acc = 0.8740234375\n",
            "Batch 13: loss = 0.3737044334411621, acc = 0.875\n",
            "Batch 14: loss = 0.3206842541694641, acc = 0.8916015625\n",
            "Batch 15: loss = 0.3637252748012543, acc = 0.8740234375\n",
            "Batch 16: loss = 0.3884328603744507, acc = 0.869140625\n",
            "Batch 17: loss = 0.3351418375968933, acc = 0.88671875\n",
            "Batch 18: loss = 0.3976910710334778, acc = 0.86328125\n",
            "Batch 19: loss = 0.3840961456298828, acc = 0.87890625\n",
            "Batch 20: loss = 0.3774663209915161, acc = 0.8681640625\n",
            "Batch 21: loss = 0.4376363456249237, acc = 0.861328125\n",
            "Batch 22: loss = 0.3869073987007141, acc = 0.876953125\n",
            "Batch 23: loss = 0.44254690408706665, acc = 0.853515625\n",
            "Batch 24: loss = 0.3736060857772827, acc = 0.865234375\n",
            "Batch 25: loss = 0.4069688618183136, acc = 0.865234375\n",
            "Batch 26: loss = 0.36126255989074707, acc = 0.876953125\n",
            "Batch 27: loss = 0.34359902143478394, acc = 0.8740234375\n",
            "Batch 28: loss = 0.3938528001308441, acc = 0.86328125\n",
            "Batch 29: loss = 0.43247729539871216, acc = 0.8486328125\n",
            "Batch 30: loss = 0.39358508586883545, acc = 0.865234375\n",
            "Batch 31: loss = 0.4036089777946472, acc = 0.8662109375\n",
            "Batch 32: loss = 0.4286421835422516, acc = 0.861328125\n",
            "Batch 33: loss = 0.38014525175094604, acc = 0.869140625\n",
            "Batch 34: loss = 0.390630841255188, acc = 0.86328125\n",
            "Batch 35: loss = 0.37911272048950195, acc = 0.8671875\n",
            "Batch 36: loss = 0.3628803789615631, acc = 0.876953125\n",
            "Batch 37: loss = 0.3467899262905121, acc = 0.8828125\n",
            "Batch 38: loss = 0.3585518002510071, acc = 0.884765625\n",
            "Batch 39: loss = 0.35485005378723145, acc = 0.8759765625\n",
            "Batch 40: loss = 0.34909865260124207, acc = 0.8779296875\n",
            "Batch 41: loss = 0.33952829241752625, acc = 0.890625\n",
            "Batch 42: loss = 0.3596643805503845, acc = 0.876953125\n",
            "Batch 43: loss = 0.3936753273010254, acc = 0.8740234375\n",
            "Batch 44: loss = 0.34305551648139954, acc = 0.892578125\n",
            "Batch 45: loss = 0.3075595498085022, acc = 0.8935546875\n",
            "Batch 46: loss = 0.3248480558395386, acc = 0.890625\n",
            "Batch 47: loss = 0.3209803104400635, acc = 0.888671875\n",
            "Batch 48: loss = 0.3336179256439209, acc = 0.8955078125\n",
            "Batch 49: loss = 0.33912307024002075, acc = 0.8876953125\n",
            "Batch 50: loss = 0.373187392950058, acc = 0.8720703125\n",
            "Batch 51: loss = 0.3691929876804352, acc = 0.8720703125\n",
            "Batch 52: loss = 0.3806443214416504, acc = 0.8681640625\n",
            "Batch 53: loss = 0.3800123333930969, acc = 0.8681640625\n",
            "Batch 54: loss = 0.30921655893325806, acc = 0.89453125\n",
            "Batch 55: loss = 0.32111191749572754, acc = 0.8916015625\n",
            "Batch 56: loss = 0.35025107860565186, acc = 0.876953125\n",
            "Batch 57: loss = 0.4001052677631378, acc = 0.8642578125\n",
            "Batch 58: loss = 0.3838047385215759, acc = 0.876953125\n",
            "Batch 59: loss = 0.33386486768722534, acc = 0.8837890625\n",
            "Batch 60: loss = 0.36832669377326965, acc = 0.8818359375\n",
            "Batch 61: loss = 0.3433171510696411, acc = 0.8798828125\n",
            "Batch 62: loss = 0.4187805950641632, acc = 0.8623046875\n",
            "Batch 63: loss = 0.380047470331192, acc = 0.8740234375\n",
            "Batch 64: loss = 0.3384914994239807, acc = 0.8818359375\n",
            "Batch 65: loss = 0.4231565296649933, acc = 0.8623046875\n",
            "Batch 66: loss = 0.3436402678489685, acc = 0.8828125\n",
            "Batch 67: loss = 0.34169459342956543, acc = 0.8779296875\n",
            "Batch 68: loss = 0.39438265562057495, acc = 0.86328125\n",
            "Batch 69: loss = 0.3401782214641571, acc = 0.8896484375\n",
            "Batch 70: loss = 0.40978923439979553, acc = 0.87109375\n",
            "Batch 71: loss = 0.41406360268592834, acc = 0.8525390625\n",
            "Batch 72: loss = 0.3515683710575104, acc = 0.892578125\n",
            "Batch 73: loss = 0.4390988349914551, acc = 0.8505859375\n",
            "Batch 74: loss = 0.3764801621437073, acc = 0.87109375\n",
            "Batch 75: loss = 0.4627113342285156, acc = 0.8388671875\n",
            "Batch 76: loss = 0.39798516035079956, acc = 0.861328125\n",
            "Batch 77: loss = 0.36564818024635315, acc = 0.87109375\n",
            "Batch 78: loss = 0.3770667016506195, acc = 0.8642578125\n",
            "Batch 79: loss = 0.36101895570755005, acc = 0.884765625\n",
            "Batch 80: loss = 0.35897907614707947, acc = 0.8779296875\n",
            "Batch 81: loss = 0.4339924454689026, acc = 0.84375\n",
            "Batch 82: loss = 0.35521095991134644, acc = 0.884765625\n",
            "Batch 83: loss = 0.3742196559906006, acc = 0.8701171875\n",
            "Batch 84: loss = 0.350079745054245, acc = 0.8701171875\n",
            "Batch 85: loss = 0.42045244574546814, acc = 0.8544921875\n",
            "Batch 86: loss = 0.36096760630607605, acc = 0.8720703125\n",
            "Batch 87: loss = 0.3532315194606781, acc = 0.87890625\n",
            "Batch 88: loss = 0.4506230354309082, acc = 0.8486328125\n",
            "Batch 89: loss = 0.39540401101112366, acc = 0.8583984375\n",
            "Batch 90: loss = 0.43387866020202637, acc = 0.8486328125\n",
            "Batch 91: loss = 0.4186381995677948, acc = 0.86328125\n",
            "Batch 92: loss = 0.40513482689857483, acc = 0.86328125\n",
            "Batch 93: loss = 0.3638516366481781, acc = 0.87890625\n",
            "Batch 94: loss = 0.35636037588119507, acc = 0.873046875\n",
            "Batch 95: loss = 0.3276730179786682, acc = 0.88671875\n",
            "Batch 96: loss = 0.4450680911540985, acc = 0.8408203125\n",
            "Batch 97: loss = 0.37496498227119446, acc = 0.8779296875\n",
            "Batch 98: loss = 0.3467942476272583, acc = 0.8876953125\n",
            "Batch 99: loss = 0.39227402210235596, acc = 0.869140625\n",
            "Batch 100: loss = 0.3612334728240967, acc = 0.8671875\n",
            "Batch 101: loss = 0.3712843060493469, acc = 0.8662109375\n",
            "Batch 102: loss = 0.38646697998046875, acc = 0.8662109375\n",
            "Batch 103: loss = 0.39862754940986633, acc = 0.873046875\n",
            "Batch 104: loss = 0.33991366624832153, acc = 0.87890625\n",
            "Batch 105: loss = 0.38901007175445557, acc = 0.8720703125\n",
            "Batch 106: loss = 0.38511335849761963, acc = 0.8671875\n",
            "Batch 107: loss = 0.37950611114501953, acc = 0.8798828125\n",
            "Batch 108: loss = 0.362694650888443, acc = 0.8759765625\n",
            "Batch 109: loss = 0.39442336559295654, acc = 0.8671875\n",
            "Batch 110: loss = 0.32593780755996704, acc = 0.8896484375\n",
            "Batch 111: loss = 0.40243446826934814, acc = 0.8623046875\n",
            "Batch 112: loss = 0.3518441617488861, acc = 0.8671875\n",
            "Batch 113: loss = 0.36380335688591003, acc = 0.873046875\n",
            "Batch 114: loss = 0.38861772418022156, acc = 0.8623046875\n",
            "Batch 115: loss = 0.3924074172973633, acc = 0.859375\n",
            "Batch 116: loss = 0.4242529273033142, acc = 0.8662109375\n",
            "Batch 117: loss = 0.3587869107723236, acc = 0.8818359375\n",
            "Batch 118: loss = 0.3840528726577759, acc = 0.8671875\n",
            "Batch 119: loss = 0.3711049258708954, acc = 0.8759765625\n",
            "Batch 120: loss = 0.3838742971420288, acc = 0.869140625\n",
            "Batch 121: loss = 0.3613245487213135, acc = 0.8740234375\n",
            "Batch 122: loss = 0.38088107109069824, acc = 0.8740234375\n",
            "Batch 123: loss = 0.35888779163360596, acc = 0.8837890625\n",
            "Batch 124: loss = 0.41842594742774963, acc = 0.8681640625\n",
            "Batch 125: loss = 0.36138659715652466, acc = 0.8740234375\n",
            "Batch 126: loss = 0.37330174446105957, acc = 0.87890625\n",
            "Saved checkpoint to gru_weights.80.h5\n",
            "\n",
            "Epoch 81/100\n",
            "Batch 1: loss = 0.4698304235935211, acc = 0.8515625\n",
            "Batch 2: loss = 0.40494629740715027, acc = 0.869140625\n",
            "Batch 3: loss = 0.41963401436805725, acc = 0.873046875\n",
            "Batch 4: loss = 0.36249229311943054, acc = 0.8828125\n",
            "Batch 5: loss = 0.36461958289146423, acc = 0.873046875\n",
            "Batch 6: loss = 0.3987990617752075, acc = 0.8701171875\n",
            "Batch 7: loss = 0.351656436920166, acc = 0.884765625\n",
            "Batch 8: loss = 0.379710853099823, acc = 0.8759765625\n",
            "Batch 9: loss = 0.3459126055240631, acc = 0.888671875\n",
            "Batch 10: loss = 0.34687259793281555, acc = 0.890625\n",
            "Batch 11: loss = 0.4045129418373108, acc = 0.85546875\n",
            "Batch 12: loss = 0.37394681572914124, acc = 0.869140625\n",
            "Batch 13: loss = 0.3605111837387085, acc = 0.8896484375\n",
            "Batch 14: loss = 0.36246055364608765, acc = 0.8818359375\n",
            "Batch 15: loss = 0.3378179669380188, acc = 0.8896484375\n",
            "Batch 16: loss = 0.3292592465877533, acc = 0.8828125\n",
            "Batch 17: loss = 0.3574454188346863, acc = 0.873046875\n",
            "Batch 18: loss = 0.4134159982204437, acc = 0.8564453125\n",
            "Batch 19: loss = 0.3701324164867401, acc = 0.880859375\n",
            "Batch 20: loss = 0.3370059132575989, acc = 0.8828125\n",
            "Batch 21: loss = 0.46003457903862, acc = 0.8515625\n",
            "Batch 22: loss = 0.36932212114334106, acc = 0.87109375\n",
            "Batch 23: loss = 0.3966684639453888, acc = 0.869140625\n",
            "Batch 24: loss = 0.41118672490119934, acc = 0.8681640625\n",
            "Batch 25: loss = 0.36705178022384644, acc = 0.880859375\n",
            "Batch 26: loss = 0.3730182647705078, acc = 0.87109375\n",
            "Batch 27: loss = 0.4079027771949768, acc = 0.8642578125\n",
            "Batch 28: loss = 0.368122935295105, acc = 0.8818359375\n",
            "Batch 29: loss = 0.34881865978240967, acc = 0.880859375\n",
            "Batch 30: loss = 0.3620684742927551, acc = 0.8740234375\n",
            "Batch 31: loss = 0.384350061416626, acc = 0.8671875\n",
            "Batch 32: loss = 0.42370080947875977, acc = 0.857421875\n",
            "Batch 33: loss = 0.36642396450042725, acc = 0.87890625\n",
            "Batch 34: loss = 0.3749418258666992, acc = 0.8779296875\n",
            "Batch 35: loss = 0.3598673939704895, acc = 0.88671875\n",
            "Batch 36: loss = 0.338052898645401, acc = 0.8935546875\n",
            "Batch 37: loss = 0.3386229872703552, acc = 0.890625\n",
            "Batch 38: loss = 0.35741499066352844, acc = 0.8818359375\n",
            "Batch 39: loss = 0.31978699564933777, acc = 0.896484375\n",
            "Batch 40: loss = 0.37263789772987366, acc = 0.873046875\n",
            "Batch 41: loss = 0.3306519687175751, acc = 0.89453125\n",
            "Batch 42: loss = 0.36419716477394104, acc = 0.8857421875\n",
            "Batch 43: loss = 0.3964303731918335, acc = 0.869140625\n",
            "Batch 44: loss = 0.38815736770629883, acc = 0.8828125\n",
            "Batch 45: loss = 0.305726557970047, acc = 0.8896484375\n",
            "Batch 46: loss = 0.36145099997520447, acc = 0.8857421875\n",
            "Batch 47: loss = 0.3352831304073334, acc = 0.8916015625\n",
            "Batch 48: loss = 0.35855215787887573, acc = 0.8759765625\n",
            "Batch 49: loss = 0.3353986144065857, acc = 0.8935546875\n",
            "Batch 50: loss = 0.3690837323665619, acc = 0.86328125\n",
            "Batch 51: loss = 0.36260807514190674, acc = 0.8857421875\n",
            "Batch 52: loss = 0.3793269693851471, acc = 0.8779296875\n",
            "Batch 53: loss = 0.3646092116832733, acc = 0.8837890625\n",
            "Batch 54: loss = 0.30121859908103943, acc = 0.8974609375\n",
            "Batch 55: loss = 0.3619961738586426, acc = 0.8837890625\n",
            "Batch 56: loss = 0.38507652282714844, acc = 0.869140625\n",
            "Batch 57: loss = 0.4087132215499878, acc = 0.8701171875\n",
            "Batch 58: loss = 0.43059831857681274, acc = 0.8525390625\n",
            "Batch 59: loss = 0.3207966983318329, acc = 0.890625\n",
            "Batch 60: loss = 0.38714486360549927, acc = 0.85546875\n",
            "Batch 61: loss = 0.359578937292099, acc = 0.876953125\n",
            "Batch 62: loss = 0.411489874124527, acc = 0.8662109375\n",
            "Batch 63: loss = 0.31949663162231445, acc = 0.8974609375\n",
            "Batch 64: loss = 0.31994765996932983, acc = 0.888671875\n",
            "Batch 65: loss = 0.374399334192276, acc = 0.87890625\n",
            "Batch 66: loss = 0.365450918674469, acc = 0.8779296875\n",
            "Batch 67: loss = 0.35385748744010925, acc = 0.8798828125\n",
            "Batch 68: loss = 0.3930376172065735, acc = 0.8740234375\n",
            "Batch 69: loss = 0.32275739312171936, acc = 0.90234375\n",
            "Batch 70: loss = 0.41388288140296936, acc = 0.861328125\n",
            "Batch 71: loss = 0.43039941787719727, acc = 0.8447265625\n",
            "Batch 72: loss = 0.35211867094039917, acc = 0.8828125\n",
            "Batch 73: loss = 0.3972916007041931, acc = 0.861328125\n",
            "Batch 74: loss = 0.40284764766693115, acc = 0.86328125\n",
            "Batch 75: loss = 0.45410415530204773, acc = 0.8466796875\n",
            "Batch 76: loss = 0.4003448188304901, acc = 0.857421875\n",
            "Batch 77: loss = 0.35904645919799805, acc = 0.8720703125\n",
            "Batch 78: loss = 0.37832093238830566, acc = 0.8662109375\n",
            "Batch 79: loss = 0.4004301428794861, acc = 0.8701171875\n",
            "Batch 80: loss = 0.342141330242157, acc = 0.875\n",
            "Batch 81: loss = 0.3890262842178345, acc = 0.8662109375\n",
            "Batch 82: loss = 0.357039213180542, acc = 0.8828125\n",
            "Batch 83: loss = 0.3538232743740082, acc = 0.8720703125\n",
            "Batch 84: loss = 0.3455047309398651, acc = 0.884765625\n",
            "Batch 85: loss = 0.42295151948928833, acc = 0.85546875\n",
            "Batch 86: loss = 0.35097646713256836, acc = 0.8740234375\n",
            "Batch 87: loss = 0.38239189982414246, acc = 0.875\n",
            "Batch 88: loss = 0.463473916053772, acc = 0.83984375\n",
            "Batch 89: loss = 0.3437489867210388, acc = 0.875\n",
            "Batch 90: loss = 0.3651655316352844, acc = 0.8720703125\n",
            "Batch 91: loss = 0.40216612815856934, acc = 0.8642578125\n",
            "Batch 92: loss = 0.4364818334579468, acc = 0.8544921875\n",
            "Batch 93: loss = 0.37813282012939453, acc = 0.880859375\n",
            "Batch 94: loss = 0.4032100439071655, acc = 0.87109375\n",
            "Batch 95: loss = 0.35379040241241455, acc = 0.875\n",
            "Batch 96: loss = 0.4334505498409271, acc = 0.837890625\n",
            "Batch 97: loss = 0.40177610516548157, acc = 0.873046875\n",
            "Batch 98: loss = 0.40056392550468445, acc = 0.8671875\n",
            "Batch 99: loss = 0.38796767592430115, acc = 0.8720703125\n",
            "Batch 100: loss = 0.35808831453323364, acc = 0.876953125\n",
            "Batch 101: loss = 0.37760311365127563, acc = 0.8720703125\n",
            "Batch 102: loss = 0.40693944692611694, acc = 0.8681640625\n",
            "Batch 103: loss = 0.3857300877571106, acc = 0.8740234375\n",
            "Batch 104: loss = 0.30253681540489197, acc = 0.8974609375\n",
            "Batch 105: loss = 0.336347371339798, acc = 0.8857421875\n",
            "Batch 106: loss = 0.38225576281547546, acc = 0.8642578125\n",
            "Batch 107: loss = 0.3688015937805176, acc = 0.8779296875\n",
            "Batch 108: loss = 0.3648025691509247, acc = 0.8798828125\n",
            "Batch 109: loss = 0.4100533425807953, acc = 0.8544921875\n",
            "Batch 110: loss = 0.3519994914531708, acc = 0.8759765625\n",
            "Batch 111: loss = 0.4008309543132782, acc = 0.87890625\n",
            "Batch 112: loss = 0.37395742535591125, acc = 0.8779296875\n",
            "Batch 113: loss = 0.38003894686698914, acc = 0.873046875\n",
            "Batch 114: loss = 0.41271936893463135, acc = 0.8642578125\n",
            "Batch 115: loss = 0.3876495361328125, acc = 0.8740234375\n",
            "Batch 116: loss = 0.41653871536254883, acc = 0.8623046875\n",
            "Batch 117: loss = 0.3570873737335205, acc = 0.884765625\n",
            "Batch 118: loss = 0.334911584854126, acc = 0.8837890625\n",
            "Batch 119: loss = 0.33769771456718445, acc = 0.8740234375\n",
            "Batch 120: loss = 0.37609273195266724, acc = 0.873046875\n",
            "Batch 121: loss = 0.3768930435180664, acc = 0.875\n",
            "Batch 122: loss = 0.3767928183078766, acc = 0.875\n",
            "Batch 123: loss = 0.37360092997550964, acc = 0.87109375\n",
            "Batch 124: loss = 0.3539760410785675, acc = 0.880859375\n",
            "Batch 125: loss = 0.4200359284877777, acc = 0.8544921875\n",
            "Batch 126: loss = 0.4068008065223694, acc = 0.8681640625\n",
            "\n",
            "Epoch 82/100\n",
            "Batch 1: loss = 0.5253444910049438, acc = 0.84765625\n",
            "Batch 2: loss = 0.4347372353076935, acc = 0.853515625\n",
            "Batch 3: loss = 0.41129523515701294, acc = 0.8642578125\n",
            "Batch 4: loss = 0.3580991327762604, acc = 0.8798828125\n",
            "Batch 5: loss = 0.3872661590576172, acc = 0.87109375\n",
            "Batch 6: loss = 0.4490978419780731, acc = 0.85546875\n",
            "Batch 7: loss = 0.38093793392181396, acc = 0.8623046875\n",
            "Batch 8: loss = 0.37463679909706116, acc = 0.87109375\n",
            "Batch 9: loss = 0.36707767844200134, acc = 0.8720703125\n",
            "Batch 10: loss = 0.3389340341091156, acc = 0.8896484375\n",
            "Batch 11: loss = 0.36091962456703186, acc = 0.8828125\n",
            "Batch 12: loss = 0.40042829513549805, acc = 0.8671875\n",
            "Batch 13: loss = 0.3681442439556122, acc = 0.8837890625\n",
            "Batch 14: loss = 0.3307582139968872, acc = 0.8876953125\n",
            "Batch 15: loss = 0.3693261742591858, acc = 0.8798828125\n",
            "Batch 16: loss = 0.38193437457084656, acc = 0.8701171875\n",
            "Batch 17: loss = 0.32142889499664307, acc = 0.892578125\n",
            "Batch 18: loss = 0.4264981746673584, acc = 0.8544921875\n",
            "Batch 19: loss = 0.37560564279556274, acc = 0.8681640625\n",
            "Batch 20: loss = 0.3914787769317627, acc = 0.865234375\n",
            "Batch 21: loss = 0.400118350982666, acc = 0.8720703125\n",
            "Batch 22: loss = 0.37959980964660645, acc = 0.87109375\n",
            "Batch 23: loss = 0.39831244945526123, acc = 0.8671875\n",
            "Batch 24: loss = 0.399582177400589, acc = 0.87109375\n",
            "Batch 25: loss = 0.3679148554801941, acc = 0.880859375\n",
            "Batch 26: loss = 0.35518455505371094, acc = 0.8779296875\n",
            "Batch 27: loss = 0.3891580402851105, acc = 0.8740234375\n",
            "Batch 28: loss = 0.3927685022354126, acc = 0.8662109375\n",
            "Batch 29: loss = 0.37109410762786865, acc = 0.875\n",
            "Batch 30: loss = 0.3786756694316864, acc = 0.8779296875\n",
            "Batch 31: loss = 0.39515987038612366, acc = 0.8623046875\n",
            "Batch 32: loss = 0.42809489369392395, acc = 0.8505859375\n",
            "Batch 33: loss = 0.349763423204422, acc = 0.8896484375\n",
            "Batch 34: loss = 0.3635963201522827, acc = 0.8828125\n",
            "Batch 35: loss = 0.3559717833995819, acc = 0.8916015625\n",
            "Batch 36: loss = 0.34395965933799744, acc = 0.8916015625\n",
            "Batch 37: loss = 0.35574397444725037, acc = 0.8876953125\n",
            "Batch 38: loss = 0.36470723152160645, acc = 0.8671875\n",
            "Batch 39: loss = 0.3460403084754944, acc = 0.8876953125\n",
            "Batch 40: loss = 0.3915218710899353, acc = 0.8623046875\n",
            "Batch 41: loss = 0.3526398241519928, acc = 0.873046875\n",
            "Batch 42: loss = 0.35171082615852356, acc = 0.888671875\n",
            "Batch 43: loss = 0.380736380815506, acc = 0.8701171875\n",
            "Batch 44: loss = 0.32654064893722534, acc = 0.8935546875\n",
            "Batch 45: loss = 0.34119486808776855, acc = 0.8876953125\n",
            "Batch 46: loss = 0.3589998483657837, acc = 0.884765625\n",
            "Batch 47: loss = 0.3565281927585602, acc = 0.880859375\n",
            "Batch 48: loss = 0.34136924147605896, acc = 0.888671875\n",
            "Batch 49: loss = 0.3336275517940521, acc = 0.884765625\n",
            "Batch 50: loss = 0.33570826053619385, acc = 0.892578125\n",
            "Batch 51: loss = 0.3876023590564728, acc = 0.873046875\n",
            "Batch 52: loss = 0.35098642110824585, acc = 0.873046875\n",
            "Batch 53: loss = 0.4208856225013733, acc = 0.8623046875\n",
            "Batch 54: loss = 0.31074023246765137, acc = 0.89453125\n",
            "Batch 55: loss = 0.3403535783290863, acc = 0.8818359375\n",
            "Batch 56: loss = 0.330651193857193, acc = 0.8857421875\n",
            "Batch 57: loss = 0.44304561614990234, acc = 0.8525390625\n",
            "Batch 58: loss = 0.3815498650074005, acc = 0.8740234375\n",
            "Batch 59: loss = 0.351608544588089, acc = 0.8837890625\n",
            "Batch 60: loss = 0.3885624408721924, acc = 0.8779296875\n",
            "Batch 61: loss = 0.3547486960887909, acc = 0.8955078125\n",
            "Batch 62: loss = 0.4259156882762909, acc = 0.8583984375\n",
            "Batch 63: loss = 0.3697510361671448, acc = 0.8798828125\n",
            "Batch 64: loss = 0.3395160436630249, acc = 0.890625\n",
            "Batch 65: loss = 0.3792501986026764, acc = 0.8798828125\n",
            "Batch 66: loss = 0.3818829357624054, acc = 0.87890625\n",
            "Batch 67: loss = 0.38380563259124756, acc = 0.8662109375\n",
            "Batch 68: loss = 0.3454759418964386, acc = 0.8818359375\n",
            "Batch 69: loss = 0.3280392289161682, acc = 0.900390625\n",
            "Batch 70: loss = 0.39013099670410156, acc = 0.8671875\n",
            "Batch 71: loss = 0.3655599355697632, acc = 0.8798828125\n",
            "Batch 72: loss = 0.35495540499687195, acc = 0.8759765625\n",
            "Batch 73: loss = 0.43939024209976196, acc = 0.8642578125\n",
            "Batch 74: loss = 0.3751755952835083, acc = 0.873046875\n",
            "Batch 75: loss = 0.44527894258499146, acc = 0.845703125\n",
            "Batch 76: loss = 0.39489099383354187, acc = 0.8642578125\n",
            "Batch 77: loss = 0.3777223229408264, acc = 0.869140625\n",
            "Batch 78: loss = 0.34521499276161194, acc = 0.884765625\n",
            "Batch 79: loss = 0.3710278868675232, acc = 0.869140625\n",
            "Batch 80: loss = 0.33140137791633606, acc = 0.875\n",
            "Batch 81: loss = 0.38073301315307617, acc = 0.865234375\n",
            "Batch 82: loss = 0.35739266872406006, acc = 0.884765625\n",
            "Batch 83: loss = 0.3824614882469177, acc = 0.880859375\n",
            "Batch 84: loss = 0.36166584491729736, acc = 0.8642578125\n",
            "Batch 85: loss = 0.40293532609939575, acc = 0.8603515625\n",
            "Batch 86: loss = 0.3815617859363556, acc = 0.8564453125\n",
            "Batch 87: loss = 0.3825639486312866, acc = 0.8671875\n",
            "Batch 88: loss = 0.42557454109191895, acc = 0.84765625\n",
            "Batch 89: loss = 0.38099321722984314, acc = 0.8828125\n",
            "Batch 90: loss = 0.43016132712364197, acc = 0.8466796875\n",
            "Batch 91: loss = 0.43329373002052307, acc = 0.853515625\n",
            "Batch 92: loss = 0.4308479428291321, acc = 0.8505859375\n",
            "Batch 93: loss = 0.34711894392967224, acc = 0.8798828125\n",
            "Batch 94: loss = 0.34541913866996765, acc = 0.892578125\n",
            "Batch 95: loss = 0.37629491090774536, acc = 0.869140625\n",
            "Batch 96: loss = 0.4236890971660614, acc = 0.8564453125\n",
            "Batch 97: loss = 0.40248462557792664, acc = 0.8662109375\n",
            "Batch 98: loss = 0.3905997574329376, acc = 0.87109375\n",
            "Batch 99: loss = 0.38877779245376587, acc = 0.8701171875\n",
            "Batch 100: loss = 0.3872883915901184, acc = 0.8623046875\n",
            "Batch 101: loss = 0.377578467130661, acc = 0.8662109375\n",
            "Batch 102: loss = 0.41814446449279785, acc = 0.8603515625\n",
            "Batch 103: loss = 0.39456552267074585, acc = 0.8642578125\n",
            "Batch 104: loss = 0.3451375365257263, acc = 0.8837890625\n",
            "Batch 105: loss = 0.3546704053878784, acc = 0.8857421875\n",
            "Batch 106: loss = 0.37751996517181396, acc = 0.8662109375\n",
            "Batch 107: loss = 0.37215644121170044, acc = 0.8779296875\n",
            "Batch 108: loss = 0.4038110375404358, acc = 0.8662109375\n",
            "Batch 109: loss = 0.3682822287082672, acc = 0.8798828125\n",
            "Batch 110: loss = 0.37517356872558594, acc = 0.8740234375\n",
            "Batch 111: loss = 0.382260262966156, acc = 0.869140625\n",
            "Batch 112: loss = 0.3820687532424927, acc = 0.8720703125\n",
            "Batch 113: loss = 0.4095540940761566, acc = 0.85546875\n",
            "Batch 114: loss = 0.39581847190856934, acc = 0.865234375\n",
            "Batch 115: loss = 0.4046107232570648, acc = 0.8759765625\n",
            "Batch 116: loss = 0.4059426784515381, acc = 0.86328125\n",
            "Batch 117: loss = 0.36111170053482056, acc = 0.880859375\n",
            "Batch 118: loss = 0.3085405230522156, acc = 0.9013671875\n",
            "Batch 119: loss = 0.3241574764251709, acc = 0.8916015625\n",
            "Batch 120: loss = 0.3784988522529602, acc = 0.8681640625\n",
            "Batch 121: loss = 0.3570765554904938, acc = 0.8798828125\n",
            "Batch 122: loss = 0.3353891968727112, acc = 0.884765625\n",
            "Batch 123: loss = 0.38600489497184753, acc = 0.8681640625\n",
            "Batch 124: loss = 0.3893495202064514, acc = 0.8662109375\n",
            "Batch 125: loss = 0.3483317494392395, acc = 0.880859375\n",
            "Batch 126: loss = 0.4303012490272522, acc = 0.86328125\n",
            "\n",
            "Epoch 83/100\n",
            "Batch 1: loss = 0.4642642140388489, acc = 0.857421875\n",
            "Batch 2: loss = 0.37956756353378296, acc = 0.8798828125\n",
            "Batch 3: loss = 0.37714630365371704, acc = 0.87109375\n",
            "Batch 4: loss = 0.34694546461105347, acc = 0.888671875\n",
            "Batch 5: loss = 0.35779938101768494, acc = 0.890625\n",
            "Batch 6: loss = 0.46277108788490295, acc = 0.849609375\n",
            "Batch 7: loss = 0.36272650957107544, acc = 0.87109375\n",
            "Batch 8: loss = 0.40157753229141235, acc = 0.861328125\n",
            "Batch 9: loss = 0.37123677134513855, acc = 0.8740234375\n",
            "Batch 10: loss = 0.36580371856689453, acc = 0.8701171875\n",
            "Batch 11: loss = 0.3324110805988312, acc = 0.890625\n",
            "Batch 12: loss = 0.41151148080825806, acc = 0.861328125\n",
            "Batch 13: loss = 0.3577308654785156, acc = 0.8759765625\n",
            "Batch 14: loss = 0.32596057653427124, acc = 0.9013671875\n",
            "Batch 15: loss = 0.3379371464252472, acc = 0.8740234375\n",
            "Batch 16: loss = 0.35266268253326416, acc = 0.88671875\n",
            "Batch 17: loss = 0.3500179052352905, acc = 0.8798828125\n",
            "Batch 18: loss = 0.4523310959339142, acc = 0.84375\n",
            "Batch 19: loss = 0.36849328875541687, acc = 0.8720703125\n",
            "Batch 20: loss = 0.3615129292011261, acc = 0.8720703125\n",
            "Batch 21: loss = 0.3690102696418762, acc = 0.8759765625\n",
            "Batch 22: loss = 0.3925061523914337, acc = 0.865234375\n",
            "Batch 23: loss = 0.4280318319797516, acc = 0.8486328125\n",
            "Batch 24: loss = 0.38914406299591064, acc = 0.865234375\n",
            "Batch 25: loss = 0.3356386125087738, acc = 0.884765625\n",
            "Batch 26: loss = 0.36394673585891724, acc = 0.87890625\n",
            "Batch 27: loss = 0.3745141625404358, acc = 0.8671875\n",
            "Batch 28: loss = 0.38083165884017944, acc = 0.8662109375\n",
            "Batch 29: loss = 0.35215187072753906, acc = 0.8828125\n",
            "Batch 30: loss = 0.3772239089012146, acc = 0.8720703125\n",
            "Batch 31: loss = 0.39114946126937866, acc = 0.8671875\n",
            "Batch 32: loss = 0.4061822295188904, acc = 0.859375\n",
            "Batch 33: loss = 0.34476545453071594, acc = 0.8935546875\n",
            "Batch 34: loss = 0.3851001560688019, acc = 0.8681640625\n",
            "Batch 35: loss = 0.3940849304199219, acc = 0.8701171875\n",
            "Batch 36: loss = 0.3412936329841614, acc = 0.890625\n",
            "Batch 37: loss = 0.35288679599761963, acc = 0.8798828125\n",
            "Batch 38: loss = 0.3687482178211212, acc = 0.875\n",
            "Batch 39: loss = 0.3598960041999817, acc = 0.87890625\n",
            "Batch 40: loss = 0.37402036786079407, acc = 0.87109375\n",
            "Batch 41: loss = 0.3444661796092987, acc = 0.8837890625\n",
            "Batch 42: loss = 0.36506304144859314, acc = 0.8701171875\n",
            "Batch 43: loss = 0.43434467911720276, acc = 0.85546875\n",
            "Batch 44: loss = 0.3398743271827698, acc = 0.892578125\n",
            "Batch 45: loss = 0.33695757389068604, acc = 0.8818359375\n",
            "Batch 46: loss = 0.3526915907859802, acc = 0.8701171875\n",
            "Batch 47: loss = 0.3801133632659912, acc = 0.876953125\n",
            "Batch 48: loss = 0.3351104259490967, acc = 0.8857421875\n",
            "Batch 49: loss = 0.3230243921279907, acc = 0.88671875\n",
            "Batch 50: loss = 0.3456477224826813, acc = 0.884765625\n",
            "Batch 51: loss = 0.44156908988952637, acc = 0.8583984375\n",
            "Batch 52: loss = 0.375438392162323, acc = 0.8740234375\n",
            "Batch 53: loss = 0.3993261158466339, acc = 0.873046875\n",
            "Batch 54: loss = 0.2891429662704468, acc = 0.8984375\n",
            "Batch 55: loss = 0.3370237648487091, acc = 0.8896484375\n",
            "Batch 56: loss = 0.3559560775756836, acc = 0.884765625\n",
            "Batch 57: loss = 0.4485850930213928, acc = 0.8330078125\n",
            "Batch 58: loss = 0.43514376878738403, acc = 0.8515625\n",
            "Batch 59: loss = 0.32511138916015625, acc = 0.8955078125\n",
            "Batch 60: loss = 0.37767595052719116, acc = 0.8662109375\n",
            "Batch 61: loss = 0.3754150867462158, acc = 0.8828125\n",
            "Batch 62: loss = 0.43311113119125366, acc = 0.853515625\n",
            "Batch 63: loss = 0.4295603632926941, acc = 0.8623046875\n",
            "Batch 64: loss = 0.29854896664619446, acc = 0.8935546875\n",
            "Batch 65: loss = 0.397457093000412, acc = 0.869140625\n",
            "Batch 66: loss = 0.4197659492492676, acc = 0.8505859375\n",
            "Batch 67: loss = 0.33758509159088135, acc = 0.8828125\n",
            "Batch 68: loss = 0.3754919767379761, acc = 0.87109375\n",
            "Batch 69: loss = 0.36246195435523987, acc = 0.87890625\n",
            "Batch 70: loss = 0.3881101608276367, acc = 0.86328125\n",
            "Batch 71: loss = 0.36605599522590637, acc = 0.86328125\n",
            "Batch 72: loss = 0.3497680425643921, acc = 0.896484375\n",
            "Batch 73: loss = 0.4343237280845642, acc = 0.865234375\n",
            "Batch 74: loss = 0.36494141817092896, acc = 0.876953125\n",
            "Batch 75: loss = 0.4208349883556366, acc = 0.853515625\n",
            "Batch 76: loss = 0.38450339436531067, acc = 0.8720703125\n",
            "Batch 77: loss = 0.3331666588783264, acc = 0.884765625\n",
            "Batch 78: loss = 0.35576331615448, acc = 0.865234375\n",
            "Batch 79: loss = 0.36243975162506104, acc = 0.8798828125\n",
            "Batch 80: loss = 0.3599768579006195, acc = 0.8759765625\n",
            "Batch 81: loss = 0.39700764417648315, acc = 0.8662109375\n",
            "Batch 82: loss = 0.4187263250350952, acc = 0.86328125\n",
            "Batch 83: loss = 0.3854001760482788, acc = 0.8701171875\n",
            "Batch 84: loss = 0.3816254734992981, acc = 0.86328125\n",
            "Batch 85: loss = 0.42039257287979126, acc = 0.857421875\n",
            "Batch 86: loss = 0.3573007881641388, acc = 0.8779296875\n",
            "Batch 87: loss = 0.38440918922424316, acc = 0.873046875\n",
            "Batch 88: loss = 0.4088234007358551, acc = 0.8603515625\n",
            "Batch 89: loss = 0.3635447323322296, acc = 0.876953125\n",
            "Batch 90: loss = 0.3925492763519287, acc = 0.8701171875\n",
            "Batch 91: loss = 0.40707120299339294, acc = 0.861328125\n",
            "Batch 92: loss = 0.4125424027442932, acc = 0.857421875\n",
            "Batch 93: loss = 0.3615362048149109, acc = 0.8837890625\n",
            "Batch 94: loss = 0.37743502855300903, acc = 0.880859375\n",
            "Batch 95: loss = 0.35379400849342346, acc = 0.880859375\n",
            "Batch 96: loss = 0.42063766717910767, acc = 0.8662109375\n",
            "Batch 97: loss = 0.3837159276008606, acc = 0.8701171875\n",
            "Batch 98: loss = 0.41250845789909363, acc = 0.865234375\n",
            "Batch 99: loss = 0.40297746658325195, acc = 0.8779296875\n",
            "Batch 100: loss = 0.36753755807876587, acc = 0.87890625\n",
            "Batch 101: loss = 0.39635923504829407, acc = 0.8525390625\n",
            "Batch 102: loss = 0.4411308765411377, acc = 0.8505859375\n",
            "Batch 103: loss = 0.3967413902282715, acc = 0.869140625\n",
            "Batch 104: loss = 0.32573580741882324, acc = 0.8837890625\n",
            "Batch 105: loss = 0.34002742171287537, acc = 0.8974609375\n",
            "Batch 106: loss = 0.34034234285354614, acc = 0.884765625\n",
            "Batch 107: loss = 0.3885955810546875, acc = 0.8828125\n",
            "Batch 108: loss = 0.35109394788742065, acc = 0.8828125\n",
            "Batch 109: loss = 0.41938072443008423, acc = 0.8603515625\n",
            "Batch 110: loss = 0.3050282597541809, acc = 0.9013671875\n",
            "Batch 111: loss = 0.38884955644607544, acc = 0.865234375\n",
            "Batch 112: loss = 0.3916880488395691, acc = 0.8681640625\n",
            "Batch 113: loss = 0.3805043697357178, acc = 0.869140625\n",
            "Batch 114: loss = 0.42941999435424805, acc = 0.853515625\n",
            "Batch 115: loss = 0.40450623631477356, acc = 0.8720703125\n",
            "Batch 116: loss = 0.431592732667923, acc = 0.8515625\n",
            "Batch 117: loss = 0.3538879156112671, acc = 0.876953125\n",
            "Batch 118: loss = 0.35125240683555603, acc = 0.8818359375\n",
            "Batch 119: loss = 0.39856237173080444, acc = 0.8671875\n",
            "Batch 120: loss = 0.376839280128479, acc = 0.8701171875\n",
            "Batch 121: loss = 0.3573085367679596, acc = 0.875\n",
            "Batch 122: loss = 0.3635198473930359, acc = 0.888671875\n",
            "Batch 123: loss = 0.3613795340061188, acc = 0.8818359375\n",
            "Batch 124: loss = 0.41986292600631714, acc = 0.861328125\n",
            "Batch 125: loss = 0.37463781237602234, acc = 0.873046875\n",
            "Batch 126: loss = 0.4144936501979828, acc = 0.8603515625\n",
            "\n",
            "Epoch 84/100\n",
            "Batch 1: loss = 0.48341497778892517, acc = 0.84765625\n",
            "Batch 2: loss = 0.45853498578071594, acc = 0.8427734375\n",
            "Batch 3: loss = 0.38729870319366455, acc = 0.86328125\n",
            "Batch 4: loss = 0.35385382175445557, acc = 0.8818359375\n",
            "Batch 5: loss = 0.36499932408332825, acc = 0.8857421875\n",
            "Batch 6: loss = 0.3939227759838104, acc = 0.880859375\n",
            "Batch 7: loss = 0.3914433419704437, acc = 0.865234375\n",
            "Batch 8: loss = 0.37855204939842224, acc = 0.8828125\n",
            "Batch 9: loss = 0.3803388774394989, acc = 0.876953125\n",
            "Batch 10: loss = 0.3423879146575928, acc = 0.890625\n",
            "Batch 11: loss = 0.36678922176361084, acc = 0.8818359375\n",
            "Batch 12: loss = 0.34026211500167847, acc = 0.876953125\n",
            "Batch 13: loss = 0.32564976811408997, acc = 0.8984375\n",
            "Batch 14: loss = 0.3109005391597748, acc = 0.90234375\n",
            "Batch 15: loss = 0.4067438244819641, acc = 0.8623046875\n",
            "Batch 16: loss = 0.37153589725494385, acc = 0.865234375\n",
            "Batch 17: loss = 0.32196176052093506, acc = 0.9052734375\n",
            "Batch 18: loss = 0.39153558015823364, acc = 0.8701171875\n",
            "Batch 19: loss = 0.3634377121925354, acc = 0.875\n",
            "Batch 20: loss = 0.37190762162208557, acc = 0.8740234375\n",
            "Batch 21: loss = 0.4198945164680481, acc = 0.8623046875\n",
            "Batch 22: loss = 0.38346099853515625, acc = 0.8642578125\n",
            "Batch 23: loss = 0.37033841013908386, acc = 0.8681640625\n",
            "Batch 24: loss = 0.38375136256217957, acc = 0.8662109375\n",
            "Batch 25: loss = 0.3914892077445984, acc = 0.8701171875\n",
            "Batch 26: loss = 0.3471946716308594, acc = 0.890625\n",
            "Batch 27: loss = 0.4304027259349823, acc = 0.8662109375\n",
            "Batch 28: loss = 0.3788195252418518, acc = 0.85546875\n",
            "Batch 29: loss = 0.3701782524585724, acc = 0.8701171875\n",
            "Batch 30: loss = 0.3426070809364319, acc = 0.875\n",
            "Batch 31: loss = 0.41165319085121155, acc = 0.857421875\n",
            "Batch 32: loss = 0.45589354634284973, acc = 0.8505859375\n",
            "Batch 33: loss = 0.3935321867465973, acc = 0.86328125\n",
            "Batch 34: loss = 0.41056400537490845, acc = 0.8662109375\n",
            "Batch 35: loss = 0.36422207951545715, acc = 0.88671875\n",
            "Batch 36: loss = 0.37598589062690735, acc = 0.8828125\n",
            "Batch 37: loss = 0.32037603855133057, acc = 0.892578125\n",
            "Batch 38: loss = 0.34956496953964233, acc = 0.8896484375\n",
            "Batch 39: loss = 0.3285394012928009, acc = 0.884765625\n",
            "Batch 40: loss = 0.37454938888549805, acc = 0.8681640625\n",
            "Batch 41: loss = 0.3354201018810272, acc = 0.8798828125\n",
            "Batch 42: loss = 0.385017991065979, acc = 0.8759765625\n",
            "Batch 43: loss = 0.41552460193634033, acc = 0.8544921875\n",
            "Batch 44: loss = 0.36379626393318176, acc = 0.8759765625\n",
            "Batch 45: loss = 0.35351261496543884, acc = 0.8818359375\n",
            "Batch 46: loss = 0.3551022410392761, acc = 0.8818359375\n",
            "Batch 47: loss = 0.3362247943878174, acc = 0.8857421875\n",
            "Batch 48: loss = 0.3462929129600525, acc = 0.8798828125\n",
            "Batch 49: loss = 0.3482637107372284, acc = 0.8837890625\n",
            "Batch 50: loss = 0.3639654219150543, acc = 0.875\n",
            "Batch 51: loss = 0.3582698702812195, acc = 0.876953125\n",
            "Batch 52: loss = 0.33563730120658875, acc = 0.88671875\n",
            "Batch 53: loss = 0.36309266090393066, acc = 0.873046875\n",
            "Batch 54: loss = 0.2710721492767334, acc = 0.912109375\n",
            "Batch 55: loss = 0.34960535168647766, acc = 0.8701171875\n",
            "Batch 56: loss = 0.39051833748817444, acc = 0.8623046875\n",
            "Batch 57: loss = 0.4156813323497772, acc = 0.8505859375\n",
            "Batch 58: loss = 0.43477368354797363, acc = 0.8525390625\n",
            "Batch 59: loss = 0.3439803719520569, acc = 0.8828125\n",
            "Batch 60: loss = 0.4063218832015991, acc = 0.86328125\n",
            "Batch 61: loss = 0.3689461052417755, acc = 0.8818359375\n",
            "Batch 62: loss = 0.4237748980522156, acc = 0.8505859375\n",
            "Batch 63: loss = 0.3589816093444824, acc = 0.875\n",
            "Batch 64: loss = 0.3249390721321106, acc = 0.88671875\n",
            "Batch 65: loss = 0.3826635777950287, acc = 0.87109375\n",
            "Batch 66: loss = 0.4024925231933594, acc = 0.8564453125\n",
            "Batch 67: loss = 0.33215567469596863, acc = 0.8828125\n",
            "Batch 68: loss = 0.382118284702301, acc = 0.8701171875\n",
            "Batch 69: loss = 0.34557467699050903, acc = 0.888671875\n",
            "Batch 70: loss = 0.4014098346233368, acc = 0.87890625\n",
            "Batch 71: loss = 0.43863120675086975, acc = 0.8515625\n",
            "Batch 72: loss = 0.3458396792411804, acc = 0.8818359375\n",
            "Batch 73: loss = 0.45378386974334717, acc = 0.8466796875\n",
            "Batch 74: loss = 0.4093685746192932, acc = 0.84765625\n",
            "Batch 75: loss = 0.43733111023902893, acc = 0.8603515625\n",
            "Batch 76: loss = 0.3983520269393921, acc = 0.8720703125\n",
            "Batch 77: loss = 0.3678182363510132, acc = 0.880859375\n",
            "Batch 78: loss = 0.38589251041412354, acc = 0.865234375\n",
            "Batch 79: loss = 0.3734322488307953, acc = 0.876953125\n",
            "Batch 80: loss = 0.3315821886062622, acc = 0.8837890625\n",
            "Batch 81: loss = 0.3743055760860443, acc = 0.875\n",
            "Batch 82: loss = 0.3621719181537628, acc = 0.8720703125\n",
            "Batch 83: loss = 0.371477335691452, acc = 0.875\n",
            "Batch 84: loss = 0.35299837589263916, acc = 0.8798828125\n",
            "Batch 85: loss = 0.429867684841156, acc = 0.8515625\n",
            "Batch 86: loss = 0.3952900767326355, acc = 0.8564453125\n",
            "Batch 87: loss = 0.3445315957069397, acc = 0.8779296875\n",
            "Batch 88: loss = 0.4405416250228882, acc = 0.8603515625\n",
            "Batch 89: loss = 0.35644710063934326, acc = 0.8818359375\n",
            "Batch 90: loss = 0.3691343069076538, acc = 0.873046875\n",
            "Batch 91: loss = 0.43534329533576965, acc = 0.8564453125\n",
            "Batch 92: loss = 0.42558395862579346, acc = 0.8564453125\n",
            "Batch 93: loss = 0.32962602376937866, acc = 0.8896484375\n",
            "Batch 94: loss = 0.3438197076320648, acc = 0.875\n",
            "Batch 95: loss = 0.3612903356552124, acc = 0.8779296875\n",
            "Batch 96: loss = 0.3984101116657257, acc = 0.859375\n",
            "Batch 97: loss = 0.4017712473869324, acc = 0.8701171875\n",
            "Batch 98: loss = 0.3598586618900299, acc = 0.873046875\n",
            "Batch 99: loss = 0.4068623185157776, acc = 0.8623046875\n",
            "Batch 100: loss = 0.3675435185432434, acc = 0.8759765625\n",
            "Batch 101: loss = 0.3438566327095032, acc = 0.8818359375\n",
            "Batch 102: loss = 0.4079190790653229, acc = 0.8662109375\n",
            "Batch 103: loss = 0.4032534062862396, acc = 0.86328125\n",
            "Batch 104: loss = 0.33646726608276367, acc = 0.8818359375\n",
            "Batch 105: loss = 0.3509419560432434, acc = 0.8779296875\n",
            "Batch 106: loss = 0.3734318017959595, acc = 0.8720703125\n",
            "Batch 107: loss = 0.3679788112640381, acc = 0.876953125\n",
            "Batch 108: loss = 0.36301225423812866, acc = 0.8798828125\n",
            "Batch 109: loss = 0.3620022237300873, acc = 0.880859375\n",
            "Batch 110: loss = 0.32711130380630493, acc = 0.890625\n",
            "Batch 111: loss = 0.3664959669113159, acc = 0.880859375\n",
            "Batch 112: loss = 0.37381207942962646, acc = 0.8681640625\n",
            "Batch 113: loss = 0.3985607624053955, acc = 0.8681640625\n",
            "Batch 114: loss = 0.4238471984863281, acc = 0.8583984375\n",
            "Batch 115: loss = 0.42595064640045166, acc = 0.859375\n",
            "Batch 116: loss = 0.4057804048061371, acc = 0.87109375\n",
            "Batch 117: loss = 0.35722455382347107, acc = 0.8779296875\n",
            "Batch 118: loss = 0.33431148529052734, acc = 0.8828125\n",
            "Batch 119: loss = 0.3634391129016876, acc = 0.869140625\n",
            "Batch 120: loss = 0.3341468572616577, acc = 0.8828125\n",
            "Batch 121: loss = 0.35793957114219666, acc = 0.8681640625\n",
            "Batch 122: loss = 0.3684210479259491, acc = 0.8779296875\n",
            "Batch 123: loss = 0.38059374690055847, acc = 0.8779296875\n",
            "Batch 124: loss = 0.4224209189414978, acc = 0.857421875\n",
            "Batch 125: loss = 0.3799619674682617, acc = 0.8603515625\n",
            "Batch 126: loss = 0.44181451201438904, acc = 0.8583984375\n",
            "\n",
            "Epoch 85/100\n",
            "Batch 1: loss = 0.4972929358482361, acc = 0.84375\n",
            "Batch 2: loss = 0.41961902379989624, acc = 0.861328125\n",
            "Batch 3: loss = 0.4135616421699524, acc = 0.8583984375\n",
            "Batch 4: loss = 0.3455016314983368, acc = 0.880859375\n",
            "Batch 5: loss = 0.41022783517837524, acc = 0.8662109375\n",
            "Batch 6: loss = 0.4372447431087494, acc = 0.859375\n",
            "Batch 7: loss = 0.38429945707321167, acc = 0.865234375\n",
            "Batch 8: loss = 0.3887980282306671, acc = 0.873046875\n",
            "Batch 9: loss = 0.3622421622276306, acc = 0.8857421875\n",
            "Batch 10: loss = 0.3574294149875641, acc = 0.8818359375\n",
            "Batch 11: loss = 0.3781357705593109, acc = 0.8740234375\n",
            "Batch 12: loss = 0.38830411434173584, acc = 0.8759765625\n",
            "Batch 13: loss = 0.36684226989746094, acc = 0.8798828125\n",
            "Batch 14: loss = 0.3542722165584564, acc = 0.884765625\n",
            "Batch 15: loss = 0.33536890149116516, acc = 0.888671875\n",
            "Batch 16: loss = 0.393422931432724, acc = 0.859375\n",
            "Batch 17: loss = 0.36045658588409424, acc = 0.8828125\n",
            "Batch 18: loss = 0.43701907992362976, acc = 0.8505859375\n",
            "Batch 19: loss = 0.38389474153518677, acc = 0.87109375\n",
            "Batch 20: loss = 0.37654122710227966, acc = 0.87109375\n",
            "Batch 21: loss = 0.3709845244884491, acc = 0.8720703125\n",
            "Batch 22: loss = 0.36399760842323303, acc = 0.8740234375\n",
            "Batch 23: loss = 0.4023689031600952, acc = 0.8642578125\n",
            "Batch 24: loss = 0.353109747171402, acc = 0.876953125\n",
            "Batch 25: loss = 0.3872368633747101, acc = 0.8828125\n",
            "Batch 26: loss = 0.38473236560821533, acc = 0.875\n",
            "Batch 27: loss = 0.37564149498939514, acc = 0.875\n",
            "Batch 28: loss = 0.3853590190410614, acc = 0.857421875\n",
            "Batch 29: loss = 0.37669694423675537, acc = 0.8623046875\n",
            "Batch 30: loss = 0.36686456203460693, acc = 0.875\n",
            "Batch 31: loss = 0.35878872871398926, acc = 0.8701171875\n",
            "Batch 32: loss = 0.4186517894268036, acc = 0.8515625\n",
            "Batch 33: loss = 0.396537184715271, acc = 0.86328125\n",
            "Batch 34: loss = 0.35951024293899536, acc = 0.8818359375\n",
            "Batch 35: loss = 0.3561372756958008, acc = 0.8837890625\n",
            "Batch 36: loss = 0.37334921956062317, acc = 0.8857421875\n",
            "Batch 37: loss = 0.3339470624923706, acc = 0.888671875\n",
            "Batch 38: loss = 0.3978879451751709, acc = 0.8583984375\n",
            "Batch 39: loss = 0.3553055226802826, acc = 0.888671875\n",
            "Batch 40: loss = 0.3774278461933136, acc = 0.8603515625\n",
            "Batch 41: loss = 0.34592923521995544, acc = 0.8720703125\n",
            "Batch 42: loss = 0.35113489627838135, acc = 0.8740234375\n",
            "Batch 43: loss = 0.3874722123146057, acc = 0.865234375\n",
            "Batch 44: loss = 0.35084158182144165, acc = 0.888671875\n",
            "Batch 45: loss = 0.35057753324508667, acc = 0.892578125\n",
            "Batch 46: loss = 0.33169981837272644, acc = 0.8916015625\n",
            "Batch 47: loss = 0.3720661699771881, acc = 0.8720703125\n",
            "Batch 48: loss = 0.35090866684913635, acc = 0.8779296875\n",
            "Batch 49: loss = 0.33952170610427856, acc = 0.890625\n",
            "Batch 50: loss = 0.3702350854873657, acc = 0.8740234375\n",
            "Batch 51: loss = 0.3631405234336853, acc = 0.87109375\n",
            "Batch 52: loss = 0.375192791223526, acc = 0.87109375\n",
            "Batch 53: loss = 0.3549244999885559, acc = 0.8828125\n",
            "Batch 54: loss = 0.30197256803512573, acc = 0.890625\n",
            "Batch 55: loss = 0.3453260660171509, acc = 0.87890625\n",
            "Batch 56: loss = 0.36643701791763306, acc = 0.8662109375\n",
            "Batch 57: loss = 0.4163234233856201, acc = 0.86328125\n",
            "Batch 58: loss = 0.37407052516937256, acc = 0.87109375\n",
            "Batch 59: loss = 0.33323463797569275, acc = 0.8828125\n",
            "Batch 60: loss = 0.35314130783081055, acc = 0.88671875\n",
            "Batch 61: loss = 0.3943507969379425, acc = 0.876953125\n",
            "Batch 62: loss = 0.4328620731830597, acc = 0.8515625\n",
            "Batch 63: loss = 0.37485671043395996, acc = 0.869140625\n",
            "Batch 64: loss = 0.31466320157051086, acc = 0.904296875\n",
            "Batch 65: loss = 0.3800612986087799, acc = 0.8701171875\n",
            "Batch 66: loss = 0.3762851357460022, acc = 0.8720703125\n",
            "Batch 67: loss = 0.31634533405303955, acc = 0.888671875\n",
            "Batch 68: loss = 0.38651043176651, acc = 0.876953125\n",
            "Batch 69: loss = 0.3144643306732178, acc = 0.8935546875\n",
            "Batch 70: loss = 0.39480045437812805, acc = 0.869140625\n",
            "Batch 71: loss = 0.38151273131370544, acc = 0.875\n",
            "Batch 72: loss = 0.34753888845443726, acc = 0.8896484375\n",
            "Batch 73: loss = 0.4478195309638977, acc = 0.85546875\n",
            "Batch 74: loss = 0.38178539276123047, acc = 0.873046875\n",
            "Batch 75: loss = 0.4470551609992981, acc = 0.8408203125\n",
            "Batch 76: loss = 0.3721919059753418, acc = 0.873046875\n",
            "Batch 77: loss = 0.3822057843208313, acc = 0.8779296875\n",
            "Batch 78: loss = 0.3753616511821747, acc = 0.86328125\n",
            "Batch 79: loss = 0.38190317153930664, acc = 0.8720703125\n",
            "Batch 80: loss = 0.3415969908237457, acc = 0.873046875\n",
            "Batch 81: loss = 0.38103190064430237, acc = 0.87109375\n",
            "Batch 82: loss = 0.3374839127063751, acc = 0.87890625\n",
            "Batch 83: loss = 0.34755939245224, acc = 0.8916015625\n",
            "Batch 84: loss = 0.364496648311615, acc = 0.873046875\n",
            "Batch 85: loss = 0.44430655241012573, acc = 0.841796875\n",
            "Batch 86: loss = 0.37745627760887146, acc = 0.8662109375\n",
            "Batch 87: loss = 0.3852468729019165, acc = 0.87890625\n",
            "Batch 88: loss = 0.3952196538448334, acc = 0.8681640625\n",
            "Batch 89: loss = 0.392900288105011, acc = 0.869140625\n",
            "Batch 90: loss = 0.38270103931427, acc = 0.8701171875\n",
            "Batch 91: loss = 0.40279924869537354, acc = 0.8603515625\n",
            "Batch 92: loss = 0.4495927095413208, acc = 0.84375\n",
            "Batch 93: loss = 0.37376514077186584, acc = 0.8818359375\n",
            "Batch 94: loss = 0.36456775665283203, acc = 0.865234375\n",
            "Batch 95: loss = 0.34462863206863403, acc = 0.876953125\n",
            "Batch 96: loss = 0.37385979294776917, acc = 0.859375\n",
            "Batch 97: loss = 0.41160205006599426, acc = 0.861328125\n",
            "Batch 98: loss = 0.38745051622390747, acc = 0.86328125\n",
            "Batch 99: loss = 0.32713136076927185, acc = 0.88671875\n",
            "Batch 100: loss = 0.3824266195297241, acc = 0.8759765625\n",
            "Batch 101: loss = 0.37458088994026184, acc = 0.857421875\n",
            "Batch 102: loss = 0.40097540616989136, acc = 0.869140625\n",
            "Batch 103: loss = 0.3508269488811493, acc = 0.8935546875\n",
            "Batch 104: loss = 0.3352605104446411, acc = 0.8896484375\n",
            "Batch 105: loss = 0.3558836281299591, acc = 0.8818359375\n",
            "Batch 106: loss = 0.3806927502155304, acc = 0.8720703125\n",
            "Batch 107: loss = 0.34930726885795593, acc = 0.880859375\n",
            "Batch 108: loss = 0.3556390404701233, acc = 0.880859375\n",
            "Batch 109: loss = 0.36268261075019836, acc = 0.876953125\n",
            "Batch 110: loss = 0.36950990557670593, acc = 0.8818359375\n",
            "Batch 111: loss = 0.3932168185710907, acc = 0.8681640625\n",
            "Batch 112: loss = 0.3685508966445923, acc = 0.8720703125\n",
            "Batch 113: loss = 0.3587145209312439, acc = 0.87890625\n",
            "Batch 114: loss = 0.40187567472457886, acc = 0.86328125\n",
            "Batch 115: loss = 0.4112931787967682, acc = 0.865234375\n",
            "Batch 116: loss = 0.41336509585380554, acc = 0.8681640625\n",
            "Batch 117: loss = 0.35088351368904114, acc = 0.880859375\n",
            "Batch 118: loss = 0.3425740599632263, acc = 0.8818359375\n",
            "Batch 119: loss = 0.3556111752986908, acc = 0.888671875\n",
            "Batch 120: loss = 0.3445340394973755, acc = 0.8837890625\n",
            "Batch 121: loss = 0.3720863163471222, acc = 0.87890625\n",
            "Batch 122: loss = 0.3642009198665619, acc = 0.873046875\n",
            "Batch 123: loss = 0.41334548592567444, acc = 0.8642578125\n",
            "Batch 124: loss = 0.40510469675064087, acc = 0.8662109375\n",
            "Batch 125: loss = 0.39562931656837463, acc = 0.873046875\n",
            "Batch 126: loss = 0.41798967123031616, acc = 0.86328125\n",
            "\n",
            "Epoch 86/100\n",
            "Batch 1: loss = 0.48331132531166077, acc = 0.8544921875\n",
            "Batch 2: loss = 0.4078984260559082, acc = 0.861328125\n",
            "Batch 3: loss = 0.3883422315120697, acc = 0.8681640625\n",
            "Batch 4: loss = 0.37833261489868164, acc = 0.876953125\n",
            "Batch 5: loss = 0.3686787486076355, acc = 0.87109375\n",
            "Batch 6: loss = 0.41873466968536377, acc = 0.8525390625\n",
            "Batch 7: loss = 0.35128650069236755, acc = 0.888671875\n",
            "Batch 8: loss = 0.4155292510986328, acc = 0.853515625\n",
            "Batch 9: loss = 0.40553855895996094, acc = 0.8662109375\n",
            "Batch 10: loss = 0.32841572165489197, acc = 0.892578125\n",
            "Batch 11: loss = 0.36054134368896484, acc = 0.880859375\n",
            "Batch 12: loss = 0.377115398645401, acc = 0.8779296875\n",
            "Batch 13: loss = 0.35635286569595337, acc = 0.880859375\n",
            "Batch 14: loss = 0.3300073444843292, acc = 0.890625\n",
            "Batch 15: loss = 0.36774298548698425, acc = 0.8759765625\n",
            "Batch 16: loss = 0.34349972009658813, acc = 0.8798828125\n",
            "Batch 17: loss = 0.3787871301174164, acc = 0.869140625\n",
            "Batch 18: loss = 0.43144822120666504, acc = 0.8544921875\n",
            "Batch 19: loss = 0.37727317214012146, acc = 0.8662109375\n",
            "Batch 20: loss = 0.35185056924819946, acc = 0.87890625\n",
            "Batch 21: loss = 0.3952042758464813, acc = 0.86328125\n",
            "Batch 22: loss = 0.3421499729156494, acc = 0.869140625\n",
            "Batch 23: loss = 0.3759506940841675, acc = 0.8720703125\n",
            "Batch 24: loss = 0.3472547233104706, acc = 0.876953125\n",
            "Batch 25: loss = 0.3472731113433838, acc = 0.88671875\n",
            "Batch 26: loss = 0.37175050377845764, acc = 0.8740234375\n",
            "Batch 27: loss = 0.39538559317588806, acc = 0.87109375\n",
            "Batch 28: loss = 0.3685559332370758, acc = 0.876953125\n",
            "Batch 29: loss = 0.3837275207042694, acc = 0.875\n",
            "Batch 30: loss = 0.35434913635253906, acc = 0.8837890625\n",
            "Batch 31: loss = 0.3460094630718231, acc = 0.8779296875\n",
            "Batch 32: loss = 0.4080201983451843, acc = 0.865234375\n",
            "Batch 33: loss = 0.3929927945137024, acc = 0.87109375\n",
            "Batch 34: loss = 0.36556756496429443, acc = 0.875\n",
            "Batch 35: loss = 0.3175814747810364, acc = 0.89453125\n",
            "Batch 36: loss = 0.35143211483955383, acc = 0.8818359375\n",
            "Batch 37: loss = 0.3147910237312317, acc = 0.8935546875\n",
            "Batch 38: loss = 0.34020885825157166, acc = 0.896484375\n",
            "Batch 39: loss = 0.3317602276802063, acc = 0.8857421875\n",
            "Batch 40: loss = 0.36546704173088074, acc = 0.87890625\n",
            "Batch 41: loss = 0.35001230239868164, acc = 0.87890625\n",
            "Batch 42: loss = 0.3537367284297943, acc = 0.8701171875\n",
            "Batch 43: loss = 0.42306965589523315, acc = 0.8642578125\n",
            "Batch 44: loss = 0.3528379499912262, acc = 0.88671875\n",
            "Batch 45: loss = 0.3564031720161438, acc = 0.8828125\n",
            "Batch 46: loss = 0.35542958974838257, acc = 0.876953125\n",
            "Batch 47: loss = 0.3632820248603821, acc = 0.87890625\n",
            "Batch 48: loss = 0.3483947217464447, acc = 0.8857421875\n",
            "Batch 49: loss = 0.3459908962249756, acc = 0.8876953125\n",
            "Batch 50: loss = 0.35542047023773193, acc = 0.8828125\n",
            "Batch 51: loss = 0.3784193992614746, acc = 0.8701171875\n",
            "Batch 52: loss = 0.362770676612854, acc = 0.865234375\n",
            "Batch 53: loss = 0.35959380865097046, acc = 0.876953125\n",
            "Batch 54: loss = 0.3067253828048706, acc = 0.884765625\n",
            "Batch 55: loss = 0.3313564360141754, acc = 0.884765625\n",
            "Batch 56: loss = 0.3712121844291687, acc = 0.880859375\n",
            "Batch 57: loss = 0.40179818868637085, acc = 0.8701171875\n",
            "Batch 58: loss = 0.3856748640537262, acc = 0.865234375\n",
            "Batch 59: loss = 0.33737167716026306, acc = 0.888671875\n",
            "Batch 60: loss = 0.3785920739173889, acc = 0.87109375\n",
            "Batch 61: loss = 0.3229188621044159, acc = 0.8818359375\n",
            "Batch 62: loss = 0.4154317080974579, acc = 0.8623046875\n",
            "Batch 63: loss = 0.33601486682891846, acc = 0.890625\n",
            "Batch 64: loss = 0.34821853041648865, acc = 0.8818359375\n",
            "Batch 65: loss = 0.323883980512619, acc = 0.8896484375\n",
            "Batch 66: loss = 0.36343318223953247, acc = 0.869140625\n",
            "Batch 67: loss = 0.3760169744491577, acc = 0.8720703125\n",
            "Batch 68: loss = 0.42188605666160583, acc = 0.8603515625\n",
            "Batch 69: loss = 0.3822922706604004, acc = 0.873046875\n",
            "Batch 70: loss = 0.3895455598831177, acc = 0.8740234375\n",
            "Batch 71: loss = 0.39254966378211975, acc = 0.857421875\n",
            "Batch 72: loss = 0.3614376485347748, acc = 0.8759765625\n",
            "Batch 73: loss = 0.4257415533065796, acc = 0.8603515625\n",
            "Batch 74: loss = 0.40178218483924866, acc = 0.8681640625\n",
            "Batch 75: loss = 0.46109509468078613, acc = 0.849609375\n",
            "Batch 76: loss = 0.3438839912414551, acc = 0.8759765625\n",
            "Batch 77: loss = 0.3685138523578644, acc = 0.880859375\n",
            "Batch 78: loss = 0.39491206407546997, acc = 0.873046875\n",
            "Batch 79: loss = 0.3528911769390106, acc = 0.896484375\n",
            "Batch 80: loss = 0.3367365002632141, acc = 0.8876953125\n",
            "Batch 81: loss = 0.40681302547454834, acc = 0.873046875\n",
            "Batch 82: loss = 0.3721393644809723, acc = 0.8779296875\n",
            "Batch 83: loss = 0.3502536118030548, acc = 0.8779296875\n",
            "Batch 84: loss = 0.3647913932800293, acc = 0.8779296875\n",
            "Batch 85: loss = 0.40093398094177246, acc = 0.8583984375\n",
            "Batch 86: loss = 0.3581897020339966, acc = 0.8720703125\n",
            "Batch 87: loss = 0.3690686821937561, acc = 0.8818359375\n",
            "Batch 88: loss = 0.43052858114242554, acc = 0.8447265625\n",
            "Batch 89: loss = 0.37187325954437256, acc = 0.8798828125\n",
            "Batch 90: loss = 0.40251386165618896, acc = 0.8642578125\n",
            "Batch 91: loss = 0.41901895403862, acc = 0.8623046875\n",
            "Batch 92: loss = 0.4153880178928375, acc = 0.865234375\n",
            "Batch 93: loss = 0.3398940861225128, acc = 0.892578125\n",
            "Batch 94: loss = 0.352268785238266, acc = 0.876953125\n",
            "Batch 95: loss = 0.3540119230747223, acc = 0.8662109375\n",
            "Batch 96: loss = 0.38330763578414917, acc = 0.8642578125\n",
            "Batch 97: loss = 0.36192435026168823, acc = 0.8779296875\n",
            "Batch 98: loss = 0.3778720498085022, acc = 0.8759765625\n",
            "Batch 99: loss = 0.39042267203330994, acc = 0.8642578125\n",
            "Batch 100: loss = 0.3864852488040924, acc = 0.8642578125\n",
            "Batch 101: loss = 0.3644137382507324, acc = 0.8740234375\n",
            "Batch 102: loss = 0.42273178696632385, acc = 0.8564453125\n",
            "Batch 103: loss = 0.3837510943412781, acc = 0.8701171875\n",
            "Batch 104: loss = 0.301038533449173, acc = 0.900390625\n",
            "Batch 105: loss = 0.33889785408973694, acc = 0.88671875\n",
            "Batch 106: loss = 0.4037763178348541, acc = 0.8671875\n",
            "Batch 107: loss = 0.3777419626712799, acc = 0.876953125\n",
            "Batch 108: loss = 0.33470025658607483, acc = 0.8876953125\n",
            "Batch 109: loss = 0.35822540521621704, acc = 0.8740234375\n",
            "Batch 110: loss = 0.3229615390300751, acc = 0.890625\n",
            "Batch 111: loss = 0.39155614376068115, acc = 0.8671875\n",
            "Batch 112: loss = 0.3635944128036499, acc = 0.87890625\n",
            "Batch 113: loss = 0.3860364556312561, acc = 0.86328125\n",
            "Batch 114: loss = 0.40495264530181885, acc = 0.87109375\n",
            "Batch 115: loss = 0.39094310998916626, acc = 0.861328125\n",
            "Batch 116: loss = 0.38235706090927124, acc = 0.8828125\n",
            "Batch 117: loss = 0.365658700466156, acc = 0.873046875\n",
            "Batch 118: loss = 0.3386174440383911, acc = 0.884765625\n",
            "Batch 119: loss = 0.35634908080101013, acc = 0.8837890625\n",
            "Batch 120: loss = 0.3563804626464844, acc = 0.8818359375\n",
            "Batch 121: loss = 0.3699215054512024, acc = 0.8828125\n",
            "Batch 122: loss = 0.33218851685523987, acc = 0.884765625\n",
            "Batch 123: loss = 0.3818565905094147, acc = 0.875\n",
            "Batch 124: loss = 0.4199652075767517, acc = 0.859375\n",
            "Batch 125: loss = 0.3670422434806824, acc = 0.87109375\n",
            "Batch 126: loss = 0.37592366337776184, acc = 0.873046875\n",
            "\n",
            "Epoch 87/100\n",
            "Batch 1: loss = 0.5203088521957397, acc = 0.83203125\n",
            "Batch 2: loss = 0.38718780875205994, acc = 0.8759765625\n",
            "Batch 3: loss = 0.41417911648750305, acc = 0.8603515625\n",
            "Batch 4: loss = 0.32547619938850403, acc = 0.8876953125\n",
            "Batch 5: loss = 0.3414851129055023, acc = 0.8828125\n",
            "Batch 6: loss = 0.38256561756134033, acc = 0.8623046875\n",
            "Batch 7: loss = 0.360563188791275, acc = 0.8720703125\n",
            "Batch 8: loss = 0.4016467332839966, acc = 0.875\n",
            "Batch 9: loss = 0.3860241770744324, acc = 0.8662109375\n",
            "Batch 10: loss = 0.36201488971710205, acc = 0.884765625\n",
            "Batch 11: loss = 0.37027794122695923, acc = 0.869140625\n",
            "Batch 12: loss = 0.3650662302970886, acc = 0.865234375\n",
            "Batch 13: loss = 0.36248841881752014, acc = 0.888671875\n",
            "Batch 14: loss = 0.32201433181762695, acc = 0.8935546875\n",
            "Batch 15: loss = 0.3459502160549164, acc = 0.8779296875\n",
            "Batch 16: loss = 0.3866768777370453, acc = 0.873046875\n",
            "Batch 17: loss = 0.3454529941082001, acc = 0.8935546875\n",
            "Batch 18: loss = 0.3595466911792755, acc = 0.8740234375\n",
            "Batch 19: loss = 0.3743455410003662, acc = 0.8759765625\n",
            "Batch 20: loss = 0.3766173720359802, acc = 0.8779296875\n",
            "Batch 21: loss = 0.41510120034217834, acc = 0.8515625\n",
            "Batch 22: loss = 0.3680897057056427, acc = 0.8740234375\n",
            "Batch 23: loss = 0.41599375009536743, acc = 0.853515625\n",
            "Batch 24: loss = 0.38732460141181946, acc = 0.87109375\n",
            "Batch 25: loss = 0.3836793601512909, acc = 0.8740234375\n",
            "Batch 26: loss = 0.35629212856292725, acc = 0.8779296875\n",
            "Batch 27: loss = 0.38842687010765076, acc = 0.8642578125\n",
            "Batch 28: loss = 0.35444405674934387, acc = 0.875\n",
            "Batch 29: loss = 0.3509681522846222, acc = 0.8896484375\n",
            "Batch 30: loss = 0.3582311272621155, acc = 0.876953125\n",
            "Batch 31: loss = 0.3840048909187317, acc = 0.865234375\n",
            "Batch 32: loss = 0.39747709035873413, acc = 0.8701171875\n",
            "Batch 33: loss = 0.38681817054748535, acc = 0.876953125\n",
            "Batch 34: loss = 0.3313741087913513, acc = 0.8876953125\n",
            "Batch 35: loss = 0.31874021887779236, acc = 0.8876953125\n",
            "Batch 36: loss = 0.36340615153312683, acc = 0.8779296875\n",
            "Batch 37: loss = 0.3047718107700348, acc = 0.896484375\n",
            "Batch 38: loss = 0.3320559859275818, acc = 0.8916015625\n",
            "Batch 39: loss = 0.36553698778152466, acc = 0.884765625\n",
            "Batch 40: loss = 0.3578326404094696, acc = 0.8671875\n",
            "Batch 41: loss = 0.35807695984840393, acc = 0.875\n",
            "Batch 42: loss = 0.3506348729133606, acc = 0.87890625\n",
            "Batch 43: loss = 0.3916780352592468, acc = 0.869140625\n",
            "Batch 44: loss = 0.3511331379413605, acc = 0.888671875\n",
            "Batch 45: loss = 0.36917322874069214, acc = 0.875\n",
            "Batch 46: loss = 0.34067803621292114, acc = 0.8740234375\n",
            "Batch 47: loss = 0.32869890332221985, acc = 0.8955078125\n",
            "Batch 48: loss = 0.3377499580383301, acc = 0.8828125\n",
            "Batch 49: loss = 0.31940680742263794, acc = 0.89453125\n",
            "Batch 50: loss = 0.38144445419311523, acc = 0.87890625\n",
            "Batch 51: loss = 0.361138254404068, acc = 0.8837890625\n",
            "Batch 52: loss = 0.34775927662849426, acc = 0.880859375\n",
            "Batch 53: loss = 0.3457544147968292, acc = 0.88671875\n",
            "Batch 54: loss = 0.33555954694747925, acc = 0.8896484375\n",
            "Batch 55: loss = 0.35595953464508057, acc = 0.875\n",
            "Batch 56: loss = 0.3553987741470337, acc = 0.876953125\n",
            "Batch 57: loss = 0.38647913932800293, acc = 0.869140625\n",
            "Batch 58: loss = 0.3985826373100281, acc = 0.8720703125\n",
            "Batch 59: loss = 0.33845657110214233, acc = 0.8916015625\n",
            "Batch 60: loss = 0.35701557993888855, acc = 0.8740234375\n",
            "Batch 61: loss = 0.3165416717529297, acc = 0.884765625\n",
            "Batch 62: loss = 0.4397471845149994, acc = 0.8408203125\n",
            "Batch 63: loss = 0.35315942764282227, acc = 0.88671875\n",
            "Batch 64: loss = 0.3581109941005707, acc = 0.8759765625\n",
            "Batch 65: loss = 0.36531832814216614, acc = 0.8759765625\n",
            "Batch 66: loss = 0.3525695502758026, acc = 0.8701171875\n",
            "Batch 67: loss = 0.33533748984336853, acc = 0.8837890625\n",
            "Batch 68: loss = 0.4241097569465637, acc = 0.8603515625\n",
            "Batch 69: loss = 0.376692533493042, acc = 0.875\n",
            "Batch 70: loss = 0.3828457295894623, acc = 0.8779296875\n",
            "Batch 71: loss = 0.39754757285118103, acc = 0.8564453125\n",
            "Batch 72: loss = 0.36077776551246643, acc = 0.8759765625\n",
            "Batch 73: loss = 0.39353927969932556, acc = 0.880859375\n",
            "Batch 74: loss = 0.3573495149612427, acc = 0.8837890625\n",
            "Batch 75: loss = 0.4358707368373871, acc = 0.85546875\n",
            "Batch 76: loss = 0.3795161843299866, acc = 0.8720703125\n",
            "Batch 77: loss = 0.3643997609615326, acc = 0.8720703125\n",
            "Batch 78: loss = 0.3960801959037781, acc = 0.865234375\n",
            "Batch 79: loss = 0.3382730782032013, acc = 0.892578125\n",
            "Batch 80: loss = 0.3288905918598175, acc = 0.8857421875\n",
            "Batch 81: loss = 0.4116438031196594, acc = 0.859375\n",
            "Batch 82: loss = 0.35739123821258545, acc = 0.8857421875\n",
            "Batch 83: loss = 0.35194575786590576, acc = 0.8896484375\n",
            "Batch 84: loss = 0.3397364616394043, acc = 0.888671875\n",
            "Batch 85: loss = 0.41005414724349976, acc = 0.8583984375\n",
            "Batch 86: loss = 0.3422013819217682, acc = 0.8818359375\n",
            "Batch 87: loss = 0.38838207721710205, acc = 0.8798828125\n",
            "Batch 88: loss = 0.44009125232696533, acc = 0.8408203125\n",
            "Batch 89: loss = 0.3418325185775757, acc = 0.8818359375\n",
            "Batch 90: loss = 0.3975151479244232, acc = 0.8583984375\n",
            "Batch 91: loss = 0.3982568383216858, acc = 0.8642578125\n",
            "Batch 92: loss = 0.3778626322746277, acc = 0.87109375\n",
            "Batch 93: loss = 0.341357946395874, acc = 0.8876953125\n",
            "Batch 94: loss = 0.37265437841415405, acc = 0.884765625\n",
            "Batch 95: loss = 0.3029157519340515, acc = 0.8955078125\n",
            "Batch 96: loss = 0.41454726457595825, acc = 0.8486328125\n",
            "Batch 97: loss = 0.3547888398170471, acc = 0.884765625\n",
            "Batch 98: loss = 0.35692861676216125, acc = 0.8876953125\n",
            "Batch 99: loss = 0.4024576246738434, acc = 0.8701171875\n",
            "Batch 100: loss = 0.38963139057159424, acc = 0.8642578125\n",
            "Batch 101: loss = 0.327859103679657, acc = 0.8828125\n",
            "Batch 102: loss = 0.4450185000896454, acc = 0.85546875\n",
            "Batch 103: loss = 0.3919100761413574, acc = 0.8671875\n",
            "Batch 104: loss = 0.3304487466812134, acc = 0.87890625\n",
            "Batch 105: loss = 0.36772680282592773, acc = 0.876953125\n",
            "Batch 106: loss = 0.364686518907547, acc = 0.861328125\n",
            "Batch 107: loss = 0.3971428871154785, acc = 0.87109375\n",
            "Batch 108: loss = 0.3490231931209564, acc = 0.8798828125\n",
            "Batch 109: loss = 0.37319415807724, acc = 0.873046875\n",
            "Batch 110: loss = 0.3216921091079712, acc = 0.8984375\n",
            "Batch 111: loss = 0.34591102600097656, acc = 0.888671875\n",
            "Batch 112: loss = 0.3519614338874817, acc = 0.880859375\n",
            "Batch 113: loss = 0.37437403202056885, acc = 0.869140625\n",
            "Batch 114: loss = 0.38990068435668945, acc = 0.880859375\n",
            "Batch 115: loss = 0.3941679298877716, acc = 0.8779296875\n",
            "Batch 116: loss = 0.36794793605804443, acc = 0.876953125\n",
            "Batch 117: loss = 0.3468325138092041, acc = 0.8916015625\n",
            "Batch 118: loss = 0.3294652998447418, acc = 0.8896484375\n",
            "Batch 119: loss = 0.358421266078949, acc = 0.884765625\n",
            "Batch 120: loss = 0.3482130765914917, acc = 0.8798828125\n",
            "Batch 121: loss = 0.3502621054649353, acc = 0.8779296875\n",
            "Batch 122: loss = 0.3539755344390869, acc = 0.88671875\n",
            "Batch 123: loss = 0.3234958052635193, acc = 0.8955078125\n",
            "Batch 124: loss = 0.3810035288333893, acc = 0.861328125\n",
            "Batch 125: loss = 0.3824046552181244, acc = 0.865234375\n",
            "Batch 126: loss = 0.35667720437049866, acc = 0.876953125\n",
            "\n",
            "Epoch 88/100\n",
            "Batch 1: loss = 0.4908360242843628, acc = 0.8408203125\n",
            "Batch 2: loss = 0.4198535978794098, acc = 0.853515625\n",
            "Batch 3: loss = 0.4043309688568115, acc = 0.8662109375\n",
            "Batch 4: loss = 0.3497731387615204, acc = 0.880859375\n",
            "Batch 5: loss = 0.4174690842628479, acc = 0.865234375\n",
            "Batch 6: loss = 0.4176753759384155, acc = 0.8583984375\n",
            "Batch 7: loss = 0.36994698643684387, acc = 0.8857421875\n",
            "Batch 8: loss = 0.38487082719802856, acc = 0.876953125\n",
            "Batch 9: loss = 0.3778189420700073, acc = 0.8681640625\n",
            "Batch 10: loss = 0.33398428559303284, acc = 0.8798828125\n",
            "Batch 11: loss = 0.38778385519981384, acc = 0.87109375\n",
            "Batch 12: loss = 0.3664284646511078, acc = 0.869140625\n",
            "Batch 13: loss = 0.3451637923717499, acc = 0.884765625\n",
            "Batch 14: loss = 0.30061161518096924, acc = 0.9033203125\n",
            "Batch 15: loss = 0.34531691670417786, acc = 0.884765625\n",
            "Batch 16: loss = 0.37084829807281494, acc = 0.8798828125\n",
            "Batch 17: loss = 0.336534321308136, acc = 0.8857421875\n",
            "Batch 18: loss = 0.3893834948539734, acc = 0.865234375\n",
            "Batch 19: loss = 0.3441794514656067, acc = 0.8828125\n",
            "Batch 20: loss = 0.3473934829235077, acc = 0.8857421875\n",
            "Batch 21: loss = 0.4145198166370392, acc = 0.8642578125\n",
            "Batch 22: loss = 0.3999605178833008, acc = 0.87109375\n",
            "Batch 23: loss = 0.3728559911251068, acc = 0.869140625\n",
            "Batch 24: loss = 0.3465040326118469, acc = 0.8759765625\n",
            "Batch 25: loss = 0.3602837920188904, acc = 0.8779296875\n",
            "Batch 26: loss = 0.38743457198143005, acc = 0.869140625\n",
            "Batch 27: loss = 0.35947340726852417, acc = 0.8779296875\n",
            "Batch 28: loss = 0.37432047724723816, acc = 0.8701171875\n",
            "Batch 29: loss = 0.35597318410873413, acc = 0.87890625\n",
            "Batch 30: loss = 0.3665199279785156, acc = 0.876953125\n",
            "Batch 31: loss = 0.3760188817977905, acc = 0.8759765625\n",
            "Batch 32: loss = 0.4205336570739746, acc = 0.8505859375\n",
            "Batch 33: loss = 0.3558084964752197, acc = 0.880859375\n",
            "Batch 34: loss = 0.37812891602516174, acc = 0.8759765625\n",
            "Batch 35: loss = 0.3643023371696472, acc = 0.8857421875\n",
            "Batch 36: loss = 0.3606422245502472, acc = 0.87890625\n",
            "Batch 37: loss = 0.3370838165283203, acc = 0.8857421875\n",
            "Batch 38: loss = 0.32452622056007385, acc = 0.8984375\n",
            "Batch 39: loss = 0.3521052598953247, acc = 0.8759765625\n",
            "Batch 40: loss = 0.338411420583725, acc = 0.8876953125\n",
            "Batch 41: loss = 0.35766953229904175, acc = 0.8759765625\n",
            "Batch 42: loss = 0.3569335341453552, acc = 0.8828125\n",
            "Batch 43: loss = 0.38325098156929016, acc = 0.8720703125\n",
            "Batch 44: loss = 0.3681906461715698, acc = 0.8740234375\n",
            "Batch 45: loss = 0.3569630980491638, acc = 0.87890625\n",
            "Batch 46: loss = 0.33668041229248047, acc = 0.876953125\n",
            "Batch 47: loss = 0.3695611357688904, acc = 0.8798828125\n",
            "Batch 48: loss = 0.33393117785453796, acc = 0.87890625\n",
            "Batch 49: loss = 0.352895051240921, acc = 0.888671875\n",
            "Batch 50: loss = 0.3512195646762848, acc = 0.876953125\n",
            "Batch 51: loss = 0.3685886859893799, acc = 0.880859375\n",
            "Batch 52: loss = 0.33745458722114563, acc = 0.8837890625\n",
            "Batch 53: loss = 0.4164326786994934, acc = 0.8466796875\n",
            "Batch 54: loss = 0.2946357727050781, acc = 0.90234375\n",
            "Batch 55: loss = 0.3356490731239319, acc = 0.873046875\n",
            "Batch 56: loss = 0.3583053648471832, acc = 0.8837890625\n",
            "Batch 57: loss = 0.37396836280822754, acc = 0.869140625\n",
            "Batch 58: loss = 0.39977705478668213, acc = 0.85546875\n",
            "Batch 59: loss = 0.339290976524353, acc = 0.8828125\n",
            "Batch 60: loss = 0.36219650506973267, acc = 0.88671875\n",
            "Batch 61: loss = 0.32523295283317566, acc = 0.900390625\n",
            "Batch 62: loss = 0.4115200638771057, acc = 0.8515625\n",
            "Batch 63: loss = 0.3378770351409912, acc = 0.8916015625\n",
            "Batch 64: loss = 0.3397698402404785, acc = 0.890625\n",
            "Batch 65: loss = 0.31517294049263, acc = 0.8935546875\n",
            "Batch 66: loss = 0.3827330768108368, acc = 0.865234375\n",
            "Batch 67: loss = 0.34272968769073486, acc = 0.884765625\n",
            "Batch 68: loss = 0.3606964945793152, acc = 0.8720703125\n",
            "Batch 69: loss = 0.3460286259651184, acc = 0.884765625\n",
            "Batch 70: loss = 0.3807951807975769, acc = 0.87890625\n",
            "Batch 71: loss = 0.37929296493530273, acc = 0.8701171875\n",
            "Batch 72: loss = 0.3438653349876404, acc = 0.88671875\n",
            "Batch 73: loss = 0.4300762414932251, acc = 0.8642578125\n",
            "Batch 74: loss = 0.39190828800201416, acc = 0.859375\n",
            "Batch 75: loss = 0.41226726770401, acc = 0.8740234375\n",
            "Batch 76: loss = 0.3504970073699951, acc = 0.875\n",
            "Batch 77: loss = 0.3567233085632324, acc = 0.8857421875\n",
            "Batch 78: loss = 0.38648372888565063, acc = 0.8701171875\n",
            "Batch 79: loss = 0.3862552344799042, acc = 0.875\n",
            "Batch 80: loss = 0.367227703332901, acc = 0.8701171875\n",
            "Batch 81: loss = 0.3815205991268158, acc = 0.8701171875\n",
            "Batch 82: loss = 0.41482096910476685, acc = 0.853515625\n",
            "Batch 83: loss = 0.3759995698928833, acc = 0.8671875\n",
            "Batch 84: loss = 0.3522348701953888, acc = 0.87890625\n",
            "Batch 85: loss = 0.3632664084434509, acc = 0.865234375\n",
            "Batch 86: loss = 0.41590267419815063, acc = 0.849609375\n",
            "Batch 87: loss = 0.36574631929397583, acc = 0.8740234375\n",
            "Batch 88: loss = 0.4059935212135315, acc = 0.861328125\n",
            "Batch 89: loss = 0.3843558430671692, acc = 0.8740234375\n",
            "Batch 90: loss = 0.3971635401248932, acc = 0.8662109375\n",
            "Batch 91: loss = 0.33311304450035095, acc = 0.88671875\n",
            "Batch 92: loss = 0.39623382687568665, acc = 0.865234375\n",
            "Batch 93: loss = 0.35858839750289917, acc = 0.8798828125\n",
            "Batch 94: loss = 0.3760492503643036, acc = 0.8603515625\n",
            "Batch 95: loss = 0.3444609045982361, acc = 0.88671875\n",
            "Batch 96: loss = 0.3770865201950073, acc = 0.869140625\n",
            "Batch 97: loss = 0.4166743755340576, acc = 0.8671875\n",
            "Batch 98: loss = 0.37806451320648193, acc = 0.875\n",
            "Batch 99: loss = 0.3596018850803375, acc = 0.880859375\n",
            "Batch 100: loss = 0.3353302478790283, acc = 0.880859375\n",
            "Batch 101: loss = 0.3950102627277374, acc = 0.8720703125\n",
            "Batch 102: loss = 0.3913843333721161, acc = 0.8720703125\n",
            "Batch 103: loss = 0.39562392234802246, acc = 0.875\n",
            "Batch 104: loss = 0.31581318378448486, acc = 0.892578125\n",
            "Batch 105: loss = 0.34747734665870667, acc = 0.8759765625\n",
            "Batch 106: loss = 0.36978331208229065, acc = 0.8701171875\n",
            "Batch 107: loss = 0.37122902274131775, acc = 0.8818359375\n",
            "Batch 108: loss = 0.36606743931770325, acc = 0.8896484375\n",
            "Batch 109: loss = 0.35908228158950806, acc = 0.8818359375\n",
            "Batch 110: loss = 0.3460215926170349, acc = 0.8857421875\n",
            "Batch 111: loss = 0.39909330010414124, acc = 0.86328125\n",
            "Batch 112: loss = 0.36676856875419617, acc = 0.8779296875\n",
            "Batch 113: loss = 0.37842532992362976, acc = 0.8759765625\n",
            "Batch 114: loss = 0.40656256675720215, acc = 0.8583984375\n",
            "Batch 115: loss = 0.4041440784931183, acc = 0.8681640625\n",
            "Batch 116: loss = 0.35961660742759705, acc = 0.87890625\n",
            "Batch 117: loss = 0.33001917600631714, acc = 0.888671875\n",
            "Batch 118: loss = 0.3472175598144531, acc = 0.884765625\n",
            "Batch 119: loss = 0.3755219280719757, acc = 0.873046875\n",
            "Batch 120: loss = 0.372096449136734, acc = 0.869140625\n",
            "Batch 121: loss = 0.37297677993774414, acc = 0.869140625\n",
            "Batch 122: loss = 0.34375837445259094, acc = 0.884765625\n",
            "Batch 123: loss = 0.3566496670246124, acc = 0.875\n",
            "Batch 124: loss = 0.4221099615097046, acc = 0.859375\n",
            "Batch 125: loss = 0.36988046765327454, acc = 0.8759765625\n",
            "Batch 126: loss = 0.37729987502098083, acc = 0.8828125\n",
            "\n",
            "Epoch 89/100\n",
            "Batch 1: loss = 0.4486919641494751, acc = 0.857421875\n",
            "Batch 2: loss = 0.4416956305503845, acc = 0.8505859375\n",
            "Batch 3: loss = 0.3920040726661682, acc = 0.8740234375\n",
            "Batch 4: loss = 0.3534722328186035, acc = 0.8837890625\n",
            "Batch 5: loss = 0.3644449710845947, acc = 0.8740234375\n",
            "Batch 6: loss = 0.36810094118118286, acc = 0.8837890625\n",
            "Batch 7: loss = 0.34830981492996216, acc = 0.884765625\n",
            "Batch 8: loss = 0.3757002353668213, acc = 0.8740234375\n",
            "Batch 9: loss = 0.4008347988128662, acc = 0.8720703125\n",
            "Batch 10: loss = 0.32840093970298767, acc = 0.896484375\n",
            "Batch 11: loss = 0.37950634956359863, acc = 0.8759765625\n",
            "Batch 12: loss = 0.37408506870269775, acc = 0.8759765625\n",
            "Batch 13: loss = 0.36044448614120483, acc = 0.8759765625\n",
            "Batch 14: loss = 0.3128209710121155, acc = 0.880859375\n",
            "Batch 15: loss = 0.3149172365665436, acc = 0.89453125\n",
            "Batch 16: loss = 0.36216220259666443, acc = 0.8857421875\n",
            "Batch 17: loss = 0.3383778929710388, acc = 0.8798828125\n",
            "Batch 18: loss = 0.4342131018638611, acc = 0.841796875\n",
            "Batch 19: loss = 0.35078009963035583, acc = 0.88671875\n",
            "Batch 20: loss = 0.346137672662735, acc = 0.8779296875\n",
            "Batch 21: loss = 0.4347664713859558, acc = 0.8486328125\n",
            "Batch 22: loss = 0.3633372187614441, acc = 0.8779296875\n",
            "Batch 23: loss = 0.3771701455116272, acc = 0.8720703125\n",
            "Batch 24: loss = 0.36422669887542725, acc = 0.880859375\n",
            "Batch 25: loss = 0.37731263041496277, acc = 0.87890625\n",
            "Batch 26: loss = 0.3288009762763977, acc = 0.8974609375\n",
            "Batch 27: loss = 0.383544921875, acc = 0.8662109375\n",
            "Batch 28: loss = 0.39500853419303894, acc = 0.8701171875\n",
            "Batch 29: loss = 0.3434391915798187, acc = 0.884765625\n",
            "Batch 30: loss = 0.3513529300689697, acc = 0.8896484375\n",
            "Batch 31: loss = 0.37653088569641113, acc = 0.880859375\n",
            "Batch 32: loss = 0.42557764053344727, acc = 0.837890625\n",
            "Batch 33: loss = 0.34197670221328735, acc = 0.884765625\n",
            "Batch 34: loss = 0.35554468631744385, acc = 0.8857421875\n",
            "Batch 35: loss = 0.306471049785614, acc = 0.90234375\n",
            "Batch 36: loss = 0.3388380706310272, acc = 0.88671875\n",
            "Batch 37: loss = 0.3263315260410309, acc = 0.8955078125\n",
            "Batch 38: loss = 0.3606289029121399, acc = 0.8818359375\n",
            "Batch 39: loss = 0.37012046575546265, acc = 0.87109375\n",
            "Batch 40: loss = 0.3367713689804077, acc = 0.8916015625\n",
            "Batch 41: loss = 0.32209259271621704, acc = 0.8984375\n",
            "Batch 42: loss = 0.3160993158817291, acc = 0.89453125\n",
            "Batch 43: loss = 0.400674045085907, acc = 0.859375\n",
            "Batch 44: loss = 0.3422638177871704, acc = 0.884765625\n",
            "Batch 45: loss = 0.34538576006889343, acc = 0.8779296875\n",
            "Batch 46: loss = 0.3450239598751068, acc = 0.87890625\n",
            "Batch 47: loss = 0.3501400649547577, acc = 0.884765625\n",
            "Batch 48: loss = 0.3168315887451172, acc = 0.890625\n",
            "Batch 49: loss = 0.3424840569496155, acc = 0.88671875\n",
            "Batch 50: loss = 0.3320751190185547, acc = 0.8837890625\n",
            "Batch 51: loss = 0.41809162497520447, acc = 0.857421875\n",
            "Batch 52: loss = 0.3507513403892517, acc = 0.869140625\n",
            "Batch 53: loss = 0.35255032777786255, acc = 0.8759765625\n",
            "Batch 54: loss = 0.3088814914226532, acc = 0.900390625\n",
            "Batch 55: loss = 0.3538808226585388, acc = 0.876953125\n",
            "Batch 56: loss = 0.33756542205810547, acc = 0.888671875\n",
            "Batch 57: loss = 0.4104708433151245, acc = 0.85546875\n",
            "Batch 58: loss = 0.36416900157928467, acc = 0.873046875\n",
            "Batch 59: loss = 0.31892064213752747, acc = 0.9033203125\n",
            "Batch 60: loss = 0.36106958985328674, acc = 0.87890625\n",
            "Batch 61: loss = 0.34529584646224976, acc = 0.8916015625\n",
            "Batch 62: loss = 0.4088616371154785, acc = 0.8671875\n",
            "Batch 63: loss = 0.3549754023551941, acc = 0.865234375\n",
            "Batch 64: loss = 0.3422739505767822, acc = 0.8896484375\n",
            "Batch 65: loss = 0.36003878712654114, acc = 0.880859375\n",
            "Batch 66: loss = 0.3969947099685669, acc = 0.8662109375\n",
            "Batch 67: loss = 0.377250999212265, acc = 0.875\n",
            "Batch 68: loss = 0.37115225195884705, acc = 0.861328125\n",
            "Batch 69: loss = 0.35383275151252747, acc = 0.8701171875\n",
            "Batch 70: loss = 0.3715718686580658, acc = 0.87109375\n",
            "Batch 71: loss = 0.3936605453491211, acc = 0.8623046875\n",
            "Batch 72: loss = 0.3171052932739258, acc = 0.8935546875\n",
            "Batch 73: loss = 0.3951137661933899, acc = 0.8662109375\n",
            "Batch 74: loss = 0.4031263291835785, acc = 0.8662109375\n",
            "Batch 75: loss = 0.4755878448486328, acc = 0.8310546875\n",
            "Batch 76: loss = 0.413382887840271, acc = 0.84375\n",
            "Batch 77: loss = 0.36400312185287476, acc = 0.8798828125\n",
            "Batch 78: loss = 0.3806629180908203, acc = 0.8662109375\n",
            "Batch 79: loss = 0.3604470193386078, acc = 0.8798828125\n",
            "Batch 80: loss = 0.32024163007736206, acc = 0.884765625\n",
            "Batch 81: loss = 0.386560320854187, acc = 0.8701171875\n",
            "Batch 82: loss = 0.3443991541862488, acc = 0.896484375\n",
            "Batch 83: loss = 0.34715795516967773, acc = 0.880859375\n",
            "Batch 84: loss = 0.34256333112716675, acc = 0.884765625\n",
            "Batch 85: loss = 0.3964468836784363, acc = 0.857421875\n",
            "Batch 86: loss = 0.3852335214614868, acc = 0.865234375\n",
            "Batch 87: loss = 0.39361655712127686, acc = 0.869140625\n",
            "Batch 88: loss = 0.4468195140361786, acc = 0.8623046875\n",
            "Batch 89: loss = 0.3674381375312805, acc = 0.8896484375\n",
            "Batch 90: loss = 0.3757234215736389, acc = 0.875\n",
            "Batch 91: loss = 0.3868215084075928, acc = 0.873046875\n",
            "Batch 92: loss = 0.4081016182899475, acc = 0.8681640625\n",
            "Batch 93: loss = 0.3337021470069885, acc = 0.888671875\n",
            "Batch 94: loss = 0.35566285252571106, acc = 0.87890625\n",
            "Batch 95: loss = 0.31642651557922363, acc = 0.8994140625\n",
            "Batch 96: loss = 0.40165433287620544, acc = 0.86328125\n",
            "Batch 97: loss = 0.38507816195487976, acc = 0.876953125\n",
            "Batch 98: loss = 0.36097365617752075, acc = 0.8857421875\n",
            "Batch 99: loss = 0.3702744245529175, acc = 0.8828125\n",
            "Batch 100: loss = 0.4057438373565674, acc = 0.8486328125\n",
            "Batch 101: loss = 0.3132360875606537, acc = 0.8857421875\n",
            "Batch 102: loss = 0.38678908348083496, acc = 0.8740234375\n",
            "Batch 103: loss = 0.38799870014190674, acc = 0.8701171875\n",
            "Batch 104: loss = 0.3295857906341553, acc = 0.8837890625\n",
            "Batch 105: loss = 0.32296374440193176, acc = 0.892578125\n",
            "Batch 106: loss = 0.33670610189437866, acc = 0.8916015625\n",
            "Batch 107: loss = 0.3838806450366974, acc = 0.875\n",
            "Batch 108: loss = 0.38492581248283386, acc = 0.861328125\n",
            "Batch 109: loss = 0.4133150577545166, acc = 0.86328125\n",
            "Batch 110: loss = 0.31595245003700256, acc = 0.8974609375\n",
            "Batch 111: loss = 0.35707753896713257, acc = 0.8701171875\n",
            "Batch 112: loss = 0.3738134503364563, acc = 0.8701171875\n",
            "Batch 113: loss = 0.37982600927352905, acc = 0.8701171875\n",
            "Batch 114: loss = 0.35921838879585266, acc = 0.8740234375\n",
            "Batch 115: loss = 0.3862507939338684, acc = 0.8759765625\n",
            "Batch 116: loss = 0.3868083357810974, acc = 0.8681640625\n",
            "Batch 117: loss = 0.3660324513912201, acc = 0.87109375\n",
            "Batch 118: loss = 0.3814716935157776, acc = 0.8759765625\n",
            "Batch 119: loss = 0.3702499270439148, acc = 0.875\n",
            "Batch 120: loss = 0.3699074387550354, acc = 0.875\n",
            "Batch 121: loss = 0.38166290521621704, acc = 0.869140625\n",
            "Batch 122: loss = 0.35514962673187256, acc = 0.8818359375\n",
            "Batch 123: loss = 0.360021710395813, acc = 0.8759765625\n",
            "Batch 124: loss = 0.39259201288223267, acc = 0.8603515625\n",
            "Batch 125: loss = 0.38307592272758484, acc = 0.87109375\n",
            "Batch 126: loss = 0.38422125577926636, acc = 0.873046875\n",
            "\n",
            "Epoch 90/100\n",
            "Batch 1: loss = 0.46527016162872314, acc = 0.8525390625\n",
            "Batch 2: loss = 0.3905118703842163, acc = 0.8642578125\n",
            "Batch 3: loss = 0.3853893280029297, acc = 0.873046875\n",
            "Batch 4: loss = 0.3535115718841553, acc = 0.8828125\n",
            "Batch 5: loss = 0.3551497459411621, acc = 0.8818359375\n",
            "Batch 6: loss = 0.3943794369697571, acc = 0.873046875\n",
            "Batch 7: loss = 0.37549829483032227, acc = 0.865234375\n",
            "Batch 8: loss = 0.3697827160358429, acc = 0.8759765625\n",
            "Batch 9: loss = 0.400165319442749, acc = 0.8623046875\n",
            "Batch 10: loss = 0.3308320641517639, acc = 0.8916015625\n",
            "Batch 11: loss = 0.33027464151382446, acc = 0.89453125\n",
            "Batch 12: loss = 0.39071598649024963, acc = 0.8779296875\n",
            "Batch 13: loss = 0.3671758770942688, acc = 0.87890625\n",
            "Batch 14: loss = 0.3383033275604248, acc = 0.88671875\n",
            "Batch 15: loss = 0.3285548686981201, acc = 0.8916015625\n",
            "Batch 16: loss = 0.40413740277290344, acc = 0.8603515625\n",
            "Batch 17: loss = 0.31224894523620605, acc = 0.904296875\n",
            "Batch 18: loss = 0.3941747546195984, acc = 0.865234375\n",
            "Batch 19: loss = 0.3528406322002411, acc = 0.880859375\n",
            "Batch 20: loss = 0.3537830114364624, acc = 0.875\n",
            "Batch 21: loss = 0.3823978900909424, acc = 0.8671875\n",
            "Batch 22: loss = 0.36447855830192566, acc = 0.8740234375\n",
            "Batch 23: loss = 0.34798941016197205, acc = 0.884765625\n",
            "Batch 24: loss = 0.36572667956352234, acc = 0.880859375\n",
            "Batch 25: loss = 0.3694940209388733, acc = 0.8828125\n",
            "Batch 26: loss = 0.3350605368614197, acc = 0.8896484375\n",
            "Batch 27: loss = 0.378062903881073, acc = 0.8759765625\n",
            "Batch 28: loss = 0.3663887083530426, acc = 0.8740234375\n",
            "Batch 29: loss = 0.35843655467033386, acc = 0.8671875\n",
            "Batch 30: loss = 0.3207864761352539, acc = 0.88671875\n",
            "Batch 31: loss = 0.36998963356018066, acc = 0.880859375\n",
            "Batch 32: loss = 0.38631561398506165, acc = 0.8681640625\n",
            "Batch 33: loss = 0.3499411940574646, acc = 0.8916015625\n",
            "Batch 34: loss = 0.3345280885696411, acc = 0.900390625\n",
            "Batch 35: loss = 0.3146056532859802, acc = 0.888671875\n",
            "Batch 36: loss = 0.33002325892448425, acc = 0.888671875\n",
            "Batch 37: loss = 0.31873011589050293, acc = 0.8935546875\n",
            "Batch 38: loss = 0.3356103301048279, acc = 0.8916015625\n",
            "Batch 39: loss = 0.349734902381897, acc = 0.8779296875\n",
            "Batch 40: loss = 0.3693864345550537, acc = 0.873046875\n",
            "Batch 41: loss = 0.343230277299881, acc = 0.8837890625\n",
            "Batch 42: loss = 0.3605368435382843, acc = 0.8720703125\n",
            "Batch 43: loss = 0.41523998975753784, acc = 0.8603515625\n",
            "Batch 44: loss = 0.36260396242141724, acc = 0.8740234375\n",
            "Batch 45: loss = 0.3442636728286743, acc = 0.8837890625\n",
            "Batch 46: loss = 0.3398197889328003, acc = 0.880859375\n",
            "Batch 47: loss = 0.28816211223602295, acc = 0.904296875\n",
            "Batch 48: loss = 0.3303329348564148, acc = 0.8837890625\n",
            "Batch 49: loss = 0.3295724391937256, acc = 0.90234375\n",
            "Batch 50: loss = 0.3448196351528168, acc = 0.890625\n",
            "Batch 51: loss = 0.39169830083847046, acc = 0.8642578125\n",
            "Batch 52: loss = 0.36695244908332825, acc = 0.8740234375\n",
            "Batch 53: loss = 0.36771854758262634, acc = 0.8798828125\n",
            "Batch 54: loss = 0.2855210304260254, acc = 0.9091796875\n",
            "Batch 55: loss = 0.3248715400695801, acc = 0.8955078125\n",
            "Batch 56: loss = 0.34368255734443665, acc = 0.87890625\n",
            "Batch 57: loss = 0.4093330502510071, acc = 0.8486328125\n",
            "Batch 58: loss = 0.39671796560287476, acc = 0.8623046875\n",
            "Batch 59: loss = 0.29773202538490295, acc = 0.90234375\n",
            "Batch 60: loss = 0.35175821185112, acc = 0.88671875\n",
            "Batch 61: loss = 0.32602977752685547, acc = 0.890625\n",
            "Batch 62: loss = 0.3938019275665283, acc = 0.857421875\n",
            "Batch 63: loss = 0.3280571401119232, acc = 0.8857421875\n",
            "Batch 64: loss = 0.3476300835609436, acc = 0.87890625\n",
            "Batch 65: loss = 0.36896437406539917, acc = 0.8759765625\n",
            "Batch 66: loss = 0.3227854371070862, acc = 0.888671875\n",
            "Batch 67: loss = 0.3677965998649597, acc = 0.875\n",
            "Batch 68: loss = 0.3742937445640564, acc = 0.8779296875\n",
            "Batch 69: loss = 0.3563462197780609, acc = 0.8837890625\n",
            "Batch 70: loss = 0.38561582565307617, acc = 0.861328125\n",
            "Batch 71: loss = 0.3400766849517822, acc = 0.87890625\n",
            "Batch 72: loss = 0.35926324129104614, acc = 0.87890625\n",
            "Batch 73: loss = 0.4238497316837311, acc = 0.857421875\n",
            "Batch 74: loss = 0.3706124722957611, acc = 0.8798828125\n",
            "Batch 75: loss = 0.4276696741580963, acc = 0.8583984375\n",
            "Batch 76: loss = 0.3684532046318054, acc = 0.8857421875\n",
            "Batch 77: loss = 0.34476906061172485, acc = 0.884765625\n",
            "Batch 78: loss = 0.356990247964859, acc = 0.8837890625\n",
            "Batch 79: loss = 0.34905892610549927, acc = 0.884765625\n",
            "Batch 80: loss = 0.33117926120758057, acc = 0.876953125\n",
            "Batch 81: loss = 0.4056949019432068, acc = 0.8642578125\n",
            "Batch 82: loss = 0.3478970527648926, acc = 0.8857421875\n",
            "Batch 83: loss = 0.3610912561416626, acc = 0.8818359375\n",
            "Batch 84: loss = 0.3480115532875061, acc = 0.884765625\n",
            "Batch 85: loss = 0.35655516386032104, acc = 0.8681640625\n",
            "Batch 86: loss = 0.3826478123664856, acc = 0.8701171875\n",
            "Batch 87: loss = 0.3731801211833954, acc = 0.8896484375\n",
            "Batch 88: loss = 0.3893226683139801, acc = 0.861328125\n",
            "Batch 89: loss = 0.33913883566856384, acc = 0.8974609375\n",
            "Batch 90: loss = 0.3314892053604126, acc = 0.8876953125\n",
            "Batch 91: loss = 0.38457757234573364, acc = 0.8681640625\n",
            "Batch 92: loss = 0.36633870005607605, acc = 0.8720703125\n",
            "Batch 93: loss = 0.3293287456035614, acc = 0.890625\n",
            "Batch 94: loss = 0.37244629859924316, acc = 0.8759765625\n",
            "Batch 95: loss = 0.33453118801116943, acc = 0.8896484375\n",
            "Batch 96: loss = 0.3909645080566406, acc = 0.8701171875\n",
            "Batch 97: loss = 0.39941155910491943, acc = 0.8701171875\n",
            "Batch 98: loss = 0.33912578225135803, acc = 0.8916015625\n",
            "Batch 99: loss = 0.3516201972961426, acc = 0.8828125\n",
            "Batch 100: loss = 0.36757615208625793, acc = 0.8671875\n",
            "Batch 101: loss = 0.34953317046165466, acc = 0.876953125\n",
            "Batch 102: loss = 0.41852617263793945, acc = 0.8564453125\n",
            "Batch 103: loss = 0.3722918927669525, acc = 0.876953125\n",
            "Batch 104: loss = 0.3084518611431122, acc = 0.8876953125\n",
            "Batch 105: loss = 0.3115772008895874, acc = 0.90234375\n",
            "Batch 106: loss = 0.37790584564208984, acc = 0.869140625\n",
            "Batch 107: loss = 0.3438313603401184, acc = 0.87890625\n",
            "Batch 108: loss = 0.3528444766998291, acc = 0.890625\n",
            "Batch 109: loss = 0.3333531618118286, acc = 0.9072265625\n",
            "Batch 110: loss = 0.3318622410297394, acc = 0.884765625\n",
            "Batch 111: loss = 0.3950274884700775, acc = 0.8740234375\n",
            "Batch 112: loss = 0.33909887075424194, acc = 0.8837890625\n",
            "Batch 113: loss = 0.3313302993774414, acc = 0.8779296875\n",
            "Batch 114: loss = 0.41011956334114075, acc = 0.8662109375\n",
            "Batch 115: loss = 0.3661210536956787, acc = 0.8916015625\n",
            "Batch 116: loss = 0.37103456258773804, acc = 0.8779296875\n",
            "Batch 117: loss = 0.34617578983306885, acc = 0.87890625\n",
            "Batch 118: loss = 0.3390726149082184, acc = 0.8876953125\n",
            "Batch 119: loss = 0.34340938925743103, acc = 0.8857421875\n",
            "Batch 120: loss = 0.34170645475387573, acc = 0.8837890625\n",
            "Batch 121: loss = 0.33451053500175476, acc = 0.8837890625\n",
            "Batch 122: loss = 0.35917526483535767, acc = 0.869140625\n",
            "Batch 123: loss = 0.3448123633861542, acc = 0.8759765625\n",
            "Batch 124: loss = 0.37766003608703613, acc = 0.8642578125\n",
            "Batch 125: loss = 0.36942458152770996, acc = 0.8671875\n",
            "Batch 126: loss = 0.3619532883167267, acc = 0.880859375\n",
            "Saved checkpoint to gru_weights.90.h5\n",
            "\n",
            "Epoch 91/100\n",
            "Batch 1: loss = 0.47608768939971924, acc = 0.8544921875\n",
            "Batch 2: loss = 0.3816080391407013, acc = 0.85546875\n",
            "Batch 3: loss = 0.3350202739238739, acc = 0.8828125\n",
            "Batch 4: loss = 0.365304172039032, acc = 0.876953125\n",
            "Batch 5: loss = 0.3604121208190918, acc = 0.8740234375\n",
            "Batch 6: loss = 0.3743894398212433, acc = 0.8798828125\n",
            "Batch 7: loss = 0.3414534628391266, acc = 0.88671875\n",
            "Batch 8: loss = 0.3866322934627533, acc = 0.873046875\n",
            "Batch 9: loss = 0.37736910581588745, acc = 0.8642578125\n",
            "Batch 10: loss = 0.32825854420661926, acc = 0.8828125\n",
            "Batch 11: loss = 0.3523043394088745, acc = 0.87890625\n",
            "Batch 12: loss = 0.34894227981567383, acc = 0.8857421875\n",
            "Batch 13: loss = 0.3480565845966339, acc = 0.890625\n",
            "Batch 14: loss = 0.35007038712501526, acc = 0.8798828125\n",
            "Batch 15: loss = 0.31790292263031006, acc = 0.888671875\n",
            "Batch 16: loss = 0.3604893982410431, acc = 0.875\n",
            "Batch 17: loss = 0.34203600883483887, acc = 0.8935546875\n",
            "Batch 18: loss = 0.3889504373073578, acc = 0.8623046875\n",
            "Batch 19: loss = 0.4039592146873474, acc = 0.86328125\n",
            "Batch 20: loss = 0.3611997067928314, acc = 0.880859375\n",
            "Batch 21: loss = 0.4048851430416107, acc = 0.8662109375\n",
            "Batch 22: loss = 0.35038286447525024, acc = 0.88671875\n",
            "Batch 23: loss = 0.367353618144989, acc = 0.8759765625\n",
            "Batch 24: loss = 0.3792896568775177, acc = 0.8642578125\n",
            "Batch 25: loss = 0.37599608302116394, acc = 0.8779296875\n",
            "Batch 26: loss = 0.3229472041130066, acc = 0.900390625\n",
            "Batch 27: loss = 0.3807874619960785, acc = 0.876953125\n",
            "Batch 28: loss = 0.3866756856441498, acc = 0.8662109375\n",
            "Batch 29: loss = 0.3464694023132324, acc = 0.8916015625\n",
            "Batch 30: loss = 0.3426079750061035, acc = 0.8779296875\n",
            "Batch 31: loss = 0.38197118043899536, acc = 0.865234375\n",
            "Batch 32: loss = 0.371844619512558, acc = 0.873046875\n",
            "Batch 33: loss = 0.3605450391769409, acc = 0.8740234375\n",
            "Batch 34: loss = 0.3637765049934387, acc = 0.8759765625\n",
            "Batch 35: loss = 0.34046846628189087, acc = 0.8896484375\n",
            "Batch 36: loss = 0.3423641622066498, acc = 0.88671875\n",
            "Batch 37: loss = 0.36610615253448486, acc = 0.88671875\n",
            "Batch 38: loss = 0.32185834646224976, acc = 0.890625\n",
            "Batch 39: loss = 0.3208235502243042, acc = 0.912109375\n",
            "Batch 40: loss = 0.3632855713367462, acc = 0.8857421875\n",
            "Batch 41: loss = 0.3475509583950043, acc = 0.888671875\n",
            "Batch 42: loss = 0.33409932255744934, acc = 0.8857421875\n",
            "Batch 43: loss = 0.3999977707862854, acc = 0.8544921875\n",
            "Batch 44: loss = 0.3782031536102295, acc = 0.8759765625\n",
            "Batch 45: loss = 0.3205846846103668, acc = 0.8876953125\n",
            "Batch 46: loss = 0.30525192618370056, acc = 0.892578125\n",
            "Batch 47: loss = 0.3472689092159271, acc = 0.8857421875\n",
            "Batch 48: loss = 0.3539433479309082, acc = 0.8759765625\n",
            "Batch 49: loss = 0.33370861411094666, acc = 0.8916015625\n",
            "Batch 50: loss = 0.3442486822605133, acc = 0.8916015625\n",
            "Batch 51: loss = 0.3626696467399597, acc = 0.876953125\n",
            "Batch 52: loss = 0.3462388813495636, acc = 0.8896484375\n",
            "Batch 53: loss = 0.34993264079093933, acc = 0.888671875\n",
            "Batch 54: loss = 0.27193108201026917, acc = 0.9130859375\n",
            "Batch 55: loss = 0.32132720947265625, acc = 0.89453125\n",
            "Batch 56: loss = 0.38300251960754395, acc = 0.873046875\n",
            "Batch 57: loss = 0.39407452940940857, acc = 0.8642578125\n",
            "Batch 58: loss = 0.39322298765182495, acc = 0.8623046875\n",
            "Batch 59: loss = 0.30829519033432007, acc = 0.890625\n",
            "Batch 60: loss = 0.32756370306015015, acc = 0.8955078125\n",
            "Batch 61: loss = 0.32637956738471985, acc = 0.88671875\n",
            "Batch 62: loss = 0.42558562755584717, acc = 0.85546875\n",
            "Batch 63: loss = 0.3558005094528198, acc = 0.8779296875\n",
            "Batch 64: loss = 0.3122062087059021, acc = 0.8935546875\n",
            "Batch 65: loss = 0.35097378492355347, acc = 0.880859375\n",
            "Batch 66: loss = 0.38860243558883667, acc = 0.8662109375\n",
            "Batch 67: loss = 0.32670679688453674, acc = 0.8896484375\n",
            "Batch 68: loss = 0.39831382036209106, acc = 0.8623046875\n",
            "Batch 69: loss = 0.3735111951828003, acc = 0.869140625\n",
            "Batch 70: loss = 0.4110386073589325, acc = 0.861328125\n",
            "Batch 71: loss = 0.3694249987602234, acc = 0.8662109375\n",
            "Batch 72: loss = 0.3665550649166107, acc = 0.8779296875\n",
            "Batch 73: loss = 0.3880261480808258, acc = 0.865234375\n",
            "Batch 74: loss = 0.38898488879203796, acc = 0.873046875\n",
            "Batch 75: loss = 0.4309084117412567, acc = 0.8642578125\n",
            "Batch 76: loss = 0.4064539670944214, acc = 0.85546875\n",
            "Batch 77: loss = 0.37845176458358765, acc = 0.8701171875\n",
            "Batch 78: loss = 0.3442457914352417, acc = 0.8828125\n",
            "Batch 79: loss = 0.3550908863544464, acc = 0.8857421875\n",
            "Batch 80: loss = 0.3526700437068939, acc = 0.87890625\n",
            "Batch 81: loss = 0.3462814688682556, acc = 0.880859375\n",
            "Batch 82: loss = 0.36883267760276794, acc = 0.875\n",
            "Batch 83: loss = 0.34901684522628784, acc = 0.880859375\n",
            "Batch 84: loss = 0.3736536204814911, acc = 0.876953125\n",
            "Batch 85: loss = 0.4269862174987793, acc = 0.8544921875\n",
            "Batch 86: loss = 0.392630934715271, acc = 0.861328125\n",
            "Batch 87: loss = 0.36320534348487854, acc = 0.8818359375\n",
            "Batch 88: loss = 0.39816731214523315, acc = 0.869140625\n",
            "Batch 89: loss = 0.36070582270622253, acc = 0.880859375\n",
            "Batch 90: loss = 0.3742942810058594, acc = 0.8720703125\n",
            "Batch 91: loss = 0.40236395597457886, acc = 0.8779296875\n",
            "Batch 92: loss = 0.3909776210784912, acc = 0.8671875\n",
            "Batch 93: loss = 0.3763209283351898, acc = 0.8779296875\n",
            "Batch 94: loss = 0.40305668115615845, acc = 0.8642578125\n",
            "Batch 95: loss = 0.3546542823314667, acc = 0.87890625\n",
            "Batch 96: loss = 0.36145734786987305, acc = 0.875\n",
            "Batch 97: loss = 0.3433666229248047, acc = 0.8916015625\n",
            "Batch 98: loss = 0.3495112955570221, acc = 0.8759765625\n",
            "Batch 99: loss = 0.4015309512615204, acc = 0.8583984375\n",
            "Batch 100: loss = 0.39261361956596375, acc = 0.8662109375\n",
            "Batch 101: loss = 0.3312752842903137, acc = 0.8818359375\n",
            "Batch 102: loss = 0.3746907413005829, acc = 0.87109375\n",
            "Batch 103: loss = 0.40723150968551636, acc = 0.8720703125\n",
            "Batch 104: loss = 0.334294855594635, acc = 0.8779296875\n",
            "Batch 105: loss = 0.348871648311615, acc = 0.8837890625\n",
            "Batch 106: loss = 0.3627009987831116, acc = 0.8740234375\n",
            "Batch 107: loss = 0.36058759689331055, acc = 0.8798828125\n",
            "Batch 108: loss = 0.3409798741340637, acc = 0.8828125\n",
            "Batch 109: loss = 0.3645944893360138, acc = 0.8701171875\n",
            "Batch 110: loss = 0.32535040378570557, acc = 0.8916015625\n",
            "Batch 111: loss = 0.3599957823753357, acc = 0.875\n",
            "Batch 112: loss = 0.3204440176486969, acc = 0.888671875\n",
            "Batch 113: loss = 0.36505189538002014, acc = 0.8818359375\n",
            "Batch 114: loss = 0.3968312740325928, acc = 0.873046875\n",
            "Batch 115: loss = 0.38008952140808105, acc = 0.8828125\n",
            "Batch 116: loss = 0.3597967028617859, acc = 0.8779296875\n",
            "Batch 117: loss = 0.39372751116752625, acc = 0.876953125\n",
            "Batch 118: loss = 0.36691927909851074, acc = 0.884765625\n",
            "Batch 119: loss = 0.3468512296676636, acc = 0.8828125\n",
            "Batch 120: loss = 0.3345642685890198, acc = 0.8837890625\n",
            "Batch 121: loss = 0.36041146516799927, acc = 0.873046875\n",
            "Batch 122: loss = 0.3649322986602783, acc = 0.8740234375\n",
            "Batch 123: loss = 0.3629108667373657, acc = 0.8828125\n",
            "Batch 124: loss = 0.3720778822898865, acc = 0.876953125\n",
            "Batch 125: loss = 0.3665945827960968, acc = 0.873046875\n",
            "Batch 126: loss = 0.3933010697364807, acc = 0.8701171875\n",
            "\n",
            "Epoch 92/100\n",
            "Batch 1: loss = 0.4583032727241516, acc = 0.8515625\n",
            "Batch 2: loss = 0.3880249261856079, acc = 0.87890625\n",
            "Batch 3: loss = 0.3843645453453064, acc = 0.8701171875\n",
            "Batch 4: loss = 0.3663971424102783, acc = 0.8740234375\n",
            "Batch 5: loss = 0.3676219582557678, acc = 0.8720703125\n",
            "Batch 6: loss = 0.39960184693336487, acc = 0.8681640625\n",
            "Batch 7: loss = 0.33302757143974304, acc = 0.8798828125\n",
            "Batch 8: loss = 0.39653170108795166, acc = 0.865234375\n",
            "Batch 9: loss = 0.3503238558769226, acc = 0.8837890625\n",
            "Batch 10: loss = 0.32765623927116394, acc = 0.888671875\n",
            "Batch 11: loss = 0.3661597967147827, acc = 0.880859375\n",
            "Batch 12: loss = 0.3886169195175171, acc = 0.8701171875\n",
            "Batch 13: loss = 0.3901013433933258, acc = 0.8779296875\n",
            "Batch 14: loss = 0.30424004793167114, acc = 0.892578125\n",
            "Batch 15: loss = 0.3409992456436157, acc = 0.8916015625\n",
            "Batch 16: loss = 0.3813806176185608, acc = 0.8681640625\n",
            "Batch 17: loss = 0.3696943521499634, acc = 0.8779296875\n",
            "Batch 18: loss = 0.3747241497039795, acc = 0.880859375\n",
            "Batch 19: loss = 0.364419162273407, acc = 0.880859375\n",
            "Batch 20: loss = 0.35394465923309326, acc = 0.880859375\n",
            "Batch 21: loss = 0.40480682253837585, acc = 0.8681640625\n",
            "Batch 22: loss = 0.3585132360458374, acc = 0.8837890625\n",
            "Batch 23: loss = 0.3622511327266693, acc = 0.880859375\n",
            "Batch 24: loss = 0.3283654749393463, acc = 0.880859375\n",
            "Batch 25: loss = 0.3493019938468933, acc = 0.8837890625\n",
            "Batch 26: loss = 0.34060078859329224, acc = 0.892578125\n",
            "Batch 27: loss = 0.3678686022758484, acc = 0.873046875\n",
            "Batch 28: loss = 0.40573278069496155, acc = 0.865234375\n",
            "Batch 29: loss = 0.39922571182250977, acc = 0.86328125\n",
            "Batch 30: loss = 0.3499234616756439, acc = 0.8837890625\n",
            "Batch 31: loss = 0.37912458181381226, acc = 0.8759765625\n",
            "Batch 32: loss = 0.38791170716285706, acc = 0.865234375\n",
            "Batch 33: loss = 0.3630481958389282, acc = 0.87890625\n",
            "Batch 34: loss = 0.32751399278640747, acc = 0.88671875\n",
            "Batch 35: loss = 0.3316305875778198, acc = 0.890625\n",
            "Batch 36: loss = 0.3765495717525482, acc = 0.875\n",
            "Batch 37: loss = 0.32720518112182617, acc = 0.892578125\n",
            "Batch 38: loss = 0.3557063937187195, acc = 0.876953125\n",
            "Batch 39: loss = 0.3395143747329712, acc = 0.8896484375\n",
            "Batch 40: loss = 0.394107848405838, acc = 0.8701171875\n",
            "Batch 41: loss = 0.32684412598609924, acc = 0.8955078125\n",
            "Batch 42: loss = 0.34026196599006653, acc = 0.884765625\n",
            "Batch 43: loss = 0.3640904426574707, acc = 0.875\n",
            "Batch 44: loss = 0.35754063725471497, acc = 0.8896484375\n",
            "Batch 45: loss = 0.3408326506614685, acc = 0.888671875\n",
            "Batch 46: loss = 0.3497145175933838, acc = 0.890625\n",
            "Batch 47: loss = 0.3567464351654053, acc = 0.8896484375\n",
            "Batch 48: loss = 0.31280606985092163, acc = 0.8935546875\n",
            "Batch 49: loss = 0.3557741641998291, acc = 0.88671875\n",
            "Batch 50: loss = 0.345032274723053, acc = 0.88671875\n",
            "Batch 51: loss = 0.39273935556411743, acc = 0.8671875\n",
            "Batch 52: loss = 0.3949827551841736, acc = 0.8662109375\n",
            "Batch 53: loss = 0.3991111218929291, acc = 0.8759765625\n",
            "Batch 54: loss = 0.2914881706237793, acc = 0.9052734375\n",
            "Batch 55: loss = 0.3362275958061218, acc = 0.88671875\n",
            "Batch 56: loss = 0.37244269251823425, acc = 0.8701171875\n",
            "Batch 57: loss = 0.4128612279891968, acc = 0.8642578125\n",
            "Batch 58: loss = 0.3949093222618103, acc = 0.8671875\n",
            "Batch 59: loss = 0.35902947187423706, acc = 0.8896484375\n",
            "Batch 60: loss = 0.3721720576286316, acc = 0.8671875\n",
            "Batch 61: loss = 0.3772115707397461, acc = 0.876953125\n",
            "Batch 62: loss = 0.4333282709121704, acc = 0.849609375\n",
            "Batch 63: loss = 0.36958226561546326, acc = 0.8818359375\n",
            "Batch 64: loss = 0.32681408524513245, acc = 0.89453125\n",
            "Batch 65: loss = 0.3289892077445984, acc = 0.8837890625\n",
            "Batch 66: loss = 0.3862718939781189, acc = 0.865234375\n",
            "Batch 67: loss = 0.3639794886112213, acc = 0.8818359375\n",
            "Batch 68: loss = 0.36009955406188965, acc = 0.87890625\n",
            "Batch 69: loss = 0.3061215877532959, acc = 0.892578125\n",
            "Batch 70: loss = 0.3418644070625305, acc = 0.890625\n",
            "Batch 71: loss = 0.37139612436294556, acc = 0.876953125\n",
            "Batch 72: loss = 0.3340531587600708, acc = 0.8896484375\n",
            "Batch 73: loss = 0.38339871168136597, acc = 0.876953125\n",
            "Batch 74: loss = 0.3968629837036133, acc = 0.865234375\n",
            "Batch 75: loss = 0.4279615879058838, acc = 0.8525390625\n",
            "Batch 76: loss = 0.39292341470718384, acc = 0.86328125\n",
            "Batch 77: loss = 0.3510565459728241, acc = 0.87890625\n",
            "Batch 78: loss = 0.37926316261291504, acc = 0.8583984375\n",
            "Batch 79: loss = 0.3649880886077881, acc = 0.884765625\n",
            "Batch 80: loss = 0.3368242084980011, acc = 0.880859375\n",
            "Batch 81: loss = 0.3764296770095825, acc = 0.875\n",
            "Batch 82: loss = 0.3611094355583191, acc = 0.8720703125\n",
            "Batch 83: loss = 0.3441895842552185, acc = 0.8837890625\n",
            "Batch 84: loss = 0.36650556325912476, acc = 0.8759765625\n",
            "Batch 85: loss = 0.45155346393585205, acc = 0.8466796875\n",
            "Batch 86: loss = 0.36684542894363403, acc = 0.876953125\n",
            "Batch 87: loss = 0.3380435109138489, acc = 0.8916015625\n",
            "Batch 88: loss = 0.39720064401626587, acc = 0.861328125\n",
            "Batch 89: loss = 0.38110238313674927, acc = 0.873046875\n",
            "Batch 90: loss = 0.38229820132255554, acc = 0.87890625\n",
            "Batch 91: loss = 0.4010968506336212, acc = 0.8642578125\n",
            "Batch 92: loss = 0.3854428827762604, acc = 0.865234375\n",
            "Batch 93: loss = 0.35488244891166687, acc = 0.88671875\n",
            "Batch 94: loss = 0.3636837899684906, acc = 0.87890625\n",
            "Batch 95: loss = 0.3480626940727234, acc = 0.890625\n",
            "Batch 96: loss = 0.4121858775615692, acc = 0.859375\n",
            "Batch 97: loss = 0.3581019341945648, acc = 0.89453125\n",
            "Batch 98: loss = 0.36415669322013855, acc = 0.8720703125\n",
            "Batch 99: loss = 0.33911386132240295, acc = 0.892578125\n",
            "Batch 100: loss = 0.33456459641456604, acc = 0.873046875\n",
            "Batch 101: loss = 0.34309613704681396, acc = 0.880859375\n",
            "Batch 102: loss = 0.42611798644065857, acc = 0.853515625\n",
            "Batch 103: loss = 0.3753455877304077, acc = 0.8662109375\n",
            "Batch 104: loss = 0.3053857386112213, acc = 0.89453125\n",
            "Batch 105: loss = 0.30783143639564514, acc = 0.884765625\n",
            "Batch 106: loss = 0.3529476523399353, acc = 0.8740234375\n",
            "Batch 107: loss = 0.38761717081069946, acc = 0.8671875\n",
            "Batch 108: loss = 0.3581002354621887, acc = 0.8701171875\n",
            "Batch 109: loss = 0.3655342161655426, acc = 0.8701171875\n",
            "Batch 110: loss = 0.29549646377563477, acc = 0.9052734375\n",
            "Batch 111: loss = 0.39774709939956665, acc = 0.8603515625\n",
            "Batch 112: loss = 0.33676275610923767, acc = 0.888671875\n",
            "Batch 113: loss = 0.36526983976364136, acc = 0.8681640625\n",
            "Batch 114: loss = 0.36004436016082764, acc = 0.8740234375\n",
            "Batch 115: loss = 0.38813942670822144, acc = 0.8720703125\n",
            "Batch 116: loss = 0.41043686866760254, acc = 0.859375\n",
            "Batch 117: loss = 0.3295329213142395, acc = 0.8818359375\n",
            "Batch 118: loss = 0.3400530219078064, acc = 0.8896484375\n",
            "Batch 119: loss = 0.31176990270614624, acc = 0.8994140625\n",
            "Batch 120: loss = 0.3043439984321594, acc = 0.8876953125\n",
            "Batch 121: loss = 0.36689868569374084, acc = 0.8623046875\n",
            "Batch 122: loss = 0.3630533814430237, acc = 0.873046875\n",
            "Batch 123: loss = 0.364683598279953, acc = 0.884765625\n",
            "Batch 124: loss = 0.36891505122184753, acc = 0.8564453125\n",
            "Batch 125: loss = 0.3742113709449768, acc = 0.865234375\n",
            "Batch 126: loss = 0.4113752841949463, acc = 0.869140625\n",
            "\n",
            "Epoch 93/100\n",
            "Batch 1: loss = 0.4889405071735382, acc = 0.8564453125\n",
            "Batch 2: loss = 0.41334572434425354, acc = 0.859375\n",
            "Batch 3: loss = 0.364797979593277, acc = 0.875\n",
            "Batch 4: loss = 0.3228369951248169, acc = 0.8935546875\n",
            "Batch 5: loss = 0.3835563063621521, acc = 0.87890625\n",
            "Batch 6: loss = 0.4157213866710663, acc = 0.8681640625\n",
            "Batch 7: loss = 0.3176383972167969, acc = 0.8798828125\n",
            "Batch 8: loss = 0.40255624055862427, acc = 0.8681640625\n",
            "Batch 9: loss = 0.32685837149620056, acc = 0.8896484375\n",
            "Batch 10: loss = 0.3062093257904053, acc = 0.8974609375\n",
            "Batch 11: loss = 0.36136484146118164, acc = 0.875\n",
            "Batch 12: loss = 0.3550074100494385, acc = 0.8828125\n",
            "Batch 13: loss = 0.37069883942604065, acc = 0.8779296875\n",
            "Batch 14: loss = 0.2958535850048065, acc = 0.9130859375\n",
            "Batch 15: loss = 0.3118598461151123, acc = 0.890625\n",
            "Batch 16: loss = 0.3756296634674072, acc = 0.875\n",
            "Batch 17: loss = 0.3543381690979004, acc = 0.8740234375\n",
            "Batch 18: loss = 0.36749744415283203, acc = 0.876953125\n",
            "Batch 19: loss = 0.4022468328475952, acc = 0.8583984375\n",
            "Batch 20: loss = 0.38173553347587585, acc = 0.8701171875\n",
            "Batch 21: loss = 0.3827974200248718, acc = 0.8720703125\n",
            "Batch 22: loss = 0.35043901205062866, acc = 0.8837890625\n",
            "Batch 23: loss = 0.3199564516544342, acc = 0.8896484375\n",
            "Batch 24: loss = 0.32025304436683655, acc = 0.884765625\n",
            "Batch 25: loss = 0.40438681840896606, acc = 0.875\n",
            "Batch 26: loss = 0.3470343351364136, acc = 0.890625\n",
            "Batch 27: loss = 0.35576942563056946, acc = 0.884765625\n",
            "Batch 28: loss = 0.366718053817749, acc = 0.876953125\n",
            "Batch 29: loss = 0.3927268385887146, acc = 0.8603515625\n",
            "Batch 30: loss = 0.33717262744903564, acc = 0.8896484375\n",
            "Batch 31: loss = 0.3499598503112793, acc = 0.8857421875\n",
            "Batch 32: loss = 0.4176356792449951, acc = 0.857421875\n",
            "Batch 33: loss = 0.31818121671676636, acc = 0.8984375\n",
            "Batch 34: loss = 0.3247101902961731, acc = 0.8994140625\n",
            "Batch 35: loss = 0.3660683333873749, acc = 0.8759765625\n",
            "Batch 36: loss = 0.3585447371006012, acc = 0.873046875\n",
            "Batch 37: loss = 0.3466373383998871, acc = 0.876953125\n",
            "Batch 38: loss = 0.31909802556037903, acc = 0.8974609375\n",
            "Batch 39: loss = 0.3375001549720764, acc = 0.8876953125\n",
            "Batch 40: loss = 0.34109628200531006, acc = 0.884765625\n",
            "Batch 41: loss = 0.31082314252853394, acc = 0.90234375\n",
            "Batch 42: loss = 0.33040136098861694, acc = 0.884765625\n",
            "Batch 43: loss = 0.37092968821525574, acc = 0.8740234375\n",
            "Batch 44: loss = 0.32875582575798035, acc = 0.8857421875\n",
            "Batch 45: loss = 0.3161469101905823, acc = 0.8857421875\n",
            "Batch 46: loss = 0.39845913648605347, acc = 0.865234375\n",
            "Batch 47: loss = 0.35810279846191406, acc = 0.8779296875\n",
            "Batch 48: loss = 0.3314495384693146, acc = 0.8857421875\n",
            "Batch 49: loss = 0.3153218924999237, acc = 0.8994140625\n",
            "Batch 50: loss = 0.3289908766746521, acc = 0.8818359375\n",
            "Batch 51: loss = 0.35903504490852356, acc = 0.8857421875\n",
            "Batch 52: loss = 0.34930410981178284, acc = 0.8779296875\n",
            "Batch 53: loss = 0.33655065298080444, acc = 0.884765625\n",
            "Batch 54: loss = 0.2934151291847229, acc = 0.896484375\n",
            "Batch 55: loss = 0.3529621660709381, acc = 0.88671875\n",
            "Batch 56: loss = 0.3229329586029053, acc = 0.8896484375\n",
            "Batch 57: loss = 0.41904497146606445, acc = 0.861328125\n",
            "Batch 58: loss = 0.3836981952190399, acc = 0.87109375\n",
            "Batch 59: loss = 0.3162194490432739, acc = 0.880859375\n",
            "Batch 60: loss = 0.36389631032943726, acc = 0.8720703125\n",
            "Batch 61: loss = 0.30701887607574463, acc = 0.8994140625\n",
            "Batch 62: loss = 0.3892901837825775, acc = 0.8740234375\n",
            "Batch 63: loss = 0.3456224501132965, acc = 0.8798828125\n",
            "Batch 64: loss = 0.33114683628082275, acc = 0.8955078125\n",
            "Batch 65: loss = 0.3866432309150696, acc = 0.869140625\n",
            "Batch 66: loss = 0.34481027722358704, acc = 0.8837890625\n",
            "Batch 67: loss = 0.33586859703063965, acc = 0.8857421875\n",
            "Batch 68: loss = 0.42093172669410706, acc = 0.8525390625\n",
            "Batch 69: loss = 0.3048614263534546, acc = 0.9033203125\n",
            "Batch 70: loss = 0.3706592917442322, acc = 0.8740234375\n",
            "Batch 71: loss = 0.36029252409935, acc = 0.875\n",
            "Batch 72: loss = 0.3539702296257019, acc = 0.8935546875\n",
            "Batch 73: loss = 0.3746861517429352, acc = 0.8798828125\n",
            "Batch 74: loss = 0.3596474230289459, acc = 0.880859375\n",
            "Batch 75: loss = 0.3903624713420868, acc = 0.8828125\n",
            "Batch 76: loss = 0.4187738299369812, acc = 0.8671875\n",
            "Batch 77: loss = 0.3564454913139343, acc = 0.8759765625\n",
            "Batch 78: loss = 0.359018474817276, acc = 0.8740234375\n",
            "Batch 79: loss = 0.3392115831375122, acc = 0.890625\n",
            "Batch 80: loss = 0.3396577537059784, acc = 0.890625\n",
            "Batch 81: loss = 0.35032182931900024, acc = 0.875\n",
            "Batch 82: loss = 0.3866094648838043, acc = 0.8759765625\n",
            "Batch 83: loss = 0.3665720224380493, acc = 0.8798828125\n",
            "Batch 84: loss = 0.3524010181427002, acc = 0.8779296875\n",
            "Batch 85: loss = 0.3917877972126007, acc = 0.8662109375\n",
            "Batch 86: loss = 0.39172080159187317, acc = 0.8603515625\n",
            "Batch 87: loss = 0.37899714708328247, acc = 0.8779296875\n",
            "Batch 88: loss = 0.383659690618515, acc = 0.8662109375\n",
            "Batch 89: loss = 0.4081821143627167, acc = 0.8583984375\n",
            "Batch 90: loss = 0.34380972385406494, acc = 0.884765625\n",
            "Batch 91: loss = 0.4099280834197998, acc = 0.8603515625\n",
            "Batch 92: loss = 0.3847322165966034, acc = 0.86328125\n",
            "Batch 93: loss = 0.3913443684577942, acc = 0.87109375\n",
            "Batch 94: loss = 0.3389296233654022, acc = 0.8837890625\n",
            "Batch 95: loss = 0.340182900428772, acc = 0.8955078125\n",
            "Batch 96: loss = 0.3556876480579376, acc = 0.8828125\n",
            "Batch 97: loss = 0.3653276860713959, acc = 0.880859375\n",
            "Batch 98: loss = 0.3447282314300537, acc = 0.8837890625\n",
            "Batch 99: loss = 0.35590434074401855, acc = 0.875\n",
            "Batch 100: loss = 0.37692752480506897, acc = 0.865234375\n",
            "Batch 101: loss = 0.3445686995983124, acc = 0.880859375\n",
            "Batch 102: loss = 0.3856339454650879, acc = 0.8701171875\n",
            "Batch 103: loss = 0.37386879324913025, acc = 0.8720703125\n",
            "Batch 104: loss = 0.32961928844451904, acc = 0.8935546875\n",
            "Batch 105: loss = 0.319003701210022, acc = 0.8994140625\n",
            "Batch 106: loss = 0.3586316704750061, acc = 0.8671875\n",
            "Batch 107: loss = 0.3768138885498047, acc = 0.875\n",
            "Batch 108: loss = 0.3442392349243164, acc = 0.8896484375\n",
            "Batch 109: loss = 0.3297902047634125, acc = 0.88671875\n",
            "Batch 110: loss = 0.30196890234947205, acc = 0.9033203125\n",
            "Batch 111: loss = 0.379411518573761, acc = 0.8681640625\n",
            "Batch 112: loss = 0.3234270513057709, acc = 0.8955078125\n",
            "Batch 113: loss = 0.3941119909286499, acc = 0.8720703125\n",
            "Batch 114: loss = 0.39495888352394104, acc = 0.8681640625\n",
            "Batch 115: loss = 0.3984333574771881, acc = 0.8603515625\n",
            "Batch 116: loss = 0.3849099278450012, acc = 0.8740234375\n",
            "Batch 117: loss = 0.34282591938972473, acc = 0.892578125\n",
            "Batch 118: loss = 0.386482298374176, acc = 0.8623046875\n",
            "Batch 119: loss = 0.3516620695590973, acc = 0.8759765625\n",
            "Batch 120: loss = 0.3298766613006592, acc = 0.8828125\n",
            "Batch 121: loss = 0.3919697701931, acc = 0.8701171875\n",
            "Batch 122: loss = 0.36042994260787964, acc = 0.8818359375\n",
            "Batch 123: loss = 0.3686129152774811, acc = 0.880859375\n",
            "Batch 124: loss = 0.3994511365890503, acc = 0.861328125\n",
            "Batch 125: loss = 0.369856595993042, acc = 0.869140625\n",
            "Batch 126: loss = 0.4148377776145935, acc = 0.8583984375\n",
            "\n",
            "Epoch 94/100\n",
            "Batch 1: loss = 0.4544362425804138, acc = 0.8564453125\n",
            "Batch 2: loss = 0.44458115100860596, acc = 0.8466796875\n",
            "Batch 3: loss = 0.37094151973724365, acc = 0.873046875\n",
            "Batch 4: loss = 0.336670845746994, acc = 0.884765625\n",
            "Batch 5: loss = 0.33323439955711365, acc = 0.8955078125\n",
            "Batch 6: loss = 0.38805025815963745, acc = 0.876953125\n",
            "Batch 7: loss = 0.34086230397224426, acc = 0.8916015625\n",
            "Batch 8: loss = 0.3939478397369385, acc = 0.861328125\n",
            "Batch 9: loss = 0.35723066329956055, acc = 0.88671875\n",
            "Batch 10: loss = 0.2830635607242584, acc = 0.892578125\n",
            "Batch 11: loss = 0.3909588158130646, acc = 0.86328125\n",
            "Batch 12: loss = 0.3852558135986328, acc = 0.873046875\n",
            "Batch 13: loss = 0.3488600254058838, acc = 0.8857421875\n",
            "Batch 14: loss = 0.3129293620586395, acc = 0.8935546875\n",
            "Batch 15: loss = 0.32102683186531067, acc = 0.8935546875\n",
            "Batch 16: loss = 0.37273845076560974, acc = 0.873046875\n",
            "Batch 17: loss = 0.34707656502723694, acc = 0.884765625\n",
            "Batch 18: loss = 0.3864596486091614, acc = 0.8681640625\n",
            "Batch 19: loss = 0.3783857524394989, acc = 0.873046875\n",
            "Batch 20: loss = 0.37282615900039673, acc = 0.873046875\n",
            "Batch 21: loss = 0.3798043131828308, acc = 0.8671875\n",
            "Batch 22: loss = 0.35973283648490906, acc = 0.8828125\n",
            "Batch 23: loss = 0.3573705554008484, acc = 0.8779296875\n",
            "Batch 24: loss = 0.3583446741104126, acc = 0.8720703125\n",
            "Batch 25: loss = 0.333660751581192, acc = 0.8916015625\n",
            "Batch 26: loss = 0.3490595519542694, acc = 0.8798828125\n",
            "Batch 27: loss = 0.3959427773952484, acc = 0.865234375\n",
            "Batch 28: loss = 0.3413624167442322, acc = 0.8798828125\n",
            "Batch 29: loss = 0.35811951756477356, acc = 0.8740234375\n",
            "Batch 30: loss = 0.36430829763412476, acc = 0.875\n",
            "Batch 31: loss = 0.3724900186061859, acc = 0.8662109375\n",
            "Batch 32: loss = 0.3549475371837616, acc = 0.888671875\n",
            "Batch 33: loss = 0.3501313030719757, acc = 0.8876953125\n",
            "Batch 34: loss = 0.33889561891555786, acc = 0.8779296875\n",
            "Batch 35: loss = 0.328580766916275, acc = 0.9013671875\n",
            "Batch 36: loss = 0.33328109979629517, acc = 0.8828125\n",
            "Batch 37: loss = 0.32106316089630127, acc = 0.89453125\n",
            "Batch 38: loss = 0.32184845209121704, acc = 0.8896484375\n",
            "Batch 39: loss = 0.34688109159469604, acc = 0.880859375\n",
            "Batch 40: loss = 0.3732335865497589, acc = 0.8642578125\n",
            "Batch 41: loss = 0.3045860826969147, acc = 0.8994140625\n",
            "Batch 42: loss = 0.3546369671821594, acc = 0.8798828125\n",
            "Batch 43: loss = 0.3722662925720215, acc = 0.8720703125\n",
            "Batch 44: loss = 0.37460562586784363, acc = 0.8828125\n",
            "Batch 45: loss = 0.34508541226387024, acc = 0.8779296875\n",
            "Batch 46: loss = 0.3399679362773895, acc = 0.884765625\n",
            "Batch 47: loss = 0.32696226239204407, acc = 0.892578125\n",
            "Batch 48: loss = 0.31194645166397095, acc = 0.8916015625\n",
            "Batch 49: loss = 0.33309635519981384, acc = 0.8974609375\n",
            "Batch 50: loss = 0.3576638996601105, acc = 0.8798828125\n",
            "Batch 51: loss = 0.3798840641975403, acc = 0.880859375\n",
            "Batch 52: loss = 0.33996984362602234, acc = 0.89453125\n",
            "Batch 53: loss = 0.33638861775398254, acc = 0.888671875\n",
            "Batch 54: loss = 0.26858699321746826, acc = 0.9091796875\n",
            "Batch 55: loss = 0.3260363042354584, acc = 0.8896484375\n",
            "Batch 56: loss = 0.3582485616207123, acc = 0.876953125\n",
            "Batch 57: loss = 0.37500491738319397, acc = 0.8779296875\n",
            "Batch 58: loss = 0.3431597352027893, acc = 0.8994140625\n",
            "Batch 59: loss = 0.3380546271800995, acc = 0.884765625\n",
            "Batch 60: loss = 0.3651955723762512, acc = 0.8701171875\n",
            "Batch 61: loss = 0.34080269932746887, acc = 0.8857421875\n",
            "Batch 62: loss = 0.4304097592830658, acc = 0.8515625\n",
            "Batch 63: loss = 0.32076311111450195, acc = 0.8994140625\n",
            "Batch 64: loss = 0.32461488246917725, acc = 0.8876953125\n",
            "Batch 65: loss = 0.3632858991622925, acc = 0.87890625\n",
            "Batch 66: loss = 0.3758176863193512, acc = 0.8818359375\n",
            "Batch 67: loss = 0.2935278117656708, acc = 0.8876953125\n",
            "Batch 68: loss = 0.35876286029815674, acc = 0.8779296875\n",
            "Batch 69: loss = 0.31942611932754517, acc = 0.8984375\n",
            "Batch 70: loss = 0.392534464597702, acc = 0.8583984375\n",
            "Batch 71: loss = 0.36897921562194824, acc = 0.87109375\n",
            "Batch 72: loss = 0.36588534712791443, acc = 0.876953125\n",
            "Batch 73: loss = 0.41577816009521484, acc = 0.8603515625\n",
            "Batch 74: loss = 0.3518446981906891, acc = 0.875\n",
            "Batch 75: loss = 0.45463722944259644, acc = 0.837890625\n",
            "Batch 76: loss = 0.4085436165332794, acc = 0.8564453125\n",
            "Batch 77: loss = 0.3554089665412903, acc = 0.8828125\n",
            "Batch 78: loss = 0.39171862602233887, acc = 0.857421875\n",
            "Batch 79: loss = 0.3391154706478119, acc = 0.8896484375\n",
            "Batch 80: loss = 0.35963699221611023, acc = 0.884765625\n",
            "Batch 81: loss = 0.38134610652923584, acc = 0.876953125\n",
            "Batch 82: loss = 0.36705759167671204, acc = 0.8828125\n",
            "Batch 83: loss = 0.3534308671951294, acc = 0.88671875\n",
            "Batch 84: loss = 0.37142980098724365, acc = 0.8681640625\n",
            "Batch 85: loss = 0.37324896454811096, acc = 0.8720703125\n",
            "Batch 86: loss = 0.39670729637145996, acc = 0.8623046875\n",
            "Batch 87: loss = 0.36804670095443726, acc = 0.8837890625\n",
            "Batch 88: loss = 0.3784753978252411, acc = 0.8759765625\n",
            "Batch 89: loss = 0.3280544579029083, acc = 0.888671875\n",
            "Batch 90: loss = 0.35730308294296265, acc = 0.8798828125\n",
            "Batch 91: loss = 0.37242844700813293, acc = 0.869140625\n",
            "Batch 92: loss = 0.3961660861968994, acc = 0.8662109375\n",
            "Batch 93: loss = 0.37431272864341736, acc = 0.8779296875\n",
            "Batch 94: loss = 0.36757874488830566, acc = 0.8837890625\n",
            "Batch 95: loss = 0.3080300986766815, acc = 0.8916015625\n",
            "Batch 96: loss = 0.3865649402141571, acc = 0.865234375\n",
            "Batch 97: loss = 0.36882275342941284, acc = 0.8896484375\n",
            "Batch 98: loss = 0.3577229678630829, acc = 0.876953125\n",
            "Batch 99: loss = 0.34501323103904724, acc = 0.8896484375\n",
            "Batch 100: loss = 0.3771272599697113, acc = 0.8671875\n",
            "Batch 101: loss = 0.3503531515598297, acc = 0.87890625\n",
            "Batch 102: loss = 0.42546364665031433, acc = 0.8525390625\n",
            "Batch 103: loss = 0.37954336404800415, acc = 0.8720703125\n",
            "Batch 104: loss = 0.31512942910194397, acc = 0.8876953125\n",
            "Batch 105: loss = 0.32747259736061096, acc = 0.892578125\n",
            "Batch 106: loss = 0.35139888525009155, acc = 0.8857421875\n",
            "Batch 107: loss = 0.36821219325065613, acc = 0.8740234375\n",
            "Batch 108: loss = 0.32938122749328613, acc = 0.88671875\n",
            "Batch 109: loss = 0.37869685888290405, acc = 0.876953125\n",
            "Batch 110: loss = 0.3376620411872864, acc = 0.8955078125\n",
            "Batch 111: loss = 0.343217134475708, acc = 0.875\n",
            "Batch 112: loss = 0.34279128909111023, acc = 0.876953125\n",
            "Batch 113: loss = 0.38825222849845886, acc = 0.8701171875\n",
            "Batch 114: loss = 0.37639182806015015, acc = 0.873046875\n",
            "Batch 115: loss = 0.37159910798072815, acc = 0.8857421875\n",
            "Batch 116: loss = 0.38926589488983154, acc = 0.87890625\n",
            "Batch 117: loss = 0.3489460051059723, acc = 0.8818359375\n",
            "Batch 118: loss = 0.34339800477027893, acc = 0.884765625\n",
            "Batch 119: loss = 0.3696097433567047, acc = 0.875\n",
            "Batch 120: loss = 0.2901860177516937, acc = 0.904296875\n",
            "Batch 121: loss = 0.3580234944820404, acc = 0.8759765625\n",
            "Batch 122: loss = 0.3370758295059204, acc = 0.888671875\n",
            "Batch 123: loss = 0.39000988006591797, acc = 0.87109375\n",
            "Batch 124: loss = 0.3957997262477875, acc = 0.8671875\n",
            "Batch 125: loss = 0.368472695350647, acc = 0.8740234375\n",
            "Batch 126: loss = 0.3776702284812927, acc = 0.8740234375\n",
            "\n",
            "Epoch 95/100\n",
            "Batch 1: loss = 0.45946893095970154, acc = 0.861328125\n",
            "Batch 2: loss = 0.40064701437950134, acc = 0.8603515625\n",
            "Batch 3: loss = 0.38779935240745544, acc = 0.87109375\n",
            "Batch 4: loss = 0.31170469522476196, acc = 0.8916015625\n",
            "Batch 5: loss = 0.37599897384643555, acc = 0.8720703125\n",
            "Batch 6: loss = 0.4008667469024658, acc = 0.869140625\n",
            "Batch 7: loss = 0.36048653721809387, acc = 0.869140625\n",
            "Batch 8: loss = 0.39905568957328796, acc = 0.8681640625\n",
            "Batch 9: loss = 0.33945006132125854, acc = 0.8837890625\n",
            "Batch 10: loss = 0.3244344890117645, acc = 0.8876953125\n",
            "Batch 11: loss = 0.34318286180496216, acc = 0.8876953125\n",
            "Batch 12: loss = 0.3380200266838074, acc = 0.8876953125\n",
            "Batch 13: loss = 0.34830862283706665, acc = 0.8828125\n",
            "Batch 14: loss = 0.33641183376312256, acc = 0.8916015625\n",
            "Batch 15: loss = 0.3347894847393036, acc = 0.8896484375\n",
            "Batch 16: loss = 0.3612699508666992, acc = 0.87109375\n",
            "Batch 17: loss = 0.34859591722488403, acc = 0.8779296875\n",
            "Batch 18: loss = 0.3635537624359131, acc = 0.8798828125\n",
            "Batch 19: loss = 0.3558037579059601, acc = 0.8779296875\n",
            "Batch 20: loss = 0.33846667408943176, acc = 0.8828125\n",
            "Batch 21: loss = 0.3522305190563202, acc = 0.888671875\n",
            "Batch 22: loss = 0.3707951307296753, acc = 0.8740234375\n",
            "Batch 23: loss = 0.35251039266586304, acc = 0.8740234375\n",
            "Batch 24: loss = 0.36598360538482666, acc = 0.865234375\n",
            "Batch 25: loss = 0.3653774857521057, acc = 0.8779296875\n",
            "Batch 26: loss = 0.3289099335670471, acc = 0.884765625\n",
            "Batch 27: loss = 0.39186611771583557, acc = 0.8701171875\n",
            "Batch 28: loss = 0.372994601726532, acc = 0.873046875\n",
            "Batch 29: loss = 0.3211061358451843, acc = 0.900390625\n",
            "Batch 30: loss = 0.33648213744163513, acc = 0.8896484375\n",
            "Batch 31: loss = 0.3748359978199005, acc = 0.8828125\n",
            "Batch 32: loss = 0.39644700288772583, acc = 0.86328125\n",
            "Batch 33: loss = 0.33609476685523987, acc = 0.8896484375\n",
            "Batch 34: loss = 0.3187359571456909, acc = 0.8916015625\n",
            "Batch 35: loss = 0.31050989031791687, acc = 0.8984375\n",
            "Batch 36: loss = 0.32639601826667786, acc = 0.900390625\n",
            "Batch 37: loss = 0.35340461134910583, acc = 0.8828125\n",
            "Batch 38: loss = 0.3724610209465027, acc = 0.8740234375\n",
            "Batch 39: loss = 0.3244384527206421, acc = 0.8828125\n",
            "Batch 40: loss = 0.33926448225975037, acc = 0.87890625\n",
            "Batch 41: loss = 0.3190150856971741, acc = 0.888671875\n",
            "Batch 42: loss = 0.34645435214042664, acc = 0.8837890625\n",
            "Batch 43: loss = 0.3618633449077606, acc = 0.8671875\n",
            "Batch 44: loss = 0.3569965362548828, acc = 0.87890625\n",
            "Batch 45: loss = 0.33109748363494873, acc = 0.8896484375\n",
            "Batch 46: loss = 0.30043497681617737, acc = 0.8974609375\n",
            "Batch 47: loss = 0.3388133943080902, acc = 0.8798828125\n",
            "Batch 48: loss = 0.31982266902923584, acc = 0.8955078125\n",
            "Batch 49: loss = 0.3403821885585785, acc = 0.8828125\n",
            "Batch 50: loss = 0.33294418454170227, acc = 0.89453125\n",
            "Batch 51: loss = 0.35212841629981995, acc = 0.8876953125\n",
            "Batch 52: loss = 0.3617090880870819, acc = 0.875\n",
            "Batch 53: loss = 0.39568933844566345, acc = 0.87109375\n",
            "Batch 54: loss = 0.3136448860168457, acc = 0.8984375\n",
            "Batch 55: loss = 0.29873913526535034, acc = 0.896484375\n",
            "Batch 56: loss = 0.35202556848526, acc = 0.8798828125\n",
            "Batch 57: loss = 0.38433361053466797, acc = 0.875\n",
            "Batch 58: loss = 0.37113088369369507, acc = 0.875\n",
            "Batch 59: loss = 0.3474009335041046, acc = 0.8837890625\n",
            "Batch 60: loss = 0.37468165159225464, acc = 0.8740234375\n",
            "Batch 61: loss = 0.3476550579071045, acc = 0.884765625\n",
            "Batch 62: loss = 0.39238572120666504, acc = 0.86328125\n",
            "Batch 63: loss = 0.3624177575111389, acc = 0.8740234375\n",
            "Batch 64: loss = 0.3380732238292694, acc = 0.8896484375\n",
            "Batch 65: loss = 0.34760230779647827, acc = 0.880859375\n",
            "Batch 66: loss = 0.3550683259963989, acc = 0.87890625\n",
            "Batch 67: loss = 0.33658567070961, acc = 0.880859375\n",
            "Batch 68: loss = 0.3543010652065277, acc = 0.8818359375\n",
            "Batch 69: loss = 0.34705448150634766, acc = 0.8857421875\n",
            "Batch 70: loss = 0.4036638140678406, acc = 0.861328125\n",
            "Batch 71: loss = 0.39487430453300476, acc = 0.86328125\n",
            "Batch 72: loss = 0.35803794860839844, acc = 0.8828125\n",
            "Batch 73: loss = 0.3722400665283203, acc = 0.876953125\n",
            "Batch 74: loss = 0.3864583969116211, acc = 0.869140625\n",
            "Batch 75: loss = 0.41056379675865173, acc = 0.86328125\n",
            "Batch 76: loss = 0.3855764865875244, acc = 0.8818359375\n",
            "Batch 77: loss = 0.34668803215026855, acc = 0.8720703125\n",
            "Batch 78: loss = 0.40500250458717346, acc = 0.8564453125\n",
            "Batch 79: loss = 0.3186717629432678, acc = 0.888671875\n",
            "Batch 80: loss = 0.30829817056655884, acc = 0.88671875\n",
            "Batch 81: loss = 0.31995832920074463, acc = 0.888671875\n",
            "Batch 82: loss = 0.3291100859642029, acc = 0.884765625\n",
            "Batch 83: loss = 0.3374292552471161, acc = 0.8896484375\n",
            "Batch 84: loss = 0.36503902077674866, acc = 0.8623046875\n",
            "Batch 85: loss = 0.4110150635242462, acc = 0.853515625\n",
            "Batch 86: loss = 0.385530561208725, acc = 0.85546875\n",
            "Batch 87: loss = 0.3598858118057251, acc = 0.884765625\n",
            "Batch 88: loss = 0.41957950592041016, acc = 0.8544921875\n",
            "Batch 89: loss = 0.34065377712249756, acc = 0.8974609375\n",
            "Batch 90: loss = 0.34098970890045166, acc = 0.8857421875\n",
            "Batch 91: loss = 0.37988775968551636, acc = 0.8701171875\n",
            "Batch 92: loss = 0.40870046615600586, acc = 0.86328125\n",
            "Batch 93: loss = 0.33968299627304077, acc = 0.8876953125\n",
            "Batch 94: loss = 0.3624979555606842, acc = 0.8828125\n",
            "Batch 95: loss = 0.30441680550575256, acc = 0.8896484375\n",
            "Batch 96: loss = 0.37171974778175354, acc = 0.873046875\n",
            "Batch 97: loss = 0.3512425124645233, acc = 0.89453125\n",
            "Batch 98: loss = 0.3939976096153259, acc = 0.8662109375\n",
            "Batch 99: loss = 0.3623444437980652, acc = 0.8740234375\n",
            "Batch 100: loss = 0.3635120391845703, acc = 0.8759765625\n",
            "Batch 101: loss = 0.3605673313140869, acc = 0.873046875\n",
            "Batch 102: loss = 0.3944375514984131, acc = 0.865234375\n",
            "Batch 103: loss = 0.3880637288093567, acc = 0.865234375\n",
            "Batch 104: loss = 0.30328723788261414, acc = 0.896484375\n",
            "Batch 105: loss = 0.30483686923980713, acc = 0.89453125\n",
            "Batch 106: loss = 0.3419411778450012, acc = 0.890625\n",
            "Batch 107: loss = 0.36881402134895325, acc = 0.8740234375\n",
            "Batch 108: loss = 0.3329309821128845, acc = 0.892578125\n",
            "Batch 109: loss = 0.37793970108032227, acc = 0.8701171875\n",
            "Batch 110: loss = 0.3340229094028473, acc = 0.8896484375\n",
            "Batch 111: loss = 0.38896259665489197, acc = 0.8623046875\n",
            "Batch 112: loss = 0.3306185007095337, acc = 0.8876953125\n",
            "Batch 113: loss = 0.3544468283653259, acc = 0.8876953125\n",
            "Batch 114: loss = 0.3312479555606842, acc = 0.8828125\n",
            "Batch 115: loss = 0.38720065355300903, acc = 0.8876953125\n",
            "Batch 116: loss = 0.3494397699832916, acc = 0.8837890625\n",
            "Batch 117: loss = 0.35033684968948364, acc = 0.89453125\n",
            "Batch 118: loss = 0.3553674817085266, acc = 0.8818359375\n",
            "Batch 119: loss = 0.35507825016975403, acc = 0.88671875\n",
            "Batch 120: loss = 0.33061739802360535, acc = 0.8837890625\n",
            "Batch 121: loss = 0.38027727603912354, acc = 0.8671875\n",
            "Batch 122: loss = 0.3322525918483734, acc = 0.880859375\n",
            "Batch 123: loss = 0.39918139576911926, acc = 0.8681640625\n",
            "Batch 124: loss = 0.3821333944797516, acc = 0.8779296875\n",
            "Batch 125: loss = 0.3739837110042572, acc = 0.8720703125\n",
            "Batch 126: loss = 0.3610113859176636, acc = 0.888671875\n",
            "\n",
            "Epoch 96/100\n",
            "Batch 1: loss = 0.4343387484550476, acc = 0.8642578125\n",
            "Batch 2: loss = 0.40247637033462524, acc = 0.875\n",
            "Batch 3: loss = 0.38084861636161804, acc = 0.880859375\n",
            "Batch 4: loss = 0.31667935848236084, acc = 0.8935546875\n",
            "Batch 5: loss = 0.3721751272678375, acc = 0.8828125\n",
            "Batch 6: loss = 0.39292287826538086, acc = 0.87109375\n",
            "Batch 7: loss = 0.34019696712493896, acc = 0.8818359375\n",
            "Batch 8: loss = 0.3453989028930664, acc = 0.8994140625\n",
            "Batch 9: loss = 0.34103623032569885, acc = 0.8955078125\n",
            "Batch 10: loss = 0.3202880918979645, acc = 0.8896484375\n",
            "Batch 11: loss = 0.35214313864707947, acc = 0.8818359375\n",
            "Batch 12: loss = 0.349946528673172, acc = 0.8818359375\n",
            "Batch 13: loss = 0.399983674287796, acc = 0.865234375\n",
            "Batch 14: loss = 0.36149126291275024, acc = 0.87890625\n",
            "Batch 15: loss = 0.34588679671287537, acc = 0.8759765625\n",
            "Batch 16: loss = 0.360520601272583, acc = 0.875\n",
            "Batch 17: loss = 0.3729454278945923, acc = 0.8857421875\n",
            "Batch 18: loss = 0.3953782021999359, acc = 0.875\n",
            "Batch 19: loss = 0.34707537293434143, acc = 0.8759765625\n",
            "Batch 20: loss = 0.3317070007324219, acc = 0.8837890625\n",
            "Batch 21: loss = 0.38203662633895874, acc = 0.86328125\n",
            "Batch 22: loss = 0.3476247489452362, acc = 0.8701171875\n",
            "Batch 23: loss = 0.35807982087135315, acc = 0.8837890625\n",
            "Batch 24: loss = 0.33476394414901733, acc = 0.8759765625\n",
            "Batch 25: loss = 0.35591086745262146, acc = 0.884765625\n",
            "Batch 26: loss = 0.31849899888038635, acc = 0.8935546875\n",
            "Batch 27: loss = 0.34482136368751526, acc = 0.8837890625\n",
            "Batch 28: loss = 0.37663984298706055, acc = 0.865234375\n",
            "Batch 29: loss = 0.39111483097076416, acc = 0.853515625\n",
            "Batch 30: loss = 0.34268325567245483, acc = 0.890625\n",
            "Batch 31: loss = 0.35304543375968933, acc = 0.884765625\n",
            "Batch 32: loss = 0.3309129476547241, acc = 0.8876953125\n",
            "Batch 33: loss = 0.3715411126613617, acc = 0.880859375\n",
            "Batch 34: loss = 0.35396409034729004, acc = 0.8837890625\n",
            "Batch 35: loss = 0.32642531394958496, acc = 0.890625\n",
            "Batch 36: loss = 0.34313204884529114, acc = 0.8955078125\n",
            "Batch 37: loss = 0.3172743320465088, acc = 0.896484375\n",
            "Batch 38: loss = 0.3452305197715759, acc = 0.888671875\n",
            "Batch 39: loss = 0.34593403339385986, acc = 0.87890625\n",
            "Batch 40: loss = 0.3589949607849121, acc = 0.884765625\n",
            "Batch 41: loss = 0.31525200605392456, acc = 0.892578125\n",
            "Batch 42: loss = 0.29821354150772095, acc = 0.90234375\n",
            "Batch 43: loss = 0.352222204208374, acc = 0.884765625\n",
            "Batch 44: loss = 0.34098634123802185, acc = 0.8896484375\n",
            "Batch 45: loss = 0.341607004404068, acc = 0.884765625\n",
            "Batch 46: loss = 0.31306421756744385, acc = 0.8935546875\n",
            "Batch 47: loss = 0.31556642055511475, acc = 0.8935546875\n",
            "Batch 48: loss = 0.28504082560539246, acc = 0.9052734375\n",
            "Batch 49: loss = 0.34179434180259705, acc = 0.884765625\n",
            "Batch 50: loss = 0.36067694425582886, acc = 0.87890625\n",
            "Batch 51: loss = 0.36228668689727783, acc = 0.86328125\n",
            "Batch 52: loss = 0.3512916564941406, acc = 0.8740234375\n",
            "Batch 53: loss = 0.33704981207847595, acc = 0.88671875\n",
            "Batch 54: loss = 0.27935969829559326, acc = 0.9033203125\n",
            "Batch 55: loss = 0.31324127316474915, acc = 0.896484375\n",
            "Batch 56: loss = 0.3315994143486023, acc = 0.8818359375\n",
            "Batch 57: loss = 0.36614829301834106, acc = 0.8759765625\n",
            "Batch 58: loss = 0.37253904342651367, acc = 0.8623046875\n",
            "Batch 59: loss = 0.3278920352458954, acc = 0.8857421875\n",
            "Batch 60: loss = 0.3378215432167053, acc = 0.884765625\n",
            "Batch 61: loss = 0.33056649565696716, acc = 0.9013671875\n",
            "Batch 62: loss = 0.397365540266037, acc = 0.87109375\n",
            "Batch 63: loss = 0.32532477378845215, acc = 0.8974609375\n",
            "Batch 64: loss = 0.316973477602005, acc = 0.8955078125\n",
            "Batch 65: loss = 0.39278173446655273, acc = 0.8642578125\n",
            "Batch 66: loss = 0.3914564251899719, acc = 0.8720703125\n",
            "Batch 67: loss = 0.32747429609298706, acc = 0.8828125\n",
            "Batch 68: loss = 0.3756062686443329, acc = 0.876953125\n",
            "Batch 69: loss = 0.3149029314517975, acc = 0.8955078125\n",
            "Batch 70: loss = 0.35349443554878235, acc = 0.884765625\n",
            "Batch 71: loss = 0.3520878553390503, acc = 0.876953125\n",
            "Batch 72: loss = 0.34094446897506714, acc = 0.8837890625\n",
            "Batch 73: loss = 0.38302260637283325, acc = 0.875\n",
            "Batch 74: loss = 0.3950607180595398, acc = 0.875\n",
            "Batch 75: loss = 0.43896591663360596, acc = 0.853515625\n",
            "Batch 76: loss = 0.34107309579849243, acc = 0.88671875\n",
            "Batch 77: loss = 0.3277910053730011, acc = 0.8779296875\n",
            "Batch 78: loss = 0.36736947298049927, acc = 0.8828125\n",
            "Batch 79: loss = 0.34339475631713867, acc = 0.888671875\n",
            "Batch 80: loss = 0.3465976119041443, acc = 0.8798828125\n",
            "Batch 81: loss = 0.3794219493865967, acc = 0.8681640625\n",
            "Batch 82: loss = 0.3656077980995178, acc = 0.87890625\n",
            "Batch 83: loss = 0.3299124836921692, acc = 0.8974609375\n",
            "Batch 84: loss = 0.3399551212787628, acc = 0.876953125\n",
            "Batch 85: loss = 0.3850061893463135, acc = 0.865234375\n",
            "Batch 86: loss = 0.3774580955505371, acc = 0.8798828125\n",
            "Batch 87: loss = 0.3732774257659912, acc = 0.87890625\n",
            "Batch 88: loss = 0.4205290377140045, acc = 0.8505859375\n",
            "Batch 89: loss = 0.36029964685440063, acc = 0.8701171875\n",
            "Batch 90: loss = 0.4167337417602539, acc = 0.869140625\n",
            "Batch 91: loss = 0.3865824043750763, acc = 0.8681640625\n",
            "Batch 92: loss = 0.39632171392440796, acc = 0.8642578125\n",
            "Batch 93: loss = 0.3481787145137787, acc = 0.892578125\n",
            "Batch 94: loss = 0.3433457314968109, acc = 0.880859375\n",
            "Batch 95: loss = 0.3384907841682434, acc = 0.888671875\n",
            "Batch 96: loss = 0.39942634105682373, acc = 0.8642578125\n",
            "Batch 97: loss = 0.37662819027900696, acc = 0.8818359375\n",
            "Batch 98: loss = 0.3774231970310211, acc = 0.8779296875\n",
            "Batch 99: loss = 0.3809993267059326, acc = 0.8828125\n",
            "Batch 100: loss = 0.34267574548721313, acc = 0.876953125\n",
            "Batch 101: loss = 0.3042106330394745, acc = 0.8955078125\n",
            "Batch 102: loss = 0.41896504163742065, acc = 0.861328125\n",
            "Batch 103: loss = 0.3806632161140442, acc = 0.8759765625\n",
            "Batch 104: loss = 0.308023065328598, acc = 0.8974609375\n",
            "Batch 105: loss = 0.29759180545806885, acc = 0.8955078125\n",
            "Batch 106: loss = 0.3153951168060303, acc = 0.8798828125\n",
            "Batch 107: loss = 0.3569149076938629, acc = 0.873046875\n",
            "Batch 108: loss = 0.3052883744239807, acc = 0.890625\n",
            "Batch 109: loss = 0.34728720784187317, acc = 0.8828125\n",
            "Batch 110: loss = 0.345761239528656, acc = 0.8740234375\n",
            "Batch 111: loss = 0.36709529161453247, acc = 0.8779296875\n",
            "Batch 112: loss = 0.3075973391532898, acc = 0.8984375\n",
            "Batch 113: loss = 0.35328778624534607, acc = 0.873046875\n",
            "Batch 114: loss = 0.40198105573654175, acc = 0.859375\n",
            "Batch 115: loss = 0.35810714960098267, acc = 0.8681640625\n",
            "Batch 116: loss = 0.3969336748123169, acc = 0.8623046875\n",
            "Batch 117: loss = 0.37164661288261414, acc = 0.8857421875\n",
            "Batch 118: loss = 0.36552193760871887, acc = 0.8759765625\n",
            "Batch 119: loss = 0.38877275586128235, acc = 0.8662109375\n",
            "Batch 120: loss = 0.33079251646995544, acc = 0.8935546875\n",
            "Batch 121: loss = 0.312792032957077, acc = 0.900390625\n",
            "Batch 122: loss = 0.3299710154533386, acc = 0.8916015625\n",
            "Batch 123: loss = 0.3517724275588989, acc = 0.8798828125\n",
            "Batch 124: loss = 0.339799165725708, acc = 0.8837890625\n",
            "Batch 125: loss = 0.3974076211452484, acc = 0.8740234375\n",
            "Batch 126: loss = 0.3469632863998413, acc = 0.880859375\n",
            "\n",
            "Epoch 97/100\n",
            "Batch 1: loss = 0.48566681146621704, acc = 0.861328125\n",
            "Batch 2: loss = 0.43083083629608154, acc = 0.8671875\n",
            "Batch 3: loss = 0.3816487193107605, acc = 0.865234375\n",
            "Batch 4: loss = 0.29339364171028137, acc = 0.9033203125\n",
            "Batch 5: loss = 0.36308300495147705, acc = 0.876953125\n",
            "Batch 6: loss = 0.4177004098892212, acc = 0.8681640625\n",
            "Batch 7: loss = 0.34124940633773804, acc = 0.888671875\n",
            "Batch 8: loss = 0.37196114659309387, acc = 0.8837890625\n",
            "Batch 9: loss = 0.3672001361846924, acc = 0.8798828125\n",
            "Batch 10: loss = 0.32850414514541626, acc = 0.87890625\n",
            "Batch 11: loss = 0.34030407667160034, acc = 0.8828125\n",
            "Batch 12: loss = 0.3709949553012848, acc = 0.873046875\n",
            "Batch 13: loss = 0.33684849739074707, acc = 0.880859375\n",
            "Batch 14: loss = 0.29885679483413696, acc = 0.9013671875\n",
            "Batch 15: loss = 0.33633238077163696, acc = 0.8916015625\n",
            "Batch 16: loss = 0.33926522731781006, acc = 0.869140625\n",
            "Batch 17: loss = 0.34032583236694336, acc = 0.8818359375\n",
            "Batch 18: loss = 0.36789581179618835, acc = 0.8720703125\n",
            "Batch 19: loss = 0.3649037778377533, acc = 0.873046875\n",
            "Batch 20: loss = 0.35624903440475464, acc = 0.8828125\n",
            "Batch 21: loss = 0.375864714384079, acc = 0.869140625\n",
            "Batch 22: loss = 0.37125471234321594, acc = 0.8671875\n",
            "Batch 23: loss = 0.3701942265033722, acc = 0.8720703125\n",
            "Batch 24: loss = 0.3400844931602478, acc = 0.8828125\n",
            "Batch 25: loss = 0.34627315402030945, acc = 0.884765625\n",
            "Batch 26: loss = 0.34293609857559204, acc = 0.880859375\n",
            "Batch 27: loss = 0.3570331335067749, acc = 0.876953125\n",
            "Batch 28: loss = 0.3637000322341919, acc = 0.8671875\n",
            "Batch 29: loss = 0.36218759417533875, acc = 0.8740234375\n",
            "Batch 30: loss = 0.34838199615478516, acc = 0.888671875\n",
            "Batch 31: loss = 0.38708099722862244, acc = 0.8720703125\n",
            "Batch 32: loss = 0.3732737898826599, acc = 0.8642578125\n",
            "Batch 33: loss = 0.3376453220844269, acc = 0.888671875\n",
            "Batch 34: loss = 0.33352887630462646, acc = 0.8955078125\n",
            "Batch 35: loss = 0.33893027901649475, acc = 0.8896484375\n",
            "Batch 36: loss = 0.30697518587112427, acc = 0.9052734375\n",
            "Batch 37: loss = 0.34112560749053955, acc = 0.89453125\n",
            "Batch 38: loss = 0.31781062483787537, acc = 0.892578125\n",
            "Batch 39: loss = 0.3448769152164459, acc = 0.88671875\n",
            "Batch 40: loss = 0.3446205258369446, acc = 0.8837890625\n",
            "Batch 41: loss = 0.3277728855609894, acc = 0.8916015625\n",
            "Batch 42: loss = 0.3365728259086609, acc = 0.8828125\n",
            "Batch 43: loss = 0.38011690974235535, acc = 0.8681640625\n",
            "Batch 44: loss = 0.36380401253700256, acc = 0.8876953125\n",
            "Batch 45: loss = 0.3428856134414673, acc = 0.8837890625\n",
            "Batch 46: loss = 0.3063630163669586, acc = 0.8916015625\n",
            "Batch 47: loss = 0.3376363515853882, acc = 0.8935546875\n",
            "Batch 48: loss = 0.3316924571990967, acc = 0.87890625\n",
            "Batch 49: loss = 0.2907865047454834, acc = 0.900390625\n",
            "Batch 50: loss = 0.3518142104148865, acc = 0.8779296875\n",
            "Batch 51: loss = 0.4090566337108612, acc = 0.857421875\n",
            "Batch 52: loss = 0.32404330372810364, acc = 0.888671875\n",
            "Batch 53: loss = 0.3266984522342682, acc = 0.8984375\n",
            "Batch 54: loss = 0.27433696389198303, acc = 0.900390625\n",
            "Batch 55: loss = 0.3412307798862457, acc = 0.8759765625\n",
            "Batch 56: loss = 0.36403894424438477, acc = 0.87890625\n",
            "Batch 57: loss = 0.39005962014198303, acc = 0.8515625\n",
            "Batch 58: loss = 0.39453914761543274, acc = 0.875\n",
            "Batch 59: loss = 0.31585997343063354, acc = 0.8994140625\n",
            "Batch 60: loss = 0.34763601422309875, acc = 0.8798828125\n",
            "Batch 61: loss = 0.3497889041900635, acc = 0.8876953125\n",
            "Batch 62: loss = 0.37545013427734375, acc = 0.876953125\n",
            "Batch 63: loss = 0.3561335504055023, acc = 0.8818359375\n",
            "Batch 64: loss = 0.30166494846343994, acc = 0.896484375\n",
            "Batch 65: loss = 0.34453725814819336, acc = 0.8837890625\n",
            "Batch 66: loss = 0.33503469824790955, acc = 0.8798828125\n",
            "Batch 67: loss = 0.3672065734863281, acc = 0.875\n",
            "Batch 68: loss = 0.4172145426273346, acc = 0.861328125\n",
            "Batch 69: loss = 0.31554555892944336, acc = 0.892578125\n",
            "Batch 70: loss = 0.36336377263069153, acc = 0.8701171875\n",
            "Batch 71: loss = 0.36988598108291626, acc = 0.8720703125\n",
            "Batch 72: loss = 0.3418402373790741, acc = 0.9072265625\n",
            "Batch 73: loss = 0.3541018068790436, acc = 0.87890625\n",
            "Batch 74: loss = 0.3845849633216858, acc = 0.8603515625\n",
            "Batch 75: loss = 0.4151144027709961, acc = 0.8662109375\n",
            "Batch 76: loss = 0.3779379725456238, acc = 0.869140625\n",
            "Batch 77: loss = 0.3252294063568115, acc = 0.89453125\n",
            "Batch 78: loss = 0.358182430267334, acc = 0.8759765625\n",
            "Batch 79: loss = 0.3371888995170593, acc = 0.8876953125\n",
            "Batch 80: loss = 0.35730934143066406, acc = 0.87890625\n",
            "Batch 81: loss = 0.3725103735923767, acc = 0.8701171875\n",
            "Batch 82: loss = 0.3339526355266571, acc = 0.8916015625\n",
            "Batch 83: loss = 0.3636438548564911, acc = 0.876953125\n",
            "Batch 84: loss = 0.36165985465049744, acc = 0.873046875\n",
            "Batch 85: loss = 0.41858184337615967, acc = 0.8740234375\n",
            "Batch 86: loss = 0.3643311858177185, acc = 0.8701171875\n",
            "Batch 87: loss = 0.3982633352279663, acc = 0.8701171875\n",
            "Batch 88: loss = 0.37558722496032715, acc = 0.869140625\n",
            "Batch 89: loss = 0.3363037109375, acc = 0.896484375\n",
            "Batch 90: loss = 0.3701479434967041, acc = 0.8798828125\n",
            "Batch 91: loss = 0.3650548756122589, acc = 0.87890625\n",
            "Batch 92: loss = 0.43288010358810425, acc = 0.8466796875\n",
            "Batch 93: loss = 0.335392564535141, acc = 0.8857421875\n",
            "Batch 94: loss = 0.35824793577194214, acc = 0.8837890625\n",
            "Batch 95: loss = 0.30401960015296936, acc = 0.892578125\n",
            "Batch 96: loss = 0.35310623049736023, acc = 0.875\n",
            "Batch 97: loss = 0.3295523524284363, acc = 0.8837890625\n",
            "Batch 98: loss = 0.3564291298389435, acc = 0.87890625\n",
            "Batch 99: loss = 0.3064207136631012, acc = 0.88671875\n",
            "Batch 100: loss = 0.37256789207458496, acc = 0.8720703125\n",
            "Batch 101: loss = 0.3379920721054077, acc = 0.8759765625\n",
            "Batch 102: loss = 0.3990485668182373, acc = 0.8603515625\n",
            "Batch 103: loss = 0.37224438786506653, acc = 0.873046875\n",
            "Batch 104: loss = 0.3170885145664215, acc = 0.89453125\n",
            "Batch 105: loss = 0.31760188937187195, acc = 0.88671875\n",
            "Batch 106: loss = 0.3623942732810974, acc = 0.87109375\n",
            "Batch 107: loss = 0.339791864156723, acc = 0.8916015625\n",
            "Batch 108: loss = 0.3328671157360077, acc = 0.8818359375\n",
            "Batch 109: loss = 0.37168291211128235, acc = 0.8740234375\n",
            "Batch 110: loss = 0.3196594715118408, acc = 0.892578125\n",
            "Batch 111: loss = 0.36619991064071655, acc = 0.880859375\n",
            "Batch 112: loss = 0.3514656722545624, acc = 0.87890625\n",
            "Batch 113: loss = 0.3681124448776245, acc = 0.876953125\n",
            "Batch 114: loss = 0.3806520700454712, acc = 0.8720703125\n",
            "Batch 115: loss = 0.33877667784690857, acc = 0.8916015625\n",
            "Batch 116: loss = 0.4097428619861603, acc = 0.87109375\n",
            "Batch 117: loss = 0.3741424083709717, acc = 0.8671875\n",
            "Batch 118: loss = 0.3221563398838043, acc = 0.89453125\n",
            "Batch 119: loss = 0.35130712389945984, acc = 0.8896484375\n",
            "Batch 120: loss = 0.33677437901496887, acc = 0.8896484375\n",
            "Batch 121: loss = 0.35824117064476013, acc = 0.8759765625\n",
            "Batch 122: loss = 0.33874818682670593, acc = 0.8798828125\n",
            "Batch 123: loss = 0.3943295180797577, acc = 0.869140625\n",
            "Batch 124: loss = 0.3613578677177429, acc = 0.875\n",
            "Batch 125: loss = 0.34109050035476685, acc = 0.8974609375\n",
            "Batch 126: loss = 0.3438400328159332, acc = 0.87890625\n",
            "\n",
            "Epoch 98/100\n",
            "Batch 1: loss = 0.4385725259780884, acc = 0.8671875\n",
            "Batch 2: loss = 0.3903418183326721, acc = 0.8662109375\n",
            "Batch 3: loss = 0.3381935954093933, acc = 0.8955078125\n",
            "Batch 4: loss = 0.3369266986846924, acc = 0.8857421875\n",
            "Batch 5: loss = 0.34965085983276367, acc = 0.8779296875\n",
            "Batch 6: loss = 0.3973177671432495, acc = 0.8681640625\n",
            "Batch 7: loss = 0.3510136604309082, acc = 0.892578125\n",
            "Batch 8: loss = 0.3848915100097656, acc = 0.8740234375\n",
            "Batch 9: loss = 0.3499985635280609, acc = 0.87890625\n",
            "Batch 10: loss = 0.3263925015926361, acc = 0.8876953125\n",
            "Batch 11: loss = 0.3411962687969208, acc = 0.8798828125\n",
            "Batch 12: loss = 0.33619609475135803, acc = 0.880859375\n",
            "Batch 13: loss = 0.360321044921875, acc = 0.8779296875\n",
            "Batch 14: loss = 0.33240100741386414, acc = 0.8837890625\n",
            "Batch 15: loss = 0.3243498206138611, acc = 0.8837890625\n",
            "Batch 16: loss = 0.3949466347694397, acc = 0.875\n",
            "Batch 17: loss = 0.32105565071105957, acc = 0.8896484375\n",
            "Batch 18: loss = 0.3962705135345459, acc = 0.8671875\n",
            "Batch 19: loss = 0.33261510729789734, acc = 0.873046875\n",
            "Batch 20: loss = 0.34466439485549927, acc = 0.8876953125\n",
            "Batch 21: loss = 0.3859764039516449, acc = 0.8720703125\n",
            "Batch 22: loss = 0.3959709107875824, acc = 0.8701171875\n",
            "Batch 23: loss = 0.3422197103500366, acc = 0.88671875\n",
            "Batch 24: loss = 0.35025298595428467, acc = 0.8837890625\n",
            "Batch 25: loss = 0.33231741189956665, acc = 0.888671875\n",
            "Batch 26: loss = 0.3684879541397095, acc = 0.865234375\n",
            "Batch 27: loss = 0.37378913164138794, acc = 0.8642578125\n",
            "Batch 28: loss = 0.3658732771873474, acc = 0.875\n",
            "Batch 29: loss = 0.33362528681755066, acc = 0.8818359375\n",
            "Batch 30: loss = 0.3602577745914459, acc = 0.87890625\n",
            "Batch 31: loss = 0.36733078956604004, acc = 0.8837890625\n",
            "Batch 32: loss = 0.3696915805339813, acc = 0.8828125\n",
            "Batch 33: loss = 0.3341827988624573, acc = 0.8798828125\n",
            "Batch 34: loss = 0.3520985543727875, acc = 0.8740234375\n",
            "Batch 35: loss = 0.32186201214790344, acc = 0.8896484375\n",
            "Batch 36: loss = 0.3517110347747803, acc = 0.88671875\n",
            "Batch 37: loss = 0.32179969549179077, acc = 0.8984375\n",
            "Batch 38: loss = 0.32534918189048767, acc = 0.89453125\n",
            "Batch 39: loss = 0.30732667446136475, acc = 0.8935546875\n",
            "Batch 40: loss = 0.3208557963371277, acc = 0.888671875\n",
            "Batch 41: loss = 0.34718841314315796, acc = 0.8857421875\n",
            "Batch 42: loss = 0.33196455240249634, acc = 0.8837890625\n",
            "Batch 43: loss = 0.3530016839504242, acc = 0.875\n",
            "Batch 44: loss = 0.35250726342201233, acc = 0.890625\n",
            "Batch 45: loss = 0.3188456892967224, acc = 0.884765625\n",
            "Batch 46: loss = 0.373214989900589, acc = 0.8662109375\n",
            "Batch 47: loss = 0.3226431608200073, acc = 0.896484375\n",
            "Batch 48: loss = 0.35736268758773804, acc = 0.87890625\n",
            "Batch 49: loss = 0.31789296865463257, acc = 0.8916015625\n",
            "Batch 50: loss = 0.34350505471229553, acc = 0.8876953125\n",
            "Batch 51: loss = 0.4018615186214447, acc = 0.8681640625\n",
            "Batch 52: loss = 0.37691354751586914, acc = 0.87109375\n",
            "Batch 53: loss = 0.34342658519744873, acc = 0.890625\n",
            "Batch 54: loss = 0.275357186794281, acc = 0.91015625\n",
            "Batch 55: loss = 0.34633952379226685, acc = 0.880859375\n",
            "Batch 56: loss = 0.3767247796058655, acc = 0.8720703125\n",
            "Batch 57: loss = 0.40058404207229614, acc = 0.8681640625\n",
            "Batch 58: loss = 0.3949207365512848, acc = 0.875\n",
            "Batch 59: loss = 0.32922184467315674, acc = 0.892578125\n",
            "Batch 60: loss = 0.3698924779891968, acc = 0.880859375\n",
            "Batch 61: loss = 0.3447023332118988, acc = 0.8896484375\n",
            "Batch 62: loss = 0.360876202583313, acc = 0.8828125\n",
            "Batch 63: loss = 0.32310718297958374, acc = 0.890625\n",
            "Batch 64: loss = 0.2972637414932251, acc = 0.9013671875\n",
            "Batch 65: loss = 0.3263072371482849, acc = 0.8896484375\n",
            "Batch 66: loss = 0.33161336183547974, acc = 0.8916015625\n",
            "Batch 67: loss = 0.34643492102622986, acc = 0.884765625\n",
            "Batch 68: loss = 0.39996805787086487, acc = 0.865234375\n",
            "Batch 69: loss = 0.35565558075904846, acc = 0.8759765625\n",
            "Batch 70: loss = 0.3870013654232025, acc = 0.8701171875\n",
            "Batch 71: loss = 0.3729190230369568, acc = 0.8583984375\n",
            "Batch 72: loss = 0.33618026971817017, acc = 0.8857421875\n",
            "Batch 73: loss = 0.37533465027809143, acc = 0.8740234375\n",
            "Batch 74: loss = 0.3301028609275818, acc = 0.888671875\n",
            "Batch 75: loss = 0.39286649227142334, acc = 0.8623046875\n",
            "Batch 76: loss = 0.399938702583313, acc = 0.8564453125\n",
            "Batch 77: loss = 0.33668002486228943, acc = 0.8935546875\n",
            "Batch 78: loss = 0.35762566328048706, acc = 0.8701171875\n",
            "Batch 79: loss = 0.3445391356945038, acc = 0.8896484375\n",
            "Batch 80: loss = 0.3355991840362549, acc = 0.888671875\n",
            "Batch 81: loss = 0.372846782207489, acc = 0.8759765625\n",
            "Batch 82: loss = 0.33583712577819824, acc = 0.8974609375\n",
            "Batch 83: loss = 0.379944384098053, acc = 0.8740234375\n",
            "Batch 84: loss = 0.3681265711784363, acc = 0.8662109375\n",
            "Batch 85: loss = 0.3595617115497589, acc = 0.873046875\n",
            "Batch 86: loss = 0.35348305106163025, acc = 0.8837890625\n",
            "Batch 87: loss = 0.34937578439712524, acc = 0.875\n",
            "Batch 88: loss = 0.4117884933948517, acc = 0.8623046875\n",
            "Batch 89: loss = 0.3605935871601105, acc = 0.8779296875\n",
            "Batch 90: loss = 0.36045321822166443, acc = 0.8740234375\n",
            "Batch 91: loss = 0.3325706720352173, acc = 0.8828125\n",
            "Batch 92: loss = 0.38595280051231384, acc = 0.8837890625\n",
            "Batch 93: loss = 0.34624266624450684, acc = 0.88671875\n",
            "Batch 94: loss = 0.3938489556312561, acc = 0.8681640625\n",
            "Batch 95: loss = 0.34625816345214844, acc = 0.8857421875\n",
            "Batch 96: loss = 0.3682408630847931, acc = 0.8701171875\n",
            "Batch 97: loss = 0.39346015453338623, acc = 0.8671875\n",
            "Batch 98: loss = 0.3598908483982086, acc = 0.8857421875\n",
            "Batch 99: loss = 0.36829671263694763, acc = 0.8779296875\n",
            "Batch 100: loss = 0.3601638376712799, acc = 0.87890625\n",
            "Batch 101: loss = 0.3431583046913147, acc = 0.8818359375\n",
            "Batch 102: loss = 0.38104262948036194, acc = 0.8623046875\n",
            "Batch 103: loss = 0.36374518275260925, acc = 0.87109375\n",
            "Batch 104: loss = 0.339523047208786, acc = 0.888671875\n",
            "Batch 105: loss = 0.3246895968914032, acc = 0.89453125\n",
            "Batch 106: loss = 0.3603484630584717, acc = 0.8603515625\n",
            "Batch 107: loss = 0.35167938470840454, acc = 0.8828125\n",
            "Batch 108: loss = 0.35336074233055115, acc = 0.875\n",
            "Batch 109: loss = 0.33643683791160583, acc = 0.8837890625\n",
            "Batch 110: loss = 0.31425362825393677, acc = 0.904296875\n",
            "Batch 111: loss = 0.385233998298645, acc = 0.8681640625\n",
            "Batch 112: loss = 0.35455790162086487, acc = 0.8837890625\n",
            "Batch 113: loss = 0.3620772361755371, acc = 0.876953125\n",
            "Batch 114: loss = 0.38399630784988403, acc = 0.85546875\n",
            "Batch 115: loss = 0.3638356626033783, acc = 0.8759765625\n",
            "Batch 116: loss = 0.3937256336212158, acc = 0.865234375\n",
            "Batch 117: loss = 0.3769731819629669, acc = 0.8759765625\n",
            "Batch 118: loss = 0.35767266154289246, acc = 0.880859375\n",
            "Batch 119: loss = 0.3507609963417053, acc = 0.884765625\n",
            "Batch 120: loss = 0.3599453568458557, acc = 0.8662109375\n",
            "Batch 121: loss = 0.3678087294101715, acc = 0.8701171875\n",
            "Batch 122: loss = 0.341717392206192, acc = 0.8818359375\n",
            "Batch 123: loss = 0.3655385375022888, acc = 0.873046875\n",
            "Batch 124: loss = 0.3432479798793793, acc = 0.876953125\n",
            "Batch 125: loss = 0.3846677541732788, acc = 0.8759765625\n",
            "Batch 126: loss = 0.4316655099391937, acc = 0.853515625\n",
            "\n",
            "Epoch 99/100\n",
            "Batch 1: loss = 0.5060746669769287, acc = 0.8359375\n",
            "Batch 2: loss = 0.38466963171958923, acc = 0.869140625\n",
            "Batch 3: loss = 0.36814582347869873, acc = 0.88671875\n",
            "Batch 4: loss = 0.3476950228214264, acc = 0.888671875\n",
            "Batch 5: loss = 0.3657894730567932, acc = 0.875\n",
            "Batch 6: loss = 0.38931751251220703, acc = 0.8662109375\n",
            "Batch 7: loss = 0.35441508889198303, acc = 0.8798828125\n",
            "Batch 8: loss = 0.38564565777778625, acc = 0.8701171875\n",
            "Batch 9: loss = 0.39385056495666504, acc = 0.8681640625\n",
            "Batch 10: loss = 0.3449121117591858, acc = 0.884765625\n",
            "Batch 11: loss = 0.34747883677482605, acc = 0.890625\n",
            "Batch 12: loss = 0.37342044711112976, acc = 0.8818359375\n",
            "Batch 13: loss = 0.3671870529651642, acc = 0.875\n",
            "Batch 14: loss = 0.33675602078437805, acc = 0.88671875\n",
            "Batch 15: loss = 0.2942843437194824, acc = 0.90625\n",
            "Batch 16: loss = 0.33355847001075745, acc = 0.87890625\n",
            "Batch 17: loss = 0.31971991062164307, acc = 0.89453125\n",
            "Batch 18: loss = 0.38546156883239746, acc = 0.8681640625\n",
            "Batch 19: loss = 0.33139288425445557, acc = 0.8935546875\n",
            "Batch 20: loss = 0.3383346199989319, acc = 0.8857421875\n",
            "Batch 21: loss = 0.375008761882782, acc = 0.884765625\n",
            "Batch 22: loss = 0.3657994270324707, acc = 0.875\n",
            "Batch 23: loss = 0.323222279548645, acc = 0.888671875\n",
            "Batch 24: loss = 0.37812426686286926, acc = 0.875\n",
            "Batch 25: loss = 0.3329102098941803, acc = 0.8837890625\n",
            "Batch 26: loss = 0.3081423044204712, acc = 0.9033203125\n",
            "Batch 27: loss = 0.38184115290641785, acc = 0.8798828125\n",
            "Batch 28: loss = 0.36462435126304626, acc = 0.8662109375\n",
            "Batch 29: loss = 0.3724103569984436, acc = 0.87890625\n",
            "Batch 30: loss = 0.39452606439590454, acc = 0.8681640625\n",
            "Batch 31: loss = 0.37526771426200867, acc = 0.8681640625\n",
            "Batch 32: loss = 0.383503794670105, acc = 0.873046875\n",
            "Batch 33: loss = 0.37389373779296875, acc = 0.8720703125\n",
            "Batch 34: loss = 0.34648674726486206, acc = 0.873046875\n",
            "Batch 35: loss = 0.35781270265579224, acc = 0.8798828125\n",
            "Batch 36: loss = 0.35832303762435913, acc = 0.8681640625\n",
            "Batch 37: loss = 0.32878735661506653, acc = 0.8955078125\n",
            "Batch 38: loss = 0.2893417477607727, acc = 0.9052734375\n",
            "Batch 39: loss = 0.34147199988365173, acc = 0.8818359375\n",
            "Batch 40: loss = 0.35534802079200745, acc = 0.8798828125\n",
            "Batch 41: loss = 0.3292790949344635, acc = 0.8955078125\n",
            "Batch 42: loss = 0.3097078800201416, acc = 0.900390625\n",
            "Batch 43: loss = 0.36419907212257385, acc = 0.8720703125\n",
            "Batch 44: loss = 0.3365739583969116, acc = 0.88671875\n",
            "Batch 45: loss = 0.3199349045753479, acc = 0.8896484375\n",
            "Batch 46: loss = 0.33397042751312256, acc = 0.8916015625\n",
            "Batch 47: loss = 0.3263891339302063, acc = 0.8837890625\n",
            "Batch 48: loss = 0.30392220616340637, acc = 0.8955078125\n",
            "Batch 49: loss = 0.3227919936180115, acc = 0.890625\n",
            "Batch 50: loss = 0.3473459780216217, acc = 0.88671875\n",
            "Batch 51: loss = 0.3490409553050995, acc = 0.880859375\n",
            "Batch 52: loss = 0.3318597078323364, acc = 0.8916015625\n",
            "Batch 53: loss = 0.34610608220100403, acc = 0.8896484375\n",
            "Batch 54: loss = 0.298870712518692, acc = 0.8984375\n",
            "Batch 55: loss = 0.32655608654022217, acc = 0.8818359375\n",
            "Batch 56: loss = 0.364749014377594, acc = 0.8720703125\n",
            "Batch 57: loss = 0.35527560114860535, acc = 0.8876953125\n",
            "Batch 58: loss = 0.3688533306121826, acc = 0.8818359375\n",
            "Batch 59: loss = 0.2881973087787628, acc = 0.908203125\n",
            "Batch 60: loss = 0.34985116124153137, acc = 0.890625\n",
            "Batch 61: loss = 0.3111891448497772, acc = 0.892578125\n",
            "Batch 62: loss = 0.3797437846660614, acc = 0.8623046875\n",
            "Batch 63: loss = 0.33966416120529175, acc = 0.88671875\n",
            "Batch 64: loss = 0.3004717528820038, acc = 0.89453125\n",
            "Batch 65: loss = 0.3677469491958618, acc = 0.8720703125\n",
            "Batch 66: loss = 0.3946077525615692, acc = 0.861328125\n",
            "Batch 67: loss = 0.33917176723480225, acc = 0.8837890625\n",
            "Batch 68: loss = 0.3582930564880371, acc = 0.8857421875\n",
            "Batch 69: loss = 0.34240904450416565, acc = 0.884765625\n",
            "Batch 70: loss = 0.3954427242279053, acc = 0.8681640625\n",
            "Batch 71: loss = 0.3858095109462738, acc = 0.861328125\n",
            "Batch 72: loss = 0.35670462250709534, acc = 0.8876953125\n",
            "Batch 73: loss = 0.3913438320159912, acc = 0.86328125\n",
            "Batch 74: loss = 0.37010738253593445, acc = 0.873046875\n",
            "Batch 75: loss = 0.38559362292289734, acc = 0.8701171875\n",
            "Batch 76: loss = 0.3626875579357147, acc = 0.87109375\n",
            "Batch 77: loss = 0.329304575920105, acc = 0.8837890625\n",
            "Batch 78: loss = 0.3721613585948944, acc = 0.8701171875\n",
            "Batch 79: loss = 0.3754286468029022, acc = 0.8701171875\n",
            "Batch 80: loss = 0.3533097505569458, acc = 0.87890625\n",
            "Batch 81: loss = 0.3610844016075134, acc = 0.8798828125\n",
            "Batch 82: loss = 0.3377915918827057, acc = 0.88671875\n",
            "Batch 83: loss = 0.357656329870224, acc = 0.876953125\n",
            "Batch 84: loss = 0.32214704155921936, acc = 0.880859375\n",
            "Batch 85: loss = 0.3836469054222107, acc = 0.86328125\n",
            "Batch 86: loss = 0.382926344871521, acc = 0.861328125\n",
            "Batch 87: loss = 0.3525616526603699, acc = 0.8779296875\n",
            "Batch 88: loss = 0.4208334684371948, acc = 0.8583984375\n",
            "Batch 89: loss = 0.34720683097839355, acc = 0.8857421875\n",
            "Batch 90: loss = 0.36846715211868286, acc = 0.8671875\n",
            "Batch 91: loss = 0.379638671875, acc = 0.873046875\n",
            "Batch 92: loss = 0.3842048645019531, acc = 0.869140625\n",
            "Batch 93: loss = 0.3695286810398102, acc = 0.8798828125\n",
            "Batch 94: loss = 0.35304364562034607, acc = 0.8828125\n",
            "Batch 95: loss = 0.31365442276000977, acc = 0.8896484375\n",
            "Batch 96: loss = 0.3971625566482544, acc = 0.8623046875\n",
            "Batch 97: loss = 0.3754461109638214, acc = 0.8818359375\n",
            "Batch 98: loss = 0.36867383122444153, acc = 0.869140625\n",
            "Batch 99: loss = 0.3264589011669159, acc = 0.8896484375\n",
            "Batch 100: loss = 0.35462749004364014, acc = 0.87109375\n",
            "Batch 101: loss = 0.35550689697265625, acc = 0.8740234375\n",
            "Batch 102: loss = 0.427864670753479, acc = 0.8583984375\n",
            "Batch 103: loss = 0.3630451560020447, acc = 0.884765625\n",
            "Batch 104: loss = 0.31921350955963135, acc = 0.88671875\n",
            "Batch 105: loss = 0.3386824131011963, acc = 0.892578125\n",
            "Batch 106: loss = 0.35669296979904175, acc = 0.8779296875\n",
            "Batch 107: loss = 0.38857153058052063, acc = 0.869140625\n",
            "Batch 108: loss = 0.3561125695705414, acc = 0.8798828125\n",
            "Batch 109: loss = 0.35511696338653564, acc = 0.87890625\n",
            "Batch 110: loss = 0.3125806748867035, acc = 0.9013671875\n",
            "Batch 111: loss = 0.35093796253204346, acc = 0.8828125\n",
            "Batch 112: loss = 0.3462292551994324, acc = 0.8837890625\n",
            "Batch 113: loss = 0.3674158453941345, acc = 0.8779296875\n",
            "Batch 114: loss = 0.3876376748085022, acc = 0.87890625\n",
            "Batch 115: loss = 0.370571494102478, acc = 0.873046875\n",
            "Batch 116: loss = 0.3899169862270355, acc = 0.86328125\n",
            "Batch 117: loss = 0.37894412875175476, acc = 0.8740234375\n",
            "Batch 118: loss = 0.30538588762283325, acc = 0.8974609375\n",
            "Batch 119: loss = 0.3389982283115387, acc = 0.8837890625\n",
            "Batch 120: loss = 0.365208238363266, acc = 0.8818359375\n",
            "Batch 121: loss = 0.3222487270832062, acc = 0.8828125\n",
            "Batch 122: loss = 0.30251193046569824, acc = 0.8994140625\n",
            "Batch 123: loss = 0.34355196356773376, acc = 0.8798828125\n",
            "Batch 124: loss = 0.3618346154689789, acc = 0.875\n",
            "Batch 125: loss = 0.3701340854167938, acc = 0.8681640625\n",
            "Batch 126: loss = 0.36496153473854065, acc = 0.87890625\n",
            "\n",
            "Epoch 100/100\n",
            "Batch 1: loss = 0.43731608986854553, acc = 0.865234375\n",
            "Batch 2: loss = 0.39055871963500977, acc = 0.8740234375\n",
            "Batch 3: loss = 0.35073381662368774, acc = 0.8818359375\n",
            "Batch 4: loss = 0.33785808086395264, acc = 0.8779296875\n",
            "Batch 5: loss = 0.3383611738681793, acc = 0.8994140625\n",
            "Batch 6: loss = 0.40054750442504883, acc = 0.865234375\n",
            "Batch 7: loss = 0.33247318863868713, acc = 0.8955078125\n",
            "Batch 8: loss = 0.3834935128688812, acc = 0.87109375\n",
            "Batch 9: loss = 0.38023829460144043, acc = 0.8642578125\n",
            "Batch 10: loss = 0.31066226959228516, acc = 0.9052734375\n",
            "Batch 11: loss = 0.34867388010025024, acc = 0.87890625\n",
            "Batch 12: loss = 0.31569555401802063, acc = 0.8916015625\n",
            "Batch 13: loss = 0.34652942419052124, acc = 0.8837890625\n",
            "Batch 14: loss = 0.33963799476623535, acc = 0.892578125\n",
            "Batch 15: loss = 0.35174861550331116, acc = 0.880859375\n",
            "Batch 16: loss = 0.35424044728279114, acc = 0.8818359375\n",
            "Batch 17: loss = 0.3174518644809723, acc = 0.8955078125\n",
            "Batch 18: loss = 0.4032207429409027, acc = 0.8642578125\n",
            "Batch 19: loss = 0.34774357080459595, acc = 0.875\n",
            "Batch 20: loss = 0.32203978300094604, acc = 0.888671875\n",
            "Batch 21: loss = 0.3801962733268738, acc = 0.8662109375\n",
            "Batch 22: loss = 0.37572428584098816, acc = 0.875\n",
            "Batch 23: loss = 0.3106505274772644, acc = 0.8955078125\n",
            "Batch 24: loss = 0.3651520013809204, acc = 0.87109375\n",
            "Batch 25: loss = 0.33171817660331726, acc = 0.89453125\n",
            "Batch 26: loss = 0.31545495986938477, acc = 0.8916015625\n",
            "Batch 27: loss = 0.345882385969162, acc = 0.86328125\n",
            "Batch 28: loss = 0.37572354078292847, acc = 0.87890625\n",
            "Batch 29: loss = 0.35960692167282104, acc = 0.8818359375\n",
            "Batch 30: loss = 0.3277851343154907, acc = 0.896484375\n",
            "Batch 31: loss = 0.3513270914554596, acc = 0.884765625\n",
            "Batch 32: loss = 0.3731869161128998, acc = 0.873046875\n",
            "Batch 33: loss = 0.3880777657032013, acc = 0.8671875\n",
            "Batch 34: loss = 0.3872036635875702, acc = 0.8701171875\n",
            "Batch 35: loss = 0.2883232831954956, acc = 0.9013671875\n",
            "Batch 36: loss = 0.3346673846244812, acc = 0.8984375\n",
            "Batch 37: loss = 0.3710932433605194, acc = 0.8837890625\n",
            "Batch 38: loss = 0.2898990511894226, acc = 0.9072265625\n",
            "Batch 39: loss = 0.3221818208694458, acc = 0.888671875\n",
            "Batch 40: loss = 0.35448992252349854, acc = 0.880859375\n",
            "Batch 41: loss = 0.3137631118297577, acc = 0.8837890625\n",
            "Batch 42: loss = 0.3274657726287842, acc = 0.8818359375\n",
            "Batch 43: loss = 0.35455450415611267, acc = 0.8740234375\n",
            "Batch 44: loss = 0.3178023099899292, acc = 0.896484375\n",
            "Batch 45: loss = 0.30766645073890686, acc = 0.90234375\n",
            "Batch 46: loss = 0.3290075957775116, acc = 0.88671875\n",
            "Batch 47: loss = 0.33500784635543823, acc = 0.880859375\n",
            "Batch 48: loss = 0.307866632938385, acc = 0.890625\n",
            "Batch 49: loss = 0.32801830768585205, acc = 0.8798828125\n",
            "Batch 50: loss = 0.3467749059200287, acc = 0.876953125\n",
            "Batch 51: loss = 0.36059653759002686, acc = 0.8779296875\n",
            "Batch 52: loss = 0.3529897630214691, acc = 0.8837890625\n",
            "Batch 53: loss = 0.3709481954574585, acc = 0.873046875\n",
            "Batch 54: loss = 0.2601771056652069, acc = 0.90234375\n",
            "Batch 55: loss = 0.31883758306503296, acc = 0.8974609375\n",
            "Batch 56: loss = 0.3558279871940613, acc = 0.8759765625\n",
            "Batch 57: loss = 0.39512330293655396, acc = 0.873046875\n",
            "Batch 58: loss = 0.38699182868003845, acc = 0.8720703125\n",
            "Batch 59: loss = 0.29418742656707764, acc = 0.90625\n",
            "Batch 60: loss = 0.339918851852417, acc = 0.880859375\n",
            "Batch 61: loss = 0.35696786642074585, acc = 0.888671875\n",
            "Batch 62: loss = 0.39810556173324585, acc = 0.8642578125\n",
            "Batch 63: loss = 0.33281922340393066, acc = 0.890625\n",
            "Batch 64: loss = 0.32656875252723694, acc = 0.8955078125\n",
            "Batch 65: loss = 0.36018237471580505, acc = 0.8818359375\n",
            "Batch 66: loss = 0.3607129454612732, acc = 0.87890625\n",
            "Batch 67: loss = 0.3254304826259613, acc = 0.8896484375\n",
            "Batch 68: loss = 0.36344143748283386, acc = 0.880859375\n",
            "Batch 69: loss = 0.31094661355018616, acc = 0.8955078125\n",
            "Batch 70: loss = 0.38576191663742065, acc = 0.876953125\n",
            "Batch 71: loss = 0.36337849497795105, acc = 0.875\n",
            "Batch 72: loss = 0.32927075028419495, acc = 0.8837890625\n",
            "Batch 73: loss = 0.35914602875709534, acc = 0.8642578125\n",
            "Batch 74: loss = 0.3602767586708069, acc = 0.8662109375\n",
            "Batch 75: loss = 0.41665637493133545, acc = 0.8583984375\n",
            "Batch 76: loss = 0.3810975253582001, acc = 0.8720703125\n",
            "Batch 77: loss = 0.3299539089202881, acc = 0.88671875\n",
            "Batch 78: loss = 0.368958055973053, acc = 0.87109375\n",
            "Batch 79: loss = 0.3921089768409729, acc = 0.86328125\n",
            "Batch 80: loss = 0.3586703836917877, acc = 0.8759765625\n",
            "Batch 81: loss = 0.34512248635292053, acc = 0.88671875\n",
            "Batch 82: loss = 0.35657134652137756, acc = 0.875\n",
            "Batch 83: loss = 0.31187424063682556, acc = 0.890625\n",
            "Batch 84: loss = 0.3505067229270935, acc = 0.880859375\n",
            "Batch 85: loss = 0.41990652680397034, acc = 0.8544921875\n",
            "Batch 86: loss = 0.3963945508003235, acc = 0.869140625\n",
            "Batch 87: loss = 0.36952531337738037, acc = 0.8857421875\n",
            "Batch 88: loss = 0.44111067056655884, acc = 0.8427734375\n",
            "Batch 89: loss = 0.33199039101600647, acc = 0.8974609375\n",
            "Batch 90: loss = 0.3715980648994446, acc = 0.8720703125\n",
            "Batch 91: loss = 0.3918692469596863, acc = 0.8671875\n",
            "Batch 92: loss = 0.3829367458820343, acc = 0.865234375\n",
            "Batch 93: loss = 0.3192892372608185, acc = 0.904296875\n",
            "Batch 94: loss = 0.3577316701412201, acc = 0.8828125\n",
            "Batch 95: loss = 0.33079156279563904, acc = 0.88671875\n",
            "Batch 96: loss = 0.3825967609882355, acc = 0.8544921875\n",
            "Batch 97: loss = 0.376772403717041, acc = 0.875\n",
            "Batch 98: loss = 0.3809140920639038, acc = 0.8681640625\n",
            "Batch 99: loss = 0.33856093883514404, acc = 0.8857421875\n",
            "Batch 100: loss = 0.352557897567749, acc = 0.8818359375\n",
            "Batch 101: loss = 0.31645485758781433, acc = 0.8798828125\n",
            "Batch 102: loss = 0.4147273302078247, acc = 0.857421875\n",
            "Batch 103: loss = 0.37725189328193665, acc = 0.865234375\n",
            "Batch 104: loss = 0.34418272972106934, acc = 0.8828125\n",
            "Batch 105: loss = 0.2952795624732971, acc = 0.90234375\n",
            "Batch 106: loss = 0.3188324272632599, acc = 0.8935546875\n",
            "Batch 107: loss = 0.3614806532859802, acc = 0.8720703125\n",
            "Batch 108: loss = 0.3866179883480072, acc = 0.875\n",
            "Batch 109: loss = 0.3328830897808075, acc = 0.880859375\n",
            "Batch 110: loss = 0.3102967143058777, acc = 0.8955078125\n",
            "Batch 111: loss = 0.35806939005851746, acc = 0.8759765625\n",
            "Batch 112: loss = 0.31554141640663147, acc = 0.8974609375\n",
            "Batch 113: loss = 0.37452179193496704, acc = 0.8701171875\n",
            "Batch 114: loss = 0.3590920567512512, acc = 0.8828125\n",
            "Batch 115: loss = 0.3863832950592041, acc = 0.8701171875\n",
            "Batch 116: loss = 0.395145982503891, acc = 0.873046875\n",
            "Batch 117: loss = 0.3659149408340454, acc = 0.884765625\n",
            "Batch 118: loss = 0.3402805030345917, acc = 0.8740234375\n",
            "Batch 119: loss = 0.3021414279937744, acc = 0.9033203125\n",
            "Batch 120: loss = 0.3283389210700989, acc = 0.869140625\n",
            "Batch 121: loss = 0.34719425439834595, acc = 0.880859375\n",
            "Batch 122: loss = 0.34216269850730896, acc = 0.888671875\n",
            "Batch 123: loss = 0.37249043583869934, acc = 0.869140625\n",
            "Batch 124: loss = 0.3635707199573517, acc = 0.8701171875\n",
            "Batch 125: loss = 0.3270755112171173, acc = 0.8896484375\n",
            "Batch 126: loss = 0.3190268278121948, acc = 0.8857421875\n",
            "Saved checkpoint to gru_weights.100.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf-8j_z9NerU"
      },
      "source": [
        "## Model Output\n",
        "\n",
        "### Run below code and enter epoch number , starting character index, sequence lenght"
      ],
      "id": "qf-8j_z9NerU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "empty-alliance",
        "outputId": "9b5d86e8-0735-4125-de59-378a86d5d8cc"
      },
      "source": [
        "epoch_number = int(input(\"Enter epoch number between 1 to 100 only multiples of 10: \"))\n",
        "character_index = int(input(\"Enter any number between 0 to 86 to sequence generation: \"))\n",
        "Sequence_length = int(input(\"Number of characters to generate: \"))\n",
        "\n",
        "Generated_music_sequence_gru = sample_seq_generator_gru(epoch_number, character_index, Sequence_length)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(Generated_music_sequence_gru)"
      ],
      "id": "empty-alliance",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter epoch number between 1 to 100 only multiples of 10: 100\n",
            "Enter any number between 0 to 86 to sequence generation: 1\n",
            "Number of characters to generate: 600\n",
            "\n",
            "\n",
            " N74\n",
            "T:The Lacking Corks\n",
            "% Nottingham Music Database\n",
            "S:NPTB, via EF\n",
            "Y:AB\n",
            "M:6/8\n",
            "K:A\n",
            "P:A\n",
            "E|\"A\"A2A AGA|\"G\"B2G \"D\"A2A|\"G\"Bcd \"A\"ecA|\"G\"BAG \"A\"A3:|\n",
            "P:B\n",
            "\"A\"age \"D\"fga|\"A\"efe ece|\"D\"fga \"A\"edc|\"D\"d3 :|\n",
            "\n",
            "\n",
            "X: 297\n",
            "T:Spenleft 1,y Ronushan\n",
            "% Nottingham Music Database\n",
            "S:Trad, arr Phil Rowe\n",
            "M:6/8\n",
            "K:A\n",
            "\"E7\"GAB |\"A\"c2e \"E7\"B2e|\"A\"Ace cAB|\"A\"c2e \"E7\"B2e|\"A\"A3 |:\"D\"d2f a2f|\"A\"efe ece|\"Bm\"f2d f2d|\"E\"f3 \"A\"c2e|\n",
            "\"A\"Ace ecA|\"D\"d2f a2f|\"A\"efe cBA|\"Bm\"dcd \"E7\"Bcd|\"A\"c3 -ceg|\"Bm\"f2d \"E7\"c2B|\n",
            "\"A\"Ace cde|\"D\"fgf fga|\"A\"ecA AcB|A2A AAc|\"Bm\"Bcd \"E7\"c2B|\n",
            "\"A\"A2c cde|\"D\"f2d fga|\"A\"cdc cBc|\"A\"Ace \"D\"agf|\"A\"efe \"E7\"e2d|\\\n",
            "\"A\"c2A \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXkDDt3Mlf8d"
      },
      "source": [
        "# **Single layer LSTM Model**\n",
        "### In this iteration we expermiented with single layer LSTM with 256 LSTM units"
      ],
      "id": "QXkDDt3Mlf8d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhaMtK_-vN-n"
      },
      "source": [
        "MODEL_DIR = '/content/drive/My Drive/Deep_Learning_Project/model/LSTM_iteration2'"
      ],
      "id": "vhaMtK_-vN-n",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzgxvZ6KL726"
      },
      "source": [
        "## Model Architecture"
      ],
      "id": "dzgxvZ6KL726"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TzPt7wGpiT5"
      },
      "source": [
        "In Iteration 2 we have considered only  1 LSTM layer"
      ],
      "id": "3TzPt7wGpiT5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuuB1HwAiXTc"
      },
      "source": [
        "def save_weights(epoch, model):\n",
        "    if not os.path.exists(MODEL_DIR):\n",
        "        os.makedirs(MODEL_DIR)\n",
        "    model.save_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n",
        "\n",
        "def load_weights(epoch, model):\n",
        "    model.load_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n",
        "\n",
        "def build_model(batch_size, seq_len, vocab_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 256, batch_input_shape=(batch_size, seq_len)))\n",
        "    model.add(LSTM(256, return_sequences=True, stateful=True))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(TimeDistributed(Dense(vocab_size))) \n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    return model"
      ],
      "id": "QuuB1HwAiXTc",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIyp8NstMH_i"
      },
      "source": [
        "## Single layer LSTM Training function"
      ],
      "id": "CIyp8NstMH_i"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzNjxYhvlU8d"
      },
      "source": [
        "def train(text, epochs=100, save_freq=10):\n",
        "\n",
        "    # Sorting the characters in the text file and assigning index numbers to each character. \n",
        "    # char_to_idx will be a dictionary of unique characters as key and index number is the value pair of dictionary\n",
        "    # To summarize we are comverting each character to a numerical index\n",
        "    \n",
        "    char_to_idx = { ch: i for (i, ch) in enumerate(sorted(list(set(text)))) }\n",
        "    print(\"Number of unique characters: \" + str(len(char_to_idx))) #86\n",
        "\n",
        "    ## Saving the char_to_idx to a json file\n",
        "    with open(os.path.join(DATA_DIR, 'char_to_idx.json'), 'w') as f:\n",
        "        json.dump(char_to_idx, f)\n",
        "\n",
        "    #3 Here we are creating index to character mapping, i.e. given an index we want to get the character for that index\n",
        "    idx_to_char = { i: ch for (ch, i) in char_to_idx.items() }\n",
        "    vocab_size = len(char_to_idx)\n",
        "\n",
        "    ######################################\n",
        "    ######### Model architecture #########\n",
        "    ######################################\n",
        "    \n",
        "    model = build_model(BATCH_SIZE, SEQ_LENGTH, vocab_size)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    ###################################\n",
        "    ###### Train data generation ######\n",
        "    ###################################\n",
        "    \n",
        "    #convert complete text into numerical indices\n",
        "    T = np.asarray([char_to_idx[c] for c in text], dtype=np.int64)\n",
        "\n",
        "    print(\"Length of text:\" + str(T.size)) #129,665\n",
        "\n",
        "    steps_per_epoch = (len(text) / BATCH_SIZE - 1) / SEQ_LENGTH  \n",
        "    \n",
        "    epc,losses, accs = [], [], []\n",
        "    ### This for loop will run for 100 epochs\n",
        "    for epoch in range(epochs):\n",
        "        print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
        "        \n",
        "        \n",
        "\n",
        "        # For each epoch it will generate a batch of X , Y values. For each batch we will train the model. \n",
        "        for i, (X, Y) in enumerate(read_batches(T, vocab_size)):\n",
        "            \n",
        "            #print(X);\n",
        "            \n",
        "            ## Details about train_on_batch here: https://keras.io/models/sequential/\n",
        "            loss, acc = model.train_on_batch(X, Y)\n",
        "            print('Batch {}: loss = {}, acc = {}'.format(i + 1, loss, acc))\n",
        "            epc.append(epoch + 1)\n",
        "            losses.append(loss)\n",
        "            accs.append(acc)\n",
        "        \n",
        "      \n",
        "        # Saving the model after every 10 epochs\n",
        "        if (epoch + 1) % save_freq == 0:\n",
        "            save_weights(epoch + 1, model)\n",
        "            print('Saved checkpoint to', 'weights.{}.h5'.format(epoch + 1))\n",
        "    \n",
        "    df1 = pd.DataFrame(epc)\n",
        "    df2 = pd.DataFrame(losses)\n",
        "    df3 = pd.DataFrame(accs)\n",
        "    frames = [df1,df2, df3]\n",
        "\n",
        "    result = pd.concat(frames, axis =1)\n",
        "    result.columns=['Epoch','Losses','Accuracy']\n",
        "\n",
        "    grouped_multiple = result.groupby(['Epoch'], as_index=False).agg({'Losses': 'mean'\n",
        "                                              ,'Accuracy':'mean'})\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return grouped_multiple"
      ],
      "id": "MzNjxYhvlU8d",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0GGd1BCOIpO"
      },
      "source": [
        "## Sequence generator build function"
      ],
      "id": "_0GGd1BCOIpO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obeVoSq-rBev"
      },
      "source": [
        "def build_model_seq_gen(unique_chars):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(unique_chars, 256, batch_input_shape=(1, 1)))\n",
        "    model.add(LSTM(256, return_sequences=True, stateful=True))\n",
        "    #model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(unique_chars))\n",
        "    model.add(Activation('softmax'))\n",
        "    plot_model(model, to_file='model.png')\n",
        "    return model"
      ],
      "id": "obeVoSq-rBev",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJOQhoUeOSB-"
      },
      "source": [
        "## Code for generating sample character sequences"
      ],
      "id": "UJOQhoUeOSB-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk8rluljs47v"
      },
      "source": [
        "def sample_seq_generator(epoch_num, character_index, seq_length):\n",
        "    with open(os.path.join(DATA_DIR, 'char_to_idx.json')) as f:\n",
        "        char_to_index = json.load(f)\n",
        "    index_to_char = {i:ch for ch, i in char_to_index.items()}\n",
        "    vocab_size = len(index_to_char)\n",
        "\n",
        "    ##########################################################\n",
        "    ######### Sequence generator model architecture  #########\n",
        "    ##########################################################\n",
        "    \n",
        "    model = build_model_seq_gen(vocab_size)\n",
        "    model.load_weights(MODEL_DIR + '/weights.{}.h5'.format(epoch_num))\n",
        "     \n",
        "    sequence_index = [character_index]\n",
        "    \n",
        "    for _ in range(seq_length):\n",
        "        batch = np.zeros((1, 1))\n",
        "        batch[0, 0] = sequence_index[-1]\n",
        "        \n",
        "        predicted_probs = model.predict(batch).reshape(-1)\n",
        "        #print(predicted_probs)\n",
        "        sample = np.random.choice(range(vocab_size), size = 1, p = predicted_probs)\n",
        "        \n",
        "        sequence_index.append(sample[0])\n",
        "    \n",
        "    seq = ''.join(index_to_char[c] for c in sequence_index)\n",
        "    return seq"
      ],
      "id": "Vk8rluljs47v",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfKo8CTvOWua"
      },
      "source": [
        "## Calling Train function"
      ],
      "id": "gfKo8CTvOWua"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ogoCOKpq__N",
        "outputId": "5f94f6a8-77c8-4eb0-fd2b-dd02d7170e98"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    epochs = 100\n",
        "    freq = 10\n",
        "    \n",
        "    ### Calling the train function to read the data from input.txt file \n",
        "    Train_epoch_loss_acc = train(open(os.path.join(DATA_DIR, filename)).read(), epochs, freq)"
      ],
      "id": "7ogoCOKpq__N",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch 13: loss = 0.3836827874183655, acc = 0.8720703125\n",
            "Batch 14: loss = 0.3852841854095459, acc = 0.8740234375\n",
            "Batch 15: loss = 0.3599187433719635, acc = 0.880859375\n",
            "Batch 16: loss = 0.4484732151031494, acc = 0.84765625\n",
            "Batch 17: loss = 0.43767279386520386, acc = 0.859375\n",
            "Batch 18: loss = 0.4159946143627167, acc = 0.8603515625\n",
            "Batch 19: loss = 0.39604824781417847, acc = 0.8701171875\n",
            "Batch 20: loss = 0.4148021936416626, acc = 0.8681640625\n",
            "Batch 21: loss = 0.3906278908252716, acc = 0.8740234375\n",
            "Batch 22: loss = 0.4137052893638611, acc = 0.849609375\n",
            "Batch 23: loss = 0.37692081928253174, acc = 0.8720703125\n",
            "Batch 24: loss = 0.3445672392845154, acc = 0.880859375\n",
            "Batch 25: loss = 0.40696319937705994, acc = 0.8623046875\n",
            "Batch 26: loss = 0.3818502426147461, acc = 0.873046875\n",
            "Batch 27: loss = 0.44986143708229065, acc = 0.8544921875\n",
            "Batch 28: loss = 0.40830063819885254, acc = 0.8505859375\n",
            "Batch 29: loss = 0.4129514694213867, acc = 0.865234375\n",
            "Batch 30: loss = 0.3723352253437042, acc = 0.876953125\n",
            "Batch 31: loss = 0.414266437292099, acc = 0.87109375\n",
            "Batch 32: loss = 0.4498177468776703, acc = 0.8583984375\n",
            "Batch 33: loss = 0.3752340078353882, acc = 0.8779296875\n",
            "Batch 34: loss = 0.4332631826400757, acc = 0.8603515625\n",
            "Batch 35: loss = 0.4251065254211426, acc = 0.85546875\n",
            "Batch 36: loss = 0.3706355094909668, acc = 0.8818359375\n",
            "Batch 37: loss = 0.3629140257835388, acc = 0.8828125\n",
            "Batch 38: loss = 0.3851310610771179, acc = 0.8779296875\n",
            "Batch 39: loss = 0.340833455324173, acc = 0.8935546875\n",
            "Batch 40: loss = 0.3766270875930786, acc = 0.8740234375\n",
            "Batch 41: loss = 0.39203405380249023, acc = 0.8701171875\n",
            "Batch 42: loss = 0.39707210659980774, acc = 0.87109375\n",
            "Batch 43: loss = 0.4520606994628906, acc = 0.8505859375\n",
            "Batch 44: loss = 0.34344935417175293, acc = 0.8818359375\n",
            "Batch 45: loss = 0.35345184803009033, acc = 0.8779296875\n",
            "Batch 46: loss = 0.34666889905929565, acc = 0.8916015625\n",
            "Batch 47: loss = 0.3775046169757843, acc = 0.873046875\n",
            "Batch 48: loss = 0.37508484721183777, acc = 0.869140625\n",
            "Batch 49: loss = 0.3826923966407776, acc = 0.8681640625\n",
            "Batch 50: loss = 0.3579879105091095, acc = 0.8720703125\n",
            "Batch 51: loss = 0.3626592755317688, acc = 0.8701171875\n",
            "Batch 52: loss = 0.41179999709129333, acc = 0.86328125\n",
            "Batch 53: loss = 0.39060255885124207, acc = 0.865234375\n",
            "Batch 54: loss = 0.3235071301460266, acc = 0.892578125\n",
            "Batch 55: loss = 0.3420162796974182, acc = 0.8955078125\n",
            "Batch 56: loss = 0.3962152898311615, acc = 0.8662109375\n",
            "Batch 57: loss = 0.4199967384338379, acc = 0.853515625\n",
            "Batch 58: loss = 0.4453287124633789, acc = 0.849609375\n",
            "Batch 59: loss = 0.3579491376876831, acc = 0.8837890625\n",
            "Batch 60: loss = 0.40988612174987793, acc = 0.8642578125\n",
            "Batch 61: loss = 0.358379065990448, acc = 0.8740234375\n",
            "Batch 62: loss = 0.44755008816719055, acc = 0.8583984375\n",
            "Batch 63: loss = 0.3836672604084015, acc = 0.873046875\n",
            "Batch 64: loss = 0.35948067903518677, acc = 0.884765625\n",
            "Batch 65: loss = 0.43487221002578735, acc = 0.8466796875\n",
            "Batch 66: loss = 0.404155433177948, acc = 0.8603515625\n",
            "Batch 67: loss = 0.4277229309082031, acc = 0.8603515625\n",
            "Batch 68: loss = 0.4097565710544586, acc = 0.8603515625\n",
            "Batch 69: loss = 0.4164690673351288, acc = 0.8671875\n",
            "Batch 70: loss = 0.46665680408477783, acc = 0.845703125\n",
            "Batch 71: loss = 0.43880513310432434, acc = 0.8515625\n",
            "Batch 72: loss = 0.38801759481430054, acc = 0.86328125\n",
            "Batch 73: loss = 0.425965815782547, acc = 0.8564453125\n",
            "Batch 74: loss = 0.41702213883399963, acc = 0.8544921875\n",
            "Batch 75: loss = 0.5209996104240417, acc = 0.837890625\n",
            "Batch 76: loss = 0.4418051838874817, acc = 0.8330078125\n",
            "Batch 77: loss = 0.39401593804359436, acc = 0.8681640625\n",
            "Batch 78: loss = 0.43619683384895325, acc = 0.8525390625\n",
            "Batch 79: loss = 0.3889376223087311, acc = 0.861328125\n",
            "Batch 80: loss = 0.4189933240413666, acc = 0.8447265625\n",
            "Batch 81: loss = 0.4367222189903259, acc = 0.8515625\n",
            "Batch 82: loss = 0.4023453891277313, acc = 0.8662109375\n",
            "Batch 83: loss = 0.37104809284210205, acc = 0.8720703125\n",
            "Batch 84: loss = 0.44295430183410645, acc = 0.845703125\n",
            "Batch 85: loss = 0.47056716680526733, acc = 0.84375\n",
            "Batch 86: loss = 0.40495234727859497, acc = 0.8603515625\n",
            "Batch 87: loss = 0.44694364070892334, acc = 0.845703125\n",
            "Batch 88: loss = 0.45372581481933594, acc = 0.8544921875\n",
            "Batch 89: loss = 0.4138355851173401, acc = 0.8623046875\n",
            "Batch 90: loss = 0.45458394289016724, acc = 0.84765625\n",
            "Batch 91: loss = 0.4492182731628418, acc = 0.84765625\n",
            "Batch 92: loss = 0.45863544940948486, acc = 0.84375\n",
            "Batch 93: loss = 0.3826778531074524, acc = 0.888671875\n",
            "Batch 94: loss = 0.402828574180603, acc = 0.8720703125\n",
            "Batch 95: loss = 0.4013638496398926, acc = 0.8740234375\n",
            "Batch 96: loss = 0.4569164216518402, acc = 0.83203125\n",
            "Batch 97: loss = 0.4520708918571472, acc = 0.8525390625\n",
            "Batch 98: loss = 0.3983226418495178, acc = 0.8671875\n",
            "Batch 99: loss = 0.45637452602386475, acc = 0.849609375\n",
            "Batch 100: loss = 0.4542770981788635, acc = 0.8447265625\n",
            "Batch 101: loss = 0.3950539231300354, acc = 0.8671875\n",
            "Batch 102: loss = 0.45350244641304016, acc = 0.8564453125\n",
            "Batch 103: loss = 0.44350165128707886, acc = 0.8505859375\n",
            "Batch 104: loss = 0.3634277880191803, acc = 0.8720703125\n",
            "Batch 105: loss = 0.3764440715312958, acc = 0.8779296875\n",
            "Batch 106: loss = 0.4276549220085144, acc = 0.845703125\n",
            "Batch 107: loss = 0.43919116258621216, acc = 0.8466796875\n",
            "Batch 108: loss = 0.38817405700683594, acc = 0.8759765625\n",
            "Batch 109: loss = 0.40913087129592896, acc = 0.85546875\n",
            "Batch 110: loss = 0.3712157905101776, acc = 0.87890625\n",
            "Batch 111: loss = 0.44112157821655273, acc = 0.86328125\n",
            "Batch 112: loss = 0.482220858335495, acc = 0.8369140625\n",
            "Batch 113: loss = 0.4236820936203003, acc = 0.8505859375\n",
            "Batch 114: loss = 0.4217120409011841, acc = 0.8447265625\n",
            "Batch 115: loss = 0.4379767179489136, acc = 0.8662109375\n",
            "Batch 116: loss = 0.46484479308128357, acc = 0.8388671875\n",
            "Batch 117: loss = 0.39315804839134216, acc = 0.873046875\n",
            "Batch 118: loss = 0.3690619170665741, acc = 0.884765625\n",
            "Batch 119: loss = 0.41168028116226196, acc = 0.8662109375\n",
            "Batch 120: loss = 0.3981521427631378, acc = 0.861328125\n",
            "Batch 121: loss = 0.3992081582546234, acc = 0.876953125\n",
            "Batch 122: loss = 0.41927579045295715, acc = 0.8623046875\n",
            "Batch 123: loss = 0.399873286485672, acc = 0.87109375\n",
            "Batch 124: loss = 0.45921918749809265, acc = 0.84765625\n",
            "Batch 125: loss = 0.42234230041503906, acc = 0.8583984375\n",
            "Batch 126: loss = 0.4243066608905792, acc = 0.8671875\n",
            "\n",
            "Epoch 63/100\n",
            "Batch 1: loss = 0.5697169303894043, acc = 0.8232421875\n",
            "Batch 2: loss = 0.43251338601112366, acc = 0.86328125\n",
            "Batch 3: loss = 0.4392288625240326, acc = 0.861328125\n",
            "Batch 4: loss = 0.39701929688453674, acc = 0.8740234375\n",
            "Batch 5: loss = 0.41497188806533813, acc = 0.865234375\n",
            "Batch 6: loss = 0.45577335357666016, acc = 0.8525390625\n",
            "Batch 7: loss = 0.38477230072021484, acc = 0.8662109375\n",
            "Batch 8: loss = 0.4028512239456177, acc = 0.8681640625\n",
            "Batch 9: loss = 0.4071882665157318, acc = 0.869140625\n",
            "Batch 10: loss = 0.339304119348526, acc = 0.9013671875\n",
            "Batch 11: loss = 0.43014097213745117, acc = 0.869140625\n",
            "Batch 12: loss = 0.39990702271461487, acc = 0.8525390625\n",
            "Batch 13: loss = 0.39022624492645264, acc = 0.865234375\n",
            "Batch 14: loss = 0.3786802291870117, acc = 0.8857421875\n",
            "Batch 15: loss = 0.3749852180480957, acc = 0.8828125\n",
            "Batch 16: loss = 0.44567370414733887, acc = 0.8515625\n",
            "Batch 17: loss = 0.3994288444519043, acc = 0.8544921875\n",
            "Batch 18: loss = 0.4620898365974426, acc = 0.8564453125\n",
            "Batch 19: loss = 0.39545494318008423, acc = 0.8701171875\n",
            "Batch 20: loss = 0.39533531665802, acc = 0.873046875\n",
            "Batch 21: loss = 0.40124422311782837, acc = 0.8662109375\n",
            "Batch 22: loss = 0.39161860942840576, acc = 0.869140625\n",
            "Batch 23: loss = 0.4134356677532196, acc = 0.8681640625\n",
            "Batch 24: loss = 0.3636971712112427, acc = 0.8828125\n",
            "Batch 25: loss = 0.40622109174728394, acc = 0.865234375\n",
            "Batch 26: loss = 0.3786797523498535, acc = 0.865234375\n",
            "Batch 27: loss = 0.45807284116744995, acc = 0.84765625\n",
            "Batch 28: loss = 0.42162221670150757, acc = 0.853515625\n",
            "Batch 29: loss = 0.4042084515094757, acc = 0.86328125\n",
            "Batch 30: loss = 0.37624025344848633, acc = 0.865234375\n",
            "Batch 31: loss = 0.4154043197631836, acc = 0.87109375\n",
            "Batch 32: loss = 0.4910561740398407, acc = 0.84375\n",
            "Batch 33: loss = 0.41668999195098877, acc = 0.857421875\n",
            "Batch 34: loss = 0.438371866941452, acc = 0.8466796875\n",
            "Batch 35: loss = 0.39332324266433716, acc = 0.875\n",
            "Batch 36: loss = 0.3507847189903259, acc = 0.88671875\n",
            "Batch 37: loss = 0.36069703102111816, acc = 0.8916015625\n",
            "Batch 38: loss = 0.371467649936676, acc = 0.87109375\n",
            "Batch 39: loss = 0.38419756293296814, acc = 0.8642578125\n",
            "Batch 40: loss = 0.4074528217315674, acc = 0.8564453125\n",
            "Batch 41: loss = 0.37887150049209595, acc = 0.8681640625\n",
            "Batch 42: loss = 0.39260056614875793, acc = 0.8681640625\n",
            "Batch 43: loss = 0.4185641407966614, acc = 0.8544921875\n",
            "Batch 44: loss = 0.3464433252811432, acc = 0.8798828125\n",
            "Batch 45: loss = 0.3578794598579407, acc = 0.876953125\n",
            "Batch 46: loss = 0.3486320376396179, acc = 0.8916015625\n",
            "Batch 47: loss = 0.3537011742591858, acc = 0.8828125\n",
            "Batch 48: loss = 0.3681243360042572, acc = 0.8857421875\n",
            "Batch 49: loss = 0.3883018493652344, acc = 0.8740234375\n",
            "Batch 50: loss = 0.35806116461753845, acc = 0.876953125\n",
            "Batch 51: loss = 0.3847329318523407, acc = 0.87890625\n",
            "Batch 52: loss = 0.4066312909126282, acc = 0.8583984375\n",
            "Batch 53: loss = 0.3737962245941162, acc = 0.8779296875\n",
            "Batch 54: loss = 0.3178308308124542, acc = 0.908203125\n",
            "Batch 55: loss = 0.3445940613746643, acc = 0.88671875\n",
            "Batch 56: loss = 0.4201356768608093, acc = 0.859375\n",
            "Batch 57: loss = 0.44827887415885925, acc = 0.84765625\n",
            "Batch 58: loss = 0.43437787890434265, acc = 0.8447265625\n",
            "Batch 59: loss = 0.3359127342700958, acc = 0.89453125\n",
            "Batch 60: loss = 0.38147127628326416, acc = 0.8759765625\n",
            "Batch 61: loss = 0.3983820974826813, acc = 0.8583984375\n",
            "Batch 62: loss = 0.48224636912345886, acc = 0.837890625\n",
            "Batch 63: loss = 0.41530242562294006, acc = 0.8544921875\n",
            "Batch 64: loss = 0.35652077198028564, acc = 0.884765625\n",
            "Batch 65: loss = 0.4162115454673767, acc = 0.865234375\n",
            "Batch 66: loss = 0.43287143111228943, acc = 0.853515625\n",
            "Batch 67: loss = 0.41245201230049133, acc = 0.8603515625\n",
            "Batch 68: loss = 0.43556469678878784, acc = 0.84765625\n",
            "Batch 69: loss = 0.39433756470680237, acc = 0.8701171875\n",
            "Batch 70: loss = 0.43350571393966675, acc = 0.8623046875\n",
            "Batch 71: loss = 0.4242057204246521, acc = 0.8505859375\n",
            "Batch 72: loss = 0.38079380989074707, acc = 0.8720703125\n",
            "Batch 73: loss = 0.4434643089771271, acc = 0.8486328125\n",
            "Batch 74: loss = 0.41482359170913696, acc = 0.8603515625\n",
            "Batch 75: loss = 0.5011515021324158, acc = 0.8369140625\n",
            "Batch 76: loss = 0.4250900447368622, acc = 0.85546875\n",
            "Batch 77: loss = 0.3993651866912842, acc = 0.86328125\n",
            "Batch 78: loss = 0.4624805152416229, acc = 0.8447265625\n",
            "Batch 79: loss = 0.4249381721019745, acc = 0.84375\n",
            "Batch 80: loss = 0.3970024585723877, acc = 0.849609375\n",
            "Batch 81: loss = 0.4129888415336609, acc = 0.869140625\n",
            "Batch 82: loss = 0.39601123332977295, acc = 0.861328125\n",
            "Batch 83: loss = 0.359620064496994, acc = 0.8720703125\n",
            "Batch 84: loss = 0.4052336812019348, acc = 0.869140625\n",
            "Batch 85: loss = 0.4409679174423218, acc = 0.8623046875\n",
            "Batch 86: loss = 0.4166625142097473, acc = 0.865234375\n",
            "Batch 87: loss = 0.4348369836807251, acc = 0.8603515625\n",
            "Batch 88: loss = 0.4332825243473053, acc = 0.8544921875\n",
            "Batch 89: loss = 0.3835298418998718, acc = 0.876953125\n",
            "Batch 90: loss = 0.4348093569278717, acc = 0.8515625\n",
            "Batch 91: loss = 0.45565780997276306, acc = 0.8486328125\n",
            "Batch 92: loss = 0.4421001076698303, acc = 0.8505859375\n",
            "Batch 93: loss = 0.341548353433609, acc = 0.8955078125\n",
            "Batch 94: loss = 0.410641610622406, acc = 0.8623046875\n",
            "Batch 95: loss = 0.3915776014328003, acc = 0.8642578125\n",
            "Batch 96: loss = 0.4431253671646118, acc = 0.8515625\n",
            "Batch 97: loss = 0.40141305327415466, acc = 0.8681640625\n",
            "Batch 98: loss = 0.4129602611064911, acc = 0.8671875\n",
            "Batch 99: loss = 0.4346694350242615, acc = 0.85546875\n",
            "Batch 100: loss = 0.48076093196868896, acc = 0.82421875\n",
            "Batch 101: loss = 0.39426174759864807, acc = 0.8671875\n",
            "Batch 102: loss = 0.44738471508026123, acc = 0.8544921875\n",
            "Batch 103: loss = 0.4491662383079529, acc = 0.8515625\n",
            "Batch 104: loss = 0.3751939535140991, acc = 0.869140625\n",
            "Batch 105: loss = 0.3609767556190491, acc = 0.875\n",
            "Batch 106: loss = 0.40473467111587524, acc = 0.8681640625\n",
            "Batch 107: loss = 0.4211857318878174, acc = 0.8623046875\n",
            "Batch 108: loss = 0.4059050679206848, acc = 0.865234375\n",
            "Batch 109: loss = 0.4079954922199249, acc = 0.8505859375\n",
            "Batch 110: loss = 0.35707545280456543, acc = 0.8818359375\n",
            "Batch 111: loss = 0.4034031629562378, acc = 0.8671875\n",
            "Batch 112: loss = 0.4714619815349579, acc = 0.84765625\n",
            "Batch 113: loss = 0.4085184633731842, acc = 0.861328125\n",
            "Batch 114: loss = 0.42805442214012146, acc = 0.8671875\n",
            "Batch 115: loss = 0.40229326486587524, acc = 0.8740234375\n",
            "Batch 116: loss = 0.43219515681266785, acc = 0.8486328125\n",
            "Batch 117: loss = 0.3868551254272461, acc = 0.8671875\n",
            "Batch 118: loss = 0.37097644805908203, acc = 0.875\n",
            "Batch 119: loss = 0.3682708442211151, acc = 0.8818359375\n",
            "Batch 120: loss = 0.3833208382129669, acc = 0.8759765625\n",
            "Batch 121: loss = 0.39376407861709595, acc = 0.8759765625\n",
            "Batch 122: loss = 0.38742008805274963, acc = 0.8642578125\n",
            "Batch 123: loss = 0.4149245619773865, acc = 0.865234375\n",
            "Batch 124: loss = 0.4692000150680542, acc = 0.8359375\n",
            "Batch 125: loss = 0.42697587609291077, acc = 0.849609375\n",
            "Batch 126: loss = 0.43066510558128357, acc = 0.8515625\n",
            "\n",
            "Epoch 64/100\n",
            "Batch 1: loss = 0.5417166352272034, acc = 0.8388671875\n",
            "Batch 2: loss = 0.417677104473114, acc = 0.8740234375\n",
            "Batch 3: loss = 0.40509870648384094, acc = 0.875\n",
            "Batch 4: loss = 0.4025527834892273, acc = 0.8701171875\n",
            "Batch 5: loss = 0.38309186697006226, acc = 0.87109375\n",
            "Batch 6: loss = 0.3995714485645294, acc = 0.8720703125\n",
            "Batch 7: loss = 0.39097583293914795, acc = 0.865234375\n",
            "Batch 8: loss = 0.3827275335788727, acc = 0.8818359375\n",
            "Batch 9: loss = 0.38509559631347656, acc = 0.87890625\n",
            "Batch 10: loss = 0.351244181394577, acc = 0.890625\n",
            "Batch 11: loss = 0.4063069224357605, acc = 0.87109375\n",
            "Batch 12: loss = 0.3940673768520355, acc = 0.861328125\n",
            "Batch 13: loss = 0.35497474670410156, acc = 0.8857421875\n",
            "Batch 14: loss = 0.398533433675766, acc = 0.8671875\n",
            "Batch 15: loss = 0.3627059757709503, acc = 0.8798828125\n",
            "Batch 16: loss = 0.41388368606567383, acc = 0.8671875\n",
            "Batch 17: loss = 0.43079710006713867, acc = 0.853515625\n",
            "Batch 18: loss = 0.4431249499320984, acc = 0.86328125\n",
            "Batch 19: loss = 0.37510526180267334, acc = 0.87890625\n",
            "Batch 20: loss = 0.3869641423225403, acc = 0.8623046875\n",
            "Batch 21: loss = 0.40959376096725464, acc = 0.8642578125\n",
            "Batch 22: loss = 0.37483304738998413, acc = 0.8818359375\n",
            "Batch 23: loss = 0.39934760332107544, acc = 0.87109375\n",
            "Batch 24: loss = 0.37198734283447266, acc = 0.88671875\n",
            "Batch 25: loss = 0.3867188096046448, acc = 0.8642578125\n",
            "Batch 26: loss = 0.3645781874656677, acc = 0.8720703125\n",
            "Batch 27: loss = 0.4436764717102051, acc = 0.849609375\n",
            "Batch 28: loss = 0.39362218976020813, acc = 0.8759765625\n",
            "Batch 29: loss = 0.4265992045402527, acc = 0.8505859375\n",
            "Batch 30: loss = 0.3550666868686676, acc = 0.880859375\n",
            "Batch 31: loss = 0.4345604479312897, acc = 0.865234375\n",
            "Batch 32: loss = 0.4929259121417999, acc = 0.85546875\n",
            "Batch 33: loss = 0.3827402591705322, acc = 0.8681640625\n",
            "Batch 34: loss = 0.42078784108161926, acc = 0.865234375\n",
            "Batch 35: loss = 0.38426730036735535, acc = 0.8681640625\n",
            "Batch 36: loss = 0.3441731929779053, acc = 0.87890625\n",
            "Batch 37: loss = 0.34430548548698425, acc = 0.8935546875\n",
            "Batch 38: loss = 0.3806062340736389, acc = 0.8837890625\n",
            "Batch 39: loss = 0.38039106130599976, acc = 0.873046875\n",
            "Batch 40: loss = 0.3599819839000702, acc = 0.876953125\n",
            "Batch 41: loss = 0.36780306696891785, acc = 0.8837890625\n",
            "Batch 42: loss = 0.40726998448371887, acc = 0.861328125\n",
            "Batch 43: loss = 0.4457971751689911, acc = 0.8505859375\n",
            "Batch 44: loss = 0.36554062366485596, acc = 0.8857421875\n",
            "Batch 45: loss = 0.3425982594490051, acc = 0.8896484375\n",
            "Batch 46: loss = 0.33210358023643494, acc = 0.8837890625\n",
            "Batch 47: loss = 0.3598729968070984, acc = 0.87890625\n",
            "Batch 48: loss = 0.39987653493881226, acc = 0.8662109375\n",
            "Batch 49: loss = 0.363899290561676, acc = 0.888671875\n",
            "Batch 50: loss = 0.3543371856212616, acc = 0.88671875\n",
            "Batch 51: loss = 0.38019394874572754, acc = 0.865234375\n",
            "Batch 52: loss = 0.3884759545326233, acc = 0.8779296875\n",
            "Batch 53: loss = 0.3929028809070587, acc = 0.8740234375\n",
            "Batch 54: loss = 0.33078011870384216, acc = 0.88671875\n",
            "Batch 55: loss = 0.34478047490119934, acc = 0.890625\n",
            "Batch 56: loss = 0.397207647562027, acc = 0.8720703125\n",
            "Batch 57: loss = 0.43137484788894653, acc = 0.853515625\n",
            "Batch 58: loss = 0.44977155327796936, acc = 0.845703125\n",
            "Batch 59: loss = 0.340277761220932, acc = 0.8837890625\n",
            "Batch 60: loss = 0.41577839851379395, acc = 0.857421875\n",
            "Batch 61: loss = 0.35424113273620605, acc = 0.8818359375\n",
            "Batch 62: loss = 0.44620397686958313, acc = 0.8564453125\n",
            "Batch 63: loss = 0.4191904366016388, acc = 0.865234375\n",
            "Batch 64: loss = 0.3703051209449768, acc = 0.8798828125\n",
            "Batch 65: loss = 0.4093708395957947, acc = 0.859375\n",
            "Batch 66: loss = 0.42720693349838257, acc = 0.8427734375\n",
            "Batch 67: loss = 0.4231589436531067, acc = 0.859375\n",
            "Batch 68: loss = 0.429373562335968, acc = 0.8623046875\n",
            "Batch 69: loss = 0.3956604599952698, acc = 0.8671875\n",
            "Batch 70: loss = 0.45632046461105347, acc = 0.8515625\n",
            "Batch 71: loss = 0.4407076835632324, acc = 0.859375\n",
            "Batch 72: loss = 0.3520350456237793, acc = 0.876953125\n",
            "Batch 73: loss = 0.42251166701316833, acc = 0.861328125\n",
            "Batch 74: loss = 0.4291191101074219, acc = 0.853515625\n",
            "Batch 75: loss = 0.463872492313385, acc = 0.8505859375\n",
            "Batch 76: loss = 0.4262462556362152, acc = 0.849609375\n",
            "Batch 77: loss = 0.39642205834388733, acc = 0.8671875\n",
            "Batch 78: loss = 0.4179340600967407, acc = 0.8525390625\n",
            "Batch 79: loss = 0.3898857533931732, acc = 0.861328125\n",
            "Batch 80: loss = 0.36598828434944153, acc = 0.8740234375\n",
            "Batch 81: loss = 0.41111865639686584, acc = 0.86328125\n",
            "Batch 82: loss = 0.37765830755233765, acc = 0.8759765625\n",
            "Batch 83: loss = 0.3621482253074646, acc = 0.8798828125\n",
            "Batch 84: loss = 0.41231557726860046, acc = 0.86328125\n",
            "Batch 85: loss = 0.41485193371772766, acc = 0.861328125\n",
            "Batch 86: loss = 0.3953472971916199, acc = 0.8681640625\n",
            "Batch 87: loss = 0.4058230221271515, acc = 0.859375\n",
            "Batch 88: loss = 0.4353438913822174, acc = 0.8505859375\n",
            "Batch 89: loss = 0.3737682104110718, acc = 0.8681640625\n",
            "Batch 90: loss = 0.406338632106781, acc = 0.857421875\n",
            "Batch 91: loss = 0.4195620119571686, acc = 0.8603515625\n",
            "Batch 92: loss = 0.438748300075531, acc = 0.8505859375\n",
            "Batch 93: loss = 0.37741902470588684, acc = 0.8779296875\n",
            "Batch 94: loss = 0.4258963465690613, acc = 0.8701171875\n",
            "Batch 95: loss = 0.3966085612773895, acc = 0.86328125\n",
            "Batch 96: loss = 0.46671438217163086, acc = 0.83984375\n",
            "Batch 97: loss = 0.40854203701019287, acc = 0.8662109375\n",
            "Batch 98: loss = 0.3911040425300598, acc = 0.8671875\n",
            "Batch 99: loss = 0.4173964262008667, acc = 0.8466796875\n",
            "Batch 100: loss = 0.4633433222770691, acc = 0.841796875\n",
            "Batch 101: loss = 0.3572649359703064, acc = 0.8837890625\n",
            "Batch 102: loss = 0.4118269383907318, acc = 0.857421875\n",
            "Batch 103: loss = 0.42610207200050354, acc = 0.8564453125\n",
            "Batch 104: loss = 0.35259130597114563, acc = 0.875\n",
            "Batch 105: loss = 0.37334802746772766, acc = 0.8681640625\n",
            "Batch 106: loss = 0.44129952788352966, acc = 0.857421875\n",
            "Batch 107: loss = 0.40349021553993225, acc = 0.8662109375\n",
            "Batch 108: loss = 0.3851490020751953, acc = 0.865234375\n",
            "Batch 109: loss = 0.3928857445716858, acc = 0.869140625\n",
            "Batch 110: loss = 0.3620186150074005, acc = 0.87890625\n",
            "Batch 111: loss = 0.4077785015106201, acc = 0.86328125\n",
            "Batch 112: loss = 0.4565916061401367, acc = 0.85546875\n",
            "Batch 113: loss = 0.4063403010368347, acc = 0.87109375\n",
            "Batch 114: loss = 0.4554923474788666, acc = 0.84375\n",
            "Batch 115: loss = 0.40999549627304077, acc = 0.8671875\n",
            "Batch 116: loss = 0.4316234290599823, acc = 0.861328125\n",
            "Batch 117: loss = 0.39818477630615234, acc = 0.8818359375\n",
            "Batch 118: loss = 0.3577842116355896, acc = 0.8876953125\n",
            "Batch 119: loss = 0.40444955229759216, acc = 0.8662109375\n",
            "Batch 120: loss = 0.376624196767807, acc = 0.8720703125\n",
            "Batch 121: loss = 0.4058661162853241, acc = 0.8681640625\n",
            "Batch 122: loss = 0.3867923617362976, acc = 0.8720703125\n",
            "Batch 123: loss = 0.39916083216667175, acc = 0.8603515625\n",
            "Batch 124: loss = 0.43762093782424927, acc = 0.8486328125\n",
            "Batch 125: loss = 0.3955106735229492, acc = 0.8603515625\n",
            "Batch 126: loss = 0.40313372015953064, acc = 0.865234375\n",
            "\n",
            "Epoch 65/100\n",
            "Batch 1: loss = 0.5415644645690918, acc = 0.84375\n",
            "Batch 2: loss = 0.4382546842098236, acc = 0.8603515625\n",
            "Batch 3: loss = 0.39908984303474426, acc = 0.876953125\n",
            "Batch 4: loss = 0.3907031714916229, acc = 0.8779296875\n",
            "Batch 5: loss = 0.4044366180896759, acc = 0.865234375\n",
            "Batch 6: loss = 0.43227851390838623, acc = 0.8681640625\n",
            "Batch 7: loss = 0.38332754373550415, acc = 0.8720703125\n",
            "Batch 8: loss = 0.391469269990921, acc = 0.861328125\n",
            "Batch 9: loss = 0.36887127161026, acc = 0.8759765625\n",
            "Batch 10: loss = 0.322376549243927, acc = 0.888671875\n",
            "Batch 11: loss = 0.41704994440078735, acc = 0.86328125\n",
            "Batch 12: loss = 0.39431285858154297, acc = 0.86328125\n",
            "Batch 13: loss = 0.36391177773475647, acc = 0.876953125\n",
            "Batch 14: loss = 0.35974109172821045, acc = 0.8974609375\n",
            "Batch 15: loss = 0.3900314271450043, acc = 0.869140625\n",
            "Batch 16: loss = 0.4165295362472534, acc = 0.8681640625\n",
            "Batch 17: loss = 0.41615524888038635, acc = 0.861328125\n",
            "Batch 18: loss = 0.40837836265563965, acc = 0.8642578125\n",
            "Batch 19: loss = 0.3998030126094818, acc = 0.8740234375\n",
            "Batch 20: loss = 0.38305625319480896, acc = 0.8720703125\n",
            "Batch 21: loss = 0.4203853905200958, acc = 0.8642578125\n",
            "Batch 22: loss = 0.38020968437194824, acc = 0.8720703125\n",
            "Batch 23: loss = 0.4058998227119446, acc = 0.86328125\n",
            "Batch 24: loss = 0.345162957906723, acc = 0.8798828125\n",
            "Batch 25: loss = 0.385672390460968, acc = 0.8759765625\n",
            "Batch 26: loss = 0.3572486937046051, acc = 0.87890625\n",
            "Batch 27: loss = 0.4271061420440674, acc = 0.861328125\n",
            "Batch 28: loss = 0.37820908427238464, acc = 0.873046875\n",
            "Batch 29: loss = 0.39508408308029175, acc = 0.8798828125\n",
            "Batch 30: loss = 0.37781858444213867, acc = 0.8720703125\n",
            "Batch 31: loss = 0.4104069471359253, acc = 0.869140625\n",
            "Batch 32: loss = 0.44800230860710144, acc = 0.861328125\n",
            "Batch 33: loss = 0.3692006468772888, acc = 0.87890625\n",
            "Batch 34: loss = 0.40236037969589233, acc = 0.8759765625\n",
            "Batch 35: loss = 0.3747336268424988, acc = 0.869140625\n",
            "Batch 36: loss = 0.3387550413608551, acc = 0.8818359375\n",
            "Batch 37: loss = 0.3351464569568634, acc = 0.904296875\n",
            "Batch 38: loss = 0.3803551197052002, acc = 0.8671875\n",
            "Batch 39: loss = 0.340440034866333, acc = 0.900390625\n",
            "Batch 40: loss = 0.3557066321372986, acc = 0.888671875\n",
            "Batch 41: loss = 0.35832229256629944, acc = 0.8828125\n",
            "Batch 42: loss = 0.3913350999355316, acc = 0.869140625\n",
            "Batch 43: loss = 0.44098350405693054, acc = 0.8408203125\n",
            "Batch 44: loss = 0.32000797986984253, acc = 0.8896484375\n",
            "Batch 45: loss = 0.346020370721817, acc = 0.880859375\n",
            "Batch 46: loss = 0.34477177262306213, acc = 0.8837890625\n",
            "Batch 47: loss = 0.36610883474349976, acc = 0.8828125\n",
            "Batch 48: loss = 0.3795384168624878, acc = 0.8671875\n",
            "Batch 49: loss = 0.35084259510040283, acc = 0.8916015625\n",
            "Batch 50: loss = 0.3600733280181885, acc = 0.8837890625\n",
            "Batch 51: loss = 0.3669629693031311, acc = 0.87109375\n",
            "Batch 52: loss = 0.403300940990448, acc = 0.8662109375\n",
            "Batch 53: loss = 0.3896128237247467, acc = 0.876953125\n",
            "Batch 54: loss = 0.3429734706878662, acc = 0.8896484375\n",
            "Batch 55: loss = 0.30215194821357727, acc = 0.9130859375\n",
            "Batch 56: loss = 0.4120591878890991, acc = 0.8544921875\n",
            "Batch 57: loss = 0.41178858280181885, acc = 0.857421875\n",
            "Batch 58: loss = 0.3951873183250427, acc = 0.873046875\n",
            "Batch 59: loss = 0.3133126199245453, acc = 0.888671875\n",
            "Batch 60: loss = 0.40975043177604675, acc = 0.85546875\n",
            "Batch 61: loss = 0.366273432970047, acc = 0.888671875\n",
            "Batch 62: loss = 0.42153042554855347, acc = 0.865234375\n",
            "Batch 63: loss = 0.3939327597618103, acc = 0.875\n",
            "Batch 64: loss = 0.34733471274375916, acc = 0.8779296875\n",
            "Batch 65: loss = 0.38945940136909485, acc = 0.861328125\n",
            "Batch 66: loss = 0.38293492794036865, acc = 0.8662109375\n",
            "Batch 67: loss = 0.3926734924316406, acc = 0.8671875\n",
            "Batch 68: loss = 0.4170854687690735, acc = 0.8642578125\n",
            "Batch 69: loss = 0.3810839354991913, acc = 0.8837890625\n",
            "Batch 70: loss = 0.4385196566581726, acc = 0.849609375\n",
            "Batch 71: loss = 0.41353920102119446, acc = 0.8623046875\n",
            "Batch 72: loss = 0.3805822730064392, acc = 0.8720703125\n",
            "Batch 73: loss = 0.4369864761829376, acc = 0.857421875\n",
            "Batch 74: loss = 0.42654165625572205, acc = 0.84765625\n",
            "Batch 75: loss = 0.4684660732746124, acc = 0.8427734375\n",
            "Batch 76: loss = 0.43961697816848755, acc = 0.853515625\n",
            "Batch 77: loss = 0.4180978536605835, acc = 0.8603515625\n",
            "Batch 78: loss = 0.4443819224834442, acc = 0.8544921875\n",
            "Batch 79: loss = 0.38960081338882446, acc = 0.865234375\n",
            "Batch 80: loss = 0.36898940801620483, acc = 0.873046875\n",
            "Batch 81: loss = 0.38386356830596924, acc = 0.8681640625\n",
            "Batch 82: loss = 0.3687235414981842, acc = 0.87890625\n",
            "Batch 83: loss = 0.35361552238464355, acc = 0.8759765625\n",
            "Batch 84: loss = 0.44626158475875854, acc = 0.84765625\n",
            "Batch 85: loss = 0.4325854182243347, acc = 0.8583984375\n",
            "Batch 86: loss = 0.38014474511146545, acc = 0.8740234375\n",
            "Batch 87: loss = 0.391735315322876, acc = 0.865234375\n",
            "Batch 88: loss = 0.44443273544311523, acc = 0.8486328125\n",
            "Batch 89: loss = 0.3856346011161804, acc = 0.8623046875\n",
            "Batch 90: loss = 0.4051702916622162, acc = 0.86328125\n",
            "Batch 91: loss = 0.4314989745616913, acc = 0.8486328125\n",
            "Batch 92: loss = 0.4402920603752136, acc = 0.8486328125\n",
            "Batch 93: loss = 0.3636181950569153, acc = 0.8720703125\n",
            "Batch 94: loss = 0.3851475417613983, acc = 0.884765625\n",
            "Batch 95: loss = 0.38107967376708984, acc = 0.875\n",
            "Batch 96: loss = 0.44769641757011414, acc = 0.849609375\n",
            "Batch 97: loss = 0.3835839331150055, acc = 0.8740234375\n",
            "Batch 98: loss = 0.3615490198135376, acc = 0.875\n",
            "Batch 99: loss = 0.42208975553512573, acc = 0.857421875\n",
            "Batch 100: loss = 0.45420944690704346, acc = 0.8466796875\n",
            "Batch 101: loss = 0.37867408990859985, acc = 0.8818359375\n",
            "Batch 102: loss = 0.4042529761791229, acc = 0.8623046875\n",
            "Batch 103: loss = 0.43941688537597656, acc = 0.849609375\n",
            "Batch 104: loss = 0.36410951614379883, acc = 0.8671875\n",
            "Batch 105: loss = 0.36888203024864197, acc = 0.8671875\n",
            "Batch 106: loss = 0.37441182136535645, acc = 0.876953125\n",
            "Batch 107: loss = 0.3919677138328552, acc = 0.8583984375\n",
            "Batch 108: loss = 0.4091896414756775, acc = 0.8701171875\n",
            "Batch 109: loss = 0.38277968764305115, acc = 0.8671875\n",
            "Batch 110: loss = 0.3802521228790283, acc = 0.869140625\n",
            "Batch 111: loss = 0.4134848713874817, acc = 0.873046875\n",
            "Batch 112: loss = 0.45295917987823486, acc = 0.85546875\n",
            "Batch 113: loss = 0.43628984689712524, acc = 0.8564453125\n",
            "Batch 114: loss = 0.4453442692756653, acc = 0.8583984375\n",
            "Batch 115: loss = 0.40018951892852783, acc = 0.8720703125\n",
            "Batch 116: loss = 0.437551885843277, acc = 0.8583984375\n",
            "Batch 117: loss = 0.38373124599456787, acc = 0.875\n",
            "Batch 118: loss = 0.3772351145744324, acc = 0.888671875\n",
            "Batch 119: loss = 0.382149338722229, acc = 0.869140625\n",
            "Batch 120: loss = 0.40013012290000916, acc = 0.8798828125\n",
            "Batch 121: loss = 0.3631919324398041, acc = 0.8740234375\n",
            "Batch 122: loss = 0.37534403800964355, acc = 0.876953125\n",
            "Batch 123: loss = 0.4137498140335083, acc = 0.86328125\n",
            "Batch 124: loss = 0.40343549847602844, acc = 0.8603515625\n",
            "Batch 125: loss = 0.42226359248161316, acc = 0.857421875\n",
            "Batch 126: loss = 0.3967479467391968, acc = 0.8740234375\n",
            "\n",
            "Epoch 66/100\n",
            "Batch 1: loss = 0.559394359588623, acc = 0.8271484375\n",
            "Batch 2: loss = 0.4068215787410736, acc = 0.875\n",
            "Batch 3: loss = 0.4233432412147522, acc = 0.8583984375\n",
            "Batch 4: loss = 0.3715830445289612, acc = 0.884765625\n",
            "Batch 5: loss = 0.37593868374824524, acc = 0.880859375\n",
            "Batch 6: loss = 0.4033457934856415, acc = 0.8623046875\n",
            "Batch 7: loss = 0.3979685604572296, acc = 0.8671875\n",
            "Batch 8: loss = 0.3914323151111603, acc = 0.875\n",
            "Batch 9: loss = 0.35234174132347107, acc = 0.8818359375\n",
            "Batch 10: loss = 0.3302963376045227, acc = 0.8955078125\n",
            "Batch 11: loss = 0.44668227434158325, acc = 0.8525390625\n",
            "Batch 12: loss = 0.4191935658454895, acc = 0.8427734375\n",
            "Batch 13: loss = 0.38613811135292053, acc = 0.873046875\n",
            "Batch 14: loss = 0.4050765335559845, acc = 0.876953125\n",
            "Batch 15: loss = 0.3308359384536743, acc = 0.8955078125\n",
            "Batch 16: loss = 0.3941313326358795, acc = 0.8779296875\n",
            "Batch 17: loss = 0.4158112108707428, acc = 0.859375\n",
            "Batch 18: loss = 0.425193727016449, acc = 0.853515625\n",
            "Batch 19: loss = 0.3752462565898895, acc = 0.8779296875\n",
            "Batch 20: loss = 0.3955360949039459, acc = 0.8623046875\n",
            "Batch 21: loss = 0.4186074137687683, acc = 0.86328125\n",
            "Batch 22: loss = 0.38137102127075195, acc = 0.8740234375\n",
            "Batch 23: loss = 0.3913915157318115, acc = 0.8759765625\n",
            "Batch 24: loss = 0.33669525384902954, acc = 0.890625\n",
            "Batch 25: loss = 0.36250171065330505, acc = 0.876953125\n",
            "Batch 26: loss = 0.36993086338043213, acc = 0.8759765625\n",
            "Batch 27: loss = 0.44075706601142883, acc = 0.8544921875\n",
            "Batch 28: loss = 0.3930016756057739, acc = 0.85546875\n",
            "Batch 29: loss = 0.3878619074821472, acc = 0.8701171875\n",
            "Batch 30: loss = 0.3427976071834564, acc = 0.8857421875\n",
            "Batch 31: loss = 0.38870254158973694, acc = 0.8681640625\n",
            "Batch 32: loss = 0.4234699606895447, acc = 0.87109375\n",
            "Batch 33: loss = 0.36542701721191406, acc = 0.8798828125\n",
            "Batch 34: loss = 0.4302254319190979, acc = 0.8525390625\n",
            "Batch 35: loss = 0.39877375960350037, acc = 0.865234375\n",
            "Batch 36: loss = 0.3341652452945709, acc = 0.8955078125\n",
            "Batch 37: loss = 0.34223389625549316, acc = 0.9052734375\n",
            "Batch 38: loss = 0.3692564368247986, acc = 0.880859375\n",
            "Batch 39: loss = 0.33738037943840027, acc = 0.8876953125\n",
            "Batch 40: loss = 0.36762896180152893, acc = 0.880859375\n",
            "Batch 41: loss = 0.3409159779548645, acc = 0.884765625\n",
            "Batch 42: loss = 0.37297892570495605, acc = 0.87109375\n",
            "Batch 43: loss = 0.4315793514251709, acc = 0.857421875\n",
            "Batch 44: loss = 0.33303576707839966, acc = 0.900390625\n",
            "Batch 45: loss = 0.31667080521583557, acc = 0.8994140625\n",
            "Batch 46: loss = 0.3453162908554077, acc = 0.890625\n",
            "Batch 47: loss = 0.3615841269493103, acc = 0.8857421875\n",
            "Batch 48: loss = 0.36270052194595337, acc = 0.873046875\n",
            "Batch 49: loss = 0.35985952615737915, acc = 0.884765625\n",
            "Batch 50: loss = 0.3693438172340393, acc = 0.8798828125\n",
            "Batch 51: loss = 0.37934941053390503, acc = 0.8828125\n",
            "Batch 52: loss = 0.3827098309993744, acc = 0.8681640625\n",
            "Batch 53: loss = 0.3620154857635498, acc = 0.884765625\n",
            "Batch 54: loss = 0.30765300989151, acc = 0.900390625\n",
            "Batch 55: loss = 0.31562578678131104, acc = 0.8984375\n",
            "Batch 56: loss = 0.38146910071372986, acc = 0.8662109375\n",
            "Batch 57: loss = 0.41654545068740845, acc = 0.861328125\n",
            "Batch 58: loss = 0.39923667907714844, acc = 0.8564453125\n",
            "Batch 59: loss = 0.3018176555633545, acc = 0.90234375\n",
            "Batch 60: loss = 0.3991793394088745, acc = 0.8642578125\n",
            "Batch 61: loss = 0.35187041759490967, acc = 0.8818359375\n",
            "Batch 62: loss = 0.420052707195282, acc = 0.85546875\n",
            "Batch 63: loss = 0.3914114832878113, acc = 0.876953125\n",
            "Batch 64: loss = 0.34321945905685425, acc = 0.8876953125\n",
            "Batch 65: loss = 0.39049848914146423, acc = 0.87890625\n",
            "Batch 66: loss = 0.3944663405418396, acc = 0.8671875\n",
            "Batch 67: loss = 0.4116919934749603, acc = 0.8515625\n",
            "Batch 68: loss = 0.3845142722129822, acc = 0.869140625\n",
            "Batch 69: loss = 0.39246922731399536, acc = 0.8740234375\n",
            "Batch 70: loss = 0.4043756127357483, acc = 0.8720703125\n",
            "Batch 71: loss = 0.4148424565792084, acc = 0.8544921875\n",
            "Batch 72: loss = 0.36789193749427795, acc = 0.8681640625\n",
            "Batch 73: loss = 0.4389997720718384, acc = 0.8544921875\n",
            "Batch 74: loss = 0.40593597292900085, acc = 0.8623046875\n",
            "Batch 75: loss = 0.45114731788635254, acc = 0.8525390625\n",
            "Batch 76: loss = 0.4207862615585327, acc = 0.859375\n",
            "Batch 77: loss = 0.39159706234931946, acc = 0.869140625\n",
            "Batch 78: loss = 0.4279792606830597, acc = 0.84765625\n",
            "Batch 79: loss = 0.3656083643436432, acc = 0.8798828125\n",
            "Batch 80: loss = 0.3641298711299896, acc = 0.876953125\n",
            "Batch 81: loss = 0.39092448353767395, acc = 0.8642578125\n",
            "Batch 82: loss = 0.3570062220096588, acc = 0.8935546875\n",
            "Batch 83: loss = 0.3761056065559387, acc = 0.8779296875\n",
            "Batch 84: loss = 0.40384623408317566, acc = 0.865234375\n",
            "Batch 85: loss = 0.43791407346725464, acc = 0.849609375\n",
            "Batch 86: loss = 0.3790307343006134, acc = 0.8779296875\n",
            "Batch 87: loss = 0.3979952931404114, acc = 0.87109375\n",
            "Batch 88: loss = 0.4124131500720978, acc = 0.8720703125\n",
            "Batch 89: loss = 0.3925725519657135, acc = 0.8642578125\n",
            "Batch 90: loss = 0.42085230350494385, acc = 0.8603515625\n",
            "Batch 91: loss = 0.40613293647766113, acc = 0.8642578125\n",
            "Batch 92: loss = 0.4327631890773773, acc = 0.85546875\n",
            "Batch 93: loss = 0.3525703251361847, acc = 0.880859375\n",
            "Batch 94: loss = 0.39611881971359253, acc = 0.87109375\n",
            "Batch 95: loss = 0.37509992718696594, acc = 0.875\n",
            "Batch 96: loss = 0.43875551223754883, acc = 0.84765625\n",
            "Batch 97: loss = 0.3923490345478058, acc = 0.880859375\n",
            "Batch 98: loss = 0.3878943622112274, acc = 0.87109375\n",
            "Batch 99: loss = 0.43039482831954956, acc = 0.8603515625\n",
            "Batch 100: loss = 0.4453379213809967, acc = 0.837890625\n",
            "Batch 101: loss = 0.37527793645858765, acc = 0.8701171875\n",
            "Batch 102: loss = 0.389964759349823, acc = 0.884765625\n",
            "Batch 103: loss = 0.4269424080848694, acc = 0.857421875\n",
            "Batch 104: loss = 0.3687523305416107, acc = 0.8828125\n",
            "Batch 105: loss = 0.3471768796443939, acc = 0.8818359375\n",
            "Batch 106: loss = 0.4294956922531128, acc = 0.85546875\n",
            "Batch 107: loss = 0.426382452249527, acc = 0.8408203125\n",
            "Batch 108: loss = 0.4037337899208069, acc = 0.8662109375\n",
            "Batch 109: loss = 0.39523833990097046, acc = 0.8681640625\n",
            "Batch 110: loss = 0.3529151678085327, acc = 0.8798828125\n",
            "Batch 111: loss = 0.4134571850299835, acc = 0.8583984375\n",
            "Batch 112: loss = 0.44364723563194275, acc = 0.859375\n",
            "Batch 113: loss = 0.3977561295032501, acc = 0.86328125\n",
            "Batch 114: loss = 0.40031856298446655, acc = 0.8740234375\n",
            "Batch 115: loss = 0.39359521865844727, acc = 0.861328125\n",
            "Batch 116: loss = 0.4327613413333893, acc = 0.861328125\n",
            "Batch 117: loss = 0.36918172240257263, acc = 0.87109375\n",
            "Batch 118: loss = 0.35788214206695557, acc = 0.888671875\n",
            "Batch 119: loss = 0.4081120789051056, acc = 0.8671875\n",
            "Batch 120: loss = 0.3857680559158325, acc = 0.8642578125\n",
            "Batch 121: loss = 0.3866136074066162, acc = 0.8701171875\n",
            "Batch 122: loss = 0.38230690360069275, acc = 0.873046875\n",
            "Batch 123: loss = 0.3963436484336853, acc = 0.8623046875\n",
            "Batch 124: loss = 0.4212053716182709, acc = 0.86328125\n",
            "Batch 125: loss = 0.4213118255138397, acc = 0.8544921875\n",
            "Batch 126: loss = 0.3895686864852905, acc = 0.876953125\n",
            "\n",
            "Epoch 67/100\n",
            "Batch 1: loss = 0.5628965497016907, acc = 0.8447265625\n",
            "Batch 2: loss = 0.4161064028739929, acc = 0.8720703125\n",
            "Batch 3: loss = 0.41275355219841003, acc = 0.8662109375\n",
            "Batch 4: loss = 0.3849462866783142, acc = 0.8740234375\n",
            "Batch 5: loss = 0.3973289430141449, acc = 0.86328125\n",
            "Batch 6: loss = 0.4027835428714752, acc = 0.8623046875\n",
            "Batch 7: loss = 0.3553171753883362, acc = 0.87890625\n",
            "Batch 8: loss = 0.3666946291923523, acc = 0.8837890625\n",
            "Batch 9: loss = 0.375853031873703, acc = 0.87890625\n",
            "Batch 10: loss = 0.3290964365005493, acc = 0.896484375\n",
            "Batch 11: loss = 0.39437586069107056, acc = 0.8740234375\n",
            "Batch 12: loss = 0.3537987470626831, acc = 0.8720703125\n",
            "Batch 13: loss = 0.38097572326660156, acc = 0.869140625\n",
            "Batch 14: loss = 0.39161062240600586, acc = 0.880859375\n",
            "Batch 15: loss = 0.37843039631843567, acc = 0.880859375\n",
            "Batch 16: loss = 0.4369027018547058, acc = 0.861328125\n",
            "Batch 17: loss = 0.39459899067878723, acc = 0.875\n",
            "Batch 18: loss = 0.4302833378314972, acc = 0.859375\n",
            "Batch 19: loss = 0.3551902770996094, acc = 0.8876953125\n",
            "Batch 20: loss = 0.36773681640625, acc = 0.875\n",
            "Batch 21: loss = 0.356736421585083, acc = 0.8828125\n",
            "Batch 22: loss = 0.3744816184043884, acc = 0.87109375\n",
            "Batch 23: loss = 0.3770770728588104, acc = 0.87109375\n",
            "Batch 24: loss = 0.33558040857315063, acc = 0.880859375\n",
            "Batch 25: loss = 0.37654435634613037, acc = 0.8837890625\n",
            "Batch 26: loss = 0.34365367889404297, acc = 0.880859375\n",
            "Batch 27: loss = 0.44705402851104736, acc = 0.83984375\n",
            "Batch 28: loss = 0.3759675920009613, acc = 0.876953125\n",
            "Batch 29: loss = 0.40706950426101685, acc = 0.861328125\n",
            "Batch 30: loss = 0.37986600399017334, acc = 0.8671875\n",
            "Batch 31: loss = 0.40192559361457825, acc = 0.865234375\n",
            "Batch 32: loss = 0.4351969361305237, acc = 0.8623046875\n",
            "Batch 33: loss = 0.361215204000473, acc = 0.8828125\n",
            "Batch 34: loss = 0.41635969281196594, acc = 0.8623046875\n",
            "Batch 35: loss = 0.3827267289161682, acc = 0.8876953125\n",
            "Batch 36: loss = 0.3437439799308777, acc = 0.888671875\n",
            "Batch 37: loss = 0.3441509008407593, acc = 0.8876953125\n",
            "Batch 38: loss = 0.3703438937664032, acc = 0.880859375\n",
            "Batch 39: loss = 0.32972538471221924, acc = 0.888671875\n",
            "Batch 40: loss = 0.36630067229270935, acc = 0.876953125\n",
            "Batch 41: loss = 0.3397828936576843, acc = 0.8857421875\n",
            "Batch 42: loss = 0.3839546740055084, acc = 0.865234375\n",
            "Batch 43: loss = 0.43000519275665283, acc = 0.853515625\n",
            "Batch 44: loss = 0.33646804094314575, acc = 0.884765625\n",
            "Batch 45: loss = 0.3098672926425934, acc = 0.89453125\n",
            "Batch 46: loss = 0.332660436630249, acc = 0.8857421875\n",
            "Batch 47: loss = 0.3287971317768097, acc = 0.8837890625\n",
            "Batch 48: loss = 0.3613917827606201, acc = 0.873046875\n",
            "Batch 49: loss = 0.34645578265190125, acc = 0.890625\n",
            "Batch 50: loss = 0.33240756392478943, acc = 0.8837890625\n",
            "Batch 51: loss = 0.3505001962184906, acc = 0.8916015625\n",
            "Batch 52: loss = 0.3853382468223572, acc = 0.8720703125\n",
            "Batch 53: loss = 0.37099799513816833, acc = 0.8759765625\n",
            "Batch 54: loss = 0.3039940893650055, acc = 0.8984375\n",
            "Batch 55: loss = 0.3377690315246582, acc = 0.8994140625\n",
            "Batch 56: loss = 0.39971300959587097, acc = 0.8603515625\n",
            "Batch 57: loss = 0.39840346574783325, acc = 0.87109375\n",
            "Batch 58: loss = 0.42705920338630676, acc = 0.8681640625\n",
            "Batch 59: loss = 0.2946932911872864, acc = 0.896484375\n",
            "Batch 60: loss = 0.3967331349849701, acc = 0.861328125\n",
            "Batch 61: loss = 0.3320465087890625, acc = 0.896484375\n",
            "Batch 62: loss = 0.426403671503067, acc = 0.853515625\n",
            "Batch 63: loss = 0.39503076672554016, acc = 0.87890625\n",
            "Batch 64: loss = 0.3488764464855194, acc = 0.8896484375\n",
            "Batch 65: loss = 0.368206650018692, acc = 0.8818359375\n",
            "Batch 66: loss = 0.38977253437042236, acc = 0.86328125\n",
            "Batch 67: loss = 0.3982291519641876, acc = 0.859375\n",
            "Batch 68: loss = 0.4001345932483673, acc = 0.87109375\n",
            "Batch 69: loss = 0.3791254758834839, acc = 0.873046875\n",
            "Batch 70: loss = 0.4407236576080322, acc = 0.85546875\n",
            "Batch 71: loss = 0.4227887690067291, acc = 0.85546875\n",
            "Batch 72: loss = 0.38286811113357544, acc = 0.8779296875\n",
            "Batch 73: loss = 0.4295731782913208, acc = 0.86328125\n",
            "Batch 74: loss = 0.3959995210170746, acc = 0.865234375\n",
            "Batch 75: loss = 0.4685117304325104, acc = 0.8603515625\n",
            "Batch 76: loss = 0.42026594281196594, acc = 0.8544921875\n",
            "Batch 77: loss = 0.41770222783088684, acc = 0.8623046875\n",
            "Batch 78: loss = 0.4266784191131592, acc = 0.8544921875\n",
            "Batch 79: loss = 0.37434110045433044, acc = 0.87109375\n",
            "Batch 80: loss = 0.3710354268550873, acc = 0.873046875\n",
            "Batch 81: loss = 0.37905916571617126, acc = 0.8642578125\n",
            "Batch 82: loss = 0.3845023810863495, acc = 0.8662109375\n",
            "Batch 83: loss = 0.348222017288208, acc = 0.8818359375\n",
            "Batch 84: loss = 0.3835950493812561, acc = 0.8798828125\n",
            "Batch 85: loss = 0.416840523481369, acc = 0.865234375\n",
            "Batch 86: loss = 0.36567234992980957, acc = 0.890625\n",
            "Batch 87: loss = 0.40484169125556946, acc = 0.859375\n",
            "Batch 88: loss = 0.3985370993614197, acc = 0.861328125\n",
            "Batch 89: loss = 0.3825925588607788, acc = 0.8701171875\n",
            "Batch 90: loss = 0.4228912591934204, acc = 0.853515625\n",
            "Batch 91: loss = 0.4422183632850647, acc = 0.8486328125\n",
            "Batch 92: loss = 0.4057983160018921, acc = 0.865234375\n",
            "Batch 93: loss = 0.3350890278816223, acc = 0.8935546875\n",
            "Batch 94: loss = 0.3919149935245514, acc = 0.86328125\n",
            "Batch 95: loss = 0.39273887872695923, acc = 0.859375\n",
            "Batch 96: loss = 0.4514453411102295, acc = 0.8544921875\n",
            "Batch 97: loss = 0.38175904750823975, acc = 0.8779296875\n",
            "Batch 98: loss = 0.358676940202713, acc = 0.876953125\n",
            "Batch 99: loss = 0.3788772523403168, acc = 0.8876953125\n",
            "Batch 100: loss = 0.43886205554008484, acc = 0.8447265625\n",
            "Batch 101: loss = 0.3737734854221344, acc = 0.8740234375\n",
            "Batch 102: loss = 0.40158766508102417, acc = 0.8681640625\n",
            "Batch 103: loss = 0.4010111391544342, acc = 0.8759765625\n",
            "Batch 104: loss = 0.37792038917541504, acc = 0.8759765625\n",
            "Batch 105: loss = 0.34446027874946594, acc = 0.8876953125\n",
            "Batch 106: loss = 0.38505983352661133, acc = 0.8681640625\n",
            "Batch 107: loss = 0.398568332195282, acc = 0.8662109375\n",
            "Batch 108: loss = 0.39173728227615356, acc = 0.8701171875\n",
            "Batch 109: loss = 0.37039053440093994, acc = 0.873046875\n",
            "Batch 110: loss = 0.35619741678237915, acc = 0.8818359375\n",
            "Batch 111: loss = 0.3985062539577484, acc = 0.8662109375\n",
            "Batch 112: loss = 0.46456724405288696, acc = 0.857421875\n",
            "Batch 113: loss = 0.3845086097717285, acc = 0.87109375\n",
            "Batch 114: loss = 0.3999326527118683, acc = 0.8779296875\n",
            "Batch 115: loss = 0.38712263107299805, acc = 0.8759765625\n",
            "Batch 116: loss = 0.3901984989643097, acc = 0.87890625\n",
            "Batch 117: loss = 0.36207687854766846, acc = 0.884765625\n",
            "Batch 118: loss = 0.36573055386543274, acc = 0.8759765625\n",
            "Batch 119: loss = 0.35015225410461426, acc = 0.8798828125\n",
            "Batch 120: loss = 0.356148898601532, acc = 0.8837890625\n",
            "Batch 121: loss = 0.39635977149009705, acc = 0.8740234375\n",
            "Batch 122: loss = 0.35920798778533936, acc = 0.8857421875\n",
            "Batch 123: loss = 0.3963530659675598, acc = 0.880859375\n",
            "Batch 124: loss = 0.41523903608322144, acc = 0.8603515625\n",
            "Batch 125: loss = 0.3936440348625183, acc = 0.8759765625\n",
            "Batch 126: loss = 0.41187113523483276, acc = 0.859375\n",
            "\n",
            "Epoch 68/100\n",
            "Batch 1: loss = 0.5463459491729736, acc = 0.8427734375\n",
            "Batch 2: loss = 0.42186644673347473, acc = 0.8544921875\n",
            "Batch 3: loss = 0.4004727005958557, acc = 0.87890625\n",
            "Batch 4: loss = 0.35381555557250977, acc = 0.8896484375\n",
            "Batch 5: loss = 0.3857996463775635, acc = 0.87109375\n",
            "Batch 6: loss = 0.417341023683548, acc = 0.87109375\n",
            "Batch 7: loss = 0.33955127000808716, acc = 0.8828125\n",
            "Batch 8: loss = 0.37095892429351807, acc = 0.8896484375\n",
            "Batch 9: loss = 0.3611990213394165, acc = 0.8828125\n",
            "Batch 10: loss = 0.3195440471172333, acc = 0.8955078125\n",
            "Batch 11: loss = 0.3698895573616028, acc = 0.8916015625\n",
            "Batch 12: loss = 0.3824284076690674, acc = 0.8798828125\n",
            "Batch 13: loss = 0.37588074803352356, acc = 0.8720703125\n",
            "Batch 14: loss = 0.35917747020721436, acc = 0.884765625\n",
            "Batch 15: loss = 0.3657136559486389, acc = 0.8779296875\n",
            "Batch 16: loss = 0.382016122341156, acc = 0.8779296875\n",
            "Batch 17: loss = 0.37222155928611755, acc = 0.884765625\n",
            "Batch 18: loss = 0.4125795364379883, acc = 0.876953125\n",
            "Batch 19: loss = 0.36322858929634094, acc = 0.890625\n",
            "Batch 20: loss = 0.40820664167404175, acc = 0.857421875\n",
            "Batch 21: loss = 0.3836449682712555, acc = 0.8779296875\n",
            "Batch 22: loss = 0.39821693301200867, acc = 0.8642578125\n",
            "Batch 23: loss = 0.3723180890083313, acc = 0.87890625\n",
            "Batch 24: loss = 0.3351665735244751, acc = 0.88671875\n",
            "Batch 25: loss = 0.38195687532424927, acc = 0.873046875\n",
            "Batch 26: loss = 0.3361373543739319, acc = 0.8876953125\n",
            "Batch 27: loss = 0.42738276720046997, acc = 0.845703125\n",
            "Batch 28: loss = 0.3683895468711853, acc = 0.8818359375\n",
            "Batch 29: loss = 0.3886565566062927, acc = 0.861328125\n",
            "Batch 30: loss = 0.3447408080101013, acc = 0.8818359375\n",
            "Batch 31: loss = 0.37919196486473083, acc = 0.87109375\n",
            "Batch 32: loss = 0.4639173150062561, acc = 0.8544921875\n",
            "Batch 33: loss = 0.34072345495224, acc = 0.896484375\n",
            "Batch 34: loss = 0.3990946412086487, acc = 0.8759765625\n",
            "Batch 35: loss = 0.37325233221054077, acc = 0.8837890625\n",
            "Batch 36: loss = 0.34013670682907104, acc = 0.890625\n",
            "Batch 37: loss = 0.3036072850227356, acc = 0.9052734375\n",
            "Batch 38: loss = 0.3924797773361206, acc = 0.8671875\n",
            "Batch 39: loss = 0.3200780153274536, acc = 0.89453125\n",
            "Batch 40: loss = 0.3458944857120514, acc = 0.890625\n",
            "Batch 41: loss = 0.322609007358551, acc = 0.890625\n",
            "Batch 42: loss = 0.37329214811325073, acc = 0.8818359375\n",
            "Batch 43: loss = 0.4091140627861023, acc = 0.8623046875\n",
            "Batch 44: loss = 0.33894285559654236, acc = 0.8818359375\n",
            "Batch 45: loss = 0.3262685537338257, acc = 0.9033203125\n",
            "Batch 46: loss = 0.3237759470939636, acc = 0.8984375\n",
            "Batch 47: loss = 0.3462253510951996, acc = 0.8828125\n",
            "Batch 48: loss = 0.3649074137210846, acc = 0.884765625\n",
            "Batch 49: loss = 0.3439624309539795, acc = 0.89453125\n",
            "Batch 50: loss = 0.37135323882102966, acc = 0.8818359375\n",
            "Batch 51: loss = 0.3637131452560425, acc = 0.880859375\n",
            "Batch 52: loss = 0.37169793248176575, acc = 0.87890625\n",
            "Batch 53: loss = 0.3720819056034088, acc = 0.884765625\n",
            "Batch 54: loss = 0.31658247113227844, acc = 0.888671875\n",
            "Batch 55: loss = 0.2954709529876709, acc = 0.919921875\n",
            "Batch 56: loss = 0.39268192648887634, acc = 0.865234375\n",
            "Batch 57: loss = 0.39007410407066345, acc = 0.8642578125\n",
            "Batch 58: loss = 0.39585989713668823, acc = 0.865234375\n",
            "Batch 59: loss = 0.30001717805862427, acc = 0.90625\n",
            "Batch 60: loss = 0.38345277309417725, acc = 0.8740234375\n",
            "Batch 61: loss = 0.30965524911880493, acc = 0.908203125\n",
            "Batch 62: loss = 0.4041352868080139, acc = 0.86328125\n",
            "Batch 63: loss = 0.3925292491912842, acc = 0.859375\n",
            "Batch 64: loss = 0.3406936526298523, acc = 0.8955078125\n",
            "Batch 65: loss = 0.3997325301170349, acc = 0.869140625\n",
            "Batch 66: loss = 0.3763780891895294, acc = 0.8798828125\n",
            "Batch 67: loss = 0.3947325348854065, acc = 0.857421875\n",
            "Batch 68: loss = 0.3896850049495697, acc = 0.8759765625\n",
            "Batch 69: loss = 0.38276126980781555, acc = 0.876953125\n",
            "Batch 70: loss = 0.4207789897918701, acc = 0.8671875\n",
            "Batch 71: loss = 0.3923102021217346, acc = 0.8720703125\n",
            "Batch 72: loss = 0.37964513897895813, acc = 0.880859375\n",
            "Batch 73: loss = 0.4121932089328766, acc = 0.865234375\n",
            "Batch 74: loss = 0.39513951539993286, acc = 0.859375\n",
            "Batch 75: loss = 0.47163650393486023, acc = 0.837890625\n",
            "Batch 76: loss = 0.4356233775615692, acc = 0.84765625\n",
            "Batch 77: loss = 0.3931136727333069, acc = 0.8671875\n",
            "Batch 78: loss = 0.4134928584098816, acc = 0.8623046875\n",
            "Batch 79: loss = 0.3762392997741699, acc = 0.87109375\n",
            "Batch 80: loss = 0.3783031105995178, acc = 0.873046875\n",
            "Batch 81: loss = 0.39685744047164917, acc = 0.8759765625\n",
            "Batch 82: loss = 0.39187148213386536, acc = 0.8759765625\n",
            "Batch 83: loss = 0.36234429478645325, acc = 0.8681640625\n",
            "Batch 84: loss = 0.4056411683559418, acc = 0.86328125\n",
            "Batch 85: loss = 0.40769901871681213, acc = 0.8642578125\n",
            "Batch 86: loss = 0.37828221917152405, acc = 0.869140625\n",
            "Batch 87: loss = 0.38331297039985657, acc = 0.875\n",
            "Batch 88: loss = 0.4045323431491852, acc = 0.865234375\n",
            "Batch 89: loss = 0.37775731086730957, acc = 0.890625\n",
            "Batch 90: loss = 0.4192814826965332, acc = 0.8642578125\n",
            "Batch 91: loss = 0.4169817566871643, acc = 0.8671875\n",
            "Batch 92: loss = 0.4304412007331848, acc = 0.859375\n",
            "Batch 93: loss = 0.37636491656303406, acc = 0.876953125\n",
            "Batch 94: loss = 0.3840606212615967, acc = 0.8740234375\n",
            "Batch 95: loss = 0.40326061844825745, acc = 0.8603515625\n",
            "Batch 96: loss = 0.43472620844841003, acc = 0.8427734375\n",
            "Batch 97: loss = 0.3754112124443054, acc = 0.87890625\n",
            "Batch 98: loss = 0.35141122341156006, acc = 0.88671875\n",
            "Batch 99: loss = 0.4221741259098053, acc = 0.869140625\n",
            "Batch 100: loss = 0.41472819447517395, acc = 0.849609375\n",
            "Batch 101: loss = 0.3649561405181885, acc = 0.8701171875\n",
            "Batch 102: loss = 0.40735697746276855, acc = 0.865234375\n",
            "Batch 103: loss = 0.40748682618141174, acc = 0.86328125\n",
            "Batch 104: loss = 0.36571204662323, acc = 0.869140625\n",
            "Batch 105: loss = 0.351942241191864, acc = 0.8818359375\n",
            "Batch 106: loss = 0.41748252511024475, acc = 0.861328125\n",
            "Batch 107: loss = 0.3989638090133667, acc = 0.85546875\n",
            "Batch 108: loss = 0.38193249702453613, acc = 0.87109375\n",
            "Batch 109: loss = 0.3583741784095764, acc = 0.8818359375\n",
            "Batch 110: loss = 0.35788360238075256, acc = 0.884765625\n",
            "Batch 111: loss = 0.37892842292785645, acc = 0.8779296875\n",
            "Batch 112: loss = 0.4171161353588104, acc = 0.869140625\n",
            "Batch 113: loss = 0.38049545884132385, acc = 0.873046875\n",
            "Batch 114: loss = 0.4322390556335449, acc = 0.859375\n",
            "Batch 115: loss = 0.3833431601524353, acc = 0.869140625\n",
            "Batch 116: loss = 0.42108580470085144, acc = 0.853515625\n",
            "Batch 117: loss = 0.37358927726745605, acc = 0.8837890625\n",
            "Batch 118: loss = 0.34132522344589233, acc = 0.888671875\n",
            "Batch 119: loss = 0.38908374309539795, acc = 0.8603515625\n",
            "Batch 120: loss = 0.3218532204627991, acc = 0.89453125\n",
            "Batch 121: loss = 0.3711155652999878, acc = 0.876953125\n",
            "Batch 122: loss = 0.3599260449409485, acc = 0.876953125\n",
            "Batch 123: loss = 0.39598506689071655, acc = 0.8662109375\n",
            "Batch 124: loss = 0.41139400005340576, acc = 0.857421875\n",
            "Batch 125: loss = 0.4049889147281647, acc = 0.8671875\n",
            "Batch 126: loss = 0.4231104850769043, acc = 0.865234375\n",
            "\n",
            "Epoch 69/100\n",
            "Batch 1: loss = 0.504710853099823, acc = 0.8427734375\n",
            "Batch 2: loss = 0.41072776913642883, acc = 0.880859375\n",
            "Batch 3: loss = 0.3966381251811981, acc = 0.8671875\n",
            "Batch 4: loss = 0.3699420392513275, acc = 0.8759765625\n",
            "Batch 5: loss = 0.3832210302352905, acc = 0.8701171875\n",
            "Batch 6: loss = 0.3926394581794739, acc = 0.87890625\n",
            "Batch 7: loss = 0.3806530237197876, acc = 0.8701171875\n",
            "Batch 8: loss = 0.3712450861930847, acc = 0.884765625\n",
            "Batch 9: loss = 0.355192095041275, acc = 0.892578125\n",
            "Batch 10: loss = 0.33297574520111084, acc = 0.892578125\n",
            "Batch 11: loss = 0.392365038394928, acc = 0.8720703125\n",
            "Batch 12: loss = 0.3719964921474457, acc = 0.8671875\n",
            "Batch 13: loss = 0.32969948649406433, acc = 0.8955078125\n",
            "Batch 14: loss = 0.3482995629310608, acc = 0.888671875\n",
            "Batch 15: loss = 0.3422887921333313, acc = 0.89453125\n",
            "Batch 16: loss = 0.41185104846954346, acc = 0.8701171875\n",
            "Batch 17: loss = 0.3636772632598877, acc = 0.8876953125\n",
            "Batch 18: loss = 0.4218361973762512, acc = 0.8564453125\n",
            "Batch 19: loss = 0.36628463864326477, acc = 0.8857421875\n",
            "Batch 20: loss = 0.39276060461997986, acc = 0.8662109375\n",
            "Batch 21: loss = 0.39311662316322327, acc = 0.8779296875\n",
            "Batch 22: loss = 0.3700030744075775, acc = 0.875\n",
            "Batch 23: loss = 0.3722241520881653, acc = 0.8779296875\n",
            "Batch 24: loss = 0.35292768478393555, acc = 0.8720703125\n",
            "Batch 25: loss = 0.3898034691810608, acc = 0.8662109375\n",
            "Batch 26: loss = 0.33609944581985474, acc = 0.8876953125\n",
            "Batch 27: loss = 0.40651053190231323, acc = 0.8671875\n",
            "Batch 28: loss = 0.3800331652164459, acc = 0.8720703125\n",
            "Batch 29: loss = 0.36494842171669006, acc = 0.8681640625\n",
            "Batch 30: loss = 0.37152642011642456, acc = 0.8779296875\n",
            "Batch 31: loss = 0.38374340534210205, acc = 0.861328125\n",
            "Batch 32: loss = 0.4351370930671692, acc = 0.8583984375\n",
            "Batch 33: loss = 0.36704909801483154, acc = 0.8740234375\n",
            "Batch 34: loss = 0.42643454670906067, acc = 0.857421875\n",
            "Batch 35: loss = 0.37345588207244873, acc = 0.8662109375\n",
            "Batch 36: loss = 0.3528073728084564, acc = 0.892578125\n",
            "Batch 37: loss = 0.3255991041660309, acc = 0.8984375\n",
            "Batch 38: loss = 0.36865875124931335, acc = 0.880859375\n",
            "Batch 39: loss = 0.33161139488220215, acc = 0.8935546875\n",
            "Batch 40: loss = 0.3310723304748535, acc = 0.8974609375\n",
            "Batch 41: loss = 0.3387732207775116, acc = 0.8916015625\n",
            "Batch 42: loss = 0.3751399517059326, acc = 0.8876953125\n",
            "Batch 43: loss = 0.42890244722366333, acc = 0.8642578125\n",
            "Batch 44: loss = 0.3471885323524475, acc = 0.8740234375\n",
            "Batch 45: loss = 0.32619720697402954, acc = 0.8994140625\n",
            "Batch 46: loss = 0.31074777245521545, acc = 0.8974609375\n",
            "Batch 47: loss = 0.3716883659362793, acc = 0.8740234375\n",
            "Batch 48: loss = 0.35361599922180176, acc = 0.8857421875\n",
            "Batch 49: loss = 0.33961352705955505, acc = 0.89453125\n",
            "Batch 50: loss = 0.352863609790802, acc = 0.8837890625\n",
            "Batch 51: loss = 0.3588596284389496, acc = 0.8779296875\n",
            "Batch 52: loss = 0.3617682158946991, acc = 0.8828125\n",
            "Batch 53: loss = 0.3702037036418915, acc = 0.87109375\n",
            "Batch 54: loss = 0.3022415339946747, acc = 0.8994140625\n",
            "Batch 55: loss = 0.31936168670654297, acc = 0.89453125\n",
            "Batch 56: loss = 0.41192519664764404, acc = 0.8662109375\n",
            "Batch 57: loss = 0.37898996472358704, acc = 0.875\n",
            "Batch 58: loss = 0.3987066149711609, acc = 0.8623046875\n",
            "Batch 59: loss = 0.30405935645103455, acc = 0.896484375\n",
            "Batch 60: loss = 0.36576589941978455, acc = 0.8759765625\n",
            "Batch 61: loss = 0.3304257392883301, acc = 0.8818359375\n",
            "Batch 62: loss = 0.4161854088306427, acc = 0.861328125\n",
            "Batch 63: loss = 0.388661652803421, acc = 0.86328125\n",
            "Batch 64: loss = 0.36343875527381897, acc = 0.8740234375\n",
            "Batch 65: loss = 0.3950519859790802, acc = 0.86328125\n",
            "Batch 66: loss = 0.38564252853393555, acc = 0.865234375\n",
            "Batch 67: loss = 0.40359291434288025, acc = 0.859375\n",
            "Batch 68: loss = 0.4114084243774414, acc = 0.8623046875\n",
            "Batch 69: loss = 0.3719102144241333, acc = 0.875\n",
            "Batch 70: loss = 0.4425255060195923, acc = 0.849609375\n",
            "Batch 71: loss = 0.4059631824493408, acc = 0.8544921875\n",
            "Batch 72: loss = 0.36507952213287354, acc = 0.880859375\n",
            "Batch 73: loss = 0.41222530603408813, acc = 0.87109375\n",
            "Batch 74: loss = 0.40974387526512146, acc = 0.853515625\n",
            "Batch 75: loss = 0.4505271911621094, acc = 0.8564453125\n",
            "Batch 76: loss = 0.42756131291389465, acc = 0.8525390625\n",
            "Batch 77: loss = 0.3993062376976013, acc = 0.857421875\n",
            "Batch 78: loss = 0.4455922842025757, acc = 0.84765625\n",
            "Batch 79: loss = 0.38755732774734497, acc = 0.859375\n",
            "Batch 80: loss = 0.38185960054397583, acc = 0.8671875\n",
            "Batch 81: loss = 0.36807966232299805, acc = 0.880859375\n",
            "Batch 82: loss = 0.37187516689300537, acc = 0.8759765625\n",
            "Batch 83: loss = 0.34672844409942627, acc = 0.873046875\n",
            "Batch 84: loss = 0.39674264192581177, acc = 0.87109375\n",
            "Batch 85: loss = 0.40243589878082275, acc = 0.8603515625\n",
            "Batch 86: loss = 0.36703169345855713, acc = 0.880859375\n",
            "Batch 87: loss = 0.40683531761169434, acc = 0.8681640625\n",
            "Batch 88: loss = 0.4333908259868622, acc = 0.8583984375\n",
            "Batch 89: loss = 0.3724549114704132, acc = 0.8681640625\n",
            "Batch 90: loss = 0.39447635412216187, acc = 0.8671875\n",
            "Batch 91: loss = 0.4216705858707428, acc = 0.859375\n",
            "Batch 92: loss = 0.4086295962333679, acc = 0.865234375\n",
            "Batch 93: loss = 0.36805906891822815, acc = 0.8779296875\n",
            "Batch 94: loss = 0.3971267342567444, acc = 0.8720703125\n",
            "Batch 95: loss = 0.40832555294036865, acc = 0.8642578125\n",
            "Batch 96: loss = 0.4627208709716797, acc = 0.8349609375\n",
            "Batch 97: loss = 0.39891141653060913, acc = 0.869140625\n",
            "Batch 98: loss = 0.3811914026737213, acc = 0.8779296875\n",
            "Batch 99: loss = 0.42584559321403503, acc = 0.865234375\n",
            "Batch 100: loss = 0.44300585985183716, acc = 0.8427734375\n",
            "Batch 101: loss = 0.34615832567214966, acc = 0.8798828125\n",
            "Batch 102: loss = 0.4084230065345764, acc = 0.869140625\n",
            "Batch 103: loss = 0.3839613199234009, acc = 0.8720703125\n",
            "Batch 104: loss = 0.3646405339241028, acc = 0.876953125\n",
            "Batch 105: loss = 0.34976112842559814, acc = 0.8818359375\n",
            "Batch 106: loss = 0.3970849812030792, acc = 0.87109375\n",
            "Batch 107: loss = 0.40075984597206116, acc = 0.87109375\n",
            "Batch 108: loss = 0.36445721983909607, acc = 0.8798828125\n",
            "Batch 109: loss = 0.36041778326034546, acc = 0.8681640625\n",
            "Batch 110: loss = 0.35565853118896484, acc = 0.8828125\n",
            "Batch 111: loss = 0.39755141735076904, acc = 0.884765625\n",
            "Batch 112: loss = 0.4429788589477539, acc = 0.861328125\n",
            "Batch 113: loss = 0.3703831732273102, acc = 0.876953125\n",
            "Batch 114: loss = 0.4186873733997345, acc = 0.8720703125\n",
            "Batch 115: loss = 0.38376864790916443, acc = 0.865234375\n",
            "Batch 116: loss = 0.39852049946784973, acc = 0.8642578125\n",
            "Batch 117: loss = 0.39169710874557495, acc = 0.8701171875\n",
            "Batch 118: loss = 0.3774733245372772, acc = 0.876953125\n",
            "Batch 119: loss = 0.35503944754600525, acc = 0.87890625\n",
            "Batch 120: loss = 0.3404765725135803, acc = 0.888671875\n",
            "Batch 121: loss = 0.3619813323020935, acc = 0.8798828125\n",
            "Batch 122: loss = 0.35036519169807434, acc = 0.8876953125\n",
            "Batch 123: loss = 0.3712810277938843, acc = 0.8837890625\n",
            "Batch 124: loss = 0.4169589877128601, acc = 0.8515625\n",
            "Batch 125: loss = 0.3986300528049469, acc = 0.865234375\n",
            "Batch 126: loss = 0.41292375326156616, acc = 0.8564453125\n",
            "\n",
            "Epoch 70/100\n",
            "Batch 1: loss = 0.5138949155807495, acc = 0.8642578125\n",
            "Batch 2: loss = 0.4251261353492737, acc = 0.85546875\n",
            "Batch 3: loss = 0.40137097239494324, acc = 0.8701171875\n",
            "Batch 4: loss = 0.3854074478149414, acc = 0.875\n",
            "Batch 5: loss = 0.3494507372379303, acc = 0.880859375\n",
            "Batch 6: loss = 0.3838682770729065, acc = 0.8837890625\n",
            "Batch 7: loss = 0.3723376393318176, acc = 0.876953125\n",
            "Batch 8: loss = 0.3498639166355133, acc = 0.87890625\n",
            "Batch 9: loss = 0.3751705586910248, acc = 0.8759765625\n",
            "Batch 10: loss = 0.32674261927604675, acc = 0.8935546875\n",
            "Batch 11: loss = 0.41033440828323364, acc = 0.8662109375\n",
            "Batch 12: loss = 0.35018593072891235, acc = 0.87890625\n",
            "Batch 13: loss = 0.3483209013938904, acc = 0.87890625\n",
            "Batch 14: loss = 0.36083248257637024, acc = 0.8857421875\n",
            "Batch 15: loss = 0.36087971925735474, acc = 0.8896484375\n",
            "Batch 16: loss = 0.41490334272384644, acc = 0.8623046875\n",
            "Batch 17: loss = 0.3978300988674164, acc = 0.873046875\n",
            "Batch 18: loss = 0.44066229462623596, acc = 0.8642578125\n",
            "Batch 19: loss = 0.3572176694869995, acc = 0.8837890625\n",
            "Batch 20: loss = 0.37789326906204224, acc = 0.8642578125\n",
            "Batch 21: loss = 0.3912161886692047, acc = 0.869140625\n",
            "Batch 22: loss = 0.36929017305374146, acc = 0.873046875\n",
            "Batch 23: loss = 0.35270899534225464, acc = 0.880859375\n",
            "Batch 24: loss = 0.3338024914264679, acc = 0.880859375\n",
            "Batch 25: loss = 0.3803250789642334, acc = 0.873046875\n",
            "Batch 26: loss = 0.3227834105491638, acc = 0.89453125\n",
            "Batch 27: loss = 0.3868682384490967, acc = 0.8701171875\n",
            "Batch 28: loss = 0.3901748061180115, acc = 0.8740234375\n",
            "Batch 29: loss = 0.3510550856590271, acc = 0.8828125\n",
            "Batch 30: loss = 0.34254083037376404, acc = 0.8857421875\n",
            "Batch 31: loss = 0.3690216541290283, acc = 0.869140625\n",
            "Batch 32: loss = 0.4304003119468689, acc = 0.8642578125\n",
            "Batch 33: loss = 0.34070393443107605, acc = 0.88671875\n",
            "Batch 34: loss = 0.3900732100009918, acc = 0.88671875\n",
            "Batch 35: loss = 0.3664308488368988, acc = 0.880859375\n",
            "Batch 36: loss = 0.34412506222724915, acc = 0.8876953125\n",
            "Batch 37: loss = 0.32492828369140625, acc = 0.8974609375\n",
            "Batch 38: loss = 0.3739764094352722, acc = 0.880859375\n",
            "Batch 39: loss = 0.31681472063064575, acc = 0.8974609375\n",
            "Batch 40: loss = 0.3555150628089905, acc = 0.8837890625\n",
            "Batch 41: loss = 0.34529927372932434, acc = 0.8896484375\n",
            "Batch 42: loss = 0.33773329854011536, acc = 0.89453125\n",
            "Batch 43: loss = 0.42571526765823364, acc = 0.837890625\n",
            "Batch 44: loss = 0.317875474691391, acc = 0.8994140625\n",
            "Batch 45: loss = 0.33229368925094604, acc = 0.8916015625\n",
            "Batch 46: loss = 0.33765843510627747, acc = 0.888671875\n",
            "Batch 47: loss = 0.3301544487476349, acc = 0.892578125\n",
            "Batch 48: loss = 0.33371150493621826, acc = 0.890625\n",
            "Batch 49: loss = 0.3421032130718231, acc = 0.880859375\n",
            "Batch 50: loss = 0.3423711657524109, acc = 0.896484375\n",
            "Batch 51: loss = 0.3340396583080292, acc = 0.8984375\n",
            "Batch 52: loss = 0.37498998641967773, acc = 0.8720703125\n",
            "Batch 53: loss = 0.39080214500427246, acc = 0.8740234375\n",
            "Batch 54: loss = 0.3478066921234131, acc = 0.8828125\n",
            "Batch 55: loss = 0.2991052269935608, acc = 0.91796875\n",
            "Batch 56: loss = 0.3826405107975006, acc = 0.86328125\n",
            "Batch 57: loss = 0.38728898763656616, acc = 0.8779296875\n",
            "Batch 58: loss = 0.37603074312210083, acc = 0.8828125\n",
            "Batch 59: loss = 0.287701815366745, acc = 0.9033203125\n",
            "Batch 60: loss = 0.36647599935531616, acc = 0.880859375\n",
            "Batch 61: loss = 0.348796546459198, acc = 0.8857421875\n",
            "Batch 62: loss = 0.40037134289741516, acc = 0.85546875\n",
            "Batch 63: loss = 0.3423442542552948, acc = 0.8837890625\n",
            "Batch 64: loss = 0.3249446749687195, acc = 0.8955078125\n",
            "Batch 65: loss = 0.37160640954971313, acc = 0.8828125\n",
            "Batch 66: loss = 0.36592698097229004, acc = 0.8837890625\n",
            "Batch 67: loss = 0.38852551579475403, acc = 0.865234375\n",
            "Batch 68: loss = 0.39450445771217346, acc = 0.8662109375\n",
            "Batch 69: loss = 0.37506964802742004, acc = 0.8759765625\n",
            "Batch 70: loss = 0.39774566888809204, acc = 0.8759765625\n",
            "Batch 71: loss = 0.40927475690841675, acc = 0.8603515625\n",
            "Batch 72: loss = 0.35251349210739136, acc = 0.892578125\n",
            "Batch 73: loss = 0.384847491979599, acc = 0.8740234375\n",
            "Batch 74: loss = 0.417044073343277, acc = 0.865234375\n",
            "Batch 75: loss = 0.4525790810585022, acc = 0.8515625\n",
            "Batch 76: loss = 0.3905683159828186, acc = 0.865234375\n",
            "Batch 77: loss = 0.37876373529434204, acc = 0.8720703125\n",
            "Batch 78: loss = 0.3938741981983185, acc = 0.8642578125\n",
            "Batch 79: loss = 0.365890771150589, acc = 0.8583984375\n",
            "Batch 80: loss = 0.4100537896156311, acc = 0.86328125\n",
            "Batch 81: loss = 0.3752126693725586, acc = 0.873046875\n",
            "Batch 82: loss = 0.36969876289367676, acc = 0.8857421875\n",
            "Batch 83: loss = 0.3343152403831482, acc = 0.8876953125\n",
            "Batch 84: loss = 0.4013800323009491, acc = 0.8662109375\n",
            "Batch 85: loss = 0.43331435322761536, acc = 0.8515625\n",
            "Batch 86: loss = 0.38886919617652893, acc = 0.869140625\n",
            "Batch 87: loss = 0.39708834886550903, acc = 0.8515625\n",
            "Batch 88: loss = 0.43405160307884216, acc = 0.8564453125\n",
            "Batch 89: loss = 0.34923645853996277, acc = 0.8857421875\n",
            "Batch 90: loss = 0.3880678415298462, acc = 0.8681640625\n",
            "Batch 91: loss = 0.39994245767593384, acc = 0.8740234375\n",
            "Batch 92: loss = 0.40969234704971313, acc = 0.8720703125\n",
            "Batch 93: loss = 0.3586157262325287, acc = 0.884765625\n",
            "Batch 94: loss = 0.38707321882247925, acc = 0.87109375\n",
            "Batch 95: loss = 0.3723091185092926, acc = 0.8759765625\n",
            "Batch 96: loss = 0.4435082674026489, acc = 0.8583984375\n",
            "Batch 97: loss = 0.4066247045993805, acc = 0.8671875\n",
            "Batch 98: loss = 0.40200403332710266, acc = 0.861328125\n",
            "Batch 99: loss = 0.442302942276001, acc = 0.8525390625\n",
            "Batch 100: loss = 0.4062419533729553, acc = 0.859375\n",
            "Batch 101: loss = 0.3862144947052002, acc = 0.875\n",
            "Batch 102: loss = 0.44201284646987915, acc = 0.8466796875\n",
            "Batch 103: loss = 0.4125511348247528, acc = 0.8671875\n",
            "Batch 104: loss = 0.38177555799484253, acc = 0.869140625\n",
            "Batch 105: loss = 0.3465145230293274, acc = 0.875\n",
            "Batch 106: loss = 0.3856259882450104, acc = 0.8720703125\n",
            "Batch 107: loss = 0.41501718759536743, acc = 0.8544921875\n",
            "Batch 108: loss = 0.3807884454727173, acc = 0.8662109375\n",
            "Batch 109: loss = 0.3843003511428833, acc = 0.861328125\n",
            "Batch 110: loss = 0.34336134791374207, acc = 0.88671875\n",
            "Batch 111: loss = 0.40536582469940186, acc = 0.8671875\n",
            "Batch 112: loss = 0.419573575258255, acc = 0.8642578125\n",
            "Batch 113: loss = 0.3826901912689209, acc = 0.87109375\n",
            "Batch 114: loss = 0.41243213415145874, acc = 0.86328125\n",
            "Batch 115: loss = 0.3850933909416199, acc = 0.875\n",
            "Batch 116: loss = 0.43758517503738403, acc = 0.8603515625\n",
            "Batch 117: loss = 0.3746776580810547, acc = 0.875\n",
            "Batch 118: loss = 0.35090479254722595, acc = 0.8837890625\n",
            "Batch 119: loss = 0.34696075320243835, acc = 0.875\n",
            "Batch 120: loss = 0.3366362750530243, acc = 0.8779296875\n",
            "Batch 121: loss = 0.37589555978775024, acc = 0.87890625\n",
            "Batch 122: loss = 0.36997759342193604, acc = 0.873046875\n",
            "Batch 123: loss = 0.3595649003982544, acc = 0.8916015625\n",
            "Batch 124: loss = 0.41196340322494507, acc = 0.8515625\n",
            "Batch 125: loss = 0.41880160570144653, acc = 0.8525390625\n",
            "Batch 126: loss = 0.4223458766937256, acc = 0.859375\n",
            "Saved checkpoint to weights.70.h5\n",
            "\n",
            "Epoch 71/100\n",
            "Batch 1: loss = 0.5128995776176453, acc = 0.8525390625\n",
            "Batch 2: loss = 0.4211813807487488, acc = 0.8603515625\n",
            "Batch 3: loss = 0.4072117209434509, acc = 0.87109375\n",
            "Batch 4: loss = 0.3798956274986267, acc = 0.8896484375\n",
            "Batch 5: loss = 0.37634918093681335, acc = 0.8701171875\n",
            "Batch 6: loss = 0.40618574619293213, acc = 0.8671875\n",
            "Batch 7: loss = 0.3611792027950287, acc = 0.875\n",
            "Batch 8: loss = 0.3706141710281372, acc = 0.8740234375\n",
            "Batch 9: loss = 0.31679093837738037, acc = 0.888671875\n",
            "Batch 10: loss = 0.31872957944869995, acc = 0.8994140625\n",
            "Batch 11: loss = 0.3989463448524475, acc = 0.8720703125\n",
            "Batch 12: loss = 0.39982542395591736, acc = 0.861328125\n",
            "Batch 13: loss = 0.358817994594574, acc = 0.873046875\n",
            "Batch 14: loss = 0.3612087070941925, acc = 0.8828125\n",
            "Batch 15: loss = 0.34441864490509033, acc = 0.904296875\n",
            "Batch 16: loss = 0.39938828349113464, acc = 0.876953125\n",
            "Batch 17: loss = 0.3852890729904175, acc = 0.8818359375\n",
            "Batch 18: loss = 0.42174166440963745, acc = 0.8486328125\n",
            "Batch 19: loss = 0.36019331216812134, acc = 0.8828125\n",
            "Batch 20: loss = 0.3745141625404358, acc = 0.8671875\n",
            "Batch 21: loss = 0.38470661640167236, acc = 0.869140625\n",
            "Batch 22: loss = 0.3515644967556, acc = 0.8828125\n",
            "Batch 23: loss = 0.35090598464012146, acc = 0.888671875\n",
            "Batch 24: loss = 0.355935662984848, acc = 0.87890625\n",
            "Batch 25: loss = 0.387134850025177, acc = 0.8701171875\n",
            "Batch 26: loss = 0.352139949798584, acc = 0.875\n",
            "Batch 27: loss = 0.4064458906650543, acc = 0.861328125\n",
            "Batch 28: loss = 0.35767799615859985, acc = 0.875\n",
            "Batch 29: loss = 0.35685473680496216, acc = 0.890625\n",
            "Batch 30: loss = 0.35704895853996277, acc = 0.880859375\n",
            "Batch 31: loss = 0.4000436067581177, acc = 0.8662109375\n",
            "Batch 32: loss = 0.41245442628860474, acc = 0.8623046875\n",
            "Batch 33: loss = 0.345152348279953, acc = 0.8916015625\n",
            "Batch 34: loss = 0.3844100534915924, acc = 0.873046875\n",
            "Batch 35: loss = 0.35804593563079834, acc = 0.8798828125\n",
            "Batch 36: loss = 0.31515568494796753, acc = 0.8974609375\n",
            "Batch 37: loss = 0.3341260552406311, acc = 0.8935546875\n",
            "Batch 38: loss = 0.38053199648857117, acc = 0.8720703125\n",
            "Batch 39: loss = 0.34535494446754456, acc = 0.8916015625\n",
            "Batch 40: loss = 0.33977147936820984, acc = 0.890625\n",
            "Batch 41: loss = 0.37690043449401855, acc = 0.8603515625\n",
            "Batch 42: loss = 0.37156158685684204, acc = 0.875\n",
            "Batch 43: loss = 0.4001926779747009, acc = 0.8671875\n",
            "Batch 44: loss = 0.3128267526626587, acc = 0.8916015625\n",
            "Batch 45: loss = 0.3216719329357147, acc = 0.896484375\n",
            "Batch 46: loss = 0.3116247057914734, acc = 0.904296875\n",
            "Batch 47: loss = 0.3335202634334564, acc = 0.8857421875\n",
            "Batch 48: loss = 0.31431272625923157, acc = 0.8916015625\n",
            "Batch 49: loss = 0.34157657623291016, acc = 0.892578125\n",
            "Batch 50: loss = 0.3373655676841736, acc = 0.88671875\n",
            "Batch 51: loss = 0.32565391063690186, acc = 0.89453125\n",
            "Batch 52: loss = 0.34971317648887634, acc = 0.880859375\n",
            "Batch 53: loss = 0.37380629777908325, acc = 0.8759765625\n",
            "Batch 54: loss = 0.2937886416912079, acc = 0.8974609375\n",
            "Batch 55: loss = 0.2955154478549957, acc = 0.912109375\n",
            "Batch 56: loss = 0.36766159534454346, acc = 0.875\n",
            "Batch 57: loss = 0.36377593874931335, acc = 0.880859375\n",
            "Batch 58: loss = 0.36221155524253845, acc = 0.8759765625\n",
            "Batch 59: loss = 0.28263208270072937, acc = 0.912109375\n",
            "Batch 60: loss = 0.3546157479286194, acc = 0.875\n",
            "Batch 61: loss = 0.3124317228794098, acc = 0.91015625\n",
            "Batch 62: loss = 0.40094563364982605, acc = 0.87109375\n",
            "Batch 63: loss = 0.3550902307033539, acc = 0.8818359375\n",
            "Batch 64: loss = 0.31935787200927734, acc = 0.896484375\n",
            "Batch 65: loss = 0.3771441578865051, acc = 0.8798828125\n",
            "Batch 66: loss = 0.35794657468795776, acc = 0.8798828125\n",
            "Batch 67: loss = 0.34777796268463135, acc = 0.884765625\n",
            "Batch 68: loss = 0.39493176341056824, acc = 0.8544921875\n",
            "Batch 69: loss = 0.37496599555015564, acc = 0.890625\n",
            "Batch 70: loss = 0.424530953168869, acc = 0.8623046875\n",
            "Batch 71: loss = 0.4137560725212097, acc = 0.857421875\n",
            "Batch 72: loss = 0.36551374197006226, acc = 0.8740234375\n",
            "Batch 73: loss = 0.3959125876426697, acc = 0.8681640625\n",
            "Batch 74: loss = 0.38616061210632324, acc = 0.8623046875\n",
            "Batch 75: loss = 0.42514896392822266, acc = 0.86328125\n",
            "Batch 76: loss = 0.3823827803134918, acc = 0.8642578125\n",
            "Batch 77: loss = 0.3571811318397522, acc = 0.875\n",
            "Batch 78: loss = 0.4034869968891144, acc = 0.8642578125\n",
            "Batch 79: loss = 0.3735486567020416, acc = 0.8720703125\n",
            "Batch 80: loss = 0.392970472574234, acc = 0.8525390625\n",
            "Batch 81: loss = 0.36703869700431824, acc = 0.8701171875\n",
            "Batch 82: loss = 0.3662026524543762, acc = 0.873046875\n",
            "Batch 83: loss = 0.3195439577102661, acc = 0.8916015625\n",
            "Batch 84: loss = 0.3820277452468872, acc = 0.8798828125\n",
            "Batch 85: loss = 0.3919994533061981, acc = 0.8642578125\n",
            "Batch 86: loss = 0.36786869168281555, acc = 0.8720703125\n",
            "Batch 87: loss = 0.36687928438186646, acc = 0.8720703125\n",
            "Batch 88: loss = 0.4192587435245514, acc = 0.8544921875\n",
            "Batch 89: loss = 0.35358160734176636, acc = 0.8818359375\n",
            "Batch 90: loss = 0.385165274143219, acc = 0.865234375\n",
            "Batch 91: loss = 0.4320680797100067, acc = 0.8623046875\n",
            "Batch 92: loss = 0.3948262631893158, acc = 0.8720703125\n",
            "Batch 93: loss = 0.3425566256046295, acc = 0.8857421875\n",
            "Batch 94: loss = 0.36380916833877563, acc = 0.8837890625\n",
            "Batch 95: loss = 0.39315083622932434, acc = 0.86328125\n",
            "Batch 96: loss = 0.44643062353134155, acc = 0.85546875\n",
            "Batch 97: loss = 0.3623872399330139, acc = 0.890625\n",
            "Batch 98: loss = 0.3616997003555298, acc = 0.884765625\n",
            "Batch 99: loss = 0.44760650396347046, acc = 0.8505859375\n",
            "Batch 100: loss = 0.4201359450817108, acc = 0.8623046875\n",
            "Batch 101: loss = 0.38054579496383667, acc = 0.8740234375\n",
            "Batch 102: loss = 0.40038415789604187, acc = 0.8701171875\n",
            "Batch 103: loss = 0.41723546385765076, acc = 0.8564453125\n",
            "Batch 104: loss = 0.349752813577652, acc = 0.8818359375\n",
            "Batch 105: loss = 0.36523982882499695, acc = 0.8828125\n",
            "Batch 106: loss = 0.38051655888557434, acc = 0.8740234375\n",
            "Batch 107: loss = 0.37446457147598267, acc = 0.8671875\n",
            "Batch 108: loss = 0.36755815148353577, acc = 0.8818359375\n",
            "Batch 109: loss = 0.357841432094574, acc = 0.8720703125\n",
            "Batch 110: loss = 0.35605064034461975, acc = 0.876953125\n",
            "Batch 111: loss = 0.3932114839553833, acc = 0.8720703125\n",
            "Batch 112: loss = 0.3927196264266968, acc = 0.875\n",
            "Batch 113: loss = 0.38274532556533813, acc = 0.876953125\n",
            "Batch 114: loss = 0.39175736904144287, acc = 0.873046875\n",
            "Batch 115: loss = 0.3985103368759155, acc = 0.861328125\n",
            "Batch 116: loss = 0.4003760814666748, acc = 0.8623046875\n",
            "Batch 117: loss = 0.3624395728111267, acc = 0.8740234375\n",
            "Batch 118: loss = 0.32593420147895813, acc = 0.9033203125\n",
            "Batch 119: loss = 0.3582133948802948, acc = 0.8798828125\n",
            "Batch 120: loss = 0.32820773124694824, acc = 0.8935546875\n",
            "Batch 121: loss = 0.3580508530139923, acc = 0.880859375\n",
            "Batch 122: loss = 0.3318259119987488, acc = 0.876953125\n",
            "Batch 123: loss = 0.36330002546310425, acc = 0.888671875\n",
            "Batch 124: loss = 0.38094213604927063, acc = 0.8740234375\n",
            "Batch 125: loss = 0.3829263150691986, acc = 0.8671875\n",
            "Batch 126: loss = 0.3844364583492279, acc = 0.880859375\n",
            "\n",
            "Epoch 72/100\n",
            "Batch 1: loss = 0.5127558708190918, acc = 0.853515625\n",
            "Batch 2: loss = 0.4191974401473999, acc = 0.873046875\n",
            "Batch 3: loss = 0.418753981590271, acc = 0.8681640625\n",
            "Batch 4: loss = 0.3633470833301544, acc = 0.8798828125\n",
            "Batch 5: loss = 0.3637754023075104, acc = 0.8837890625\n",
            "Batch 6: loss = 0.390542209148407, acc = 0.8828125\n",
            "Batch 7: loss = 0.3740800619125366, acc = 0.8642578125\n",
            "Batch 8: loss = 0.3692246079444885, acc = 0.873046875\n",
            "Batch 9: loss = 0.3529490828514099, acc = 0.8876953125\n",
            "Batch 10: loss = 0.3136395215988159, acc = 0.90234375\n",
            "Batch 11: loss = 0.3847046494483948, acc = 0.890625\n",
            "Batch 12: loss = 0.35115501284599304, acc = 0.87890625\n",
            "Batch 13: loss = 0.3283337652683258, acc = 0.8828125\n",
            "Batch 14: loss = 0.32562336325645447, acc = 0.890625\n",
            "Batch 15: loss = 0.3197564482688904, acc = 0.9013671875\n",
            "Batch 16: loss = 0.3737434148788452, acc = 0.8828125\n",
            "Batch 17: loss = 0.4190429449081421, acc = 0.861328125\n",
            "Batch 18: loss = 0.39044272899627686, acc = 0.8798828125\n",
            "Batch 19: loss = 0.3486020863056183, acc = 0.8779296875\n",
            "Batch 20: loss = 0.3637486696243286, acc = 0.8720703125\n",
            "Batch 21: loss = 0.34473010897636414, acc = 0.892578125\n",
            "Batch 22: loss = 0.3554622232913971, acc = 0.8818359375\n",
            "Batch 23: loss = 0.33214011788368225, acc = 0.8896484375\n",
            "Batch 24: loss = 0.32190650701522827, acc = 0.888671875\n",
            "Batch 25: loss = 0.37941837310791016, acc = 0.8798828125\n",
            "Batch 26: loss = 0.3411288857460022, acc = 0.8828125\n",
            "Batch 27: loss = 0.41104114055633545, acc = 0.857421875\n",
            "Batch 28: loss = 0.34046581387519836, acc = 0.8935546875\n",
            "Batch 29: loss = 0.34461233019828796, acc = 0.8818359375\n",
            "Batch 30: loss = 0.33161234855651855, acc = 0.90234375\n",
            "Batch 31: loss = 0.36465275287628174, acc = 0.8720703125\n",
            "Batch 32: loss = 0.4100000262260437, acc = 0.865234375\n",
            "Batch 33: loss = 0.3282894492149353, acc = 0.896484375\n",
            "Batch 34: loss = 0.38572877645492554, acc = 0.8701171875\n",
            "Batch 35: loss = 0.3449788987636566, acc = 0.87890625\n",
            "Batch 36: loss = 0.3172471225261688, acc = 0.8974609375\n",
            "Batch 37: loss = 0.30724185705184937, acc = 0.90234375\n",
            "Batch 38: loss = 0.331747829914093, acc = 0.8837890625\n",
            "Batch 39: loss = 0.3226679265499115, acc = 0.8935546875\n",
            "Batch 40: loss = 0.34636691212654114, acc = 0.8916015625\n",
            "Batch 41: loss = 0.32268595695495605, acc = 0.8896484375\n",
            "Batch 42: loss = 0.36916932463645935, acc = 0.869140625\n",
            "Batch 43: loss = 0.3909505307674408, acc = 0.8603515625\n",
            "Batch 44: loss = 0.3218212425708771, acc = 0.888671875\n",
            "Batch 45: loss = 0.3018241226673126, acc = 0.8974609375\n",
            "Batch 46: loss = 0.30419307947158813, acc = 0.8994140625\n",
            "Batch 47: loss = 0.32582059502601624, acc = 0.89453125\n",
            "Batch 48: loss = 0.33344346284866333, acc = 0.88671875\n",
            "Batch 49: loss = 0.3433825671672821, acc = 0.890625\n",
            "Batch 50: loss = 0.32696452736854553, acc = 0.8994140625\n",
            "Batch 51: loss = 0.3646255135536194, acc = 0.869140625\n",
            "Batch 52: loss = 0.3725828528404236, acc = 0.873046875\n",
            "Batch 53: loss = 0.3672347068786621, acc = 0.8798828125\n",
            "Batch 54: loss = 0.2759902775287628, acc = 0.908203125\n",
            "Batch 55: loss = 0.3007699251174927, acc = 0.91015625\n",
            "Batch 56: loss = 0.40296047925949097, acc = 0.8583984375\n",
            "Batch 57: loss = 0.35560286045074463, acc = 0.8720703125\n",
            "Batch 58: loss = 0.3568543791770935, acc = 0.87890625\n",
            "Batch 59: loss = 0.2860126495361328, acc = 0.90625\n",
            "Batch 60: loss = 0.3488873541355133, acc = 0.876953125\n",
            "Batch 61: loss = 0.3014531135559082, acc = 0.9111328125\n",
            "Batch 62: loss = 0.4333956241607666, acc = 0.857421875\n",
            "Batch 63: loss = 0.3671199381351471, acc = 0.8740234375\n",
            "Batch 64: loss = 0.304596483707428, acc = 0.9013671875\n",
            "Batch 65: loss = 0.35526132583618164, acc = 0.8837890625\n",
            "Batch 66: loss = 0.3807070851325989, acc = 0.884765625\n",
            "Batch 67: loss = 0.36395758390426636, acc = 0.869140625\n",
            "Batch 68: loss = 0.37808704376220703, acc = 0.8681640625\n",
            "Batch 69: loss = 0.3518317639827728, acc = 0.8759765625\n",
            "Batch 70: loss = 0.4059027433395386, acc = 0.85546875\n",
            "Batch 71: loss = 0.3616696298122406, acc = 0.869140625\n",
            "Batch 72: loss = 0.33938610553741455, acc = 0.8916015625\n",
            "Batch 73: loss = 0.407366544008255, acc = 0.8505859375\n",
            "Batch 74: loss = 0.40679964423179626, acc = 0.8740234375\n",
            "Batch 75: loss = 0.43521615862846375, acc = 0.85546875\n",
            "Batch 76: loss = 0.38500523567199707, acc = 0.865234375\n",
            "Batch 77: loss = 0.3884688913822174, acc = 0.8671875\n",
            "Batch 78: loss = 0.38149604201316833, acc = 0.875\n",
            "Batch 79: loss = 0.3564855456352234, acc = 0.8759765625\n",
            "Batch 80: loss = 0.3575778603553772, acc = 0.8798828125\n",
            "Batch 81: loss = 0.36894792318344116, acc = 0.873046875\n",
            "Batch 82: loss = 0.37021276354789734, acc = 0.876953125\n",
            "Batch 83: loss = 0.3313109278678894, acc = 0.880859375\n",
            "Batch 84: loss = 0.3888511657714844, acc = 0.865234375\n",
            "Batch 85: loss = 0.41832950711250305, acc = 0.8525390625\n",
            "Batch 86: loss = 0.36979520320892334, acc = 0.8740234375\n",
            "Batch 87: loss = 0.3773044943809509, acc = 0.8759765625\n",
            "Batch 88: loss = 0.46150973439216614, acc = 0.841796875\n",
            "Batch 89: loss = 0.3490481972694397, acc = 0.890625\n",
            "Batch 90: loss = 0.3819185495376587, acc = 0.873046875\n",
            "Batch 91: loss = 0.4035528898239136, acc = 0.8642578125\n",
            "Batch 92: loss = 0.39536628127098083, acc = 0.8740234375\n",
            "Batch 93: loss = 0.31781262159347534, acc = 0.8828125\n",
            "Batch 94: loss = 0.3559707701206207, acc = 0.880859375\n",
            "Batch 95: loss = 0.3700355291366577, acc = 0.8720703125\n",
            "Batch 96: loss = 0.42808660864830017, acc = 0.83984375\n",
            "Batch 97: loss = 0.36703816056251526, acc = 0.88671875\n",
            "Batch 98: loss = 0.36947423219680786, acc = 0.888671875\n",
            "Batch 99: loss = 0.42712724208831787, acc = 0.8603515625\n",
            "Batch 100: loss = 0.4312831461429596, acc = 0.8515625\n",
            "Batch 101: loss = 0.3404838442802429, acc = 0.880859375\n",
            "Batch 102: loss = 0.4186146557331085, acc = 0.8681640625\n",
            "Batch 103: loss = 0.41408756375312805, acc = 0.8681640625\n",
            "Batch 104: loss = 0.3338671028614044, acc = 0.890625\n",
            "Batch 105: loss = 0.3231636583805084, acc = 0.892578125\n",
            "Batch 106: loss = 0.3769172430038452, acc = 0.8701171875\n",
            "Batch 107: loss = 0.4181286692619324, acc = 0.84375\n",
            "Batch 108: loss = 0.3581375181674957, acc = 0.88671875\n",
            "Batch 109: loss = 0.3475795090198517, acc = 0.8876953125\n",
            "Batch 110: loss = 0.35919448733329773, acc = 0.89453125\n",
            "Batch 111: loss = 0.4072414040565491, acc = 0.8583984375\n",
            "Batch 112: loss = 0.42944854497909546, acc = 0.8642578125\n",
            "Batch 113: loss = 0.36570265889167786, acc = 0.87890625\n",
            "Batch 114: loss = 0.37353983521461487, acc = 0.876953125\n",
            "Batch 115: loss = 0.40150076150894165, acc = 0.869140625\n",
            "Batch 116: loss = 0.3993447422981262, acc = 0.8603515625\n",
            "Batch 117: loss = 0.3834518790245056, acc = 0.8828125\n",
            "Batch 118: loss = 0.3247917592525482, acc = 0.892578125\n",
            "Batch 119: loss = 0.35244014859199524, acc = 0.8720703125\n",
            "Batch 120: loss = 0.3308650553226471, acc = 0.8916015625\n",
            "Batch 121: loss = 0.32947641611099243, acc = 0.8955078125\n",
            "Batch 122: loss = 0.35318872332572937, acc = 0.8818359375\n",
            "Batch 123: loss = 0.3970794379711151, acc = 0.8681640625\n",
            "Batch 124: loss = 0.37958306074142456, acc = 0.8671875\n",
            "Batch 125: loss = 0.3871288299560547, acc = 0.8701171875\n",
            "Batch 126: loss = 0.40190207958221436, acc = 0.8603515625\n",
            "\n",
            "Epoch 73/100\n",
            "Batch 1: loss = 0.5215051174163818, acc = 0.845703125\n",
            "Batch 2: loss = 0.39374396204948425, acc = 0.8681640625\n",
            "Batch 3: loss = 0.3627927005290985, acc = 0.8798828125\n",
            "Batch 4: loss = 0.3356406092643738, acc = 0.8916015625\n",
            "Batch 5: loss = 0.36439576745033264, acc = 0.8779296875\n",
            "Batch 6: loss = 0.37675741314888, acc = 0.88671875\n",
            "Batch 7: loss = 0.33638796210289, acc = 0.8955078125\n",
            "Batch 8: loss = 0.3838925361633301, acc = 0.8896484375\n",
            "Batch 9: loss = 0.35084161162376404, acc = 0.8916015625\n",
            "Batch 10: loss = 0.3220213055610657, acc = 0.8818359375\n",
            "Batch 11: loss = 0.3839428424835205, acc = 0.8720703125\n",
            "Batch 12: loss = 0.3485284447669983, acc = 0.8857421875\n",
            "Batch 13: loss = 0.35823532938957214, acc = 0.8759765625\n",
            "Batch 14: loss = 0.3538520932197571, acc = 0.884765625\n",
            "Batch 15: loss = 0.3218157887458801, acc = 0.8955078125\n",
            "Batch 16: loss = 0.37192052602767944, acc = 0.8828125\n",
            "Batch 17: loss = 0.373128205537796, acc = 0.876953125\n",
            "Batch 18: loss = 0.40074342489242554, acc = 0.8720703125\n",
            "Batch 19: loss = 0.3615928590297699, acc = 0.87890625\n",
            "Batch 20: loss = 0.3642999529838562, acc = 0.876953125\n",
            "Batch 21: loss = 0.37282049655914307, acc = 0.875\n",
            "Batch 22: loss = 0.33388373255729675, acc = 0.888671875\n",
            "Batch 23: loss = 0.3761192560195923, acc = 0.8681640625\n",
            "Batch 24: loss = 0.30448028445243835, acc = 0.896484375\n",
            "Batch 25: loss = 0.36227893829345703, acc = 0.8896484375\n",
            "Batch 26: loss = 0.3528248369693756, acc = 0.8779296875\n",
            "Batch 27: loss = 0.38576439023017883, acc = 0.8759765625\n",
            "Batch 28: loss = 0.3687598705291748, acc = 0.8642578125\n",
            "Batch 29: loss = 0.33929330110549927, acc = 0.875\n",
            "Batch 30: loss = 0.3376493752002716, acc = 0.8896484375\n",
            "Batch 31: loss = 0.3505689203739166, acc = 0.8759765625\n",
            "Batch 32: loss = 0.4103275239467621, acc = 0.8623046875\n",
            "Batch 33: loss = 0.350076287984848, acc = 0.888671875\n",
            "Batch 34: loss = 0.410675048828125, acc = 0.8544921875\n",
            "Batch 35: loss = 0.33626317977905273, acc = 0.8896484375\n",
            "Batch 36: loss = 0.3183625042438507, acc = 0.8935546875\n",
            "Batch 37: loss = 0.31115061044692993, acc = 0.9091796875\n",
            "Batch 38: loss = 0.3180408179759979, acc = 0.89453125\n",
            "Batch 39: loss = 0.293000727891922, acc = 0.91015625\n",
            "Batch 40: loss = 0.3338797688484192, acc = 0.8955078125\n",
            "Batch 41: loss = 0.3118700087070465, acc = 0.8935546875\n",
            "Batch 42: loss = 0.3627810776233673, acc = 0.8828125\n",
            "Batch 43: loss = 0.40757837891578674, acc = 0.87109375\n",
            "Batch 44: loss = 0.3164665400981903, acc = 0.888671875\n",
            "Batch 45: loss = 0.33310800790786743, acc = 0.884765625\n",
            "Batch 46: loss = 0.3104895353317261, acc = 0.8994140625\n",
            "Batch 47: loss = 0.30732375383377075, acc = 0.8896484375\n",
            "Batch 48: loss = 0.3144909739494324, acc = 0.88671875\n",
            "Batch 49: loss = 0.3371833264827728, acc = 0.8974609375\n",
            "Batch 50: loss = 0.31738975644111633, acc = 0.9111328125\n",
            "Batch 51: loss = 0.3324393630027771, acc = 0.890625\n",
            "Batch 52: loss = 0.34955379366874695, acc = 0.8828125\n",
            "Batch 53: loss = 0.3510548770427704, acc = 0.880859375\n",
            "Batch 54: loss = 0.2852761149406433, acc = 0.9072265625\n",
            "Batch 55: loss = 0.28840455412864685, acc = 0.9052734375\n",
            "Batch 56: loss = 0.39476218819618225, acc = 0.861328125\n",
            "Batch 57: loss = 0.3749842345714569, acc = 0.8642578125\n",
            "Batch 58: loss = 0.39818328619003296, acc = 0.869140625\n",
            "Batch 59: loss = 0.3086859881877899, acc = 0.9033203125\n",
            "Batch 60: loss = 0.33626872301101685, acc = 0.8828125\n",
            "Batch 61: loss = 0.32615771889686584, acc = 0.9052734375\n",
            "Batch 62: loss = 0.42529061436653137, acc = 0.8583984375\n",
            "Batch 63: loss = 0.325941801071167, acc = 0.8876953125\n",
            "Batch 64: loss = 0.3084869384765625, acc = 0.90625\n",
            "Batch 65: loss = 0.38357335329055786, acc = 0.8720703125\n",
            "Batch 66: loss = 0.3543730080127716, acc = 0.88671875\n",
            "Batch 67: loss = 0.35201308131217957, acc = 0.884765625\n",
            "Batch 68: loss = 0.39363983273506165, acc = 0.8740234375\n",
            "Batch 69: loss = 0.3395163118839264, acc = 0.888671875\n",
            "Batch 70: loss = 0.42151618003845215, acc = 0.85546875\n",
            "Batch 71: loss = 0.4034586250782013, acc = 0.8544921875\n",
            "Batch 72: loss = 0.35843491554260254, acc = 0.8779296875\n",
            "Batch 73: loss = 0.34530770778656006, acc = 0.88671875\n",
            "Batch 74: loss = 0.3768071234226227, acc = 0.861328125\n",
            "Batch 75: loss = 0.42641928791999817, acc = 0.845703125\n",
            "Batch 76: loss = 0.40830308198928833, acc = 0.8515625\n",
            "Batch 77: loss = 0.3691835403442383, acc = 0.87890625\n",
            "Batch 78: loss = 0.4133419692516327, acc = 0.8564453125\n",
            "Batch 79: loss = 0.34850436449050903, acc = 0.892578125\n",
            "Batch 80: loss = 0.3351763188838959, acc = 0.875\n",
            "Batch 81: loss = 0.3849761188030243, acc = 0.865234375\n",
            "Batch 82: loss = 0.3246689438819885, acc = 0.892578125\n",
            "Batch 83: loss = 0.3453868627548218, acc = 0.87890625\n",
            "Batch 84: loss = 0.4056980311870575, acc = 0.8583984375\n",
            "Batch 85: loss = 0.3645729422569275, acc = 0.884765625\n",
            "Batch 86: loss = 0.36112159490585327, acc = 0.873046875\n",
            "Batch 87: loss = 0.37559619545936584, acc = 0.869140625\n",
            "Batch 88: loss = 0.3896227478981018, acc = 0.8681640625\n",
            "Batch 89: loss = 0.352251797914505, acc = 0.87890625\n",
            "Batch 90: loss = 0.359894722700119, acc = 0.87890625\n",
            "Batch 91: loss = 0.37425103783607483, acc = 0.875\n",
            "Batch 92: loss = 0.3986234962940216, acc = 0.861328125\n",
            "Batch 93: loss = 0.3336539566516876, acc = 0.8935546875\n",
            "Batch 94: loss = 0.3408074975013733, acc = 0.8857421875\n",
            "Batch 95: loss = 0.3571895360946655, acc = 0.8779296875\n",
            "Batch 96: loss = 0.39991217851638794, acc = 0.857421875\n",
            "Batch 97: loss = 0.38664987683296204, acc = 0.8740234375\n",
            "Batch 98: loss = 0.37272948026657104, acc = 0.873046875\n",
            "Batch 99: loss = 0.4020402729511261, acc = 0.869140625\n",
            "Batch 100: loss = 0.4265466034412384, acc = 0.8544921875\n",
            "Batch 101: loss = 0.3683564066886902, acc = 0.87890625\n",
            "Batch 102: loss = 0.445301353931427, acc = 0.8623046875\n",
            "Batch 103: loss = 0.3888881206512451, acc = 0.8740234375\n",
            "Batch 104: loss = 0.3345802426338196, acc = 0.8857421875\n",
            "Batch 105: loss = 0.31937357783317566, acc = 0.892578125\n",
            "Batch 106: loss = 0.3317081928253174, acc = 0.88671875\n",
            "Batch 107: loss = 0.3947257399559021, acc = 0.8642578125\n",
            "Batch 108: loss = 0.38094261288642883, acc = 0.876953125\n",
            "Batch 109: loss = 0.36865732073783875, acc = 0.875\n",
            "Batch 110: loss = 0.342194527387619, acc = 0.896484375\n",
            "Batch 111: loss = 0.37425747513771057, acc = 0.87890625\n",
            "Batch 112: loss = 0.3954259157180786, acc = 0.87890625\n",
            "Batch 113: loss = 0.3486000895500183, acc = 0.8798828125\n",
            "Batch 114: loss = 0.3579748868942261, acc = 0.8876953125\n",
            "Batch 115: loss = 0.3651184141635895, acc = 0.8828125\n",
            "Batch 116: loss = 0.4053340554237366, acc = 0.873046875\n",
            "Batch 117: loss = 0.3723483979701996, acc = 0.8857421875\n",
            "Batch 118: loss = 0.3183346688747406, acc = 0.91015625\n",
            "Batch 119: loss = 0.3513067662715912, acc = 0.8740234375\n",
            "Batch 120: loss = 0.30989503860473633, acc = 0.900390625\n",
            "Batch 121: loss = 0.3628186583518982, acc = 0.88671875\n",
            "Batch 122: loss = 0.34909313917160034, acc = 0.8896484375\n",
            "Batch 123: loss = 0.34970077872276306, acc = 0.8759765625\n",
            "Batch 124: loss = 0.412336528301239, acc = 0.859375\n",
            "Batch 125: loss = 0.38674572110176086, acc = 0.865234375\n",
            "Batch 126: loss = 0.38764357566833496, acc = 0.873046875\n",
            "\n",
            "Epoch 74/100\n",
            "Batch 1: loss = 0.48334893584251404, acc = 0.87109375\n",
            "Batch 2: loss = 0.3654063940048218, acc = 0.87890625\n",
            "Batch 3: loss = 0.36231017112731934, acc = 0.88671875\n",
            "Batch 4: loss = 0.3590361475944519, acc = 0.87890625\n",
            "Batch 5: loss = 0.3548179864883423, acc = 0.880859375\n",
            "Batch 6: loss = 0.4132586717605591, acc = 0.8623046875\n",
            "Batch 7: loss = 0.3456577956676483, acc = 0.8984375\n",
            "Batch 8: loss = 0.34704500436782837, acc = 0.896484375\n",
            "Batch 9: loss = 0.35560327768325806, acc = 0.88671875\n",
            "Batch 10: loss = 0.31234070658683777, acc = 0.8994140625\n",
            "Batch 11: loss = 0.38334786891937256, acc = 0.8837890625\n",
            "Batch 12: loss = 0.33314430713653564, acc = 0.8896484375\n",
            "Batch 13: loss = 0.35699549317359924, acc = 0.87890625\n",
            "Batch 14: loss = 0.3323465585708618, acc = 0.900390625\n",
            "Batch 15: loss = 0.3234984278678894, acc = 0.8984375\n",
            "Batch 16: loss = 0.3655737638473511, acc = 0.8837890625\n",
            "Batch 17: loss = 0.3803670406341553, acc = 0.875\n",
            "Batch 18: loss = 0.39100709557533264, acc = 0.8671875\n",
            "Batch 19: loss = 0.3170068562030792, acc = 0.8955078125\n",
            "Batch 20: loss = 0.3736037313938141, acc = 0.865234375\n",
            "Batch 21: loss = 0.37381336092948914, acc = 0.875\n",
            "Batch 22: loss = 0.34772467613220215, acc = 0.8818359375\n",
            "Batch 23: loss = 0.3615378141403198, acc = 0.876953125\n",
            "Batch 24: loss = 0.32234126329421997, acc = 0.890625\n",
            "Batch 25: loss = 0.336284875869751, acc = 0.8916015625\n",
            "Batch 26: loss = 0.33640557527542114, acc = 0.87890625\n",
            "Batch 27: loss = 0.3773420453071594, acc = 0.8740234375\n",
            "Batch 28: loss = 0.32857802510261536, acc = 0.8876953125\n",
            "Batch 29: loss = 0.3701891601085663, acc = 0.8779296875\n",
            "Batch 30: loss = 0.3155912160873413, acc = 0.8994140625\n",
            "Batch 31: loss = 0.3369012176990509, acc = 0.880859375\n",
            "Batch 32: loss = 0.42780590057373047, acc = 0.8505859375\n",
            "Batch 33: loss = 0.31531429290771484, acc = 0.8955078125\n",
            "Batch 34: loss = 0.3489183485507965, acc = 0.8935546875\n",
            "Batch 35: loss = 0.3655496835708618, acc = 0.876953125\n",
            "Batch 36: loss = 0.31938230991363525, acc = 0.8994140625\n",
            "Batch 37: loss = 0.315165638923645, acc = 0.8994140625\n",
            "Batch 38: loss = 0.319898784160614, acc = 0.892578125\n",
            "Batch 39: loss = 0.3273279666900635, acc = 0.8984375\n",
            "Batch 40: loss = 0.3186267018318176, acc = 0.8955078125\n",
            "Batch 41: loss = 0.3286624550819397, acc = 0.884765625\n",
            "Batch 42: loss = 0.326541543006897, acc = 0.8818359375\n",
            "Batch 43: loss = 0.36792004108428955, acc = 0.880859375\n",
            "Batch 44: loss = 0.2907041013240814, acc = 0.90625\n",
            "Batch 45: loss = 0.31482160091400146, acc = 0.8828125\n",
            "Batch 46: loss = 0.2991064488887787, acc = 0.8994140625\n",
            "Batch 47: loss = 0.34102392196655273, acc = 0.8837890625\n",
            "Batch 48: loss = 0.3391996920108795, acc = 0.8984375\n",
            "Batch 49: loss = 0.34434112906455994, acc = 0.8896484375\n",
            "Batch 50: loss = 0.3194342255592346, acc = 0.8935546875\n",
            "Batch 51: loss = 0.33828306198120117, acc = 0.87109375\n",
            "Batch 52: loss = 0.36225762963294983, acc = 0.8798828125\n",
            "Batch 53: loss = 0.3389453887939453, acc = 0.888671875\n",
            "Batch 54: loss = 0.29702669382095337, acc = 0.896484375\n",
            "Batch 55: loss = 0.28531521558761597, acc = 0.9140625\n",
            "Batch 56: loss = 0.34492847323417664, acc = 0.88671875\n",
            "Batch 57: loss = 0.3520020842552185, acc = 0.875\n",
            "Batch 58: loss = 0.3754289746284485, acc = 0.8681640625\n",
            "Batch 59: loss = 0.2794320583343506, acc = 0.9052734375\n",
            "Batch 60: loss = 0.3552088141441345, acc = 0.876953125\n",
            "Batch 61: loss = 0.3181816041469574, acc = 0.8955078125\n",
            "Batch 62: loss = 0.41049522161483765, acc = 0.8564453125\n",
            "Batch 63: loss = 0.3307053744792938, acc = 0.890625\n",
            "Batch 64: loss = 0.3290002644062042, acc = 0.8994140625\n",
            "Batch 65: loss = 0.3663257956504822, acc = 0.8681640625\n",
            "Batch 66: loss = 0.34593623876571655, acc = 0.87890625\n",
            "Batch 67: loss = 0.3511451780796051, acc = 0.8759765625\n",
            "Batch 68: loss = 0.35407277941703796, acc = 0.8876953125\n",
            "Batch 69: loss = 0.3149639666080475, acc = 0.892578125\n",
            "Batch 70: loss = 0.38760489225387573, acc = 0.876953125\n",
            "Batch 71: loss = 0.36167699098587036, acc = 0.8759765625\n",
            "Batch 72: loss = 0.3561209440231323, acc = 0.8896484375\n",
            "Batch 73: loss = 0.39761975407600403, acc = 0.8701171875\n",
            "Batch 74: loss = 0.399311363697052, acc = 0.86328125\n",
            "Batch 75: loss = 0.4474940299987793, acc = 0.8525390625\n",
            "Batch 76: loss = 0.3774234652519226, acc = 0.8779296875\n",
            "Batch 77: loss = 0.35323384404182434, acc = 0.8896484375\n",
            "Batch 78: loss = 0.3941234350204468, acc = 0.8828125\n",
            "Batch 79: loss = 0.363260418176651, acc = 0.87109375\n",
            "Batch 80: loss = 0.359804630279541, acc = 0.8701171875\n",
            "Batch 81: loss = 0.3796165883541107, acc = 0.876953125\n",
            "Batch 82: loss = 0.3431616723537445, acc = 0.890625\n",
            "Batch 83: loss = 0.31709909439086914, acc = 0.8916015625\n",
            "Batch 84: loss = 0.38290226459503174, acc = 0.876953125\n",
            "Batch 85: loss = 0.4313812255859375, acc = 0.8544921875\n",
            "Batch 86: loss = 0.3585360646247864, acc = 0.8740234375\n",
            "Batch 87: loss = 0.3510911166667938, acc = 0.8720703125\n",
            "Batch 88: loss = 0.407401978969574, acc = 0.865234375\n",
            "Batch 89: loss = 0.34023672342300415, acc = 0.8974609375\n",
            "Batch 90: loss = 0.3480866253376007, acc = 0.880859375\n",
            "Batch 91: loss = 0.3839651942253113, acc = 0.8720703125\n",
            "Batch 92: loss = 0.39151859283447266, acc = 0.87109375\n",
            "Batch 93: loss = 0.3290841281414032, acc = 0.8935546875\n",
            "Batch 94: loss = 0.3853224217891693, acc = 0.8671875\n",
            "Batch 95: loss = 0.37666141986846924, acc = 0.8740234375\n",
            "Batch 96: loss = 0.4153980612754822, acc = 0.8515625\n",
            "Batch 97: loss = 0.3942517936229706, acc = 0.8779296875\n",
            "Batch 98: loss = 0.345345139503479, acc = 0.87890625\n",
            "Batch 99: loss = 0.36570942401885986, acc = 0.8701171875\n",
            "Batch 100: loss = 0.383737713098526, acc = 0.8681640625\n",
            "Batch 101: loss = 0.3687741458415985, acc = 0.875\n",
            "Batch 102: loss = 0.3929007053375244, acc = 0.8740234375\n",
            "Batch 103: loss = 0.3832337558269501, acc = 0.8740234375\n",
            "Batch 104: loss = 0.3480556607246399, acc = 0.884765625\n",
            "Batch 105: loss = 0.3177125155925751, acc = 0.8935546875\n",
            "Batch 106: loss = 0.3445361852645874, acc = 0.8740234375\n",
            "Batch 107: loss = 0.3396475613117218, acc = 0.884765625\n",
            "Batch 108: loss = 0.3805604875087738, acc = 0.8681640625\n",
            "Batch 109: loss = 0.33561497926712036, acc = 0.876953125\n",
            "Batch 110: loss = 0.3209911584854126, acc = 0.896484375\n",
            "Batch 111: loss = 0.367070734500885, acc = 0.8798828125\n",
            "Batch 112: loss = 0.4000800848007202, acc = 0.873046875\n",
            "Batch 113: loss = 0.371788889169693, acc = 0.8759765625\n",
            "Batch 114: loss = 0.3926975131034851, acc = 0.8798828125\n",
            "Batch 115: loss = 0.34953773021698, acc = 0.8876953125\n",
            "Batch 116: loss = 0.3907272219657898, acc = 0.873046875\n",
            "Batch 117: loss = 0.33740493655204773, acc = 0.8828125\n",
            "Batch 118: loss = 0.30100443959236145, acc = 0.9052734375\n",
            "Batch 119: loss = 0.350638747215271, acc = 0.8828125\n",
            "Batch 120: loss = 0.3243485689163208, acc = 0.896484375\n",
            "Batch 121: loss = 0.34435132145881653, acc = 0.8857421875\n",
            "Batch 122: loss = 0.33513951301574707, acc = 0.8818359375\n",
            "Batch 123: loss = 0.35518866777420044, acc = 0.8935546875\n",
            "Batch 124: loss = 0.345824271440506, acc = 0.8857421875\n",
            "Batch 125: loss = 0.3798674941062927, acc = 0.875\n",
            "Batch 126: loss = 0.38680410385131836, acc = 0.87109375\n",
            "\n",
            "Epoch 75/100\n",
            "Batch 1: loss = 0.4822128415107727, acc = 0.8642578125\n",
            "Batch 2: loss = 0.3965962529182434, acc = 0.8701171875\n",
            "Batch 3: loss = 0.3449021279811859, acc = 0.888671875\n",
            "Batch 4: loss = 0.34278181195259094, acc = 0.8876953125\n",
            "Batch 5: loss = 0.33977028727531433, acc = 0.8935546875\n",
            "Batch 6: loss = 0.4022939205169678, acc = 0.865234375\n",
            "Batch 7: loss = 0.35464105010032654, acc = 0.875\n",
            "Batch 8: loss = 0.34699177742004395, acc = 0.875\n",
            "Batch 9: loss = 0.3442417085170746, acc = 0.8994140625\n",
            "Batch 10: loss = 0.32277631759643555, acc = 0.88671875\n",
            "Batch 11: loss = 0.3765842318534851, acc = 0.8828125\n",
            "Batch 12: loss = 0.36548539996147156, acc = 0.8740234375\n",
            "Batch 13: loss = 0.3367066979408264, acc = 0.8857421875\n",
            "Batch 14: loss = 0.3594215214252472, acc = 0.8896484375\n",
            "Batch 15: loss = 0.32666510343551636, acc = 0.8916015625\n",
            "Batch 16: loss = 0.3640241026878357, acc = 0.884765625\n",
            "Batch 17: loss = 0.3648558557033539, acc = 0.880859375\n",
            "Batch 18: loss = 0.404599666595459, acc = 0.8701171875\n",
            "Batch 19: loss = 0.31683170795440674, acc = 0.8935546875\n",
            "Batch 20: loss = 0.3629710078239441, acc = 0.869140625\n",
            "Batch 21: loss = 0.3574836552143097, acc = 0.8828125\n",
            "Batch 22: loss = 0.3353796899318695, acc = 0.8828125\n",
            "Batch 23: loss = 0.34625136852264404, acc = 0.880859375\n",
            "Batch 24: loss = 0.288643479347229, acc = 0.9091796875\n",
            "Batch 25: loss = 0.3686017692089081, acc = 0.8828125\n",
            "Batch 26: loss = 0.3269546627998352, acc = 0.88671875\n",
            "Batch 27: loss = 0.3769181966781616, acc = 0.8740234375\n",
            "Batch 28: loss = 0.3417775630950928, acc = 0.884765625\n",
            "Batch 29: loss = 0.3451169729232788, acc = 0.884765625\n",
            "Batch 30: loss = 0.3246282935142517, acc = 0.900390625\n",
            "Batch 31: loss = 0.352315753698349, acc = 0.8818359375\n",
            "Batch 32: loss = 0.3877309560775757, acc = 0.8720703125\n",
            "Batch 33: loss = 0.3243923783302307, acc = 0.8935546875\n",
            "Batch 34: loss = 0.35630133748054504, acc = 0.892578125\n",
            "Batch 35: loss = 0.34956538677215576, acc = 0.8896484375\n",
            "Batch 36: loss = 0.31500500440597534, acc = 0.8955078125\n",
            "Batch 37: loss = 0.3037605285644531, acc = 0.8916015625\n",
            "Batch 38: loss = 0.33925861120224, acc = 0.890625\n",
            "Batch 39: loss = 0.2924053370952606, acc = 0.9111328125\n",
            "Batch 40: loss = 0.3317949175834656, acc = 0.8876953125\n",
            "Batch 41: loss = 0.3405298590660095, acc = 0.87890625\n",
            "Batch 42: loss = 0.35787340998649597, acc = 0.8779296875\n",
            "Batch 43: loss = 0.38843920826911926, acc = 0.8740234375\n",
            "Batch 44: loss = 0.3201982378959656, acc = 0.8818359375\n",
            "Batch 45: loss = 0.3047550916671753, acc = 0.896484375\n",
            "Batch 46: loss = 0.2888453006744385, acc = 0.9013671875\n",
            "Batch 47: loss = 0.32003679871559143, acc = 0.88671875\n",
            "Batch 48: loss = 0.33458584547042847, acc = 0.890625\n",
            "Batch 49: loss = 0.31884971261024475, acc = 0.904296875\n",
            "Batch 50: loss = 0.32428523898124695, acc = 0.8994140625\n",
            "Batch 51: loss = 0.3431640565395355, acc = 0.8818359375\n",
            "Batch 52: loss = 0.32845717668533325, acc = 0.8916015625\n",
            "Batch 53: loss = 0.33888334035873413, acc = 0.8828125\n",
            "Batch 54: loss = 0.2643357813358307, acc = 0.904296875\n",
            "Batch 55: loss = 0.2997768521308899, acc = 0.9072265625\n",
            "Batch 56: loss = 0.385661780834198, acc = 0.8671875\n",
            "Batch 57: loss = 0.3482683598995209, acc = 0.8759765625\n",
            "Batch 58: loss = 0.3976013958454132, acc = 0.8681640625\n",
            "Batch 59: loss = 0.29058924317359924, acc = 0.9111328125\n",
            "Batch 60: loss = 0.3305380046367645, acc = 0.8857421875\n",
            "Batch 61: loss = 0.31009620428085327, acc = 0.9033203125\n",
            "Batch 62: loss = 0.38771554827690125, acc = 0.8720703125\n",
            "Batch 63: loss = 0.321419894695282, acc = 0.8876953125\n",
            "Batch 64: loss = 0.29416415095329285, acc = 0.90234375\n",
            "Batch 65: loss = 0.3640629053115845, acc = 0.89453125\n",
            "Batch 66: loss = 0.3661583364009857, acc = 0.8740234375\n",
            "Batch 67: loss = 0.37270215153694153, acc = 0.8720703125\n",
            "Batch 68: loss = 0.39538854360580444, acc = 0.8818359375\n",
            "Batch 69: loss = 0.3530096411705017, acc = 0.888671875\n",
            "Batch 70: loss = 0.37455153465270996, acc = 0.875\n",
            "Batch 71: loss = 0.3531710207462311, acc = 0.8837890625\n",
            "Batch 72: loss = 0.3154200613498688, acc = 0.8974609375\n",
            "Batch 73: loss = 0.40483802556991577, acc = 0.86328125\n",
            "Batch 74: loss = 0.37668293714523315, acc = 0.8798828125\n",
            "Batch 75: loss = 0.395172655582428, acc = 0.86328125\n",
            "Batch 76: loss = 0.3470251262187958, acc = 0.87890625\n",
            "Batch 77: loss = 0.3292500078678131, acc = 0.8994140625\n",
            "Batch 78: loss = 0.3643946051597595, acc = 0.8701171875\n",
            "Batch 79: loss = 0.35620105266571045, acc = 0.8837890625\n",
            "Batch 80: loss = 0.3620988428592682, acc = 0.8701171875\n",
            "Batch 81: loss = 0.32354408502578735, acc = 0.890625\n",
            "Batch 82: loss = 0.3366644084453583, acc = 0.8720703125\n",
            "Batch 83: loss = 0.2986840009689331, acc = 0.908203125\n",
            "Batch 84: loss = 0.3873179256916046, acc = 0.87109375\n",
            "Batch 85: loss = 0.3640431761741638, acc = 0.8818359375\n",
            "Batch 86: loss = 0.3282235562801361, acc = 0.88671875\n",
            "Batch 87: loss = 0.3563034236431122, acc = 0.8857421875\n",
            "Batch 88: loss = 0.41478824615478516, acc = 0.86328125\n",
            "Batch 89: loss = 0.3753553330898285, acc = 0.86328125\n",
            "Batch 90: loss = 0.3520655632019043, acc = 0.8779296875\n",
            "Batch 91: loss = 0.367208331823349, acc = 0.8818359375\n",
            "Batch 92: loss = 0.35763847827911377, acc = 0.892578125\n",
            "Batch 93: loss = 0.3435891270637512, acc = 0.87890625\n",
            "Batch 94: loss = 0.3516981303691864, acc = 0.890625\n",
            "Batch 95: loss = 0.3749808669090271, acc = 0.8828125\n",
            "Batch 96: loss = 0.4240013659000397, acc = 0.845703125\n",
            "Batch 97: loss = 0.348137766122818, acc = 0.9033203125\n",
            "Batch 98: loss = 0.3420841097831726, acc = 0.8935546875\n",
            "Batch 99: loss = 0.4107058048248291, acc = 0.8671875\n",
            "Batch 100: loss = 0.3938373923301697, acc = 0.8515625\n",
            "Batch 101: loss = 0.3480902910232544, acc = 0.8876953125\n",
            "Batch 102: loss = 0.35685431957244873, acc = 0.88671875\n",
            "Batch 103: loss = 0.37930792570114136, acc = 0.865234375\n",
            "Batch 104: loss = 0.3241029679775238, acc = 0.8916015625\n",
            "Batch 105: loss = 0.3121618926525116, acc = 0.892578125\n",
            "Batch 106: loss = 0.3536800146102905, acc = 0.8779296875\n",
            "Batch 107: loss = 0.33838701248168945, acc = 0.88671875\n",
            "Batch 108: loss = 0.35356876254081726, acc = 0.884765625\n",
            "Batch 109: loss = 0.32968905568122864, acc = 0.8828125\n",
            "Batch 110: loss = 0.34474655985832214, acc = 0.89453125\n",
            "Batch 111: loss = 0.37305164337158203, acc = 0.8740234375\n",
            "Batch 112: loss = 0.40607818961143494, acc = 0.8681640625\n",
            "Batch 113: loss = 0.3571074604988098, acc = 0.8828125\n",
            "Batch 114: loss = 0.3571014702320099, acc = 0.880859375\n",
            "Batch 115: loss = 0.34646984934806824, acc = 0.8857421875\n",
            "Batch 116: loss = 0.40339672565460205, acc = 0.8681640625\n",
            "Batch 117: loss = 0.31649482250213623, acc = 0.8994140625\n",
            "Batch 118: loss = 0.2808932065963745, acc = 0.9140625\n",
            "Batch 119: loss = 0.3463468849658966, acc = 0.87890625\n",
            "Batch 120: loss = 0.3330445885658264, acc = 0.8818359375\n",
            "Batch 121: loss = 0.3674774765968323, acc = 0.8642578125\n",
            "Batch 122: loss = 0.3274042308330536, acc = 0.89453125\n",
            "Batch 123: loss = 0.3821285367012024, acc = 0.87890625\n",
            "Batch 124: loss = 0.3848215937614441, acc = 0.865234375\n",
            "Batch 125: loss = 0.37305936217308044, acc = 0.8701171875\n",
            "Batch 126: loss = 0.3932417631149292, acc = 0.865234375\n",
            "\n",
            "Epoch 76/100\n",
            "Batch 1: loss = 0.5118358135223389, acc = 0.853515625\n",
            "Batch 2: loss = 0.42593178153038025, acc = 0.865234375\n",
            "Batch 3: loss = 0.38972094655036926, acc = 0.88671875\n",
            "Batch 4: loss = 0.3355303108692169, acc = 0.896484375\n",
            "Batch 5: loss = 0.3563859760761261, acc = 0.892578125\n",
            "Batch 6: loss = 0.39489179849624634, acc = 0.873046875\n",
            "Batch 7: loss = 0.3634050786495209, acc = 0.8779296875\n",
            "Batch 8: loss = 0.3298732042312622, acc = 0.8935546875\n",
            "Batch 9: loss = 0.3526618182659149, acc = 0.890625\n",
            "Batch 10: loss = 0.31344905495643616, acc = 0.8974609375\n",
            "Batch 11: loss = 0.37295663356781006, acc = 0.87890625\n",
            "Batch 12: loss = 0.31538137793540955, acc = 0.8828125\n",
            "Batch 13: loss = 0.3306838870048523, acc = 0.8916015625\n",
            "Batch 14: loss = 0.3223070502281189, acc = 0.9013671875\n",
            "Batch 15: loss = 0.32371774315834045, acc = 0.900390625\n",
            "Batch 16: loss = 0.33649060130119324, acc = 0.8916015625\n",
            "Batch 17: loss = 0.33660104870796204, acc = 0.8974609375\n",
            "Batch 18: loss = 0.4074530005455017, acc = 0.8798828125\n",
            "Batch 19: loss = 0.30928850173950195, acc = 0.904296875\n",
            "Batch 20: loss = 0.3577597141265869, acc = 0.8701171875\n",
            "Batch 21: loss = 0.3406454622745514, acc = 0.890625\n",
            "Batch 22: loss = 0.33928513526916504, acc = 0.8876953125\n",
            "Batch 23: loss = 0.3583546280860901, acc = 0.8818359375\n",
            "Batch 24: loss = 0.31025582551956177, acc = 0.8935546875\n",
            "Batch 25: loss = 0.360744833946228, acc = 0.8798828125\n",
            "Batch 26: loss = 0.32287490367889404, acc = 0.8984375\n",
            "Batch 27: loss = 0.3944948613643646, acc = 0.8662109375\n",
            "Batch 28: loss = 0.33429136872291565, acc = 0.8876953125\n",
            "Batch 29: loss = 0.3668743073940277, acc = 0.865234375\n",
            "Batch 30: loss = 0.31962889432907104, acc = 0.8857421875\n",
            "Batch 31: loss = 0.3435783088207245, acc = 0.890625\n",
            "Batch 32: loss = 0.39329707622528076, acc = 0.8759765625\n",
            "Batch 33: loss = 0.3187022805213928, acc = 0.9033203125\n",
            "Batch 34: loss = 0.35736095905303955, acc = 0.8896484375\n",
            "Batch 35: loss = 0.3311070203781128, acc = 0.8876953125\n",
            "Batch 36: loss = 0.3072892129421234, acc = 0.900390625\n",
            "Batch 37: loss = 0.3190118372440338, acc = 0.8974609375\n",
            "Batch 38: loss = 0.3407675623893738, acc = 0.888671875\n",
            "Batch 39: loss = 0.31967633962631226, acc = 0.8994140625\n",
            "Batch 40: loss = 0.3232984244823456, acc = 0.8876953125\n",
            "Batch 41: loss = 0.3113287091255188, acc = 0.904296875\n",
            "Batch 42: loss = 0.32460880279541016, acc = 0.88671875\n",
            "Batch 43: loss = 0.3935311436653137, acc = 0.8544921875\n",
            "Batch 44: loss = 0.28648126125335693, acc = 0.9091796875\n",
            "Batch 45: loss = 0.3080224394798279, acc = 0.896484375\n",
            "Batch 46: loss = 0.2950737476348877, acc = 0.9013671875\n",
            "Batch 47: loss = 0.3131125271320343, acc = 0.8955078125\n",
            "Batch 48: loss = 0.32940006256103516, acc = 0.8955078125\n",
            "Batch 49: loss = 0.3099064230918884, acc = 0.90234375\n",
            "Batch 50: loss = 0.33620190620422363, acc = 0.8818359375\n",
            "Batch 51: loss = 0.3306986093521118, acc = 0.8818359375\n",
            "Batch 52: loss = 0.3301701545715332, acc = 0.8916015625\n",
            "Batch 53: loss = 0.3376067876815796, acc = 0.888671875\n",
            "Batch 54: loss = 0.28814345598220825, acc = 0.91015625\n",
            "Batch 55: loss = 0.29312610626220703, acc = 0.9033203125\n",
            "Batch 56: loss = 0.36159583926200867, acc = 0.884765625\n",
            "Batch 57: loss = 0.3387562036514282, acc = 0.8876953125\n",
            "Batch 58: loss = 0.3786044418811798, acc = 0.8759765625\n",
            "Batch 59: loss = 0.27009326219558716, acc = 0.9140625\n",
            "Batch 60: loss = 0.34620964527130127, acc = 0.8837890625\n",
            "Batch 61: loss = 0.29539263248443604, acc = 0.91015625\n",
            "Batch 62: loss = 0.3792808949947357, acc = 0.8798828125\n",
            "Batch 63: loss = 0.34512239694595337, acc = 0.88671875\n",
            "Batch 64: loss = 0.341640442609787, acc = 0.8857421875\n",
            "Batch 65: loss = 0.36310631036758423, acc = 0.87890625\n",
            "Batch 66: loss = 0.34639647603034973, acc = 0.88671875\n",
            "Batch 67: loss = 0.34795475006103516, acc = 0.8740234375\n",
            "Batch 68: loss = 0.38343295454978943, acc = 0.8876953125\n",
            "Batch 69: loss = 0.36276698112487793, acc = 0.873046875\n",
            "Batch 70: loss = 0.3889049291610718, acc = 0.8779296875\n",
            "Batch 71: loss = 0.35042041540145874, acc = 0.88671875\n",
            "Batch 72: loss = 0.3397814631462097, acc = 0.880859375\n",
            "Batch 73: loss = 0.3543185293674469, acc = 0.875\n",
            "Batch 74: loss = 0.3723853528499603, acc = 0.8759765625\n",
            "Batch 75: loss = 0.42753398418426514, acc = 0.8603515625\n",
            "Batch 76: loss = 0.38079848885536194, acc = 0.865234375\n",
            "Batch 77: loss = 0.3437831401824951, acc = 0.8857421875\n",
            "Batch 78: loss = 0.3504236340522766, acc = 0.876953125\n",
            "Batch 79: loss = 0.32326018810272217, acc = 0.8955078125\n",
            "Batch 80: loss = 0.35607415437698364, acc = 0.87890625\n",
            "Batch 81: loss = 0.3378337621688843, acc = 0.8916015625\n",
            "Batch 82: loss = 0.3146193027496338, acc = 0.904296875\n",
            "Batch 83: loss = 0.3093229830265045, acc = 0.8896484375\n",
            "Batch 84: loss = 0.35966843366622925, acc = 0.8828125\n",
            "Batch 85: loss = 0.36873698234558105, acc = 0.87890625\n",
            "Batch 86: loss = 0.35171616077423096, acc = 0.8828125\n",
            "Batch 87: loss = 0.3660721480846405, acc = 0.8779296875\n",
            "Batch 88: loss = 0.3955530524253845, acc = 0.876953125\n",
            "Batch 89: loss = 0.3261262774467468, acc = 0.892578125\n",
            "Batch 90: loss = 0.38044488430023193, acc = 0.8681640625\n",
            "Batch 91: loss = 0.36868181824684143, acc = 0.8798828125\n",
            "Batch 92: loss = 0.3687437176704407, acc = 0.8740234375\n",
            "Batch 93: loss = 0.31660306453704834, acc = 0.89453125\n",
            "Batch 94: loss = 0.34460970759391785, acc = 0.88671875\n",
            "Batch 95: loss = 0.37303993105888367, acc = 0.869140625\n",
            "Batch 96: loss = 0.37828660011291504, acc = 0.8671875\n",
            "Batch 97: loss = 0.34447532892227173, acc = 0.8857421875\n",
            "Batch 98: loss = 0.38612061738967896, acc = 0.8623046875\n",
            "Batch 99: loss = 0.36338675022125244, acc = 0.88671875\n",
            "Batch 100: loss = 0.38564157485961914, acc = 0.8583984375\n",
            "Batch 101: loss = 0.3390289545059204, acc = 0.8828125\n",
            "Batch 102: loss = 0.39218583703041077, acc = 0.8662109375\n",
            "Batch 103: loss = 0.3500175476074219, acc = 0.8798828125\n",
            "Batch 104: loss = 0.3416767120361328, acc = 0.890625\n",
            "Batch 105: loss = 0.32898277044296265, acc = 0.8955078125\n",
            "Batch 106: loss = 0.3356947600841522, acc = 0.884765625\n",
            "Batch 107: loss = 0.3848181664943695, acc = 0.8681640625\n",
            "Batch 108: loss = 0.3790867328643799, acc = 0.8671875\n",
            "Batch 109: loss = 0.38052698969841003, acc = 0.8642578125\n",
            "Batch 110: loss = 0.327210009098053, acc = 0.9033203125\n",
            "Batch 111: loss = 0.3676632046699524, acc = 0.8828125\n",
            "Batch 112: loss = 0.38934898376464844, acc = 0.873046875\n",
            "Batch 113: loss = 0.36498311161994934, acc = 0.880859375\n",
            "Batch 114: loss = 0.37367337942123413, acc = 0.888671875\n",
            "Batch 115: loss = 0.3600599765777588, acc = 0.8837890625\n",
            "Batch 116: loss = 0.37877151370048523, acc = 0.8779296875\n",
            "Batch 117: loss = 0.3524751663208008, acc = 0.875\n",
            "Batch 118: loss = 0.31807181239128113, acc = 0.88671875\n",
            "Batch 119: loss = 0.35343822836875916, acc = 0.876953125\n",
            "Batch 120: loss = 0.31979215145111084, acc = 0.880859375\n",
            "Batch 121: loss = 0.32817837595939636, acc = 0.89453125\n",
            "Batch 122: loss = 0.3313562273979187, acc = 0.896484375\n",
            "Batch 123: loss = 0.35253384709358215, acc = 0.8896484375\n",
            "Batch 124: loss = 0.38844531774520874, acc = 0.853515625\n",
            "Batch 125: loss = 0.38184958696365356, acc = 0.8720703125\n",
            "Batch 126: loss = 0.3472285270690918, acc = 0.88671875\n",
            "\n",
            "Epoch 77/100\n",
            "Batch 1: loss = 0.4798036217689514, acc = 0.8671875\n",
            "Batch 2: loss = 0.381380170583725, acc = 0.8798828125\n",
            "Batch 3: loss = 0.3902781903743744, acc = 0.8662109375\n",
            "Batch 4: loss = 0.35159310698509216, acc = 0.8994140625\n",
            "Batch 5: loss = 0.36353546380996704, acc = 0.884765625\n",
            "Batch 6: loss = 0.3971731662750244, acc = 0.87109375\n",
            "Batch 7: loss = 0.3288499116897583, acc = 0.8896484375\n",
            "Batch 8: loss = 0.3311176002025604, acc = 0.89453125\n",
            "Batch 9: loss = 0.3691314160823822, acc = 0.8779296875\n",
            "Batch 10: loss = 0.30733242630958557, acc = 0.9013671875\n",
            "Batch 11: loss = 0.3628552258014679, acc = 0.88671875\n",
            "Batch 12: loss = 0.356515109539032, acc = 0.8642578125\n",
            "Batch 13: loss = 0.3256957232952118, acc = 0.880859375\n",
            "Batch 14: loss = 0.3327055275440216, acc = 0.896484375\n",
            "Batch 15: loss = 0.33484670519828796, acc = 0.89453125\n",
            "Batch 16: loss = 0.35373300313949585, acc = 0.8857421875\n",
            "Batch 17: loss = 0.3445141613483429, acc = 0.89453125\n",
            "Batch 18: loss = 0.3897773325443268, acc = 0.87890625\n",
            "Batch 19: loss = 0.33172595500946045, acc = 0.8857421875\n",
            "Batch 20: loss = 0.3234170377254486, acc = 0.8857421875\n",
            "Batch 21: loss = 0.34222880005836487, acc = 0.8818359375\n",
            "Batch 22: loss = 0.3273603916168213, acc = 0.8935546875\n",
            "Batch 23: loss = 0.3370336592197418, acc = 0.8916015625\n",
            "Batch 24: loss = 0.27889886498451233, acc = 0.90625\n",
            "Batch 25: loss = 0.3475615382194519, acc = 0.888671875\n",
            "Batch 26: loss = 0.3037126064300537, acc = 0.8935546875\n",
            "Batch 27: loss = 0.368947297334671, acc = 0.8759765625\n",
            "Batch 28: loss = 0.3446531295776367, acc = 0.875\n",
            "Batch 29: loss = 0.35194146633148193, acc = 0.880859375\n",
            "Batch 30: loss = 0.3158453702926636, acc = 0.8935546875\n",
            "Batch 31: loss = 0.3347254991531372, acc = 0.8974609375\n",
            "Batch 32: loss = 0.35989871621131897, acc = 0.8857421875\n",
            "Batch 33: loss = 0.3243923485279083, acc = 0.8955078125\n",
            "Batch 34: loss = 0.3655499517917633, acc = 0.8916015625\n",
            "Batch 35: loss = 0.3220379054546356, acc = 0.8935546875\n",
            "Batch 36: loss = 0.3116576075553894, acc = 0.8984375\n",
            "Batch 37: loss = 0.2819738984107971, acc = 0.912109375\n",
            "Batch 38: loss = 0.2890765964984894, acc = 0.9091796875\n",
            "Batch 39: loss = 0.3007224202156067, acc = 0.9052734375\n",
            "Batch 40: loss = 0.30829209089279175, acc = 0.900390625\n",
            "Batch 41: loss = 0.3074687421321869, acc = 0.892578125\n",
            "Batch 42: loss = 0.33866557478904724, acc = 0.8916015625\n",
            "Batch 43: loss = 0.4041699469089508, acc = 0.861328125\n",
            "Batch 44: loss = 0.2761930227279663, acc = 0.912109375\n",
            "Batch 45: loss = 0.32349133491516113, acc = 0.8974609375\n",
            "Batch 46: loss = 0.2761201560497284, acc = 0.908203125\n",
            "Batch 47: loss = 0.30815473198890686, acc = 0.9013671875\n",
            "Batch 48: loss = 0.31695255637168884, acc = 0.8935546875\n",
            "Batch 49: loss = 0.31196460127830505, acc = 0.9013671875\n",
            "Batch 50: loss = 0.3067777752876282, acc = 0.896484375\n",
            "Batch 51: loss = 0.32579031586647034, acc = 0.8828125\n",
            "Batch 52: loss = 0.32947176694869995, acc = 0.88671875\n",
            "Batch 53: loss = 0.3457602262496948, acc = 0.8798828125\n",
            "Batch 54: loss = 0.270853728055954, acc = 0.9169921875\n",
            "Batch 55: loss = 0.26556894183158875, acc = 0.9208984375\n",
            "Batch 56: loss = 0.38174059987068176, acc = 0.873046875\n",
            "Batch 57: loss = 0.346470445394516, acc = 0.888671875\n",
            "Batch 58: loss = 0.35558491945266724, acc = 0.8876953125\n",
            "Batch 59: loss = 0.26237544417381287, acc = 0.9208984375\n",
            "Batch 60: loss = 0.32284480333328247, acc = 0.8984375\n",
            "Batch 61: loss = 0.2927365005016327, acc = 0.912109375\n",
            "Batch 62: loss = 0.3685374855995178, acc = 0.8779296875\n",
            "Batch 63: loss = 0.3178044259548187, acc = 0.8994140625\n",
            "Batch 64: loss = 0.30760514736175537, acc = 0.9111328125\n",
            "Batch 65: loss = 0.33968374133110046, acc = 0.8876953125\n",
            "Batch 66: loss = 0.3554077744483948, acc = 0.8837890625\n",
            "Batch 67: loss = 0.3126947283744812, acc = 0.892578125\n",
            "Batch 68: loss = 0.34371793270111084, acc = 0.8798828125\n",
            "Batch 69: loss = 0.3131878972053528, acc = 0.900390625\n",
            "Batch 70: loss = 0.363893985748291, acc = 0.8857421875\n",
            "Batch 71: loss = 0.3539128005504608, acc = 0.8779296875\n",
            "Batch 72: loss = 0.3309905529022217, acc = 0.9013671875\n",
            "Batch 73: loss = 0.3771721124649048, acc = 0.8837890625\n",
            "Batch 74: loss = 0.38873496651649475, acc = 0.8671875\n",
            "Batch 75: loss = 0.4033306837081909, acc = 0.861328125\n",
            "Batch 76: loss = 0.35176682472229004, acc = 0.87109375\n",
            "Batch 77: loss = 0.32062461972236633, acc = 0.900390625\n",
            "Batch 78: loss = 0.37435781955718994, acc = 0.8779296875\n",
            "Batch 79: loss = 0.3177565336227417, acc = 0.8857421875\n",
            "Batch 80: loss = 0.33015677332878113, acc = 0.890625\n",
            "Batch 81: loss = 0.35697847604751587, acc = 0.87890625\n",
            "Batch 82: loss = 0.3364976942539215, acc = 0.888671875\n",
            "Batch 83: loss = 0.2779654264450073, acc = 0.908203125\n",
            "Batch 84: loss = 0.35814711451530457, acc = 0.8740234375\n",
            "Batch 85: loss = 0.37110787630081177, acc = 0.873046875\n",
            "Batch 86: loss = 0.34737950563430786, acc = 0.8876953125\n",
            "Batch 87: loss = 0.33797022700309753, acc = 0.8896484375\n",
            "Batch 88: loss = 0.3701329529285431, acc = 0.873046875\n",
            "Batch 89: loss = 0.3346939980983734, acc = 0.8916015625\n",
            "Batch 90: loss = 0.3459380567073822, acc = 0.888671875\n",
            "Batch 91: loss = 0.3718723654747009, acc = 0.8828125\n",
            "Batch 92: loss = 0.36990803480148315, acc = 0.8671875\n",
            "Batch 93: loss = 0.32082870602607727, acc = 0.8955078125\n",
            "Batch 94: loss = 0.3031202554702759, acc = 0.90234375\n",
            "Batch 95: loss = 0.340481698513031, acc = 0.884765625\n",
            "Batch 96: loss = 0.39923208951950073, acc = 0.8671875\n",
            "Batch 97: loss = 0.3667328953742981, acc = 0.890625\n",
            "Batch 98: loss = 0.3441670835018158, acc = 0.8857421875\n",
            "Batch 99: loss = 0.38747164607048035, acc = 0.8759765625\n",
            "Batch 100: loss = 0.36443451046943665, acc = 0.8779296875\n",
            "Batch 101: loss = 0.34303638339042664, acc = 0.884765625\n",
            "Batch 102: loss = 0.3818143308162689, acc = 0.8798828125\n",
            "Batch 103: loss = 0.3632756173610687, acc = 0.8818359375\n",
            "Batch 104: loss = 0.3323991000652313, acc = 0.8876953125\n",
            "Batch 105: loss = 0.32277753949165344, acc = 0.8779296875\n",
            "Batch 106: loss = 0.3356458842754364, acc = 0.888671875\n",
            "Batch 107: loss = 0.37275072932243347, acc = 0.8740234375\n",
            "Batch 108: loss = 0.36599159240722656, acc = 0.88671875\n",
            "Batch 109: loss = 0.3387370705604553, acc = 0.8896484375\n",
            "Batch 110: loss = 0.3254563808441162, acc = 0.896484375\n",
            "Batch 111: loss = 0.36739644408226013, acc = 0.875\n",
            "Batch 112: loss = 0.36575964093208313, acc = 0.880859375\n",
            "Batch 113: loss = 0.3450068533420563, acc = 0.8896484375\n",
            "Batch 114: loss = 0.3418646454811096, acc = 0.8896484375\n",
            "Batch 115: loss = 0.36240479350090027, acc = 0.8740234375\n",
            "Batch 116: loss = 0.3712088167667389, acc = 0.8818359375\n",
            "Batch 117: loss = 0.3306575119495392, acc = 0.8896484375\n",
            "Batch 118: loss = 0.3250879943370819, acc = 0.8896484375\n",
            "Batch 119: loss = 0.3492944538593292, acc = 0.8818359375\n",
            "Batch 120: loss = 0.329508900642395, acc = 0.884765625\n",
            "Batch 121: loss = 0.3351040780544281, acc = 0.8876953125\n",
            "Batch 122: loss = 0.31764093041419983, acc = 0.8994140625\n",
            "Batch 123: loss = 0.32910677790641785, acc = 0.89453125\n",
            "Batch 124: loss = 0.3653067648410797, acc = 0.873046875\n",
            "Batch 125: loss = 0.3600442707538605, acc = 0.8818359375\n",
            "Batch 126: loss = 0.3371148109436035, acc = 0.892578125\n",
            "\n",
            "Epoch 78/100\n",
            "Batch 1: loss = 0.47067973017692566, acc = 0.8740234375\n",
            "Batch 2: loss = 0.3883435130119324, acc = 0.873046875\n",
            "Batch 3: loss = 0.35926389694213867, acc = 0.8828125\n",
            "Batch 4: loss = 0.3566463589668274, acc = 0.880859375\n",
            "Batch 5: loss = 0.3266981840133667, acc = 0.896484375\n",
            "Batch 6: loss = 0.3545074760913849, acc = 0.875\n",
            "Batch 7: loss = 0.3007655739784241, acc = 0.8974609375\n",
            "Batch 8: loss = 0.3669077157974243, acc = 0.888671875\n",
            "Batch 9: loss = 0.3169788718223572, acc = 0.900390625\n",
            "Batch 10: loss = 0.31277763843536377, acc = 0.880859375\n",
            "Batch 11: loss = 0.34947076439857483, acc = 0.88671875\n",
            "Batch 12: loss = 0.3338395059108734, acc = 0.8876953125\n",
            "Batch 13: loss = 0.3098090887069702, acc = 0.896484375\n",
            "Batch 14: loss = 0.3113064467906952, acc = 0.900390625\n",
            "Batch 15: loss = 0.33279985189437866, acc = 0.896484375\n",
            "Batch 16: loss = 0.37723103165626526, acc = 0.8759765625\n",
            "Batch 17: loss = 0.32872772216796875, acc = 0.90234375\n",
            "Batch 18: loss = 0.36214491724967957, acc = 0.880859375\n",
            "Batch 19: loss = 0.3212108016014099, acc = 0.888671875\n",
            "Batch 20: loss = 0.3350575566291809, acc = 0.8857421875\n",
            "Batch 21: loss = 0.3634958565235138, acc = 0.8857421875\n",
            "Batch 22: loss = 0.3520205616950989, acc = 0.8876953125\n",
            "Batch 23: loss = 0.312608003616333, acc = 0.8916015625\n",
            "Batch 24: loss = 0.30349844694137573, acc = 0.9052734375\n",
            "Batch 25: loss = 0.35484054684638977, acc = 0.8837890625\n",
            "Batch 26: loss = 0.3228246569633484, acc = 0.890625\n",
            "Batch 27: loss = 0.3831695020198822, acc = 0.8662109375\n",
            "Batch 28: loss = 0.3321075439453125, acc = 0.8876953125\n",
            "Batch 29: loss = 0.3217073678970337, acc = 0.8837890625\n",
            "Batch 30: loss = 0.3230041563510895, acc = 0.892578125\n",
            "Batch 31: loss = 0.3434494733810425, acc = 0.8818359375\n",
            "Batch 32: loss = 0.38872429728507996, acc = 0.8798828125\n",
            "Batch 33: loss = 0.32560914754867554, acc = 0.8974609375\n",
            "Batch 34: loss = 0.35389938950538635, acc = 0.8828125\n",
            "Batch 35: loss = 0.316069096326828, acc = 0.896484375\n",
            "Batch 36: loss = 0.3486471176147461, acc = 0.876953125\n",
            "Batch 37: loss = 0.2846866250038147, acc = 0.9111328125\n",
            "Batch 38: loss = 0.29511910676956177, acc = 0.91015625\n",
            "Batch 39: loss = 0.295617014169693, acc = 0.8984375\n",
            "Batch 40: loss = 0.31687188148498535, acc = 0.8876953125\n",
            "Batch 41: loss = 0.2931057810783386, acc = 0.90625\n",
            "Batch 42: loss = 0.32398688793182373, acc = 0.890625\n",
            "Batch 43: loss = 0.38282907009124756, acc = 0.8720703125\n",
            "Batch 44: loss = 0.27733534574508667, acc = 0.9091796875\n",
            "Batch 45: loss = 0.30474889278411865, acc = 0.8984375\n",
            "Batch 46: loss = 0.26261207461357117, acc = 0.9189453125\n",
            "Batch 47: loss = 0.2831743359565735, acc = 0.9130859375\n",
            "Batch 48: loss = 0.33106404542922974, acc = 0.8857421875\n",
            "Batch 49: loss = 0.3125104010105133, acc = 0.912109375\n",
            "Batch 50: loss = 0.3235504627227783, acc = 0.8984375\n",
            "Batch 51: loss = 0.3306383490562439, acc = 0.8876953125\n",
            "Batch 52: loss = 0.34384310245513916, acc = 0.880859375\n",
            "Batch 53: loss = 0.3101821839809418, acc = 0.8974609375\n",
            "Batch 54: loss = 0.2949331998825073, acc = 0.908203125\n",
            "Batch 55: loss = 0.262355774641037, acc = 0.9150390625\n",
            "Batch 56: loss = 0.36893144249916077, acc = 0.8720703125\n",
            "Batch 57: loss = 0.33539992570877075, acc = 0.8984375\n",
            "Batch 58: loss = 0.3369540572166443, acc = 0.8828125\n",
            "Batch 59: loss = 0.2523171305656433, acc = 0.916015625\n",
            "Batch 60: loss = 0.3104513883590698, acc = 0.8984375\n",
            "Batch 61: loss = 0.26799440383911133, acc = 0.919921875\n",
            "Batch 62: loss = 0.35004377365112305, acc = 0.8818359375\n",
            "Batch 63: loss = 0.31817665696144104, acc = 0.8916015625\n",
            "Batch 64: loss = 0.29076987504959106, acc = 0.912109375\n",
            "Batch 65: loss = 0.3410770297050476, acc = 0.8896484375\n",
            "Batch 66: loss = 0.3306947350502014, acc = 0.8798828125\n",
            "Batch 67: loss = 0.34111225605010986, acc = 0.8828125\n",
            "Batch 68: loss = 0.32707512378692627, acc = 0.9033203125\n",
            "Batch 69: loss = 0.31350407004356384, acc = 0.8955078125\n",
            "Batch 70: loss = 0.3752448558807373, acc = 0.8779296875\n",
            "Batch 71: loss = 0.32959800958633423, acc = 0.8984375\n",
            "Batch 72: loss = 0.33296993374824524, acc = 0.8974609375\n",
            "Batch 73: loss = 0.3444337844848633, acc = 0.8984375\n",
            "Batch 74: loss = 0.3347043991088867, acc = 0.8818359375\n",
            "Batch 75: loss = 0.41692474484443665, acc = 0.8671875\n",
            "Batch 76: loss = 0.3530910015106201, acc = 0.8779296875\n",
            "Batch 77: loss = 0.3379272520542145, acc = 0.8828125\n",
            "Batch 78: loss = 0.3713342547416687, acc = 0.8701171875\n",
            "Batch 79: loss = 0.324749231338501, acc = 0.87890625\n",
            "Batch 80: loss = 0.33692115545272827, acc = 0.89453125\n",
            "Batch 81: loss = 0.34331825375556946, acc = 0.8798828125\n",
            "Batch 82: loss = 0.34755533933639526, acc = 0.8837890625\n",
            "Batch 83: loss = 0.317156046628952, acc = 0.8935546875\n",
            "Batch 84: loss = 0.36712974309921265, acc = 0.876953125\n",
            "Batch 85: loss = 0.3689368963241577, acc = 0.8662109375\n",
            "Batch 86: loss = 0.353194922208786, acc = 0.8876953125\n",
            "Batch 87: loss = 0.3366057872772217, acc = 0.88671875\n",
            "Batch 88: loss = 0.404238760471344, acc = 0.8701171875\n",
            "Batch 89: loss = 0.32880473136901855, acc = 0.8916015625\n",
            "Batch 90: loss = 0.33048999309539795, acc = 0.88671875\n",
            "Batch 91: loss = 0.37761086225509644, acc = 0.86328125\n",
            "Batch 92: loss = 0.3609376847743988, acc = 0.88671875\n",
            "Batch 93: loss = 0.3262846767902374, acc = 0.8896484375\n",
            "Batch 94: loss = 0.35263264179229736, acc = 0.8818359375\n",
            "Batch 95: loss = 0.32618027925491333, acc = 0.8974609375\n",
            "Batch 96: loss = 0.3826167583465576, acc = 0.873046875\n",
            "Batch 97: loss = 0.3479352593421936, acc = 0.890625\n",
            "Batch 98: loss = 0.3393508195877075, acc = 0.88671875\n",
            "Batch 99: loss = 0.3635097146034241, acc = 0.8974609375\n",
            "Batch 100: loss = 0.3561800718307495, acc = 0.876953125\n",
            "Batch 101: loss = 0.3434622287750244, acc = 0.8720703125\n",
            "Batch 102: loss = 0.403303861618042, acc = 0.861328125\n",
            "Batch 103: loss = 0.37361687421798706, acc = 0.87890625\n",
            "Batch 104: loss = 0.31989967823028564, acc = 0.8974609375\n",
            "Batch 105: loss = 0.31600964069366455, acc = 0.8876953125\n",
            "Batch 106: loss = 0.33512768149375916, acc = 0.896484375\n",
            "Batch 107: loss = 0.36619970202445984, acc = 0.8603515625\n",
            "Batch 108: loss = 0.3604617714881897, acc = 0.87890625\n",
            "Batch 109: loss = 0.33221161365509033, acc = 0.8916015625\n",
            "Batch 110: loss = 0.32471153140068054, acc = 0.8974609375\n",
            "Batch 111: loss = 0.36489957571029663, acc = 0.8876953125\n",
            "Batch 112: loss = 0.3642815053462982, acc = 0.88671875\n",
            "Batch 113: loss = 0.31817686557769775, acc = 0.88671875\n",
            "Batch 114: loss = 0.34655603766441345, acc = 0.8857421875\n",
            "Batch 115: loss = 0.37378424406051636, acc = 0.8828125\n",
            "Batch 116: loss = 0.3821335732936859, acc = 0.8642578125\n",
            "Batch 117: loss = 0.32333600521087646, acc = 0.8974609375\n",
            "Batch 118: loss = 0.3332441747188568, acc = 0.892578125\n",
            "Batch 119: loss = 0.3253040015697479, acc = 0.8935546875\n",
            "Batch 120: loss = 0.3082553744316101, acc = 0.892578125\n",
            "Batch 121: loss = 0.35673195123672485, acc = 0.884765625\n",
            "Batch 122: loss = 0.32680150866508484, acc = 0.8857421875\n",
            "Batch 123: loss = 0.3527205288410187, acc = 0.880859375\n",
            "Batch 124: loss = 0.3830198645591736, acc = 0.8798828125\n",
            "Batch 125: loss = 0.34056222438812256, acc = 0.896484375\n",
            "Batch 126: loss = 0.37284034490585327, acc = 0.873046875\n",
            "\n",
            "Epoch 79/100\n",
            "Batch 1: loss = 0.4751782715320587, acc = 0.8671875\n",
            "Batch 2: loss = 0.3768969774246216, acc = 0.8681640625\n",
            "Batch 3: loss = 0.3317527770996094, acc = 0.892578125\n",
            "Batch 4: loss = 0.36587852239608765, acc = 0.873046875\n",
            "Batch 5: loss = 0.3288343548774719, acc = 0.890625\n",
            "Batch 6: loss = 0.3475351333618164, acc = 0.8857421875\n",
            "Batch 7: loss = 0.34969890117645264, acc = 0.8818359375\n",
            "Batch 8: loss = 0.34485912322998047, acc = 0.8916015625\n",
            "Batch 9: loss = 0.3462674617767334, acc = 0.87890625\n",
            "Batch 10: loss = 0.2841322422027588, acc = 0.8955078125\n",
            "Batch 11: loss = 0.3723730444908142, acc = 0.8779296875\n",
            "Batch 12: loss = 0.34430962800979614, acc = 0.8740234375\n",
            "Batch 13: loss = 0.35046979784965515, acc = 0.8935546875\n",
            "Batch 14: loss = 0.3132482171058655, acc = 0.8935546875\n",
            "Batch 15: loss = 0.32705745100975037, acc = 0.8896484375\n",
            "Batch 16: loss = 0.3483918309211731, acc = 0.8818359375\n",
            "Batch 17: loss = 0.32966282963752747, acc = 0.8935546875\n",
            "Batch 18: loss = 0.3587951064109802, acc = 0.8701171875\n",
            "Batch 19: loss = 0.32742395997047424, acc = 0.8818359375\n",
            "Batch 20: loss = 0.3405952751636505, acc = 0.8779296875\n",
            "Batch 21: loss = 0.35926371812820435, acc = 0.888671875\n",
            "Batch 22: loss = 0.3377552330493927, acc = 0.8837890625\n",
            "Batch 23: loss = 0.32512104511260986, acc = 0.892578125\n",
            "Batch 24: loss = 0.28028279542922974, acc = 0.908203125\n",
            "Batch 25: loss = 0.35178571939468384, acc = 0.8857421875\n",
            "Batch 26: loss = 0.3209248483181, acc = 0.892578125\n",
            "Batch 27: loss = 0.3624522387981415, acc = 0.8818359375\n",
            "Batch 28: loss = 0.3165869414806366, acc = 0.888671875\n",
            "Batch 29: loss = 0.3338901698589325, acc = 0.892578125\n",
            "Batch 30: loss = 0.32347139716148376, acc = 0.875\n",
            "Batch 31: loss = 0.35559093952178955, acc = 0.88671875\n",
            "Batch 32: loss = 0.36803579330444336, acc = 0.888671875\n",
            "Batch 33: loss = 0.3714872896671295, acc = 0.8779296875\n",
            "Batch 34: loss = 0.37078583240509033, acc = 0.8798828125\n",
            "Batch 35: loss = 0.32348623871803284, acc = 0.8896484375\n",
            "Batch 36: loss = 0.27286696434020996, acc = 0.9140625\n",
            "Batch 37: loss = 0.27088940143585205, acc = 0.923828125\n",
            "Batch 38: loss = 0.28109726309776306, acc = 0.9150390625\n",
            "Batch 39: loss = 0.27101948857307434, acc = 0.91796875\n",
            "Batch 40: loss = 0.3157525658607483, acc = 0.896484375\n",
            "Batch 41: loss = 0.2949085831642151, acc = 0.888671875\n",
            "Batch 42: loss = 0.32851460576057434, acc = 0.88671875\n",
            "Batch 43: loss = 0.3498251736164093, acc = 0.88671875\n",
            "Batch 44: loss = 0.2584403157234192, acc = 0.91796875\n",
            "Batch 45: loss = 0.28798580169677734, acc = 0.9052734375\n",
            "Batch 46: loss = 0.296117901802063, acc = 0.8955078125\n",
            "Batch 47: loss = 0.26864781975746155, acc = 0.91015625\n",
            "Batch 48: loss = 0.30696630477905273, acc = 0.890625\n",
            "Batch 49: loss = 0.3134972155094147, acc = 0.9033203125\n",
            "Batch 50: loss = 0.2929796576499939, acc = 0.912109375\n",
            "Batch 51: loss = 0.3298094868659973, acc = 0.890625\n",
            "Batch 52: loss = 0.3451315462589264, acc = 0.8818359375\n",
            "Batch 53: loss = 0.33644622564315796, acc = 0.8916015625\n",
            "Batch 54: loss = 0.2770659923553467, acc = 0.912109375\n",
            "Batch 55: loss = 0.26058951020240784, acc = 0.91796875\n",
            "Batch 56: loss = 0.3476206064224243, acc = 0.8671875\n",
            "Batch 57: loss = 0.3308519721031189, acc = 0.8896484375\n",
            "Batch 58: loss = 0.36679884791374207, acc = 0.8837890625\n",
            "Batch 59: loss = 0.2687626779079437, acc = 0.8994140625\n",
            "Batch 60: loss = 0.2996131181716919, acc = 0.904296875\n",
            "Batch 61: loss = 0.2748943567276001, acc = 0.908203125\n",
            "Batch 62: loss = 0.3614533543586731, acc = 0.8798828125\n",
            "Batch 63: loss = 0.336001992225647, acc = 0.8857421875\n",
            "Batch 64: loss = 0.2969474792480469, acc = 0.900390625\n",
            "Batch 65: loss = 0.32108044624328613, acc = 0.8935546875\n",
            "Batch 66: loss = 0.34162068367004395, acc = 0.8955078125\n",
            "Batch 67: loss = 0.3282971680164337, acc = 0.884765625\n",
            "Batch 68: loss = 0.3559185564517975, acc = 0.88671875\n",
            "Batch 69: loss = 0.3156008720397949, acc = 0.900390625\n",
            "Batch 70: loss = 0.3533886969089508, acc = 0.8876953125\n",
            "Batch 71: loss = 0.34707778692245483, acc = 0.8798828125\n",
            "Batch 72: loss = 0.3199564218521118, acc = 0.9033203125\n",
            "Batch 73: loss = 0.3829265832901001, acc = 0.8759765625\n",
            "Batch 74: loss = 0.32260793447494507, acc = 0.892578125\n",
            "Batch 75: loss = 0.40733426809310913, acc = 0.857421875\n",
            "Batch 76: loss = 0.38478872179985046, acc = 0.8701171875\n",
            "Batch 77: loss = 0.3304181694984436, acc = 0.888671875\n",
            "Batch 78: loss = 0.38033124804496765, acc = 0.865234375\n",
            "Batch 79: loss = 0.3400298058986664, acc = 0.8818359375\n",
            "Batch 80: loss = 0.3097754716873169, acc = 0.900390625\n",
            "Batch 81: loss = 0.34508389234542847, acc = 0.8720703125\n",
            "Batch 82: loss = 0.3241017162799835, acc = 0.896484375\n",
            "Batch 83: loss = 0.28825142979621887, acc = 0.9091796875\n",
            "Batch 84: loss = 0.3717181384563446, acc = 0.884765625\n",
            "Batch 85: loss = 0.35963529348373413, acc = 0.8740234375\n",
            "Batch 86: loss = 0.32603955268859863, acc = 0.8828125\n",
            "Batch 87: loss = 0.3382360339164734, acc = 0.8798828125\n",
            "Batch 88: loss = 0.37609556317329407, acc = 0.8759765625\n",
            "Batch 89: loss = 0.2872924506664276, acc = 0.9033203125\n",
            "Batch 90: loss = 0.35778066515922546, acc = 0.873046875\n",
            "Batch 91: loss = 0.3637595474720001, acc = 0.8603515625\n",
            "Batch 92: loss = 0.34349480271339417, acc = 0.8876953125\n",
            "Batch 93: loss = 0.3087303638458252, acc = 0.8984375\n",
            "Batch 94: loss = 0.3448311388492584, acc = 0.8876953125\n",
            "Batch 95: loss = 0.33628493547439575, acc = 0.9033203125\n",
            "Batch 96: loss = 0.3725690245628357, acc = 0.869140625\n",
            "Batch 97: loss = 0.3692096769809723, acc = 0.8818359375\n",
            "Batch 98: loss = 0.3679441809654236, acc = 0.8701171875\n",
            "Batch 99: loss = 0.3666682541370392, acc = 0.876953125\n",
            "Batch 100: loss = 0.36277443170547485, acc = 0.8759765625\n",
            "Batch 101: loss = 0.33684515953063965, acc = 0.88671875\n",
            "Batch 102: loss = 0.3690139651298523, acc = 0.873046875\n",
            "Batch 103: loss = 0.34513145685195923, acc = 0.8984375\n",
            "Batch 104: loss = 0.3303583264350891, acc = 0.8837890625\n",
            "Batch 105: loss = 0.315509557723999, acc = 0.8974609375\n",
            "Batch 106: loss = 0.3351894021034241, acc = 0.88671875\n",
            "Batch 107: loss = 0.3859928548336029, acc = 0.865234375\n",
            "Batch 108: loss = 0.3567691445350647, acc = 0.873046875\n",
            "Batch 109: loss = 0.3188539743423462, acc = 0.8876953125\n",
            "Batch 110: loss = 0.3544263541698456, acc = 0.876953125\n",
            "Batch 111: loss = 0.3371656835079193, acc = 0.896484375\n",
            "Batch 112: loss = 0.3721999526023865, acc = 0.87109375\n",
            "Batch 113: loss = 0.3498673439025879, acc = 0.8857421875\n",
            "Batch 114: loss = 0.3687231242656708, acc = 0.869140625\n",
            "Batch 115: loss = 0.3557623028755188, acc = 0.8759765625\n",
            "Batch 116: loss = 0.36917930841445923, acc = 0.880859375\n",
            "Batch 117: loss = 0.3122525215148926, acc = 0.892578125\n",
            "Batch 118: loss = 0.3109247386455536, acc = 0.88671875\n",
            "Batch 119: loss = 0.33028173446655273, acc = 0.8759765625\n",
            "Batch 120: loss = 0.28979772329330444, acc = 0.912109375\n",
            "Batch 121: loss = 0.36486995220184326, acc = 0.8876953125\n",
            "Batch 122: loss = 0.3244180679321289, acc = 0.896484375\n",
            "Batch 123: loss = 0.3340983986854553, acc = 0.90234375\n",
            "Batch 124: loss = 0.3501497209072113, acc = 0.873046875\n",
            "Batch 125: loss = 0.3726252317428589, acc = 0.8701171875\n",
            "Batch 126: loss = 0.34219634532928467, acc = 0.8720703125\n",
            "\n",
            "Epoch 80/100\n",
            "Batch 1: loss = 0.4775286018848419, acc = 0.857421875\n",
            "Batch 2: loss = 0.37981078028678894, acc = 0.8671875\n",
            "Batch 3: loss = 0.38117992877960205, acc = 0.8798828125\n",
            "Batch 4: loss = 0.34384945034980774, acc = 0.888671875\n",
            "Batch 5: loss = 0.307524710893631, acc = 0.8984375\n",
            "Batch 6: loss = 0.34754663705825806, acc = 0.8828125\n",
            "Batch 7: loss = 0.3455488383769989, acc = 0.8818359375\n",
            "Batch 8: loss = 0.34321603178977966, acc = 0.884765625\n",
            "Batch 9: loss = 0.30888068675994873, acc = 0.900390625\n",
            "Batch 10: loss = 0.30418476462364197, acc = 0.900390625\n",
            "Batch 11: loss = 0.3419649302959442, acc = 0.8857421875\n",
            "Batch 12: loss = 0.32616445422172546, acc = 0.8828125\n",
            "Batch 13: loss = 0.3133683502674103, acc = 0.90234375\n",
            "Batch 14: loss = 0.31343016028404236, acc = 0.900390625\n",
            "Batch 15: loss = 0.3139585256576538, acc = 0.9013671875\n",
            "Batch 16: loss = 0.34492045640945435, acc = 0.8916015625\n",
            "Batch 17: loss = 0.3359140455722809, acc = 0.888671875\n",
            "Batch 18: loss = 0.3551124334335327, acc = 0.892578125\n",
            "Batch 19: loss = 0.2912513315677643, acc = 0.90234375\n",
            "Batch 20: loss = 0.33587321639060974, acc = 0.8779296875\n",
            "Batch 21: loss = 0.33810195326805115, acc = 0.8828125\n",
            "Batch 22: loss = 0.312198281288147, acc = 0.896484375\n",
            "Batch 23: loss = 0.3269461989402771, acc = 0.8896484375\n",
            "Batch 24: loss = 0.297579824924469, acc = 0.9033203125\n",
            "Batch 25: loss = 0.34726911783218384, acc = 0.8876953125\n",
            "Batch 26: loss = 0.28472957015037537, acc = 0.9111328125\n",
            "Batch 27: loss = 0.3518417477607727, acc = 0.87890625\n",
            "Batch 28: loss = 0.3578116297721863, acc = 0.880859375\n",
            "Batch 29: loss = 0.32552123069763184, acc = 0.890625\n",
            "Batch 30: loss = 0.32926031947135925, acc = 0.890625\n",
            "Batch 31: loss = 0.32445305585861206, acc = 0.884765625\n",
            "Batch 32: loss = 0.36872661113739014, acc = 0.8837890625\n",
            "Batch 33: loss = 0.3358333110809326, acc = 0.8935546875\n",
            "Batch 34: loss = 0.37198546528816223, acc = 0.890625\n",
            "Batch 35: loss = 0.33966854214668274, acc = 0.890625\n",
            "Batch 36: loss = 0.29037994146347046, acc = 0.908203125\n",
            "Batch 37: loss = 0.2950625419616699, acc = 0.912109375\n",
            "Batch 38: loss = 0.300851434469223, acc = 0.904296875\n",
            "Batch 39: loss = 0.2957327961921692, acc = 0.9013671875\n",
            "Batch 40: loss = 0.2893746793270111, acc = 0.916015625\n",
            "Batch 41: loss = 0.28798601031303406, acc = 0.90625\n",
            "Batch 42: loss = 0.32941192388534546, acc = 0.8828125\n",
            "Batch 43: loss = 0.36614400148391724, acc = 0.880859375\n",
            "Batch 44: loss = 0.25386911630630493, acc = 0.90625\n",
            "Batch 45: loss = 0.28748252987861633, acc = 0.9072265625\n",
            "Batch 46: loss = 0.288714200258255, acc = 0.90234375\n",
            "Batch 47: loss = 0.28730303049087524, acc = 0.9013671875\n",
            "Batch 48: loss = 0.3020387291908264, acc = 0.8955078125\n",
            "Batch 49: loss = 0.2935596704483032, acc = 0.91015625\n",
            "Batch 50: loss = 0.3138038218021393, acc = 0.9072265625\n",
            "Batch 51: loss = 0.3331705629825592, acc = 0.8857421875\n",
            "Batch 52: loss = 0.3397383689880371, acc = 0.890625\n",
            "Batch 53: loss = 0.313316285610199, acc = 0.890625\n",
            "Batch 54: loss = 0.24783961474895477, acc = 0.9150390625\n",
            "Batch 55: loss = 0.28427940607070923, acc = 0.9140625\n",
            "Batch 56: loss = 0.3655272424221039, acc = 0.87890625\n",
            "Batch 57: loss = 0.31893229484558105, acc = 0.8955078125\n",
            "Batch 58: loss = 0.3630119562149048, acc = 0.87890625\n",
            "Batch 59: loss = 0.2618274688720703, acc = 0.919921875\n",
            "Batch 60: loss = 0.30384761095046997, acc = 0.9033203125\n",
            "Batch 61: loss = 0.27130287885665894, acc = 0.9169921875\n",
            "Batch 62: loss = 0.36348283290863037, acc = 0.88671875\n",
            "Batch 63: loss = 0.3180793523788452, acc = 0.8955078125\n",
            "Batch 64: loss = 0.27975553274154663, acc = 0.9033203125\n",
            "Batch 65: loss = 0.3436606228351593, acc = 0.8818359375\n",
            "Batch 66: loss = 0.3550490736961365, acc = 0.8740234375\n",
            "Batch 67: loss = 0.3200015425682068, acc = 0.896484375\n",
            "Batch 68: loss = 0.3389122784137726, acc = 0.892578125\n",
            "Batch 69: loss = 0.2868797183036804, acc = 0.90625\n",
            "Batch 70: loss = 0.38717398047447205, acc = 0.8642578125\n",
            "Batch 71: loss = 0.3740578591823578, acc = 0.869140625\n",
            "Batch 72: loss = 0.3157810866832733, acc = 0.896484375\n",
            "Batch 73: loss = 0.3496062159538269, acc = 0.876953125\n",
            "Batch 74: loss = 0.3321588337421417, acc = 0.888671875\n",
            "Batch 75: loss = 0.41209322214126587, acc = 0.873046875\n",
            "Batch 76: loss = 0.3353073298931122, acc = 0.8798828125\n",
            "Batch 77: loss = 0.3366931080818176, acc = 0.8828125\n",
            "Batch 78: loss = 0.35975736379623413, acc = 0.8798828125\n",
            "Batch 79: loss = 0.3306361436843872, acc = 0.880859375\n",
            "Batch 80: loss = 0.34242507815361023, acc = 0.8740234375\n",
            "Batch 81: loss = 0.3212968707084656, acc = 0.8935546875\n",
            "Batch 82: loss = 0.3243682384490967, acc = 0.8876953125\n",
            "Batch 83: loss = 0.31671857833862305, acc = 0.8896484375\n",
            "Batch 84: loss = 0.351886510848999, acc = 0.8779296875\n",
            "Batch 85: loss = 0.34795916080474854, acc = 0.8818359375\n",
            "Batch 86: loss = 0.32153016328811646, acc = 0.8876953125\n",
            "Batch 87: loss = 0.3587130606174469, acc = 0.888671875\n",
            "Batch 88: loss = 0.37948906421661377, acc = 0.87109375\n",
            "Batch 89: loss = 0.31249645352363586, acc = 0.8974609375\n",
            "Batch 90: loss = 0.35127776861190796, acc = 0.8916015625\n",
            "Batch 91: loss = 0.3505348265171051, acc = 0.8779296875\n",
            "Batch 92: loss = 0.36370354890823364, acc = 0.8828125\n",
            "Batch 93: loss = 0.30745333433151245, acc = 0.9052734375\n",
            "Batch 94: loss = 0.35210120677948, acc = 0.8916015625\n",
            "Batch 95: loss = 0.3516373038291931, acc = 0.8837890625\n",
            "Batch 96: loss = 0.39539095759391785, acc = 0.87109375\n",
            "Batch 97: loss = 0.35098543763160706, acc = 0.88671875\n",
            "Batch 98: loss = 0.32724884152412415, acc = 0.8876953125\n",
            "Batch 99: loss = 0.35818377137184143, acc = 0.8857421875\n",
            "Batch 100: loss = 0.3774638772010803, acc = 0.8681640625\n",
            "Batch 101: loss = 0.3322223722934723, acc = 0.8857421875\n",
            "Batch 102: loss = 0.38715091347694397, acc = 0.8740234375\n",
            "Batch 103: loss = 0.3973277509212494, acc = 0.87109375\n",
            "Batch 104: loss = 0.30824339389801025, acc = 0.8955078125\n",
            "Batch 105: loss = 0.3113492727279663, acc = 0.88671875\n",
            "Batch 106: loss = 0.3436238467693329, acc = 0.873046875\n",
            "Batch 107: loss = 0.33620378375053406, acc = 0.8837890625\n",
            "Batch 108: loss = 0.3213236927986145, acc = 0.87890625\n",
            "Batch 109: loss = 0.31537187099456787, acc = 0.8994140625\n",
            "Batch 110: loss = 0.2991972267627716, acc = 0.9033203125\n",
            "Batch 111: loss = 0.335499107837677, acc = 0.8876953125\n",
            "Batch 112: loss = 0.36863863468170166, acc = 0.8876953125\n",
            "Batch 113: loss = 0.3307001292705536, acc = 0.8974609375\n",
            "Batch 114: loss = 0.34197530150413513, acc = 0.8857421875\n",
            "Batch 115: loss = 0.3494590222835541, acc = 0.88671875\n",
            "Batch 116: loss = 0.3279324173927307, acc = 0.888671875\n",
            "Batch 117: loss = 0.32426267862319946, acc = 0.8935546875\n",
            "Batch 118: loss = 0.30733522772789, acc = 0.8984375\n",
            "Batch 119: loss = 0.3143714964389801, acc = 0.8984375\n",
            "Batch 120: loss = 0.2875264883041382, acc = 0.9013671875\n",
            "Batch 121: loss = 0.34357038140296936, acc = 0.884765625\n",
            "Batch 122: loss = 0.3114200234413147, acc = 0.896484375\n",
            "Batch 123: loss = 0.31133967638015747, acc = 0.896484375\n",
            "Batch 124: loss = 0.3399272561073303, acc = 0.8798828125\n",
            "Batch 125: loss = 0.36004164814949036, acc = 0.876953125\n",
            "Batch 126: loss = 0.3510648012161255, acc = 0.87890625\n",
            "Saved checkpoint to weights.80.h5\n",
            "\n",
            "Epoch 81/100\n",
            "Batch 1: loss = 0.5008661150932312, acc = 0.8466796875\n",
            "Batch 2: loss = 0.3869466185569763, acc = 0.8720703125\n",
            "Batch 3: loss = 0.3210834860801697, acc = 0.8984375\n",
            "Batch 4: loss = 0.3118411898612976, acc = 0.91015625\n",
            "Batch 5: loss = 0.3281685411930084, acc = 0.8818359375\n",
            "Batch 6: loss = 0.358314573764801, acc = 0.876953125\n",
            "Batch 7: loss = 0.2951313853263855, acc = 0.90234375\n",
            "Batch 8: loss = 0.3402695655822754, acc = 0.900390625\n",
            "Batch 9: loss = 0.3416108787059784, acc = 0.890625\n",
            "Batch 10: loss = 0.3166784346103668, acc = 0.892578125\n",
            "Batch 11: loss = 0.3461575508117676, acc = 0.888671875\n",
            "Batch 12: loss = 0.28819894790649414, acc = 0.904296875\n",
            "Batch 13: loss = 0.31991875171661377, acc = 0.8857421875\n",
            "Batch 14: loss = 0.314210444688797, acc = 0.8974609375\n",
            "Batch 15: loss = 0.30594301223754883, acc = 0.9013671875\n",
            "Batch 16: loss = 0.3428581953048706, acc = 0.8876953125\n",
            "Batch 17: loss = 0.3315049111843109, acc = 0.8955078125\n",
            "Batch 18: loss = 0.3638564646244049, acc = 0.875\n",
            "Batch 19: loss = 0.3076550364494324, acc = 0.8955078125\n",
            "Batch 20: loss = 0.3361688256263733, acc = 0.8837890625\n",
            "Batch 21: loss = 0.3250192403793335, acc = 0.8955078125\n",
            "Batch 22: loss = 0.32121455669403076, acc = 0.8935546875\n",
            "Batch 23: loss = 0.31784704327583313, acc = 0.8994140625\n",
            "Batch 24: loss = 0.30082401633262634, acc = 0.8974609375\n",
            "Batch 25: loss = 0.34140509366989136, acc = 0.890625\n",
            "Batch 26: loss = 0.27035611867904663, acc = 0.908203125\n",
            "Batch 27: loss = 0.32787030935287476, acc = 0.89453125\n",
            "Batch 28: loss = 0.3323756158351898, acc = 0.88671875\n",
            "Batch 29: loss = 0.33506521582603455, acc = 0.8779296875\n",
            "Batch 30: loss = 0.3151496648788452, acc = 0.9052734375\n",
            "Batch 31: loss = 0.36081594228744507, acc = 0.8857421875\n",
            "Batch 32: loss = 0.37484022974967957, acc = 0.8798828125\n",
            "Batch 33: loss = 0.3349018394947052, acc = 0.896484375\n",
            "Batch 34: loss = 0.34018486738204956, acc = 0.890625\n",
            "Batch 35: loss = 0.3171873390674591, acc = 0.8876953125\n",
            "Batch 36: loss = 0.27407264709472656, acc = 0.9189453125\n",
            "Batch 37: loss = 0.2678278684616089, acc = 0.9169921875\n",
            "Batch 38: loss = 0.3068457245826721, acc = 0.8896484375\n",
            "Batch 39: loss = 0.286848783493042, acc = 0.9091796875\n",
            "Batch 40: loss = 0.3566001057624817, acc = 0.8740234375\n",
            "Batch 41: loss = 0.3122643232345581, acc = 0.8896484375\n",
            "Batch 42: loss = 0.3123649060726166, acc = 0.89453125\n",
            "Batch 43: loss = 0.3621748685836792, acc = 0.8740234375\n",
            "Batch 44: loss = 0.2546129822731018, acc = 0.9208984375\n",
            "Batch 45: loss = 0.2663305699825287, acc = 0.9091796875\n",
            "Batch 46: loss = 0.2649328410625458, acc = 0.9140625\n",
            "Batch 47: loss = 0.30196985602378845, acc = 0.900390625\n",
            "Batch 48: loss = 0.32585203647613525, acc = 0.8984375\n",
            "Batch 49: loss = 0.29718300700187683, acc = 0.9091796875\n",
            "Batch 50: loss = 0.3253249228000641, acc = 0.8935546875\n",
            "Batch 51: loss = 0.32815784215927124, acc = 0.8759765625\n",
            "Batch 52: loss = 0.32189151644706726, acc = 0.88671875\n",
            "Batch 53: loss = 0.3293546140193939, acc = 0.8984375\n",
            "Batch 54: loss = 0.2772182822227478, acc = 0.9013671875\n",
            "Batch 55: loss = 0.2692665457725525, acc = 0.91015625\n",
            "Batch 56: loss = 0.35625946521759033, acc = 0.8681640625\n",
            "Batch 57: loss = 0.32760870456695557, acc = 0.89453125\n",
            "Batch 58: loss = 0.3636578917503357, acc = 0.876953125\n",
            "Batch 59: loss = 0.282844215631485, acc = 0.9091796875\n",
            "Batch 60: loss = 0.3043389916419983, acc = 0.8955078125\n",
            "Batch 61: loss = 0.318217396736145, acc = 0.8935546875\n",
            "Batch 62: loss = 0.3639165461063385, acc = 0.876953125\n",
            "Batch 63: loss = 0.2970382571220398, acc = 0.896484375\n",
            "Batch 64: loss = 0.2866171598434448, acc = 0.9013671875\n",
            "Batch 65: loss = 0.3264721632003784, acc = 0.8876953125\n",
            "Batch 66: loss = 0.31559956073760986, acc = 0.8916015625\n",
            "Batch 67: loss = 0.31692057847976685, acc = 0.880859375\n",
            "Batch 68: loss = 0.3295349180698395, acc = 0.88671875\n",
            "Batch 69: loss = 0.30234193801879883, acc = 0.9072265625\n",
            "Batch 70: loss = 0.3472510278224945, acc = 0.884765625\n",
            "Batch 71: loss = 0.3505030572414398, acc = 0.890625\n",
            "Batch 72: loss = 0.3120526075363159, acc = 0.890625\n",
            "Batch 73: loss = 0.3482523560523987, acc = 0.8828125\n",
            "Batch 74: loss = 0.31925520300865173, acc = 0.890625\n",
            "Batch 75: loss = 0.4105699956417084, acc = 0.8583984375\n",
            "Batch 76: loss = 0.34207579493522644, acc = 0.8740234375\n",
            "Batch 77: loss = 0.34494176506996155, acc = 0.884765625\n",
            "Batch 78: loss = 0.3450024425983429, acc = 0.876953125\n",
            "Batch 79: loss = 0.3153553605079651, acc = 0.8974609375\n",
            "Batch 80: loss = 0.3111681342124939, acc = 0.8916015625\n",
            "Batch 81: loss = 0.324879914522171, acc = 0.88671875\n",
            "Batch 82: loss = 0.3037027418613434, acc = 0.9013671875\n",
            "Batch 83: loss = 0.3046097755432129, acc = 0.896484375\n",
            "Batch 84: loss = 0.35409921407699585, acc = 0.8916015625\n",
            "Batch 85: loss = 0.3522377610206604, acc = 0.87890625\n",
            "Batch 86: loss = 0.2964681386947632, acc = 0.8935546875\n",
            "Batch 87: loss = 0.3422953486442566, acc = 0.87890625\n",
            "Batch 88: loss = 0.3416936695575714, acc = 0.8818359375\n",
            "Batch 89: loss = 0.3232303857803345, acc = 0.8994140625\n",
            "Batch 90: loss = 0.3374970853328705, acc = 0.8876953125\n",
            "Batch 91: loss = 0.33235520124435425, acc = 0.8857421875\n",
            "Batch 92: loss = 0.37229663133621216, acc = 0.8818359375\n",
            "Batch 93: loss = 0.29097771644592285, acc = 0.9033203125\n",
            "Batch 94: loss = 0.35061267018318176, acc = 0.8798828125\n",
            "Batch 95: loss = 0.33847612142562866, acc = 0.8837890625\n",
            "Batch 96: loss = 0.3964357376098633, acc = 0.8681640625\n",
            "Batch 97: loss = 0.35092437267303467, acc = 0.8818359375\n",
            "Batch 98: loss = 0.3271939754486084, acc = 0.8876953125\n",
            "Batch 99: loss = 0.3748512268066406, acc = 0.8759765625\n",
            "Batch 100: loss = 0.35919585824012756, acc = 0.876953125\n",
            "Batch 101: loss = 0.32927411794662476, acc = 0.8857421875\n",
            "Batch 102: loss = 0.36305028200149536, acc = 0.8837890625\n",
            "Batch 103: loss = 0.33780568838119507, acc = 0.8896484375\n",
            "Batch 104: loss = 0.3104419410228729, acc = 0.9013671875\n",
            "Batch 105: loss = 0.314365416765213, acc = 0.8857421875\n",
            "Batch 106: loss = 0.3097442090511322, acc = 0.896484375\n",
            "Batch 107: loss = 0.3435835540294647, acc = 0.8798828125\n",
            "Batch 108: loss = 0.3337700068950653, acc = 0.8876953125\n",
            "Batch 109: loss = 0.3198338449001312, acc = 0.8955078125\n",
            "Batch 110: loss = 0.342974454164505, acc = 0.8974609375\n",
            "Batch 111: loss = 0.3524628281593323, acc = 0.88671875\n",
            "Batch 112: loss = 0.35287028551101685, acc = 0.892578125\n",
            "Batch 113: loss = 0.31362277269363403, acc = 0.8955078125\n",
            "Batch 114: loss = 0.33063286542892456, acc = 0.896484375\n",
            "Batch 115: loss = 0.3396568298339844, acc = 0.8779296875\n",
            "Batch 116: loss = 0.3530220687389374, acc = 0.8837890625\n",
            "Batch 117: loss = 0.3146841824054718, acc = 0.89453125\n",
            "Batch 118: loss = 0.32937154173851013, acc = 0.8935546875\n",
            "Batch 119: loss = 0.2996324598789215, acc = 0.89453125\n",
            "Batch 120: loss = 0.3120662271976471, acc = 0.8916015625\n",
            "Batch 121: loss = 0.31358081102371216, acc = 0.8916015625\n",
            "Batch 122: loss = 0.3141409158706665, acc = 0.888671875\n",
            "Batch 123: loss = 0.31811094284057617, acc = 0.8916015625\n",
            "Batch 124: loss = 0.35089725255966187, acc = 0.888671875\n",
            "Batch 125: loss = 0.3682186007499695, acc = 0.87890625\n",
            "Batch 126: loss = 0.36389824748039246, acc = 0.8798828125\n",
            "\n",
            "Epoch 82/100\n",
            "Batch 1: loss = 0.44553324580192566, acc = 0.8681640625\n",
            "Batch 2: loss = 0.35401248931884766, acc = 0.8955078125\n",
            "Batch 3: loss = 0.3432145416736603, acc = 0.8896484375\n",
            "Batch 4: loss = 0.3324894905090332, acc = 0.89453125\n",
            "Batch 5: loss = 0.31504058837890625, acc = 0.8896484375\n",
            "Batch 6: loss = 0.31543299555778503, acc = 0.912109375\n",
            "Batch 7: loss = 0.3102545738220215, acc = 0.892578125\n",
            "Batch 8: loss = 0.32477429509162903, acc = 0.9013671875\n",
            "Batch 9: loss = 0.3295214772224426, acc = 0.890625\n",
            "Batch 10: loss = 0.26954787969589233, acc = 0.9150390625\n",
            "Batch 11: loss = 0.3515167832374573, acc = 0.88671875\n",
            "Batch 12: loss = 0.30450573563575745, acc = 0.8984375\n",
            "Batch 13: loss = 0.3099936842918396, acc = 0.900390625\n",
            "Batch 14: loss = 0.3297177851200104, acc = 0.890625\n",
            "Batch 15: loss = 0.3213617205619812, acc = 0.8994140625\n",
            "Batch 16: loss = 0.3481699526309967, acc = 0.890625\n",
            "Batch 17: loss = 0.3399982750415802, acc = 0.884765625\n",
            "Batch 18: loss = 0.34943240880966187, acc = 0.8916015625\n",
            "Batch 19: loss = 0.31471535563468933, acc = 0.8916015625\n",
            "Batch 20: loss = 0.30415236949920654, acc = 0.89453125\n",
            "Batch 21: loss = 0.32681891322135925, acc = 0.8916015625\n",
            "Batch 22: loss = 0.32898858189582825, acc = 0.8876953125\n",
            "Batch 23: loss = 0.331434428691864, acc = 0.890625\n",
            "Batch 24: loss = 0.2644168734550476, acc = 0.916015625\n",
            "Batch 25: loss = 0.34260380268096924, acc = 0.8828125\n",
            "Batch 26: loss = 0.28670790791511536, acc = 0.904296875\n",
            "Batch 27: loss = 0.33901605010032654, acc = 0.884765625\n",
            "Batch 28: loss = 0.3062017560005188, acc = 0.896484375\n",
            "Batch 29: loss = 0.3178136944770813, acc = 0.8896484375\n",
            "Batch 30: loss = 0.3016267418861389, acc = 0.896484375\n",
            "Batch 31: loss = 0.32292652130126953, acc = 0.896484375\n",
            "Batch 32: loss = 0.3492623567581177, acc = 0.896484375\n",
            "Batch 33: loss = 0.29260092973709106, acc = 0.9072265625\n",
            "Batch 34: loss = 0.3154986798763275, acc = 0.9052734375\n",
            "Batch 35: loss = 0.3171120882034302, acc = 0.8837890625\n",
            "Batch 36: loss = 0.29479530453681946, acc = 0.904296875\n",
            "Batch 37: loss = 0.30086320638656616, acc = 0.9091796875\n",
            "Batch 38: loss = 0.30028364062309265, acc = 0.904296875\n",
            "Batch 39: loss = 0.2686323821544647, acc = 0.9130859375\n",
            "Batch 40: loss = 0.3079155683517456, acc = 0.890625\n",
            "Batch 41: loss = 0.29625383019447327, acc = 0.9033203125\n",
            "Batch 42: loss = 0.3425329327583313, acc = 0.8896484375\n",
            "Batch 43: loss = 0.3750195801258087, acc = 0.87890625\n",
            "Batch 44: loss = 0.2734740674495697, acc = 0.9091796875\n",
            "Batch 45: loss = 0.2770787477493286, acc = 0.9052734375\n",
            "Batch 46: loss = 0.27733784914016724, acc = 0.90625\n",
            "Batch 47: loss = 0.3308616876602173, acc = 0.8896484375\n",
            "Batch 48: loss = 0.3108035624027252, acc = 0.900390625\n",
            "Batch 49: loss = 0.2987246513366699, acc = 0.9033203125\n",
            "Batch 50: loss = 0.27864137291908264, acc = 0.91015625\n",
            "Batch 51: loss = 0.32460859417915344, acc = 0.896484375\n",
            "Batch 52: loss = 0.32702386379241943, acc = 0.8984375\n",
            "Batch 53: loss = 0.29454168677330017, acc = 0.9130859375\n",
            "Batch 54: loss = 0.2804677188396454, acc = 0.9150390625\n",
            "Batch 55: loss = 0.24090100824832916, acc = 0.9228515625\n",
            "Batch 56: loss = 0.3442424535751343, acc = 0.876953125\n",
            "Batch 57: loss = 0.3261660933494568, acc = 0.890625\n",
            "Batch 58: loss = 0.3432701826095581, acc = 0.876953125\n",
            "Batch 59: loss = 0.29790881276130676, acc = 0.90234375\n",
            "Batch 60: loss = 0.31965023279190063, acc = 0.8955078125\n",
            "Batch 61: loss = 0.2852754294872284, acc = 0.9033203125\n",
            "Batch 62: loss = 0.3565082252025604, acc = 0.8828125\n",
            "Batch 63: loss = 0.3195890486240387, acc = 0.8994140625\n",
            "Batch 64: loss = 0.27707862854003906, acc = 0.912109375\n",
            "Batch 65: loss = 0.320247620344162, acc = 0.8974609375\n",
            "Batch 66: loss = 0.33344030380249023, acc = 0.888671875\n",
            "Batch 67: loss = 0.33244866132736206, acc = 0.88671875\n",
            "Batch 68: loss = 0.3314270079135895, acc = 0.890625\n",
            "Batch 69: loss = 0.31090307235717773, acc = 0.8984375\n",
            "Batch 70: loss = 0.3592090904712677, acc = 0.876953125\n",
            "Batch 71: loss = 0.32204338908195496, acc = 0.892578125\n",
            "Batch 72: loss = 0.3154119849205017, acc = 0.892578125\n",
            "Batch 73: loss = 0.3339577913284302, acc = 0.8837890625\n",
            "Batch 74: loss = 0.34324440360069275, acc = 0.892578125\n",
            "Batch 75: loss = 0.3703271448612213, acc = 0.8828125\n",
            "Batch 76: loss = 0.3424873352050781, acc = 0.880859375\n",
            "Batch 77: loss = 0.31323134899139404, acc = 0.892578125\n",
            "Batch 78: loss = 0.32360410690307617, acc = 0.8896484375\n",
            "Batch 79: loss = 0.3441738486289978, acc = 0.8935546875\n",
            "Batch 80: loss = 0.30015963315963745, acc = 0.890625\n",
            "Batch 81: loss = 0.31285926699638367, acc = 0.88671875\n",
            "Batch 82: loss = 0.3274822235107422, acc = 0.88671875\n",
            "Batch 83: loss = 0.30079784989356995, acc = 0.8876953125\n",
            "Batch 84: loss = 0.34293559193611145, acc = 0.8857421875\n",
            "Batch 85: loss = 0.3213612139225006, acc = 0.88671875\n",
            "Batch 86: loss = 0.30238431692123413, acc = 0.904296875\n",
            "Batch 87: loss = 0.31045645475387573, acc = 0.89453125\n",
            "Batch 88: loss = 0.3696724474430084, acc = 0.875\n",
            "Batch 89: loss = 0.3026055097579956, acc = 0.900390625\n",
            "Batch 90: loss = 0.33253374695777893, acc = 0.8759765625\n",
            "Batch 91: loss = 0.3239107131958008, acc = 0.8955078125\n",
            "Batch 92: loss = 0.34959498047828674, acc = 0.87890625\n",
            "Batch 93: loss = 0.30777931213378906, acc = 0.8974609375\n",
            "Batch 94: loss = 0.33098259568214417, acc = 0.8916015625\n",
            "Batch 95: loss = 0.3256068825721741, acc = 0.890625\n",
            "Batch 96: loss = 0.3595163822174072, acc = 0.8701171875\n",
            "Batch 97: loss = 0.3210938274860382, acc = 0.8955078125\n",
            "Batch 98: loss = 0.31885069608688354, acc = 0.89453125\n",
            "Batch 99: loss = 0.33256882429122925, acc = 0.8896484375\n",
            "Batch 100: loss = 0.3385467827320099, acc = 0.8740234375\n",
            "Batch 101: loss = 0.3206403851509094, acc = 0.8896484375\n",
            "Batch 102: loss = 0.3480928838253021, acc = 0.876953125\n",
            "Batch 103: loss = 0.36896777153015137, acc = 0.8818359375\n",
            "Batch 104: loss = 0.288421094417572, acc = 0.9033203125\n",
            "Batch 105: loss = 0.27039968967437744, acc = 0.9052734375\n",
            "Batch 106: loss = 0.34510213136672974, acc = 0.890625\n",
            "Batch 107: loss = 0.32373255491256714, acc = 0.890625\n",
            "Batch 108: loss = 0.32903215289115906, acc = 0.890625\n",
            "Batch 109: loss = 0.31670013070106506, acc = 0.89453125\n",
            "Batch 110: loss = 0.30750900506973267, acc = 0.9033203125\n",
            "Batch 111: loss = 0.3299499750137329, acc = 0.8876953125\n",
            "Batch 112: loss = 0.36125949025154114, acc = 0.8818359375\n",
            "Batch 113: loss = 0.3321159780025482, acc = 0.8837890625\n",
            "Batch 114: loss = 0.3657829165458679, acc = 0.869140625\n",
            "Batch 115: loss = 0.35039252042770386, acc = 0.8837890625\n",
            "Batch 116: loss = 0.3509219288825989, acc = 0.884765625\n",
            "Batch 117: loss = 0.3315310478210449, acc = 0.8955078125\n",
            "Batch 118: loss = 0.2789507806301117, acc = 0.904296875\n",
            "Batch 119: loss = 0.3116922676563263, acc = 0.8876953125\n",
            "Batch 120: loss = 0.3137457072734833, acc = 0.8896484375\n",
            "Batch 121: loss = 0.30523133277893066, acc = 0.8935546875\n",
            "Batch 122: loss = 0.3289135992527008, acc = 0.8896484375\n",
            "Batch 123: loss = 0.3359518349170685, acc = 0.8916015625\n",
            "Batch 124: loss = 0.3494706451892853, acc = 0.8662109375\n",
            "Batch 125: loss = 0.3654380142688751, acc = 0.873046875\n",
            "Batch 126: loss = 0.3300093114376068, acc = 0.88671875\n",
            "\n",
            "Epoch 83/100\n",
            "Batch 1: loss = 0.452311247587204, acc = 0.869140625\n",
            "Batch 2: loss = 0.3441913425922394, acc = 0.89453125\n",
            "Batch 3: loss = 0.36942145228385925, acc = 0.88671875\n",
            "Batch 4: loss = 0.31379854679107666, acc = 0.900390625\n",
            "Batch 5: loss = 0.33747708797454834, acc = 0.884765625\n",
            "Batch 6: loss = 0.3464966416358948, acc = 0.8876953125\n",
            "Batch 7: loss = 0.29278987646102905, acc = 0.908203125\n",
            "Batch 8: loss = 0.325887531042099, acc = 0.89453125\n",
            "Batch 9: loss = 0.3222988545894623, acc = 0.8876953125\n",
            "Batch 10: loss = 0.3018507957458496, acc = 0.9013671875\n",
            "Batch 11: loss = 0.35436946153640747, acc = 0.8837890625\n",
            "Batch 12: loss = 0.30417758226394653, acc = 0.8984375\n",
            "Batch 13: loss = 0.32386159896850586, acc = 0.9013671875\n",
            "Batch 14: loss = 0.32724127173423767, acc = 0.8916015625\n",
            "Batch 15: loss = 0.3074694871902466, acc = 0.9111328125\n",
            "Batch 16: loss = 0.35379788279533386, acc = 0.8896484375\n",
            "Batch 17: loss = 0.31282782554626465, acc = 0.8984375\n",
            "Batch 18: loss = 0.35298895835876465, acc = 0.880859375\n",
            "Batch 19: loss = 0.3089340329170227, acc = 0.9091796875\n",
            "Batch 20: loss = 0.33716124296188354, acc = 0.8779296875\n",
            "Batch 21: loss = 0.3201702833175659, acc = 0.8974609375\n",
            "Batch 22: loss = 0.31392452120780945, acc = 0.8955078125\n",
            "Batch 23: loss = 0.29711484909057617, acc = 0.900390625\n",
            "Batch 24: loss = 0.2824217975139618, acc = 0.900390625\n",
            "Batch 25: loss = 0.34211018681526184, acc = 0.89453125\n",
            "Batch 26: loss = 0.2611899673938751, acc = 0.916015625\n",
            "Batch 27: loss = 0.3652871251106262, acc = 0.87109375\n",
            "Batch 28: loss = 0.3271198272705078, acc = 0.8916015625\n",
            "Batch 29: loss = 0.33534178137779236, acc = 0.888671875\n",
            "Batch 30: loss = 0.29081493616104126, acc = 0.9033203125\n",
            "Batch 31: loss = 0.357063353061676, acc = 0.87109375\n",
            "Batch 32: loss = 0.3845560550689697, acc = 0.876953125\n",
            "Batch 33: loss = 0.3090929388999939, acc = 0.900390625\n",
            "Batch 34: loss = 0.3206460177898407, acc = 0.90625\n",
            "Batch 35: loss = 0.31088146567344666, acc = 0.9013671875\n",
            "Batch 36: loss = 0.2890704572200775, acc = 0.9140625\n",
            "Batch 37: loss = 0.2629487216472626, acc = 0.9208984375\n",
            "Batch 38: loss = 0.30499088764190674, acc = 0.9052734375\n",
            "Batch 39: loss = 0.2924732267856598, acc = 0.8994140625\n",
            "Batch 40: loss = 0.2978416085243225, acc = 0.9072265625\n",
            "Batch 41: loss = 0.2879965603351593, acc = 0.9052734375\n",
            "Batch 42: loss = 0.30868491530418396, acc = 0.8974609375\n",
            "Batch 43: loss = 0.3452281057834625, acc = 0.8837890625\n",
            "Batch 44: loss = 0.2674725353717804, acc = 0.923828125\n",
            "Batch 45: loss = 0.29762160778045654, acc = 0.8984375\n",
            "Batch 46: loss = 0.2587484121322632, acc = 0.9208984375\n",
            "Batch 47: loss = 0.31532710790634155, acc = 0.8935546875\n",
            "Batch 48: loss = 0.31144627928733826, acc = 0.900390625\n",
            "Batch 49: loss = 0.2973192632198334, acc = 0.91015625\n",
            "Batch 50: loss = 0.31665319204330444, acc = 0.8896484375\n",
            "Batch 51: loss = 0.3114815056324005, acc = 0.8974609375\n",
            "Batch 52: loss = 0.32949697971343994, acc = 0.8818359375\n",
            "Batch 53: loss = 0.31202104687690735, acc = 0.8955078125\n",
            "Batch 54: loss = 0.2784896492958069, acc = 0.9013671875\n",
            "Batch 55: loss = 0.25369003415107727, acc = 0.92578125\n",
            "Batch 56: loss = 0.3407551944255829, acc = 0.884765625\n",
            "Batch 57: loss = 0.34154969453811646, acc = 0.8798828125\n",
            "Batch 58: loss = 0.359958291053772, acc = 0.87890625\n",
            "Batch 59: loss = 0.2541809678077698, acc = 0.9140625\n",
            "Batch 60: loss = 0.326459139585495, acc = 0.880859375\n",
            "Batch 61: loss = 0.26963043212890625, acc = 0.921875\n",
            "Batch 62: loss = 0.3683186173439026, acc = 0.884765625\n",
            "Batch 63: loss = 0.3177693784236908, acc = 0.8935546875\n",
            "Batch 64: loss = 0.3141286373138428, acc = 0.8984375\n",
            "Batch 65: loss = 0.36158230900764465, acc = 0.880859375\n",
            "Batch 66: loss = 0.36020249128341675, acc = 0.865234375\n",
            "Batch 67: loss = 0.3145357072353363, acc = 0.892578125\n",
            "Batch 68: loss = 0.32316121459007263, acc = 0.896484375\n",
            "Batch 69: loss = 0.3116922676563263, acc = 0.896484375\n",
            "Batch 70: loss = 0.3479774296283722, acc = 0.8818359375\n",
            "Batch 71: loss = 0.35432085394859314, acc = 0.8818359375\n",
            "Batch 72: loss = 0.3146829903125763, acc = 0.8896484375\n",
            "Batch 73: loss = 0.3447428345680237, acc = 0.8876953125\n",
            "Batch 74: loss = 0.3531522750854492, acc = 0.884765625\n",
            "Batch 75: loss = 0.37423446774482727, acc = 0.8759765625\n",
            "Batch 76: loss = 0.3409937918186188, acc = 0.8818359375\n",
            "Batch 77: loss = 0.34497302770614624, acc = 0.876953125\n",
            "Batch 78: loss = 0.3571968078613281, acc = 0.873046875\n",
            "Batch 79: loss = 0.3209425210952759, acc = 0.8857421875\n",
            "Batch 80: loss = 0.29678046703338623, acc = 0.896484375\n",
            "Batch 81: loss = 0.3226853013038635, acc = 0.8916015625\n",
            "Batch 82: loss = 0.3210678696632385, acc = 0.8896484375\n",
            "Batch 83: loss = 0.2932807505130768, acc = 0.9052734375\n",
            "Batch 84: loss = 0.33883142471313477, acc = 0.8837890625\n",
            "Batch 85: loss = 0.2943921387195587, acc = 0.8994140625\n",
            "Batch 86: loss = 0.28904491662979126, acc = 0.89453125\n",
            "Batch 87: loss = 0.3366040587425232, acc = 0.8935546875\n",
            "Batch 88: loss = 0.3498590886592865, acc = 0.87890625\n",
            "Batch 89: loss = 0.29585379362106323, acc = 0.8994140625\n",
            "Batch 90: loss = 0.32603007555007935, acc = 0.8896484375\n",
            "Batch 91: loss = 0.33979660272598267, acc = 0.8837890625\n",
            "Batch 92: loss = 0.34739190340042114, acc = 0.8798828125\n",
            "Batch 93: loss = 0.32053133845329285, acc = 0.8935546875\n",
            "Batch 94: loss = 0.28426575660705566, acc = 0.900390625\n",
            "Batch 95: loss = 0.3448081910610199, acc = 0.8876953125\n",
            "Batch 96: loss = 0.36841532588005066, acc = 0.884765625\n",
            "Batch 97: loss = 0.3593898415565491, acc = 0.8857421875\n",
            "Batch 98: loss = 0.3455345332622528, acc = 0.880859375\n",
            "Batch 99: loss = 0.33832889795303345, acc = 0.890625\n",
            "Batch 100: loss = 0.32875192165374756, acc = 0.880859375\n",
            "Batch 101: loss = 0.32467779517173767, acc = 0.884765625\n",
            "Batch 102: loss = 0.34946194291114807, acc = 0.8818359375\n",
            "Batch 103: loss = 0.33827465772628784, acc = 0.8857421875\n",
            "Batch 104: loss = 0.31569796800613403, acc = 0.89453125\n",
            "Batch 105: loss = 0.291607528924942, acc = 0.9013671875\n",
            "Batch 106: loss = 0.3206966817378998, acc = 0.8935546875\n",
            "Batch 107: loss = 0.3227570056915283, acc = 0.8916015625\n",
            "Batch 108: loss = 0.32672119140625, acc = 0.8984375\n",
            "Batch 109: loss = 0.29575932025909424, acc = 0.8974609375\n",
            "Batch 110: loss = 0.31444281339645386, acc = 0.8916015625\n",
            "Batch 111: loss = 0.34123340249061584, acc = 0.88671875\n",
            "Batch 112: loss = 0.35147905349731445, acc = 0.8876953125\n",
            "Batch 113: loss = 0.31496137380599976, acc = 0.8984375\n",
            "Batch 114: loss = 0.32550421357154846, acc = 0.8818359375\n",
            "Batch 115: loss = 0.3377363979816437, acc = 0.8916015625\n",
            "Batch 116: loss = 0.3778519630432129, acc = 0.8701171875\n",
            "Batch 117: loss = 0.3209969997406006, acc = 0.89453125\n",
            "Batch 118: loss = 0.3066719174385071, acc = 0.89453125\n",
            "Batch 119: loss = 0.34702613949775696, acc = 0.8857421875\n",
            "Batch 120: loss = 0.33045506477355957, acc = 0.88671875\n",
            "Batch 121: loss = 0.31900325417518616, acc = 0.892578125\n",
            "Batch 122: loss = 0.309230774641037, acc = 0.8974609375\n",
            "Batch 123: loss = 0.3361409604549408, acc = 0.892578125\n",
            "Batch 124: loss = 0.3446570038795471, acc = 0.8681640625\n",
            "Batch 125: loss = 0.349320650100708, acc = 0.8779296875\n",
            "Batch 126: loss = 0.35161760449409485, acc = 0.8740234375\n",
            "\n",
            "Epoch 84/100\n",
            "Batch 1: loss = 0.46592074632644653, acc = 0.8583984375\n",
            "Batch 2: loss = 0.3652401864528656, acc = 0.87890625\n",
            "Batch 3: loss = 0.34540626406669617, acc = 0.888671875\n",
            "Batch 4: loss = 0.3332333266735077, acc = 0.8896484375\n",
            "Batch 5: loss = 0.3232152462005615, acc = 0.8779296875\n",
            "Batch 6: loss = 0.34267792105674744, acc = 0.88671875\n",
            "Batch 7: loss = 0.3015570342540741, acc = 0.904296875\n",
            "Batch 8: loss = 0.3496556580066681, acc = 0.8857421875\n",
            "Batch 9: loss = 0.31355538964271545, acc = 0.8994140625\n",
            "Batch 10: loss = 0.2996509373188019, acc = 0.9033203125\n",
            "Batch 11: loss = 0.34949788451194763, acc = 0.87890625\n",
            "Batch 12: loss = 0.2943989634513855, acc = 0.8916015625\n",
            "Batch 13: loss = 0.3327101469039917, acc = 0.888671875\n",
            "Batch 14: loss = 0.30891722440719604, acc = 0.8984375\n",
            "Batch 15: loss = 0.3085482716560364, acc = 0.9013671875\n",
            "Batch 16: loss = 0.33971866965293884, acc = 0.892578125\n",
            "Batch 17: loss = 0.33596423268318176, acc = 0.8896484375\n",
            "Batch 18: loss = 0.3552011549472809, acc = 0.8857421875\n",
            "Batch 19: loss = 0.3046974241733551, acc = 0.904296875\n",
            "Batch 20: loss = 0.31681954860687256, acc = 0.892578125\n",
            "Batch 21: loss = 0.34019768238067627, acc = 0.880859375\n",
            "Batch 22: loss = 0.33574196696281433, acc = 0.8896484375\n",
            "Batch 23: loss = 0.314280241727829, acc = 0.9033203125\n",
            "Batch 24: loss = 0.2945917248725891, acc = 0.9091796875\n",
            "Batch 25: loss = 0.30827751755714417, acc = 0.90234375\n",
            "Batch 26: loss = 0.2964929938316345, acc = 0.89453125\n",
            "Batch 27: loss = 0.33490514755249023, acc = 0.876953125\n",
            "Batch 28: loss = 0.3276619613170624, acc = 0.8935546875\n",
            "Batch 29: loss = 0.30744099617004395, acc = 0.8935546875\n",
            "Batch 30: loss = 0.2872501611709595, acc = 0.8984375\n",
            "Batch 31: loss = 0.33292749524116516, acc = 0.8779296875\n",
            "Batch 32: loss = 0.36991435289382935, acc = 0.876953125\n",
            "Batch 33: loss = 0.31664523482322693, acc = 0.900390625\n",
            "Batch 34: loss = 0.3348439335823059, acc = 0.8876953125\n",
            "Batch 35: loss = 0.34416577219963074, acc = 0.88671875\n",
            "Batch 36: loss = 0.26096808910369873, acc = 0.9140625\n",
            "Batch 37: loss = 0.2898690402507782, acc = 0.9111328125\n",
            "Batch 38: loss = 0.2689409852027893, acc = 0.912109375\n",
            "Batch 39: loss = 0.2679935693740845, acc = 0.916015625\n",
            "Batch 40: loss = 0.29188603162765503, acc = 0.892578125\n",
            "Batch 41: loss = 0.2599978446960449, acc = 0.9072265625\n",
            "Batch 42: loss = 0.34131866693496704, acc = 0.884765625\n",
            "Batch 43: loss = 0.3373224139213562, acc = 0.8916015625\n",
            "Batch 44: loss = 0.28966328501701355, acc = 0.8984375\n",
            "Batch 45: loss = 0.2925707697868347, acc = 0.9130859375\n",
            "Batch 46: loss = 0.2677614986896515, acc = 0.9140625\n",
            "Batch 47: loss = 0.2978481352329254, acc = 0.9130859375\n",
            "Batch 48: loss = 0.31347495317459106, acc = 0.9033203125\n",
            "Batch 49: loss = 0.2539317309856415, acc = 0.9169921875\n",
            "Batch 50: loss = 0.29965725541114807, acc = 0.8955078125\n",
            "Batch 51: loss = 0.3178773820400238, acc = 0.8818359375\n",
            "Batch 52: loss = 0.3653046488761902, acc = 0.8759765625\n",
            "Batch 53: loss = 0.29649946093559265, acc = 0.896484375\n",
            "Batch 54: loss = 0.2589801549911499, acc = 0.9091796875\n",
            "Batch 55: loss = 0.2641277313232422, acc = 0.916015625\n",
            "Batch 56: loss = 0.36820629239082336, acc = 0.87890625\n",
            "Batch 57: loss = 0.3282702565193176, acc = 0.8837890625\n",
            "Batch 58: loss = 0.35439833998680115, acc = 0.8701171875\n",
            "Batch 59: loss = 0.24829483032226562, acc = 0.921875\n",
            "Batch 60: loss = 0.3063380718231201, acc = 0.904296875\n",
            "Batch 61: loss = 0.27261388301849365, acc = 0.916015625\n",
            "Batch 62: loss = 0.35170817375183105, acc = 0.8779296875\n",
            "Batch 63: loss = 0.27844828367233276, acc = 0.90234375\n",
            "Batch 64: loss = 0.2972360849380493, acc = 0.900390625\n",
            "Batch 65: loss = 0.31425097584724426, acc = 0.8974609375\n",
            "Batch 66: loss = 0.33663931488990784, acc = 0.88671875\n",
            "Batch 67: loss = 0.30915090441703796, acc = 0.896484375\n",
            "Batch 68: loss = 0.34824338555336, acc = 0.8798828125\n",
            "Batch 69: loss = 0.29973119497299194, acc = 0.91015625\n",
            "Batch 70: loss = 0.33056655526161194, acc = 0.89453125\n",
            "Batch 71: loss = 0.32682597637176514, acc = 0.8935546875\n",
            "Batch 72: loss = 0.32273566722869873, acc = 0.890625\n",
            "Batch 73: loss = 0.34227678179740906, acc = 0.8896484375\n",
            "Batch 74: loss = 0.3363388180732727, acc = 0.880859375\n",
            "Batch 75: loss = 0.3983876705169678, acc = 0.865234375\n",
            "Batch 76: loss = 0.3593147099018097, acc = 0.87109375\n",
            "Batch 77: loss = 0.32597678899765015, acc = 0.8984375\n",
            "Batch 78: loss = 0.33215367794036865, acc = 0.888671875\n",
            "Batch 79: loss = 0.3336436450481415, acc = 0.8837890625\n",
            "Batch 80: loss = 0.2964729070663452, acc = 0.9013671875\n",
            "Batch 81: loss = 0.33763521909713745, acc = 0.8798828125\n",
            "Batch 82: loss = 0.2824345827102661, acc = 0.9150390625\n",
            "Batch 83: loss = 0.29893070459365845, acc = 0.9013671875\n",
            "Batch 84: loss = 0.3184562623500824, acc = 0.884765625\n",
            "Batch 85: loss = 0.3294807970523834, acc = 0.890625\n",
            "Batch 86: loss = 0.3101271688938141, acc = 0.896484375\n",
            "Batch 87: loss = 0.35265642404556274, acc = 0.880859375\n",
            "Batch 88: loss = 0.34597018361091614, acc = 0.8818359375\n",
            "Batch 89: loss = 0.27480265498161316, acc = 0.9091796875\n",
            "Batch 90: loss = 0.35953086614608765, acc = 0.87109375\n",
            "Batch 91: loss = 0.3458274006843567, acc = 0.880859375\n",
            "Batch 92: loss = 0.3472102880477905, acc = 0.8916015625\n",
            "Batch 93: loss = 0.29911333322525024, acc = 0.90625\n",
            "Batch 94: loss = 0.31196922063827515, acc = 0.904296875\n",
            "Batch 95: loss = 0.3275819718837738, acc = 0.8916015625\n",
            "Batch 96: loss = 0.3499545454978943, acc = 0.88671875\n",
            "Batch 97: loss = 0.3340277373790741, acc = 0.8955078125\n",
            "Batch 98: loss = 0.3425407111644745, acc = 0.89453125\n",
            "Batch 99: loss = 0.3199012577533722, acc = 0.884765625\n",
            "Batch 100: loss = 0.35732966661453247, acc = 0.873046875\n",
            "Batch 101: loss = 0.3348628580570221, acc = 0.8759765625\n",
            "Batch 102: loss = 0.35132893919944763, acc = 0.8857421875\n",
            "Batch 103: loss = 0.3450750410556793, acc = 0.884765625\n",
            "Batch 104: loss = 0.27589288353919983, acc = 0.9033203125\n",
            "Batch 105: loss = 0.2662617564201355, acc = 0.9140625\n",
            "Batch 106: loss = 0.31833580136299133, acc = 0.890625\n",
            "Batch 107: loss = 0.31138935685157776, acc = 0.888671875\n",
            "Batch 108: loss = 0.31410178542137146, acc = 0.890625\n",
            "Batch 109: loss = 0.3047295808792114, acc = 0.90234375\n",
            "Batch 110: loss = 0.26645565032958984, acc = 0.9189453125\n",
            "Batch 111: loss = 0.36712977290153503, acc = 0.8828125\n",
            "Batch 112: loss = 0.30584678053855896, acc = 0.8994140625\n",
            "Batch 113: loss = 0.3168462812900543, acc = 0.900390625\n",
            "Batch 114: loss = 0.3397577702999115, acc = 0.8828125\n",
            "Batch 115: loss = 0.32531556487083435, acc = 0.8896484375\n",
            "Batch 116: loss = 0.3344273865222931, acc = 0.8984375\n",
            "Batch 117: loss = 0.31510451436042786, acc = 0.9013671875\n",
            "Batch 118: loss = 0.3139539361000061, acc = 0.8955078125\n",
            "Batch 119: loss = 0.3098580837249756, acc = 0.892578125\n",
            "Batch 120: loss = 0.3021661639213562, acc = 0.8955078125\n",
            "Batch 121: loss = 0.2942669689655304, acc = 0.8984375\n",
            "Batch 122: loss = 0.2965441942214966, acc = 0.8994140625\n",
            "Batch 123: loss = 0.33964046835899353, acc = 0.876953125\n",
            "Batch 124: loss = 0.32619866728782654, acc = 0.896484375\n",
            "Batch 125: loss = 0.3578724265098572, acc = 0.873046875\n",
            "Batch 126: loss = 0.3285760283470154, acc = 0.89453125\n",
            "\n",
            "Epoch 85/100\n",
            "Batch 1: loss = 0.44059696793556213, acc = 0.8701171875\n",
            "Batch 2: loss = 0.35475143790245056, acc = 0.884765625\n",
            "Batch 3: loss = 0.33715733885765076, acc = 0.8896484375\n",
            "Batch 4: loss = 0.3391113579273224, acc = 0.892578125\n",
            "Batch 5: loss = 0.32307061553001404, acc = 0.8974609375\n",
            "Batch 6: loss = 0.3430403470993042, acc = 0.88671875\n",
            "Batch 7: loss = 0.31910285353660583, acc = 0.8916015625\n",
            "Batch 8: loss = 0.333168625831604, acc = 0.8837890625\n",
            "Batch 9: loss = 0.33769845962524414, acc = 0.8876953125\n",
            "Batch 10: loss = 0.2946103811264038, acc = 0.9013671875\n",
            "Batch 11: loss = 0.3395984470844269, acc = 0.8916015625\n",
            "Batch 12: loss = 0.30326828360557556, acc = 0.8955078125\n",
            "Batch 13: loss = 0.3150796890258789, acc = 0.888671875\n",
            "Batch 14: loss = 0.30650556087493896, acc = 0.9013671875\n",
            "Batch 15: loss = 0.2904374301433563, acc = 0.904296875\n",
            "Batch 16: loss = 0.3512793183326721, acc = 0.8955078125\n",
            "Batch 17: loss = 0.33052799105644226, acc = 0.8974609375\n",
            "Batch 18: loss = 0.3427661657333374, acc = 0.8974609375\n",
            "Batch 19: loss = 0.3148517906665802, acc = 0.8955078125\n",
            "Batch 20: loss = 0.3224446177482605, acc = 0.8994140625\n",
            "Batch 21: loss = 0.34992340207099915, acc = 0.8828125\n",
            "Batch 22: loss = 0.3086091876029968, acc = 0.90625\n",
            "Batch 23: loss = 0.2981494069099426, acc = 0.9052734375\n",
            "Batch 24: loss = 0.26644036173820496, acc = 0.9052734375\n",
            "Batch 25: loss = 0.3516347408294678, acc = 0.8837890625\n",
            "Batch 26: loss = 0.2846444249153137, acc = 0.90625\n",
            "Batch 27: loss = 0.34338080883026123, acc = 0.8828125\n",
            "Batch 28: loss = 0.33922630548477173, acc = 0.880859375\n",
            "Batch 29: loss = 0.32791709899902344, acc = 0.8857421875\n",
            "Batch 30: loss = 0.3065137565135956, acc = 0.8935546875\n",
            "Batch 31: loss = 0.3170899450778961, acc = 0.8974609375\n",
            "Batch 32: loss = 0.39296725392341614, acc = 0.87109375\n",
            "Batch 33: loss = 0.3223476707935333, acc = 0.8955078125\n",
            "Batch 34: loss = 0.33538496494293213, acc = 0.8984375\n",
            "Batch 35: loss = 0.3234798014163971, acc = 0.8984375\n",
            "Batch 36: loss = 0.27247899770736694, acc = 0.91015625\n",
            "Batch 37: loss = 0.2799661159515381, acc = 0.9091796875\n",
            "Batch 38: loss = 0.2914212644100189, acc = 0.90234375\n",
            "Batch 39: loss = 0.2450154572725296, acc = 0.921875\n",
            "Batch 40: loss = 0.3159370720386505, acc = 0.892578125\n",
            "Batch 41: loss = 0.27767327427864075, acc = 0.9033203125\n",
            "Batch 42: loss = 0.30191364884376526, acc = 0.90625\n",
            "Batch 43: loss = 0.34237775206565857, acc = 0.8837890625\n",
            "Batch 44: loss = 0.27088966965675354, acc = 0.9169921875\n",
            "Batch 45: loss = 0.3232971429824829, acc = 0.884765625\n",
            "Batch 46: loss = 0.2552226185798645, acc = 0.921875\n",
            "Batch 47: loss = 0.2877717614173889, acc = 0.8935546875\n",
            "Batch 48: loss = 0.2769829034805298, acc = 0.904296875\n",
            "Batch 49: loss = 0.2853216230869293, acc = 0.9150390625\n",
            "Batch 50: loss = 0.2975369989871979, acc = 0.9072265625\n",
            "Batch 51: loss = 0.312191367149353, acc = 0.8857421875\n",
            "Batch 52: loss = 0.3360796272754669, acc = 0.890625\n",
            "Batch 53: loss = 0.31343692541122437, acc = 0.890625\n",
            "Batch 54: loss = 0.24773848056793213, acc = 0.9169921875\n",
            "Batch 55: loss = 0.2579682469367981, acc = 0.9228515625\n",
            "Batch 56: loss = 0.3285994827747345, acc = 0.88671875\n",
            "Batch 57: loss = 0.3112400472164154, acc = 0.900390625\n",
            "Batch 58: loss = 0.3672647178173065, acc = 0.869140625\n",
            "Batch 59: loss = 0.26000145077705383, acc = 0.919921875\n",
            "Batch 60: loss = 0.2882656455039978, acc = 0.9111328125\n",
            "Batch 61: loss = 0.28273218870162964, acc = 0.90625\n",
            "Batch 62: loss = 0.31694525480270386, acc = 0.8994140625\n",
            "Batch 63: loss = 0.2746206223964691, acc = 0.9013671875\n",
            "Batch 64: loss = 0.26301655173301697, acc = 0.9130859375\n",
            "Batch 65: loss = 0.32194387912750244, acc = 0.8857421875\n",
            "Batch 66: loss = 0.33976930379867554, acc = 0.8857421875\n",
            "Batch 67: loss = 0.29074400663375854, acc = 0.890625\n",
            "Batch 68: loss = 0.3237166404724121, acc = 0.8935546875\n",
            "Batch 69: loss = 0.3095237612724304, acc = 0.9091796875\n",
            "Batch 70: loss = 0.3300037086009979, acc = 0.892578125\n",
            "Batch 71: loss = 0.341430127620697, acc = 0.88671875\n",
            "Batch 72: loss = 0.29963845014572144, acc = 0.9111328125\n",
            "Batch 73: loss = 0.35656577348709106, acc = 0.8828125\n",
            "Batch 74: loss = 0.34537431597709656, acc = 0.8828125\n",
            "Batch 75: loss = 0.37669679522514343, acc = 0.8759765625\n",
            "Batch 76: loss = 0.3588191270828247, acc = 0.876953125\n",
            "Batch 77: loss = 0.34414443373680115, acc = 0.8837890625\n",
            "Batch 78: loss = 0.34495171904563904, acc = 0.8759765625\n",
            "Batch 79: loss = 0.29802206158638, acc = 0.8984375\n",
            "Batch 80: loss = 0.29141154885292053, acc = 0.8994140625\n",
            "Batch 81: loss = 0.3011186122894287, acc = 0.8974609375\n",
            "Batch 82: loss = 0.32828474044799805, acc = 0.8974609375\n",
            "Batch 83: loss = 0.3132966160774231, acc = 0.8984375\n",
            "Batch 84: loss = 0.3651520013809204, acc = 0.8818359375\n",
            "Batch 85: loss = 0.34288114309310913, acc = 0.8876953125\n",
            "Batch 86: loss = 0.32424473762512207, acc = 0.890625\n",
            "Batch 87: loss = 0.32748210430145264, acc = 0.8876953125\n",
            "Batch 88: loss = 0.34037837386131287, acc = 0.8974609375\n",
            "Batch 89: loss = 0.3028159737586975, acc = 0.8974609375\n",
            "Batch 90: loss = 0.3206259310245514, acc = 0.8896484375\n",
            "Batch 91: loss = 0.33845457434654236, acc = 0.8974609375\n",
            "Batch 92: loss = 0.3270055055618286, acc = 0.8955078125\n",
            "Batch 93: loss = 0.32301920652389526, acc = 0.900390625\n",
            "Batch 94: loss = 0.3109155297279358, acc = 0.9013671875\n",
            "Batch 95: loss = 0.3274534344673157, acc = 0.8837890625\n",
            "Batch 96: loss = 0.37971770763397217, acc = 0.8701171875\n",
            "Batch 97: loss = 0.3471141457557678, acc = 0.89453125\n",
            "Batch 98: loss = 0.3515569567680359, acc = 0.8837890625\n",
            "Batch 99: loss = 0.3498413562774658, acc = 0.8828125\n",
            "Batch 100: loss = 0.3373746871948242, acc = 0.8798828125\n",
            "Batch 101: loss = 0.33489179611206055, acc = 0.8857421875\n",
            "Batch 102: loss = 0.31644150614738464, acc = 0.900390625\n",
            "Batch 103: loss = 0.3568294942378998, acc = 0.8818359375\n",
            "Batch 104: loss = 0.3224431276321411, acc = 0.8896484375\n",
            "Batch 105: loss = 0.29127609729766846, acc = 0.8994140625\n",
            "Batch 106: loss = 0.33249974250793457, acc = 0.888671875\n",
            "Batch 107: loss = 0.34475743770599365, acc = 0.8818359375\n",
            "Batch 108: loss = 0.30684277415275574, acc = 0.896484375\n",
            "Batch 109: loss = 0.3034963607788086, acc = 0.8935546875\n",
            "Batch 110: loss = 0.28409984707832336, acc = 0.90625\n",
            "Batch 111: loss = 0.3289467692375183, acc = 0.890625\n",
            "Batch 112: loss = 0.35165178775787354, acc = 0.88671875\n",
            "Batch 113: loss = 0.32063770294189453, acc = 0.896484375\n",
            "Batch 114: loss = 0.3281337022781372, acc = 0.890625\n",
            "Batch 115: loss = 0.33182957768440247, acc = 0.8896484375\n",
            "Batch 116: loss = 0.32864782214164734, acc = 0.8935546875\n",
            "Batch 117: loss = 0.328682541847229, acc = 0.8994140625\n",
            "Batch 118: loss = 0.3266752064228058, acc = 0.8896484375\n",
            "Batch 119: loss = 0.3060457706451416, acc = 0.8955078125\n",
            "Batch 120: loss = 0.2947247326374054, acc = 0.8994140625\n",
            "Batch 121: loss = 0.3144884407520294, acc = 0.900390625\n",
            "Batch 122: loss = 0.27859747409820557, acc = 0.912109375\n",
            "Batch 123: loss = 0.32133984565734863, acc = 0.888671875\n",
            "Batch 124: loss = 0.3231436312198639, acc = 0.87890625\n",
            "Batch 125: loss = 0.30751487612724304, acc = 0.896484375\n",
            "Batch 126: loss = 0.33648622035980225, acc = 0.8896484375\n",
            "\n",
            "Epoch 86/100\n",
            "Batch 1: loss = 0.4335097372531891, acc = 0.876953125\n",
            "Batch 2: loss = 0.3855070173740387, acc = 0.8837890625\n",
            "Batch 3: loss = 0.30142948031425476, acc = 0.90234375\n",
            "Batch 4: loss = 0.32768476009368896, acc = 0.9052734375\n",
            "Batch 5: loss = 0.3172670304775238, acc = 0.9033203125\n",
            "Batch 6: loss = 0.3171312212944031, acc = 0.900390625\n",
            "Batch 7: loss = 0.32211217284202576, acc = 0.88671875\n",
            "Batch 8: loss = 0.31191641092300415, acc = 0.9033203125\n",
            "Batch 9: loss = 0.3109188377857208, acc = 0.9033203125\n",
            "Batch 10: loss = 0.286103218793869, acc = 0.9013671875\n",
            "Batch 11: loss = 0.3708345890045166, acc = 0.884765625\n",
            "Batch 12: loss = 0.2934657335281372, acc = 0.9033203125\n",
            "Batch 13: loss = 0.2858226001262665, acc = 0.8955078125\n",
            "Batch 14: loss = 0.31819799542427063, acc = 0.8896484375\n",
            "Batch 15: loss = 0.2876851558685303, acc = 0.912109375\n",
            "Batch 16: loss = 0.33865609765052795, acc = 0.8876953125\n",
            "Batch 17: loss = 0.3410295844078064, acc = 0.892578125\n",
            "Batch 18: loss = 0.3281584084033966, acc = 0.8935546875\n",
            "Batch 19: loss = 0.29987213015556335, acc = 0.8984375\n",
            "Batch 20: loss = 0.3300934135913849, acc = 0.8779296875\n",
            "Batch 21: loss = 0.3236246407032013, acc = 0.9013671875\n",
            "Batch 22: loss = 0.3159170150756836, acc = 0.8974609375\n",
            "Batch 23: loss = 0.32300007343292236, acc = 0.89453125\n",
            "Batch 24: loss = 0.2734615206718445, acc = 0.904296875\n",
            "Batch 25: loss = 0.3136786222457886, acc = 0.9052734375\n",
            "Batch 26: loss = 0.30709517002105713, acc = 0.9033203125\n",
            "Batch 27: loss = 0.3256123661994934, acc = 0.892578125\n",
            "Batch 28: loss = 0.3280429244041443, acc = 0.888671875\n",
            "Batch 29: loss = 0.31288737058639526, acc = 0.8896484375\n",
            "Batch 30: loss = 0.28425559401512146, acc = 0.8974609375\n",
            "Batch 31: loss = 0.32213088870048523, acc = 0.8994140625\n",
            "Batch 32: loss = 0.36163902282714844, acc = 0.87890625\n",
            "Batch 33: loss = 0.2908991277217865, acc = 0.90234375\n",
            "Batch 34: loss = 0.32049188017845154, acc = 0.892578125\n",
            "Batch 35: loss = 0.29082590341567993, acc = 0.9072265625\n",
            "Batch 36: loss = 0.29041263461112976, acc = 0.8984375\n",
            "Batch 37: loss = 0.3075668513774872, acc = 0.9130859375\n",
            "Batch 38: loss = 0.2842963635921478, acc = 0.90625\n",
            "Batch 39: loss = 0.2974291443824768, acc = 0.892578125\n",
            "Batch 40: loss = 0.28885528445243835, acc = 0.9013671875\n",
            "Batch 41: loss = 0.3071805238723755, acc = 0.892578125\n",
            "Batch 42: loss = 0.29951488971710205, acc = 0.9013671875\n",
            "Batch 43: loss = 0.347054123878479, acc = 0.88671875\n",
            "Batch 44: loss = 0.26189354062080383, acc = 0.916015625\n",
            "Batch 45: loss = 0.29640763998031616, acc = 0.904296875\n",
            "Batch 46: loss = 0.2964802086353302, acc = 0.89453125\n",
            "Batch 47: loss = 0.3361523747444153, acc = 0.8857421875\n",
            "Batch 48: loss = 0.2977040410041809, acc = 0.8994140625\n",
            "Batch 49: loss = 0.30633804202079773, acc = 0.91015625\n",
            "Batch 50: loss = 0.3131938874721527, acc = 0.8935546875\n",
            "Batch 51: loss = 0.33539384603500366, acc = 0.8798828125\n",
            "Batch 52: loss = 0.3089832365512848, acc = 0.8935546875\n",
            "Batch 53: loss = 0.3413536846637726, acc = 0.8759765625\n",
            "Batch 54: loss = 0.25111398100852966, acc = 0.91796875\n",
            "Batch 55: loss = 0.2596237063407898, acc = 0.9208984375\n",
            "Batch 56: loss = 0.3418098986148834, acc = 0.8876953125\n",
            "Batch 57: loss = 0.31558525562286377, acc = 0.884765625\n",
            "Batch 58: loss = 0.35530680418014526, acc = 0.875\n",
            "Batch 59: loss = 0.26333093643188477, acc = 0.923828125\n",
            "Batch 60: loss = 0.31087973713874817, acc = 0.9111328125\n",
            "Batch 61: loss = 0.25583314895629883, acc = 0.921875\n",
            "Batch 62: loss = 0.3419339060783386, acc = 0.8935546875\n",
            "Batch 63: loss = 0.30282342433929443, acc = 0.8994140625\n",
            "Batch 64: loss = 0.27742424607276917, acc = 0.904296875\n",
            "Batch 65: loss = 0.3179541528224945, acc = 0.8955078125\n",
            "Batch 66: loss = 0.3097327947616577, acc = 0.90234375\n",
            "Batch 67: loss = 0.31337541341781616, acc = 0.908203125\n",
            "Batch 68: loss = 0.33800840377807617, acc = 0.892578125\n",
            "Batch 69: loss = 0.27798905968666077, acc = 0.9150390625\n",
            "Batch 70: loss = 0.35475173592567444, acc = 0.8779296875\n",
            "Batch 71: loss = 0.3335864543914795, acc = 0.8798828125\n",
            "Batch 72: loss = 0.2916678488254547, acc = 0.91015625\n",
            "Batch 73: loss = 0.30673420429229736, acc = 0.9052734375\n",
            "Batch 74: loss = 0.32870620489120483, acc = 0.8857421875\n",
            "Batch 75: loss = 0.3865349590778351, acc = 0.8720703125\n",
            "Batch 76: loss = 0.32677555084228516, acc = 0.884765625\n",
            "Batch 77: loss = 0.31249532103538513, acc = 0.89453125\n",
            "Batch 78: loss = 0.31695690751075745, acc = 0.8916015625\n",
            "Batch 79: loss = 0.3202716112136841, acc = 0.888671875\n",
            "Batch 80: loss = 0.30154597759246826, acc = 0.8994140625\n",
            "Batch 81: loss = 0.31830400228500366, acc = 0.8916015625\n",
            "Batch 82: loss = 0.3163582682609558, acc = 0.8974609375\n",
            "Batch 83: loss = 0.2652047276496887, acc = 0.9130859375\n",
            "Batch 84: loss = 0.3427240252494812, acc = 0.88671875\n",
            "Batch 85: loss = 0.3447481095790863, acc = 0.8974609375\n",
            "Batch 86: loss = 0.30198246240615845, acc = 0.896484375\n",
            "Batch 87: loss = 0.3158649206161499, acc = 0.890625\n",
            "Batch 88: loss = 0.3648609220981598, acc = 0.87890625\n",
            "Batch 89: loss = 0.3073130249977112, acc = 0.90234375\n",
            "Batch 90: loss = 0.31881433725357056, acc = 0.8876953125\n",
            "Batch 91: loss = 0.3300895690917969, acc = 0.8935546875\n",
            "Batch 92: loss = 0.34546715021133423, acc = 0.890625\n",
            "Batch 93: loss = 0.28205856680870056, acc = 0.8994140625\n",
            "Batch 94: loss = 0.32744497060775757, acc = 0.8818359375\n",
            "Batch 95: loss = 0.3304865062236786, acc = 0.8896484375\n",
            "Batch 96: loss = 0.38846099376678467, acc = 0.85546875\n",
            "Batch 97: loss = 0.3369634449481964, acc = 0.8876953125\n",
            "Batch 98: loss = 0.32966896891593933, acc = 0.888671875\n",
            "Batch 99: loss = 0.34348729252815247, acc = 0.8837890625\n",
            "Batch 100: loss = 0.3555591404438019, acc = 0.8828125\n",
            "Batch 101: loss = 0.299439936876297, acc = 0.9033203125\n",
            "Batch 102: loss = 0.3555920720100403, acc = 0.87890625\n",
            "Batch 103: loss = 0.3484589755535126, acc = 0.8857421875\n",
            "Batch 104: loss = 0.3120272755622864, acc = 0.9013671875\n",
            "Batch 105: loss = 0.27279502153396606, acc = 0.904296875\n",
            "Batch 106: loss = 0.3153321146965027, acc = 0.88671875\n",
            "Batch 107: loss = 0.32577067613601685, acc = 0.8818359375\n",
            "Batch 108: loss = 0.3249449133872986, acc = 0.890625\n",
            "Batch 109: loss = 0.30799052119255066, acc = 0.89453125\n",
            "Batch 110: loss = 0.29761776328086853, acc = 0.9052734375\n",
            "Batch 111: loss = 0.34100890159606934, acc = 0.876953125\n",
            "Batch 112: loss = 0.3462876081466675, acc = 0.8857421875\n",
            "Batch 113: loss = 0.3070950508117676, acc = 0.8984375\n",
            "Batch 114: loss = 0.3217151463031769, acc = 0.9013671875\n",
            "Batch 115: loss = 0.33406680822372437, acc = 0.8916015625\n",
            "Batch 116: loss = 0.3622588813304901, acc = 0.87109375\n",
            "Batch 117: loss = 0.32198289036750793, acc = 0.90234375\n",
            "Batch 118: loss = 0.2880457937717438, acc = 0.900390625\n",
            "Batch 119: loss = 0.2767791152000427, acc = 0.896484375\n",
            "Batch 120: loss = 0.26473385095596313, acc = 0.912109375\n",
            "Batch 121: loss = 0.2925245761871338, acc = 0.8984375\n",
            "Batch 122: loss = 0.28663989901542664, acc = 0.9013671875\n",
            "Batch 123: loss = 0.3406532108783722, acc = 0.8818359375\n",
            "Batch 124: loss = 0.33904579281806946, acc = 0.884765625\n",
            "Batch 125: loss = 0.3457246422767639, acc = 0.8857421875\n",
            "Batch 126: loss = 0.3317941427230835, acc = 0.888671875\n",
            "\n",
            "Epoch 87/100\n",
            "Batch 1: loss = 0.40784144401550293, acc = 0.88671875\n",
            "Batch 2: loss = 0.34284594655036926, acc = 0.8876953125\n",
            "Batch 3: loss = 0.3361237049102783, acc = 0.8994140625\n",
            "Batch 4: loss = 0.3093372881412506, acc = 0.8994140625\n",
            "Batch 5: loss = 0.3212713599205017, acc = 0.8994140625\n",
            "Batch 6: loss = 0.3230859041213989, acc = 0.8955078125\n",
            "Batch 7: loss = 0.315626859664917, acc = 0.88671875\n",
            "Batch 8: loss = 0.3179473876953125, acc = 0.900390625\n",
            "Batch 9: loss = 0.327380508184433, acc = 0.888671875\n",
            "Batch 10: loss = 0.2761484682559967, acc = 0.8984375\n",
            "Batch 11: loss = 0.3290022909641266, acc = 0.88671875\n",
            "Batch 12: loss = 0.3087652623653412, acc = 0.8984375\n",
            "Batch 13: loss = 0.2899876534938812, acc = 0.8984375\n",
            "Batch 14: loss = 0.29628968238830566, acc = 0.9091796875\n",
            "Batch 15: loss = 0.31846755743026733, acc = 0.904296875\n",
            "Batch 16: loss = 0.3188360333442688, acc = 0.8955078125\n",
            "Batch 17: loss = 0.29806917905807495, acc = 0.91015625\n",
            "Batch 18: loss = 0.3500218987464905, acc = 0.87890625\n",
            "Batch 19: loss = 0.2964858412742615, acc = 0.90234375\n",
            "Batch 20: loss = 0.31063351035118103, acc = 0.896484375\n",
            "Batch 21: loss = 0.3219500780105591, acc = 0.8876953125\n",
            "Batch 22: loss = 0.30847662687301636, acc = 0.884765625\n",
            "Batch 23: loss = 0.2941001057624817, acc = 0.8984375\n",
            "Batch 24: loss = 0.28312045335769653, acc = 0.9033203125\n",
            "Batch 25: loss = 0.3222418427467346, acc = 0.8876953125\n",
            "Batch 26: loss = 0.3063051104545593, acc = 0.904296875\n",
            "Batch 27: loss = 0.32955628633499146, acc = 0.8828125\n",
            "Batch 28: loss = 0.34408214688301086, acc = 0.8828125\n",
            "Batch 29: loss = 0.3336743712425232, acc = 0.8837890625\n",
            "Batch 30: loss = 0.28662315011024475, acc = 0.9013671875\n",
            "Batch 31: loss = 0.33071058988571167, acc = 0.890625\n",
            "Batch 32: loss = 0.36147305369377136, acc = 0.873046875\n",
            "Batch 33: loss = 0.28582870960235596, acc = 0.9150390625\n",
            "Batch 34: loss = 0.29453086853027344, acc = 0.91015625\n",
            "Batch 35: loss = 0.31027498841285706, acc = 0.90234375\n",
            "Batch 36: loss = 0.27386313676834106, acc = 0.9072265625\n",
            "Batch 37: loss = 0.2802059054374695, acc = 0.9111328125\n",
            "Batch 38: loss = 0.3055182993412018, acc = 0.88671875\n",
            "Batch 39: loss = 0.2556806802749634, acc = 0.9150390625\n",
            "Batch 40: loss = 0.2869062125682831, acc = 0.9013671875\n",
            "Batch 41: loss = 0.2932034730911255, acc = 0.8994140625\n",
            "Batch 42: loss = 0.30522361397743225, acc = 0.8935546875\n",
            "Batch 43: loss = 0.3379081189632416, acc = 0.8896484375\n",
            "Batch 44: loss = 0.2588624358177185, acc = 0.9150390625\n",
            "Batch 45: loss = 0.27512842416763306, acc = 0.9130859375\n",
            "Batch 46: loss = 0.24987800419330597, acc = 0.921875\n",
            "Batch 47: loss = 0.30160897970199585, acc = 0.908203125\n",
            "Batch 48: loss = 0.2943322956562042, acc = 0.90625\n",
            "Batch 49: loss = 0.28213638067245483, acc = 0.9150390625\n",
            "Batch 50: loss = 0.30100539326667786, acc = 0.8984375\n",
            "Batch 51: loss = 0.31085139513015747, acc = 0.8916015625\n",
            "Batch 52: loss = 0.31294721364974976, acc = 0.890625\n",
            "Batch 53: loss = 0.30327385663986206, acc = 0.8994140625\n",
            "Batch 54: loss = 0.25249433517456055, acc = 0.91796875\n",
            "Batch 55: loss = 0.2626555263996124, acc = 0.91796875\n",
            "Batch 56: loss = 0.34637442231178284, acc = 0.8857421875\n",
            "Batch 57: loss = 0.2789474129676819, acc = 0.904296875\n",
            "Batch 58: loss = 0.3254135251045227, acc = 0.8984375\n",
            "Batch 59: loss = 0.26160719990730286, acc = 0.91796875\n",
            "Batch 60: loss = 0.3038053810596466, acc = 0.900390625\n",
            "Batch 61: loss = 0.2891550064086914, acc = 0.904296875\n",
            "Batch 62: loss = 0.3535231649875641, acc = 0.8759765625\n",
            "Batch 63: loss = 0.30540019273757935, acc = 0.888671875\n",
            "Batch 64: loss = 0.26763197779655457, acc = 0.9130859375\n",
            "Batch 65: loss = 0.3349064886569977, acc = 0.88671875\n",
            "Batch 66: loss = 0.3302907347679138, acc = 0.8857421875\n",
            "Batch 67: loss = 0.3243357539176941, acc = 0.888671875\n",
            "Batch 68: loss = 0.3427470922470093, acc = 0.890625\n",
            "Batch 69: loss = 0.3174767792224884, acc = 0.8984375\n",
            "Batch 70: loss = 0.34502679109573364, acc = 0.8857421875\n",
            "Batch 71: loss = 0.33662885427474976, acc = 0.8837890625\n",
            "Batch 72: loss = 0.3219429552555084, acc = 0.8955078125\n",
            "Batch 73: loss = 0.32613617181777954, acc = 0.8876953125\n",
            "Batch 74: loss = 0.3419474959373474, acc = 0.8857421875\n",
            "Batch 75: loss = 0.3914014995098114, acc = 0.8671875\n",
            "Batch 76: loss = 0.36688363552093506, acc = 0.87109375\n",
            "Batch 77: loss = 0.29545703530311584, acc = 0.8974609375\n",
            "Batch 78: loss = 0.3403319716453552, acc = 0.8779296875\n",
            "Batch 79: loss = 0.322598934173584, acc = 0.8818359375\n",
            "Batch 80: loss = 0.3120344579219818, acc = 0.888671875\n",
            "Batch 81: loss = 0.3318565785884857, acc = 0.8857421875\n",
            "Batch 82: loss = 0.29423463344573975, acc = 0.90625\n",
            "Batch 83: loss = 0.2763902246952057, acc = 0.9091796875\n",
            "Batch 84: loss = 0.322952002286911, acc = 0.9013671875\n",
            "Batch 85: loss = 0.317780464887619, acc = 0.896484375\n",
            "Batch 86: loss = 0.28286096453666687, acc = 0.908203125\n",
            "Batch 87: loss = 0.323386013507843, acc = 0.8974609375\n",
            "Batch 88: loss = 0.33539795875549316, acc = 0.888671875\n",
            "Batch 89: loss = 0.28358927369117737, acc = 0.9091796875\n",
            "Batch 90: loss = 0.3230358958244324, acc = 0.88671875\n",
            "Batch 91: loss = 0.31053581833839417, acc = 0.89453125\n",
            "Batch 92: loss = 0.3477238416671753, acc = 0.890625\n",
            "Batch 93: loss = 0.286885142326355, acc = 0.9052734375\n",
            "Batch 94: loss = 0.30050355195999146, acc = 0.8916015625\n",
            "Batch 95: loss = 0.3185402750968933, acc = 0.8876953125\n",
            "Batch 96: loss = 0.3609802722930908, acc = 0.8837890625\n",
            "Batch 97: loss = 0.3384162485599518, acc = 0.888671875\n",
            "Batch 98: loss = 0.32956019043922424, acc = 0.8798828125\n",
            "Batch 99: loss = 0.3769235908985138, acc = 0.8681640625\n",
            "Batch 100: loss = 0.37113314867019653, acc = 0.87109375\n",
            "Batch 101: loss = 0.3165208101272583, acc = 0.8916015625\n",
            "Batch 102: loss = 0.3141270577907562, acc = 0.9033203125\n",
            "Batch 103: loss = 0.3841603994369507, acc = 0.8701171875\n",
            "Batch 104: loss = 0.31396347284317017, acc = 0.8916015625\n",
            "Batch 105: loss = 0.30077818036079407, acc = 0.892578125\n",
            "Batch 106: loss = 0.3009949326515198, acc = 0.8994140625\n",
            "Batch 107: loss = 0.31105050444602966, acc = 0.8916015625\n",
            "Batch 108: loss = 0.32576262950897217, acc = 0.8818359375\n",
            "Batch 109: loss = 0.301999032497406, acc = 0.904296875\n",
            "Batch 110: loss = 0.32283520698547363, acc = 0.8984375\n",
            "Batch 111: loss = 0.3144323229789734, acc = 0.900390625\n",
            "Batch 112: loss = 0.34244638681411743, acc = 0.884765625\n",
            "Batch 113: loss = 0.3185577988624573, acc = 0.888671875\n",
            "Batch 114: loss = 0.331432580947876, acc = 0.888671875\n",
            "Batch 115: loss = 0.2954237759113312, acc = 0.89453125\n",
            "Batch 116: loss = 0.34178388118743896, acc = 0.87890625\n",
            "Batch 117: loss = 0.30837276577949524, acc = 0.9033203125\n",
            "Batch 118: loss = 0.31022727489471436, acc = 0.90625\n",
            "Batch 119: loss = 0.28608590364456177, acc = 0.9052734375\n",
            "Batch 120: loss = 0.28813841938972473, acc = 0.9033203125\n",
            "Batch 121: loss = 0.338309645652771, acc = 0.884765625\n",
            "Batch 122: loss = 0.293294757604599, acc = 0.9052734375\n",
            "Batch 123: loss = 0.3178326189517975, acc = 0.8828125\n",
            "Batch 124: loss = 0.3449738919734955, acc = 0.876953125\n",
            "Batch 125: loss = 0.3193550407886505, acc = 0.904296875\n",
            "Batch 126: loss = 0.3309968411922455, acc = 0.8896484375\n",
            "\n",
            "Epoch 88/100\n",
            "Batch 1: loss = 0.42202693223953247, acc = 0.8916015625\n",
            "Batch 2: loss = 0.3298293352127075, acc = 0.8857421875\n",
            "Batch 3: loss = 0.3149510622024536, acc = 0.9052734375\n",
            "Batch 4: loss = 0.3098428547382355, acc = 0.904296875\n",
            "Batch 5: loss = 0.31428784132003784, acc = 0.9013671875\n",
            "Batch 6: loss = 0.31051650643348694, acc = 0.900390625\n",
            "Batch 7: loss = 0.31780171394348145, acc = 0.89453125\n",
            "Batch 8: loss = 0.31912970542907715, acc = 0.908203125\n",
            "Batch 9: loss = 0.31289029121398926, acc = 0.9052734375\n",
            "Batch 10: loss = 0.25539693236351013, acc = 0.9130859375\n",
            "Batch 11: loss = 0.3390669524669647, acc = 0.8955078125\n",
            "Batch 12: loss = 0.29327699542045593, acc = 0.8916015625\n",
            "Batch 13: loss = 0.3266526758670807, acc = 0.884765625\n",
            "Batch 14: loss = 0.31662148237228394, acc = 0.904296875\n",
            "Batch 15: loss = 0.26713669300079346, acc = 0.9228515625\n",
            "Batch 16: loss = 0.33842912316322327, acc = 0.8916015625\n",
            "Batch 17: loss = 0.29786282777786255, acc = 0.8994140625\n",
            "Batch 18: loss = 0.3288072645664215, acc = 0.88671875\n",
            "Batch 19: loss = 0.3062790334224701, acc = 0.8984375\n",
            "Batch 20: loss = 0.2967972159385681, acc = 0.9013671875\n",
            "Batch 21: loss = 0.2951175570487976, acc = 0.892578125\n",
            "Batch 22: loss = 0.3067809045314789, acc = 0.884765625\n",
            "Batch 23: loss = 0.2849591374397278, acc = 0.9140625\n",
            "Batch 24: loss = 0.2714568078517914, acc = 0.91015625\n",
            "Batch 25: loss = 0.3080337941646576, acc = 0.9013671875\n",
            "Batch 26: loss = 0.29846808314323425, acc = 0.89453125\n",
            "Batch 27: loss = 0.31054186820983887, acc = 0.8916015625\n",
            "Batch 28: loss = 0.30698612332344055, acc = 0.900390625\n",
            "Batch 29: loss = 0.32024359703063965, acc = 0.8935546875\n",
            "Batch 30: loss = 0.28259214758872986, acc = 0.908203125\n",
            "Batch 31: loss = 0.31441813707351685, acc = 0.8955078125\n",
            "Batch 32: loss = 0.3425259292125702, acc = 0.8818359375\n",
            "Batch 33: loss = 0.29761478304862976, acc = 0.91015625\n",
            "Batch 34: loss = 0.3288748264312744, acc = 0.90234375\n",
            "Batch 35: loss = 0.3161817789077759, acc = 0.8974609375\n",
            "Batch 36: loss = 0.2794873118400574, acc = 0.9052734375\n",
            "Batch 37: loss = 0.2325822412967682, acc = 0.921875\n",
            "Batch 38: loss = 0.2852109670639038, acc = 0.900390625\n",
            "Batch 39: loss = 0.2627754807472229, acc = 0.9130859375\n",
            "Batch 40: loss = 0.2889450192451477, acc = 0.900390625\n",
            "Batch 41: loss = 0.2897864580154419, acc = 0.9052734375\n",
            "Batch 42: loss = 0.3175678551197052, acc = 0.890625\n",
            "Batch 43: loss = 0.3339841067790985, acc = 0.8857421875\n",
            "Batch 44: loss = 0.2495722770690918, acc = 0.919921875\n",
            "Batch 45: loss = 0.27117568254470825, acc = 0.916015625\n",
            "Batch 46: loss = 0.2855967581272125, acc = 0.90234375\n",
            "Batch 47: loss = 0.2766930162906647, acc = 0.9130859375\n",
            "Batch 48: loss = 0.29575636982917786, acc = 0.8984375\n",
            "Batch 49: loss = 0.30395740270614624, acc = 0.9072265625\n",
            "Batch 50: loss = 0.30301859974861145, acc = 0.8955078125\n",
            "Batch 51: loss = 0.292243629693985, acc = 0.9111328125\n",
            "Batch 52: loss = 0.33190852403640747, acc = 0.8798828125\n",
            "Batch 53: loss = 0.30137959122657776, acc = 0.908203125\n",
            "Batch 54: loss = 0.23788747191429138, acc = 0.9140625\n",
            "Batch 55: loss = 0.2562437057495117, acc = 0.9169921875\n",
            "Batch 56: loss = 0.32152578234672546, acc = 0.90234375\n",
            "Batch 57: loss = 0.30469971895217896, acc = 0.8935546875\n",
            "Batch 58: loss = 0.3716105818748474, acc = 0.876953125\n",
            "Batch 59: loss = 0.253275066614151, acc = 0.919921875\n",
            "Batch 60: loss = 0.2923898994922638, acc = 0.9013671875\n",
            "Batch 61: loss = 0.2478005588054657, acc = 0.927734375\n",
            "Batch 62: loss = 0.33500006794929504, acc = 0.8828125\n",
            "Batch 63: loss = 0.2959049940109253, acc = 0.9052734375\n",
            "Batch 64: loss = 0.260489821434021, acc = 0.912109375\n",
            "Batch 65: loss = 0.31130507588386536, acc = 0.8935546875\n",
            "Batch 66: loss = 0.3260365128517151, acc = 0.8916015625\n",
            "Batch 67: loss = 0.2877424955368042, acc = 0.904296875\n",
            "Batch 68: loss = 0.3125675618648529, acc = 0.8896484375\n",
            "Batch 69: loss = 0.3069049119949341, acc = 0.904296875\n",
            "Batch 70: loss = 0.35282644629478455, acc = 0.88671875\n",
            "Batch 71: loss = 0.3239181935787201, acc = 0.89453125\n",
            "Batch 72: loss = 0.3066816031932831, acc = 0.890625\n",
            "Batch 73: loss = 0.3363616466522217, acc = 0.8955078125\n",
            "Batch 74: loss = 0.3462359607219696, acc = 0.87890625\n",
            "Batch 75: loss = 0.37577202916145325, acc = 0.875\n",
            "Batch 76: loss = 0.33045077323913574, acc = 0.8876953125\n",
            "Batch 77: loss = 0.297596275806427, acc = 0.9072265625\n",
            "Batch 78: loss = 0.3178867995738983, acc = 0.8896484375\n",
            "Batch 79: loss = 0.33674922585487366, acc = 0.888671875\n",
            "Batch 80: loss = 0.3087073862552643, acc = 0.8916015625\n",
            "Batch 81: loss = 0.27892038226127625, acc = 0.900390625\n",
            "Batch 82: loss = 0.3105056583881378, acc = 0.8955078125\n",
            "Batch 83: loss = 0.2940157353878021, acc = 0.900390625\n",
            "Batch 84: loss = 0.3593282103538513, acc = 0.876953125\n",
            "Batch 85: loss = 0.31346943974494934, acc = 0.896484375\n",
            "Batch 86: loss = 0.30413171648979187, acc = 0.8935546875\n",
            "Batch 87: loss = 0.31518131494522095, acc = 0.880859375\n",
            "Batch 88: loss = 0.36309370398521423, acc = 0.87109375\n",
            "Batch 89: loss = 0.2908285856246948, acc = 0.90625\n",
            "Batch 90: loss = 0.30067378282546997, acc = 0.8896484375\n",
            "Batch 91: loss = 0.3263751268386841, acc = 0.8896484375\n",
            "Batch 92: loss = 0.3420456647872925, acc = 0.892578125\n",
            "Batch 93: loss = 0.31870949268341064, acc = 0.8916015625\n",
            "Batch 94: loss = 0.29258400201797485, acc = 0.896484375\n",
            "Batch 95: loss = 0.31497684121131897, acc = 0.8896484375\n",
            "Batch 96: loss = 0.3353164792060852, acc = 0.8828125\n",
            "Batch 97: loss = 0.32947292923927307, acc = 0.8955078125\n",
            "Batch 98: loss = 0.3026996850967407, acc = 0.896484375\n",
            "Batch 99: loss = 0.32237398624420166, acc = 0.88671875\n",
            "Batch 100: loss = 0.3597874343395233, acc = 0.87890625\n",
            "Batch 101: loss = 0.3080938756465912, acc = 0.8955078125\n",
            "Batch 102: loss = 0.3260258436203003, acc = 0.8896484375\n",
            "Batch 103: loss = 0.3466170132160187, acc = 0.8720703125\n",
            "Batch 104: loss = 0.27878910303115845, acc = 0.9013671875\n",
            "Batch 105: loss = 0.28864696621894836, acc = 0.900390625\n",
            "Batch 106: loss = 0.32031455636024475, acc = 0.8955078125\n",
            "Batch 107: loss = 0.3333096206188202, acc = 0.87890625\n",
            "Batch 108: loss = 0.29635322093963623, acc = 0.90625\n",
            "Batch 109: loss = 0.295814573764801, acc = 0.8994140625\n",
            "Batch 110: loss = 0.2866940200328827, acc = 0.9013671875\n",
            "Batch 111: loss = 0.3124100863933563, acc = 0.900390625\n",
            "Batch 112: loss = 0.32476815581321716, acc = 0.890625\n",
            "Batch 113: loss = 0.3035599887371063, acc = 0.8984375\n",
            "Batch 114: loss = 0.32462090253829956, acc = 0.904296875\n",
            "Batch 115: loss = 0.29817938804626465, acc = 0.904296875\n",
            "Batch 116: loss = 0.31580138206481934, acc = 0.8984375\n",
            "Batch 117: loss = 0.26529115438461304, acc = 0.9072265625\n",
            "Batch 118: loss = 0.29207372665405273, acc = 0.90625\n",
            "Batch 119: loss = 0.3083633482456207, acc = 0.8994140625\n",
            "Batch 120: loss = 0.2911949157714844, acc = 0.900390625\n",
            "Batch 121: loss = 0.31563323736190796, acc = 0.8974609375\n",
            "Batch 122: loss = 0.31939494609832764, acc = 0.8916015625\n",
            "Batch 123: loss = 0.3234587013721466, acc = 0.8935546875\n",
            "Batch 124: loss = 0.327324241399765, acc = 0.8896484375\n",
            "Batch 125: loss = 0.3550505042076111, acc = 0.880859375\n",
            "Batch 126: loss = 0.31919369101524353, acc = 0.89453125\n",
            "\n",
            "Epoch 89/100\n",
            "Batch 1: loss = 0.466823935508728, acc = 0.865234375\n",
            "Batch 2: loss = 0.3372943103313446, acc = 0.8828125\n",
            "Batch 3: loss = 0.309283047914505, acc = 0.8984375\n",
            "Batch 4: loss = 0.31941837072372437, acc = 0.9013671875\n",
            "Batch 5: loss = 0.3030579090118408, acc = 0.8974609375\n",
            "Batch 6: loss = 0.3490302562713623, acc = 0.8857421875\n",
            "Batch 7: loss = 0.28532809019088745, acc = 0.9033203125\n",
            "Batch 8: loss = 0.2816990613937378, acc = 0.9150390625\n",
            "Batch 9: loss = 0.30820637941360474, acc = 0.90234375\n",
            "Batch 10: loss = 0.27493274211883545, acc = 0.9033203125\n",
            "Batch 11: loss = 0.3249005973339081, acc = 0.8935546875\n",
            "Batch 12: loss = 0.31790095567703247, acc = 0.89453125\n",
            "Batch 13: loss = 0.28403881192207336, acc = 0.90625\n",
            "Batch 14: loss = 0.2666427791118622, acc = 0.904296875\n",
            "Batch 15: loss = 0.30956897139549255, acc = 0.9013671875\n",
            "Batch 16: loss = 0.32479390501976013, acc = 0.892578125\n",
            "Batch 17: loss = 0.3214603066444397, acc = 0.89453125\n",
            "Batch 18: loss = 0.3116094172000885, acc = 0.900390625\n",
            "Batch 19: loss = 0.32601994276046753, acc = 0.890625\n",
            "Batch 20: loss = 0.3268573582172394, acc = 0.8876953125\n",
            "Batch 21: loss = 0.3142375349998474, acc = 0.8984375\n",
            "Batch 22: loss = 0.2914467751979828, acc = 0.9140625\n",
            "Batch 23: loss = 0.3049877882003784, acc = 0.8994140625\n",
            "Batch 24: loss = 0.27239444851875305, acc = 0.9169921875\n",
            "Batch 25: loss = 0.33262899518013, acc = 0.8828125\n",
            "Batch 26: loss = 0.3008808493614197, acc = 0.8994140625\n",
            "Batch 27: loss = 0.32797113060951233, acc = 0.88671875\n",
            "Batch 28: loss = 0.32321697473526, acc = 0.89453125\n",
            "Batch 29: loss = 0.28168389201164246, acc = 0.9111328125\n",
            "Batch 30: loss = 0.3057129681110382, acc = 0.8935546875\n",
            "Batch 31: loss = 0.29927587509155273, acc = 0.90625\n",
            "Batch 32: loss = 0.3394545614719391, acc = 0.8955078125\n",
            "Batch 33: loss = 0.29388049244880676, acc = 0.900390625\n",
            "Batch 34: loss = 0.3177354633808136, acc = 0.90234375\n",
            "Batch 35: loss = 0.3080754578113556, acc = 0.8935546875\n",
            "Batch 36: loss = 0.27055901288986206, acc = 0.9052734375\n",
            "Batch 37: loss = 0.2477068156003952, acc = 0.923828125\n",
            "Batch 38: loss = 0.2730151414871216, acc = 0.9091796875\n",
            "Batch 39: loss = 0.25917020440101624, acc = 0.916015625\n",
            "Batch 40: loss = 0.26816773414611816, acc = 0.9130859375\n",
            "Batch 41: loss = 0.2903926968574524, acc = 0.8974609375\n",
            "Batch 42: loss = 0.2720283269882202, acc = 0.9111328125\n",
            "Batch 43: loss = 0.3510358929634094, acc = 0.8896484375\n",
            "Batch 44: loss = 0.25700345635414124, acc = 0.9150390625\n",
            "Batch 45: loss = 0.2926695644855499, acc = 0.9013671875\n",
            "Batch 46: loss = 0.2754599452018738, acc = 0.9091796875\n",
            "Batch 47: loss = 0.28109443187713623, acc = 0.91015625\n",
            "Batch 48: loss = 0.2825433015823364, acc = 0.9140625\n",
            "Batch 49: loss = 0.2845994830131531, acc = 0.912109375\n",
            "Batch 50: loss = 0.30583927035331726, acc = 0.896484375\n",
            "Batch 51: loss = 0.3095186650753021, acc = 0.8876953125\n",
            "Batch 52: loss = 0.29572969675064087, acc = 0.8955078125\n",
            "Batch 53: loss = 0.2901279628276825, acc = 0.900390625\n",
            "Batch 54: loss = 0.26837921142578125, acc = 0.9072265625\n",
            "Batch 55: loss = 0.24743187427520752, acc = 0.912109375\n",
            "Batch 56: loss = 0.34554290771484375, acc = 0.87890625\n",
            "Batch 57: loss = 0.2844267785549164, acc = 0.904296875\n",
            "Batch 58: loss = 0.36323636770248413, acc = 0.876953125\n",
            "Batch 59: loss = 0.24774757027626038, acc = 0.9150390625\n",
            "Batch 60: loss = 0.293664813041687, acc = 0.8974609375\n",
            "Batch 61: loss = 0.23551848530769348, acc = 0.9267578125\n",
            "Batch 62: loss = 0.3333469331264496, acc = 0.8876953125\n",
            "Batch 63: loss = 0.2838302254676819, acc = 0.900390625\n",
            "Batch 64: loss = 0.2665877938270569, acc = 0.908203125\n",
            "Batch 65: loss = 0.29647547006607056, acc = 0.90234375\n",
            "Batch 66: loss = 0.30522620677948, acc = 0.8984375\n",
            "Batch 67: loss = 0.3037785291671753, acc = 0.8857421875\n",
            "Batch 68: loss = 0.33944299817085266, acc = 0.888671875\n",
            "Batch 69: loss = 0.2854450047016144, acc = 0.9111328125\n",
            "Batch 70: loss = 0.3420678377151489, acc = 0.88671875\n",
            "Batch 71: loss = 0.3254009485244751, acc = 0.8876953125\n",
            "Batch 72: loss = 0.29533839225769043, acc = 0.900390625\n",
            "Batch 73: loss = 0.32799169421195984, acc = 0.8974609375\n",
            "Batch 74: loss = 0.32318419218063354, acc = 0.890625\n",
            "Batch 75: loss = 0.38453495502471924, acc = 0.8818359375\n",
            "Batch 76: loss = 0.31930071115493774, acc = 0.88671875\n",
            "Batch 77: loss = 0.31999391317367554, acc = 0.8935546875\n",
            "Batch 78: loss = 0.33027711510658264, acc = 0.890625\n",
            "Batch 79: loss = 0.31362754106521606, acc = 0.896484375\n",
            "Batch 80: loss = 0.3206510543823242, acc = 0.8779296875\n",
            "Batch 81: loss = 0.3374637961387634, acc = 0.8876953125\n",
            "Batch 82: loss = 0.32991552352905273, acc = 0.900390625\n",
            "Batch 83: loss = 0.30081066489219666, acc = 0.896484375\n",
            "Batch 84: loss = 0.3155810236930847, acc = 0.89453125\n",
            "Batch 85: loss = 0.32093310356140137, acc = 0.8955078125\n",
            "Batch 86: loss = 0.2828991115093231, acc = 0.8984375\n",
            "Batch 87: loss = 0.31798070669174194, acc = 0.892578125\n",
            "Batch 88: loss = 0.3422359526157379, acc = 0.884765625\n",
            "Batch 89: loss = 0.3019714057445526, acc = 0.900390625\n",
            "Batch 90: loss = 0.32833728194236755, acc = 0.8955078125\n",
            "Batch 91: loss = 0.3250572979450226, acc = 0.8896484375\n",
            "Batch 92: loss = 0.3448956608772278, acc = 0.8828125\n",
            "Batch 93: loss = 0.28033700585365295, acc = 0.9072265625\n",
            "Batch 94: loss = 0.29581883549690247, acc = 0.90234375\n",
            "Batch 95: loss = 0.31043222546577454, acc = 0.8994140625\n",
            "Batch 96: loss = 0.34756314754486084, acc = 0.8779296875\n",
            "Batch 97: loss = 0.3085649609565735, acc = 0.9052734375\n",
            "Batch 98: loss = 0.303821861743927, acc = 0.88671875\n",
            "Batch 99: loss = 0.3594251275062561, acc = 0.880859375\n",
            "Batch 100: loss = 0.3485790491104126, acc = 0.875\n",
            "Batch 101: loss = 0.31312325596809387, acc = 0.8935546875\n",
            "Batch 102: loss = 0.29634997248649597, acc = 0.8974609375\n",
            "Batch 103: loss = 0.34666043519973755, acc = 0.89453125\n",
            "Batch 104: loss = 0.27504122257232666, acc = 0.904296875\n",
            "Batch 105: loss = 0.2918354272842407, acc = 0.8916015625\n",
            "Batch 106: loss = 0.29475170373916626, acc = 0.89453125\n",
            "Batch 107: loss = 0.30515673756599426, acc = 0.8857421875\n",
            "Batch 108: loss = 0.31417202949523926, acc = 0.8876953125\n",
            "Batch 109: loss = 0.30692335963249207, acc = 0.900390625\n",
            "Batch 110: loss = 0.3004286587238312, acc = 0.912109375\n",
            "Batch 111: loss = 0.32000911235809326, acc = 0.890625\n",
            "Batch 112: loss = 0.32026711106300354, acc = 0.90234375\n",
            "Batch 113: loss = 0.29615718126296997, acc = 0.8955078125\n",
            "Batch 114: loss = 0.33894211053848267, acc = 0.892578125\n",
            "Batch 115: loss = 0.2940913736820221, acc = 0.8974609375\n",
            "Batch 116: loss = 0.30775564908981323, acc = 0.900390625\n",
            "Batch 117: loss = 0.2845669388771057, acc = 0.89453125\n",
            "Batch 118: loss = 0.29677242040634155, acc = 0.9091796875\n",
            "Batch 119: loss = 0.31594839692115784, acc = 0.888671875\n",
            "Batch 120: loss = 0.287801593542099, acc = 0.8994140625\n",
            "Batch 121: loss = 0.3200373351573944, acc = 0.8955078125\n",
            "Batch 122: loss = 0.30559635162353516, acc = 0.8955078125\n",
            "Batch 123: loss = 0.29657983779907227, acc = 0.89453125\n",
            "Batch 124: loss = 0.32197055220603943, acc = 0.890625\n",
            "Batch 125: loss = 0.3002454340457916, acc = 0.8984375\n",
            "Batch 126: loss = 0.3278728127479553, acc = 0.884765625\n",
            "\n",
            "Epoch 90/100\n",
            "Batch 1: loss = 0.42130786180496216, acc = 0.875\n",
            "Batch 2: loss = 0.34058496356010437, acc = 0.888671875\n",
            "Batch 3: loss = 0.3268783986568451, acc = 0.9013671875\n",
            "Batch 4: loss = 0.3351108431816101, acc = 0.8896484375\n",
            "Batch 5: loss = 0.29384616017341614, acc = 0.904296875\n",
            "Batch 6: loss = 0.3368748128414154, acc = 0.884765625\n",
            "Batch 7: loss = 0.29249557852745056, acc = 0.9052734375\n",
            "Batch 8: loss = 0.29101642966270447, acc = 0.9140625\n",
            "Batch 9: loss = 0.3037591874599457, acc = 0.90234375\n",
            "Batch 10: loss = 0.2579907476902008, acc = 0.9130859375\n",
            "Batch 11: loss = 0.32791876792907715, acc = 0.888671875\n",
            "Batch 12: loss = 0.284037709236145, acc = 0.9013671875\n",
            "Batch 13: loss = 0.3147665560245514, acc = 0.896484375\n",
            "Batch 14: loss = 0.2872469127178192, acc = 0.9072265625\n",
            "Batch 15: loss = 0.26689213514328003, acc = 0.9189453125\n",
            "Batch 16: loss = 0.35088375210762024, acc = 0.8916015625\n",
            "Batch 17: loss = 0.30619096755981445, acc = 0.896484375\n",
            "Batch 18: loss = 0.33342230319976807, acc = 0.892578125\n",
            "Batch 19: loss = 0.2706429362297058, acc = 0.9111328125\n",
            "Batch 20: loss = 0.2792607247829437, acc = 0.9072265625\n",
            "Batch 21: loss = 0.3062189817428589, acc = 0.90234375\n",
            "Batch 22: loss = 0.29969027638435364, acc = 0.904296875\n",
            "Batch 23: loss = 0.3056398034095764, acc = 0.8974609375\n",
            "Batch 24: loss = 0.26810121536254883, acc = 0.9091796875\n",
            "Batch 25: loss = 0.32191839814186096, acc = 0.9013671875\n",
            "Batch 26: loss = 0.28649023175239563, acc = 0.896484375\n",
            "Batch 27: loss = 0.3100166916847229, acc = 0.8955078125\n",
            "Batch 28: loss = 0.31647586822509766, acc = 0.9052734375\n",
            "Batch 29: loss = 0.3125241994857788, acc = 0.890625\n",
            "Batch 30: loss = 0.27479875087738037, acc = 0.904296875\n",
            "Batch 31: loss = 0.29336464405059814, acc = 0.908203125\n",
            "Batch 32: loss = 0.3476426303386688, acc = 0.87890625\n",
            "Batch 33: loss = 0.2953472435474396, acc = 0.9052734375\n",
            "Batch 34: loss = 0.2936473786830902, acc = 0.9130859375\n",
            "Batch 35: loss = 0.29777538776397705, acc = 0.9033203125\n",
            "Batch 36: loss = 0.2571609318256378, acc = 0.916015625\n",
            "Batch 37: loss = 0.2538430094718933, acc = 0.9140625\n",
            "Batch 38: loss = 0.2882236838340759, acc = 0.9052734375\n",
            "Batch 39: loss = 0.26111310720443726, acc = 0.9130859375\n",
            "Batch 40: loss = 0.26641175150871277, acc = 0.91796875\n",
            "Batch 41: loss = 0.2865985929965973, acc = 0.8984375\n",
            "Batch 42: loss = 0.29032814502716064, acc = 0.9072265625\n",
            "Batch 43: loss = 0.3131446838378906, acc = 0.8935546875\n",
            "Batch 44: loss = 0.25685399770736694, acc = 0.91796875\n",
            "Batch 45: loss = 0.27910083532333374, acc = 0.9072265625\n",
            "Batch 46: loss = 0.2643314599990845, acc = 0.9150390625\n",
            "Batch 47: loss = 0.27938222885131836, acc = 0.90625\n",
            "Batch 48: loss = 0.2753411531448364, acc = 0.912109375\n",
            "Batch 49: loss = 0.2822206914424896, acc = 0.9228515625\n",
            "Batch 50: loss = 0.2671908438205719, acc = 0.9130859375\n",
            "Batch 51: loss = 0.3014219403266907, acc = 0.88671875\n",
            "Batch 52: loss = 0.29082465171813965, acc = 0.8974609375\n",
            "Batch 53: loss = 0.29687485098838806, acc = 0.8974609375\n",
            "Batch 54: loss = 0.2565980553627014, acc = 0.9130859375\n",
            "Batch 55: loss = 0.2742576599121094, acc = 0.908203125\n",
            "Batch 56: loss = 0.3255455195903778, acc = 0.8984375\n",
            "Batch 57: loss = 0.320113867521286, acc = 0.8876953125\n",
            "Batch 58: loss = 0.33942100405693054, acc = 0.888671875\n",
            "Batch 59: loss = 0.24861270189285278, acc = 0.9150390625\n",
            "Batch 60: loss = 0.2752985954284668, acc = 0.912109375\n",
            "Batch 61: loss = 0.2452157735824585, acc = 0.921875\n",
            "Batch 62: loss = 0.3405997157096863, acc = 0.8994140625\n",
            "Batch 63: loss = 0.2870085835456848, acc = 0.900390625\n",
            "Batch 64: loss = 0.2546057105064392, acc = 0.91796875\n",
            "Batch 65: loss = 0.2958691716194153, acc = 0.9033203125\n",
            "Batch 66: loss = 0.2972703278064728, acc = 0.900390625\n",
            "Batch 67: loss = 0.29810768365859985, acc = 0.900390625\n",
            "Batch 68: loss = 0.30562764406204224, acc = 0.90234375\n",
            "Batch 69: loss = 0.2912205755710602, acc = 0.9091796875\n",
            "Batch 70: loss = 0.32315465807914734, acc = 0.904296875\n",
            "Batch 71: loss = 0.3079744577407837, acc = 0.8935546875\n",
            "Batch 72: loss = 0.2755306363105774, acc = 0.912109375\n",
            "Batch 73: loss = 0.3075142204761505, acc = 0.896484375\n",
            "Batch 74: loss = 0.30245450139045715, acc = 0.8984375\n",
            "Batch 75: loss = 0.3909369707107544, acc = 0.8798828125\n",
            "Batch 76: loss = 0.33244845271110535, acc = 0.87890625\n",
            "Batch 77: loss = 0.31508100032806396, acc = 0.8984375\n",
            "Batch 78: loss = 0.3490903079509735, acc = 0.87109375\n",
            "Batch 79: loss = 0.3128759264945984, acc = 0.8916015625\n",
            "Batch 80: loss = 0.30414995551109314, acc = 0.8876953125\n",
            "Batch 81: loss = 0.31304067373275757, acc = 0.892578125\n",
            "Batch 82: loss = 0.32286930084228516, acc = 0.8974609375\n",
            "Batch 83: loss = 0.26465904712677, acc = 0.91015625\n",
            "Batch 84: loss = 0.3526676595211029, acc = 0.8818359375\n",
            "Batch 85: loss = 0.28756651282310486, acc = 0.9013671875\n",
            "Batch 86: loss = 0.31167107820510864, acc = 0.8935546875\n",
            "Batch 87: loss = 0.3261760473251343, acc = 0.8935546875\n",
            "Batch 88: loss = 0.3579200804233551, acc = 0.8818359375\n",
            "Batch 89: loss = 0.305914968252182, acc = 0.8994140625\n",
            "Batch 90: loss = 0.29672950506210327, acc = 0.88671875\n",
            "Batch 91: loss = 0.3144519329071045, acc = 0.9013671875\n",
            "Batch 92: loss = 0.294556200504303, acc = 0.9052734375\n",
            "Batch 93: loss = 0.27488723397254944, acc = 0.904296875\n",
            "Batch 94: loss = 0.30966299772262573, acc = 0.8955078125\n",
            "Batch 95: loss = 0.3070128560066223, acc = 0.8935546875\n",
            "Batch 96: loss = 0.34366410970687866, acc = 0.880859375\n",
            "Batch 97: loss = 0.33664342761039734, acc = 0.8818359375\n",
            "Batch 98: loss = 0.2970559000968933, acc = 0.9013671875\n",
            "Batch 99: loss = 0.3269960880279541, acc = 0.8916015625\n",
            "Batch 100: loss = 0.3419705033302307, acc = 0.876953125\n",
            "Batch 101: loss = 0.29125094413757324, acc = 0.900390625\n",
            "Batch 102: loss = 0.33307409286499023, acc = 0.8837890625\n",
            "Batch 103: loss = 0.3274279236793518, acc = 0.8935546875\n",
            "Batch 104: loss = 0.27048441767692566, acc = 0.9140625\n",
            "Batch 105: loss = 0.2628195285797119, acc = 0.9033203125\n",
            "Batch 106: loss = 0.29620760679244995, acc = 0.9033203125\n",
            "Batch 107: loss = 0.2986557185649872, acc = 0.89453125\n",
            "Batch 108: loss = 0.2710978090763092, acc = 0.904296875\n",
            "Batch 109: loss = 0.30194613337516785, acc = 0.8857421875\n",
            "Batch 110: loss = 0.2903509736061096, acc = 0.908203125\n",
            "Batch 111: loss = 0.32101696729660034, acc = 0.884765625\n",
            "Batch 112: loss = 0.30909624695777893, acc = 0.8994140625\n",
            "Batch 113: loss = 0.3090825080871582, acc = 0.896484375\n",
            "Batch 114: loss = 0.3012814223766327, acc = 0.904296875\n",
            "Batch 115: loss = 0.30656614899635315, acc = 0.9052734375\n",
            "Batch 116: loss = 0.32674580812454224, acc = 0.8935546875\n",
            "Batch 117: loss = 0.29068177938461304, acc = 0.908203125\n",
            "Batch 118: loss = 0.26610106229782104, acc = 0.91796875\n",
            "Batch 119: loss = 0.29501622915267944, acc = 0.904296875\n",
            "Batch 120: loss = 0.2850719094276428, acc = 0.912109375\n",
            "Batch 121: loss = 0.31548067927360535, acc = 0.8935546875\n",
            "Batch 122: loss = 0.27059078216552734, acc = 0.919921875\n",
            "Batch 123: loss = 0.310180127620697, acc = 0.8955078125\n",
            "Batch 124: loss = 0.33175918459892273, acc = 0.8779296875\n",
            "Batch 125: loss = 0.29874327778816223, acc = 0.90234375\n",
            "Batch 126: loss = 0.3067159652709961, acc = 0.896484375\n",
            "Saved checkpoint to weights.90.h5\n",
            "\n",
            "Epoch 91/100\n",
            "Batch 1: loss = 0.4433716833591461, acc = 0.8642578125\n",
            "Batch 2: loss = 0.37182900309562683, acc = 0.875\n",
            "Batch 3: loss = 0.32981058955192566, acc = 0.890625\n",
            "Batch 4: loss = 0.3160794973373413, acc = 0.908203125\n",
            "Batch 5: loss = 0.3080443739891052, acc = 0.896484375\n",
            "Batch 6: loss = 0.302827388048172, acc = 0.90234375\n",
            "Batch 7: loss = 0.31738224625587463, acc = 0.8935546875\n",
            "Batch 8: loss = 0.3298817574977875, acc = 0.8974609375\n",
            "Batch 9: loss = 0.30774152278900146, acc = 0.908203125\n",
            "Batch 10: loss = 0.276577353477478, acc = 0.9091796875\n",
            "Batch 11: loss = 0.31623896956443787, acc = 0.9033203125\n",
            "Batch 12: loss = 0.31860601902008057, acc = 0.8876953125\n",
            "Batch 13: loss = 0.30887994170188904, acc = 0.8876953125\n",
            "Batch 14: loss = 0.29516854882240295, acc = 0.9091796875\n",
            "Batch 15: loss = 0.2868196368217468, acc = 0.9033203125\n",
            "Batch 16: loss = 0.3007214069366455, acc = 0.9072265625\n",
            "Batch 17: loss = 0.32070502638816833, acc = 0.89453125\n",
            "Batch 18: loss = 0.34079769253730774, acc = 0.89453125\n",
            "Batch 19: loss = 0.286241352558136, acc = 0.9091796875\n",
            "Batch 20: loss = 0.29548555612564087, acc = 0.904296875\n",
            "Batch 21: loss = 0.2714763879776001, acc = 0.9150390625\n",
            "Batch 22: loss = 0.30478450655937195, acc = 0.9013671875\n",
            "Batch 23: loss = 0.2949765622615814, acc = 0.8974609375\n",
            "Batch 24: loss = 0.2787245810031891, acc = 0.9111328125\n",
            "Batch 25: loss = 0.296880841255188, acc = 0.91015625\n",
            "Batch 26: loss = 0.3004411458969116, acc = 0.89453125\n",
            "Batch 27: loss = 0.3232140839099884, acc = 0.890625\n",
            "Batch 28: loss = 0.309364378452301, acc = 0.8994140625\n",
            "Batch 29: loss = 0.26881644129753113, acc = 0.9111328125\n",
            "Batch 30: loss = 0.2768852412700653, acc = 0.9091796875\n",
            "Batch 31: loss = 0.322561651468277, acc = 0.8955078125\n",
            "Batch 32: loss = 0.34665367007255554, acc = 0.890625\n",
            "Batch 33: loss = 0.2708790898323059, acc = 0.9013671875\n",
            "Batch 34: loss = 0.2831062972545624, acc = 0.90625\n",
            "Batch 35: loss = 0.2986794710159302, acc = 0.916015625\n",
            "Batch 36: loss = 0.2777556777000427, acc = 0.904296875\n",
            "Batch 37: loss = 0.25467953085899353, acc = 0.927734375\n",
            "Batch 38: loss = 0.26088687777519226, acc = 0.9208984375\n",
            "Batch 39: loss = 0.2539670169353485, acc = 0.9130859375\n",
            "Batch 40: loss = 0.2759334444999695, acc = 0.91015625\n",
            "Batch 41: loss = 0.24272119998931885, acc = 0.912109375\n",
            "Batch 42: loss = 0.3076784312725067, acc = 0.89453125\n",
            "Batch 43: loss = 0.31796255707740784, acc = 0.8876953125\n",
            "Batch 44: loss = 0.22651392221450806, acc = 0.93359375\n",
            "Batch 45: loss = 0.2752717435359955, acc = 0.916015625\n",
            "Batch 46: loss = 0.2514089047908783, acc = 0.9150390625\n",
            "Batch 47: loss = 0.2955220937728882, acc = 0.892578125\n",
            "Batch 48: loss = 0.2631170153617859, acc = 0.9130859375\n",
            "Batch 49: loss = 0.2879176139831543, acc = 0.8974609375\n",
            "Batch 50: loss = 0.25663894414901733, acc = 0.9150390625\n",
            "Batch 51: loss = 0.3057582676410675, acc = 0.8974609375\n",
            "Batch 52: loss = 0.26122957468032837, acc = 0.91015625\n",
            "Batch 53: loss = 0.2710166573524475, acc = 0.908203125\n",
            "Batch 54: loss = 0.2387527972459793, acc = 0.91796875\n",
            "Batch 55: loss = 0.2503422200679779, acc = 0.921875\n",
            "Batch 56: loss = 0.29985982179641724, acc = 0.908203125\n",
            "Batch 57: loss = 0.2979956269264221, acc = 0.8974609375\n",
            "Batch 58: loss = 0.298932820558548, acc = 0.9013671875\n",
            "Batch 59: loss = 0.23722459375858307, acc = 0.9306640625\n",
            "Batch 60: loss = 0.2946471571922302, acc = 0.8994140625\n",
            "Batch 61: loss = 0.251657634973526, acc = 0.9208984375\n",
            "Batch 62: loss = 0.35645976662635803, acc = 0.8857421875\n",
            "Batch 63: loss = 0.26108649373054504, acc = 0.9169921875\n",
            "Batch 64: loss = 0.2812431752681732, acc = 0.9150390625\n",
            "Batch 65: loss = 0.2702527940273285, acc = 0.9111328125\n",
            "Batch 66: loss = 0.30588147044181824, acc = 0.9013671875\n",
            "Batch 67: loss = 0.2949398159980774, acc = 0.888671875\n",
            "Batch 68: loss = 0.30785316228866577, acc = 0.8896484375\n",
            "Batch 69: loss = 0.2742846608161926, acc = 0.9033203125\n",
            "Batch 70: loss = 0.3072822391986847, acc = 0.9033203125\n",
            "Batch 71: loss = 0.32203468680381775, acc = 0.89453125\n",
            "Batch 72: loss = 0.27754464745521545, acc = 0.9013671875\n",
            "Batch 73: loss = 0.300040602684021, acc = 0.8994140625\n",
            "Batch 74: loss = 0.29260510206222534, acc = 0.8896484375\n",
            "Batch 75: loss = 0.3720628321170807, acc = 0.87109375\n",
            "Batch 76: loss = 0.3190932273864746, acc = 0.884765625\n",
            "Batch 77: loss = 0.2834832966327667, acc = 0.9072265625\n",
            "Batch 78: loss = 0.32928645610809326, acc = 0.875\n",
            "Batch 79: loss = 0.28935515880584717, acc = 0.8955078125\n",
            "Batch 80: loss = 0.3036966919898987, acc = 0.8916015625\n",
            "Batch 81: loss = 0.3248370885848999, acc = 0.890625\n",
            "Batch 82: loss = 0.31193315982818604, acc = 0.900390625\n",
            "Batch 83: loss = 0.2876262068748474, acc = 0.912109375\n",
            "Batch 84: loss = 0.33203357458114624, acc = 0.8935546875\n",
            "Batch 85: loss = 0.30563604831695557, acc = 0.900390625\n",
            "Batch 86: loss = 0.292062908411026, acc = 0.8984375\n",
            "Batch 87: loss = 0.3166268467903137, acc = 0.8994140625\n",
            "Batch 88: loss = 0.37108859419822693, acc = 0.8818359375\n",
            "Batch 89: loss = 0.25975874066352844, acc = 0.919921875\n",
            "Batch 90: loss = 0.30525681376457214, acc = 0.892578125\n",
            "Batch 91: loss = 0.30194857716560364, acc = 0.896484375\n",
            "Batch 92: loss = 0.31636959314346313, acc = 0.8984375\n",
            "Batch 93: loss = 0.300014466047287, acc = 0.9072265625\n",
            "Batch 94: loss = 0.30552321672439575, acc = 0.900390625\n",
            "Batch 95: loss = 0.31078875064849854, acc = 0.896484375\n",
            "Batch 96: loss = 0.34168684482574463, acc = 0.8916015625\n",
            "Batch 97: loss = 0.2853683829307556, acc = 0.9091796875\n",
            "Batch 98: loss = 0.2687552571296692, acc = 0.9130859375\n",
            "Batch 99: loss = 0.32921063899993896, acc = 0.8916015625\n",
            "Batch 100: loss = 0.3458389341831207, acc = 0.8779296875\n",
            "Batch 101: loss = 0.31733113527297974, acc = 0.8818359375\n",
            "Batch 102: loss = 0.32223087549209595, acc = 0.892578125\n",
            "Batch 103: loss = 0.32656756043434143, acc = 0.9013671875\n",
            "Batch 104: loss = 0.26589882373809814, acc = 0.888671875\n",
            "Batch 105: loss = 0.28176259994506836, acc = 0.904296875\n",
            "Batch 106: loss = 0.3284146785736084, acc = 0.8955078125\n",
            "Batch 107: loss = 0.3140193223953247, acc = 0.892578125\n",
            "Batch 108: loss = 0.3204368054866791, acc = 0.8876953125\n",
            "Batch 109: loss = 0.3124057352542877, acc = 0.8984375\n",
            "Batch 110: loss = 0.2973251938819885, acc = 0.9013671875\n",
            "Batch 111: loss = 0.335304856300354, acc = 0.890625\n",
            "Batch 112: loss = 0.3143908679485321, acc = 0.8955078125\n",
            "Batch 113: loss = 0.2989957332611084, acc = 0.904296875\n",
            "Batch 114: loss = 0.3050247132778168, acc = 0.8984375\n",
            "Batch 115: loss = 0.31600093841552734, acc = 0.8876953125\n",
            "Batch 116: loss = 0.329688161611557, acc = 0.884765625\n",
            "Batch 117: loss = 0.28190985321998596, acc = 0.8994140625\n",
            "Batch 118: loss = 0.27953097224235535, acc = 0.9033203125\n",
            "Batch 119: loss = 0.2692186236381531, acc = 0.9033203125\n",
            "Batch 120: loss = 0.28583839535713196, acc = 0.9130859375\n",
            "Batch 121: loss = 0.2893142104148865, acc = 0.8974609375\n",
            "Batch 122: loss = 0.3077288866043091, acc = 0.89453125\n",
            "Batch 123: loss = 0.32023143768310547, acc = 0.8896484375\n",
            "Batch 124: loss = 0.32474789023399353, acc = 0.890625\n",
            "Batch 125: loss = 0.2791202664375305, acc = 0.9140625\n",
            "Batch 126: loss = 0.3079415559768677, acc = 0.8955078125\n",
            "\n",
            "Epoch 92/100\n",
            "Batch 1: loss = 0.43404459953308105, acc = 0.8701171875\n",
            "Batch 2: loss = 0.33969032764434814, acc = 0.8876953125\n",
            "Batch 3: loss = 0.2986446022987366, acc = 0.90234375\n",
            "Batch 4: loss = 0.28566494584083557, acc = 0.916015625\n",
            "Batch 5: loss = 0.3039591908454895, acc = 0.8984375\n",
            "Batch 6: loss = 0.29921799898147583, acc = 0.8974609375\n",
            "Batch 7: loss = 0.2881179451942444, acc = 0.91015625\n",
            "Batch 8: loss = 0.29946500062942505, acc = 0.90625\n",
            "Batch 9: loss = 0.2936488389968872, acc = 0.900390625\n",
            "Batch 10: loss = 0.24313874542713165, acc = 0.9111328125\n",
            "Batch 11: loss = 0.28096118569374084, acc = 0.904296875\n",
            "Batch 12: loss = 0.2910000681877136, acc = 0.90234375\n",
            "Batch 13: loss = 0.2716997563838959, acc = 0.90234375\n",
            "Batch 14: loss = 0.31070077419281006, acc = 0.896484375\n",
            "Batch 15: loss = 0.2811988592147827, acc = 0.912109375\n",
            "Batch 16: loss = 0.3361974060535431, acc = 0.89453125\n",
            "Batch 17: loss = 0.3036896288394928, acc = 0.9033203125\n",
            "Batch 18: loss = 0.34609565138816833, acc = 0.88671875\n",
            "Batch 19: loss = 0.3061104416847229, acc = 0.8935546875\n",
            "Batch 20: loss = 0.31168434023857117, acc = 0.884765625\n",
            "Batch 21: loss = 0.30473414063453674, acc = 0.8984375\n",
            "Batch 22: loss = 0.31550827622413635, acc = 0.8955078125\n",
            "Batch 23: loss = 0.3123161792755127, acc = 0.8994140625\n",
            "Batch 24: loss = 0.28170275688171387, acc = 0.9052734375\n",
            "Batch 25: loss = 0.32573479413986206, acc = 0.888671875\n",
            "Batch 26: loss = 0.28582996129989624, acc = 0.9072265625\n",
            "Batch 27: loss = 0.28072240948677063, acc = 0.9111328125\n",
            "Batch 28: loss = 0.2917760908603668, acc = 0.9111328125\n",
            "Batch 29: loss = 0.3083740472793579, acc = 0.8984375\n",
            "Batch 30: loss = 0.2882571220397949, acc = 0.90625\n",
            "Batch 31: loss = 0.3029515743255615, acc = 0.90234375\n",
            "Batch 32: loss = 0.3082909882068634, acc = 0.896484375\n",
            "Batch 33: loss = 0.28767573833465576, acc = 0.90234375\n",
            "Batch 34: loss = 0.29860225319862366, acc = 0.90234375\n",
            "Batch 35: loss = 0.27839863300323486, acc = 0.9140625\n",
            "Batch 36: loss = 0.27661752700805664, acc = 0.9130859375\n",
            "Batch 37: loss = 0.2574273347854614, acc = 0.92578125\n",
            "Batch 38: loss = 0.2521936595439911, acc = 0.9130859375\n",
            "Batch 39: loss = 0.24621085822582245, acc = 0.9228515625\n",
            "Batch 40: loss = 0.2760341167449951, acc = 0.9052734375\n",
            "Batch 41: loss = 0.30272743105888367, acc = 0.8935546875\n",
            "Batch 42: loss = 0.2946975529193878, acc = 0.90234375\n",
            "Batch 43: loss = 0.34440720081329346, acc = 0.8828125\n",
            "Batch 44: loss = 0.25766125321388245, acc = 0.9150390625\n",
            "Batch 45: loss = 0.2681336998939514, acc = 0.9189453125\n",
            "Batch 46: loss = 0.26612067222595215, acc = 0.9130859375\n",
            "Batch 47: loss = 0.2786480486392975, acc = 0.916015625\n",
            "Batch 48: loss = 0.2810112237930298, acc = 0.9013671875\n",
            "Batch 49: loss = 0.2628275752067566, acc = 0.919921875\n",
            "Batch 50: loss = 0.2736261487007141, acc = 0.908203125\n",
            "Batch 51: loss = 0.28189560770988464, acc = 0.8994140625\n",
            "Batch 52: loss = 0.28143802285194397, acc = 0.908203125\n",
            "Batch 53: loss = 0.2739126980304718, acc = 0.912109375\n",
            "Batch 54: loss = 0.24840736389160156, acc = 0.9228515625\n",
            "Batch 55: loss = 0.2329253852367401, acc = 0.9248046875\n",
            "Batch 56: loss = 0.3104937970638275, acc = 0.8916015625\n",
            "Batch 57: loss = 0.29339635372161865, acc = 0.8984375\n",
            "Batch 58: loss = 0.31857866048812866, acc = 0.8935546875\n",
            "Batch 59: loss = 0.22807833552360535, acc = 0.923828125\n",
            "Batch 60: loss = 0.29144594073295593, acc = 0.900390625\n",
            "Batch 61: loss = 0.2535642385482788, acc = 0.9248046875\n",
            "Batch 62: loss = 0.32759737968444824, acc = 0.896484375\n",
            "Batch 63: loss = 0.25085321068763733, acc = 0.921875\n",
            "Batch 64: loss = 0.2790848910808563, acc = 0.916015625\n",
            "Batch 65: loss = 0.30627018213272095, acc = 0.9033203125\n",
            "Batch 66: loss = 0.30218881368637085, acc = 0.90234375\n",
            "Batch 67: loss = 0.27948522567749023, acc = 0.9091796875\n",
            "Batch 68: loss = 0.3265742063522339, acc = 0.8916015625\n",
            "Batch 69: loss = 0.2817418873310089, acc = 0.9111328125\n",
            "Batch 70: loss = 0.3162277936935425, acc = 0.900390625\n",
            "Batch 71: loss = 0.32283928990364075, acc = 0.8896484375\n",
            "Batch 72: loss = 0.2886795997619629, acc = 0.9013671875\n",
            "Batch 73: loss = 0.3039712607860565, acc = 0.90234375\n",
            "Batch 74: loss = 0.30792051553726196, acc = 0.896484375\n",
            "Batch 75: loss = 0.3720510005950928, acc = 0.873046875\n",
            "Batch 76: loss = 0.3125587999820709, acc = 0.8837890625\n",
            "Batch 77: loss = 0.2948201894760132, acc = 0.9033203125\n",
            "Batch 78: loss = 0.30017298460006714, acc = 0.89453125\n",
            "Batch 79: loss = 0.30080926418304443, acc = 0.89453125\n",
            "Batch 80: loss = 0.3090364933013916, acc = 0.892578125\n",
            "Batch 81: loss = 0.3009414076805115, acc = 0.90234375\n",
            "Batch 82: loss = 0.29522886872291565, acc = 0.9091796875\n",
            "Batch 83: loss = 0.27920353412628174, acc = 0.9052734375\n",
            "Batch 84: loss = 0.32505810260772705, acc = 0.88671875\n",
            "Batch 85: loss = 0.3027544319629669, acc = 0.8984375\n",
            "Batch 86: loss = 0.2844194173812866, acc = 0.91015625\n",
            "Batch 87: loss = 0.3001098036766052, acc = 0.904296875\n",
            "Batch 88: loss = 0.30598536133766174, acc = 0.89453125\n",
            "Batch 89: loss = 0.29969969391822815, acc = 0.90234375\n",
            "Batch 90: loss = 0.30879735946655273, acc = 0.8994140625\n",
            "Batch 91: loss = 0.3172546327114105, acc = 0.9033203125\n",
            "Batch 92: loss = 0.3314765691757202, acc = 0.892578125\n",
            "Batch 93: loss = 0.2580185830593109, acc = 0.9140625\n",
            "Batch 94: loss = 0.2766363024711609, acc = 0.9111328125\n",
            "Batch 95: loss = 0.29129159450531006, acc = 0.89453125\n",
            "Batch 96: loss = 0.33032792806625366, acc = 0.8798828125\n",
            "Batch 97: loss = 0.3094435930252075, acc = 0.90625\n",
            "Batch 98: loss = 0.2946018874645233, acc = 0.908203125\n",
            "Batch 99: loss = 0.3396623432636261, acc = 0.892578125\n",
            "Batch 100: loss = 0.3207584321498871, acc = 0.8837890625\n",
            "Batch 101: loss = 0.29470840096473694, acc = 0.900390625\n",
            "Batch 102: loss = 0.35418716073036194, acc = 0.8720703125\n",
            "Batch 103: loss = 0.31764307618141174, acc = 0.9033203125\n",
            "Batch 104: loss = 0.27806612849235535, acc = 0.9013671875\n",
            "Batch 105: loss = 0.27706673741340637, acc = 0.8994140625\n",
            "Batch 106: loss = 0.3309047222137451, acc = 0.8828125\n",
            "Batch 107: loss = 0.3325875997543335, acc = 0.8876953125\n",
            "Batch 108: loss = 0.2998615503311157, acc = 0.9052734375\n",
            "Batch 109: loss = 0.2979234755039215, acc = 0.896484375\n",
            "Batch 110: loss = 0.2743757963180542, acc = 0.908203125\n",
            "Batch 111: loss = 0.3392401933670044, acc = 0.8837890625\n",
            "Batch 112: loss = 0.3186032176017761, acc = 0.8994140625\n",
            "Batch 113: loss = 0.30367159843444824, acc = 0.9013671875\n",
            "Batch 114: loss = 0.3189046382904053, acc = 0.8994140625\n",
            "Batch 115: loss = 0.3050695061683655, acc = 0.9052734375\n",
            "Batch 116: loss = 0.30757027864456177, acc = 0.904296875\n",
            "Batch 117: loss = 0.27908855676651, acc = 0.9208984375\n",
            "Batch 118: loss = 0.3019966781139374, acc = 0.9033203125\n",
            "Batch 119: loss = 0.289678692817688, acc = 0.9013671875\n",
            "Batch 120: loss = 0.28902068734169006, acc = 0.9130859375\n",
            "Batch 121: loss = 0.2715683579444885, acc = 0.90234375\n",
            "Batch 122: loss = 0.25857028365135193, acc = 0.90625\n",
            "Batch 123: loss = 0.32705727219581604, acc = 0.8896484375\n",
            "Batch 124: loss = 0.3008558750152588, acc = 0.900390625\n",
            "Batch 125: loss = 0.28545674681663513, acc = 0.9091796875\n",
            "Batch 126: loss = 0.3195696771144867, acc = 0.896484375\n",
            "\n",
            "Epoch 93/100\n",
            "Batch 1: loss = 0.39481228590011597, acc = 0.88671875\n",
            "Batch 2: loss = 0.35790830850601196, acc = 0.8759765625\n",
            "Batch 3: loss = 0.3169853091239929, acc = 0.896484375\n",
            "Batch 4: loss = 0.2830008864402771, acc = 0.9140625\n",
            "Batch 5: loss = 0.3118292987346649, acc = 0.8984375\n",
            "Batch 6: loss = 0.31616201996803284, acc = 0.908203125\n",
            "Batch 7: loss = 0.27324244379997253, acc = 0.90625\n",
            "Batch 8: loss = 0.2979314625263214, acc = 0.90625\n",
            "Batch 9: loss = 0.3048154413700104, acc = 0.90234375\n",
            "Batch 10: loss = 0.2398293912410736, acc = 0.9150390625\n",
            "Batch 11: loss = 0.30827099084854126, acc = 0.89453125\n",
            "Batch 12: loss = 0.26733237504959106, acc = 0.9130859375\n",
            "Batch 13: loss = 0.2967250943183899, acc = 0.892578125\n",
            "Batch 14: loss = 0.26917383074760437, acc = 0.9169921875\n",
            "Batch 15: loss = 0.2751423418521881, acc = 0.912109375\n",
            "Batch 16: loss = 0.3259753882884979, acc = 0.8994140625\n",
            "Batch 17: loss = 0.28411993384361267, acc = 0.9169921875\n",
            "Batch 18: loss = 0.33323654532432556, acc = 0.89453125\n",
            "Batch 19: loss = 0.27273356914520264, acc = 0.9140625\n",
            "Batch 20: loss = 0.2842973470687866, acc = 0.8974609375\n",
            "Batch 21: loss = 0.2890802025794983, acc = 0.916015625\n",
            "Batch 22: loss = 0.28995853662490845, acc = 0.90234375\n",
            "Batch 23: loss = 0.29122257232666016, acc = 0.90234375\n",
            "Batch 24: loss = 0.26280876994132996, acc = 0.9072265625\n",
            "Batch 25: loss = 0.32983940839767456, acc = 0.888671875\n",
            "Batch 26: loss = 0.2927871346473694, acc = 0.8994140625\n",
            "Batch 27: loss = 0.2780457139015198, acc = 0.9013671875\n",
            "Batch 28: loss = 0.3029339909553528, acc = 0.8994140625\n",
            "Batch 29: loss = 0.28990310430526733, acc = 0.8994140625\n",
            "Batch 30: loss = 0.27859994769096375, acc = 0.91015625\n",
            "Batch 31: loss = 0.29930615425109863, acc = 0.90234375\n",
            "Batch 32: loss = 0.3539462685585022, acc = 0.8935546875\n",
            "Batch 33: loss = 0.3089551329612732, acc = 0.90625\n",
            "Batch 34: loss = 0.3114451467990875, acc = 0.900390625\n",
            "Batch 35: loss = 0.2774033844470978, acc = 0.9072265625\n",
            "Batch 36: loss = 0.27334192395210266, acc = 0.91015625\n",
            "Batch 37: loss = 0.26387956738471985, acc = 0.919921875\n",
            "Batch 38: loss = 0.2596856951713562, acc = 0.9208984375\n",
            "Batch 39: loss = 0.24530228972434998, acc = 0.921875\n",
            "Batch 40: loss = 0.268541157245636, acc = 0.91015625\n",
            "Batch 41: loss = 0.26604318618774414, acc = 0.9033203125\n",
            "Batch 42: loss = 0.29497188329696655, acc = 0.9072265625\n",
            "Batch 43: loss = 0.2997940480709076, acc = 0.9013671875\n",
            "Batch 44: loss = 0.2206430733203888, acc = 0.927734375\n",
            "Batch 45: loss = 0.272095650434494, acc = 0.9111328125\n",
            "Batch 46: loss = 0.2531212568283081, acc = 0.9150390625\n",
            "Batch 47: loss = 0.299127459526062, acc = 0.900390625\n",
            "Batch 48: loss = 0.25876253843307495, acc = 0.91796875\n",
            "Batch 49: loss = 0.25215139985084534, acc = 0.919921875\n",
            "Batch 50: loss = 0.270384281873703, acc = 0.912109375\n",
            "Batch 51: loss = 0.2698599696159363, acc = 0.908203125\n",
            "Batch 52: loss = 0.31380927562713623, acc = 0.8857421875\n",
            "Batch 53: loss = 0.33613401651382446, acc = 0.888671875\n",
            "Batch 54: loss = 0.2378801703453064, acc = 0.9150390625\n",
            "Batch 55: loss = 0.24669712781906128, acc = 0.923828125\n",
            "Batch 56: loss = 0.32443851232528687, acc = 0.8837890625\n",
            "Batch 57: loss = 0.27996912598609924, acc = 0.9091796875\n",
            "Batch 58: loss = 0.3091721534729004, acc = 0.8955078125\n",
            "Batch 59: loss = 0.23200173676013947, acc = 0.9248046875\n",
            "Batch 60: loss = 0.27321040630340576, acc = 0.9140625\n",
            "Batch 61: loss = 0.25157204270362854, acc = 0.9169921875\n",
            "Batch 62: loss = 0.32948794960975647, acc = 0.8935546875\n",
            "Batch 63: loss = 0.2639392614364624, acc = 0.9111328125\n",
            "Batch 64: loss = 0.2555435001850128, acc = 0.9208984375\n",
            "Batch 65: loss = 0.2715488076210022, acc = 0.9208984375\n",
            "Batch 66: loss = 0.2976166605949402, acc = 0.90625\n",
            "Batch 67: loss = 0.2662244737148285, acc = 0.916015625\n",
            "Batch 68: loss = 0.3135138154029846, acc = 0.892578125\n",
            "Batch 69: loss = 0.2922169268131256, acc = 0.9111328125\n",
            "Batch 70: loss = 0.32190537452697754, acc = 0.9013671875\n",
            "Batch 71: loss = 0.30003055930137634, acc = 0.90234375\n",
            "Batch 72: loss = 0.30379801988601685, acc = 0.9033203125\n",
            "Batch 73: loss = 0.309137225151062, acc = 0.90234375\n",
            "Batch 74: loss = 0.33044058084487915, acc = 0.8798828125\n",
            "Batch 75: loss = 0.3662285804748535, acc = 0.8828125\n",
            "Batch 76: loss = 0.31419721245765686, acc = 0.880859375\n",
            "Batch 77: loss = 0.28088170289993286, acc = 0.9052734375\n",
            "Batch 78: loss = 0.31723469495773315, acc = 0.890625\n",
            "Batch 79: loss = 0.3147622346878052, acc = 0.896484375\n",
            "Batch 80: loss = 0.28037229180336, acc = 0.90625\n",
            "Batch 81: loss = 0.29979461431503296, acc = 0.9033203125\n",
            "Batch 82: loss = 0.3001134693622589, acc = 0.8994140625\n",
            "Batch 83: loss = 0.2622981667518616, acc = 0.9130859375\n",
            "Batch 84: loss = 0.3338260054588318, acc = 0.890625\n",
            "Batch 85: loss = 0.2935093343257904, acc = 0.9072265625\n",
            "Batch 86: loss = 0.2831297814846039, acc = 0.9091796875\n",
            "Batch 87: loss = 0.31330281496047974, acc = 0.8955078125\n",
            "Batch 88: loss = 0.2847995162010193, acc = 0.900390625\n",
            "Batch 89: loss = 0.2810181975364685, acc = 0.9091796875\n",
            "Batch 90: loss = 0.30949753522872925, acc = 0.8935546875\n",
            "Batch 91: loss = 0.33451351523399353, acc = 0.8779296875\n",
            "Batch 92: loss = 0.3159177303314209, acc = 0.89453125\n",
            "Batch 93: loss = 0.2777402997016907, acc = 0.9033203125\n",
            "Batch 94: loss = 0.3129001557826996, acc = 0.8994140625\n",
            "Batch 95: loss = 0.2910303473472595, acc = 0.9013671875\n",
            "Batch 96: loss = 0.3186118006706238, acc = 0.890625\n",
            "Batch 97: loss = 0.2945002317428589, acc = 0.904296875\n",
            "Batch 98: loss = 0.288618803024292, acc = 0.8984375\n",
            "Batch 99: loss = 0.30416637659072876, acc = 0.904296875\n",
            "Batch 100: loss = 0.290101021528244, acc = 0.892578125\n",
            "Batch 101: loss = 0.2953014671802521, acc = 0.892578125\n",
            "Batch 102: loss = 0.33808809518814087, acc = 0.896484375\n",
            "Batch 103: loss = 0.3108079135417938, acc = 0.8984375\n",
            "Batch 104: loss = 0.24962085485458374, acc = 0.9111328125\n",
            "Batch 105: loss = 0.2544393241405487, acc = 0.9169921875\n",
            "Batch 106: loss = 0.2926987111568451, acc = 0.90234375\n",
            "Batch 107: loss = 0.27808159589767456, acc = 0.9111328125\n",
            "Batch 108: loss = 0.2860693037509918, acc = 0.90625\n",
            "Batch 109: loss = 0.27861660718917847, acc = 0.900390625\n",
            "Batch 110: loss = 0.264373779296875, acc = 0.91015625\n",
            "Batch 111: loss = 0.2971422076225281, acc = 0.896484375\n",
            "Batch 112: loss = 0.33229807019233704, acc = 0.9013671875\n",
            "Batch 113: loss = 0.28261733055114746, acc = 0.9033203125\n",
            "Batch 114: loss = 0.3278408348560333, acc = 0.896484375\n",
            "Batch 115: loss = 0.2975909411907196, acc = 0.900390625\n",
            "Batch 116: loss = 0.29987725615501404, acc = 0.900390625\n",
            "Batch 117: loss = 0.27688363194465637, acc = 0.904296875\n",
            "Batch 118: loss = 0.23712016642093658, acc = 0.9296875\n",
            "Batch 119: loss = 0.2733669579029083, acc = 0.9033203125\n",
            "Batch 120: loss = 0.27266746759414673, acc = 0.9072265625\n",
            "Batch 121: loss = 0.28489798307418823, acc = 0.8994140625\n",
            "Batch 122: loss = 0.2569766044616699, acc = 0.9169921875\n",
            "Batch 123: loss = 0.27626490592956543, acc = 0.9091796875\n",
            "Batch 124: loss = 0.3142929673194885, acc = 0.8916015625\n",
            "Batch 125: loss = 0.2668392062187195, acc = 0.9140625\n",
            "Batch 126: loss = 0.3202628195285797, acc = 0.8935546875\n",
            "\n",
            "Epoch 94/100\n",
            "Batch 1: loss = 0.4174000918865204, acc = 0.8876953125\n",
            "Batch 2: loss = 0.32892847061157227, acc = 0.8837890625\n",
            "Batch 3: loss = 0.29590386152267456, acc = 0.908203125\n",
            "Batch 4: loss = 0.293984055519104, acc = 0.90625\n",
            "Batch 5: loss = 0.29276174306869507, acc = 0.9052734375\n",
            "Batch 6: loss = 0.31255632638931274, acc = 0.8984375\n",
            "Batch 7: loss = 0.2857156991958618, acc = 0.9052734375\n",
            "Batch 8: loss = 0.32612428069114685, acc = 0.892578125\n",
            "Batch 9: loss = 0.3061051666736603, acc = 0.888671875\n",
            "Batch 10: loss = 0.2461453229188919, acc = 0.9189453125\n",
            "Batch 11: loss = 0.32936719059944153, acc = 0.890625\n",
            "Batch 12: loss = 0.2723131775856018, acc = 0.91015625\n",
            "Batch 13: loss = 0.28547143936157227, acc = 0.90625\n",
            "Batch 14: loss = 0.3060888648033142, acc = 0.8994140625\n",
            "Batch 15: loss = 0.2719682455062866, acc = 0.916015625\n",
            "Batch 16: loss = 0.2855606973171234, acc = 0.9052734375\n",
            "Batch 17: loss = 0.32704755663871765, acc = 0.8974609375\n",
            "Batch 18: loss = 0.31341028213500977, acc = 0.9033203125\n",
            "Batch 19: loss = 0.2786385715007782, acc = 0.8974609375\n",
            "Batch 20: loss = 0.29432356357574463, acc = 0.8984375\n",
            "Batch 21: loss = 0.290641725063324, acc = 0.8984375\n",
            "Batch 22: loss = 0.3175661861896515, acc = 0.888671875\n",
            "Batch 23: loss = 0.28809598088264465, acc = 0.904296875\n",
            "Batch 24: loss = 0.2667681872844696, acc = 0.9052734375\n",
            "Batch 25: loss = 0.3000984787940979, acc = 0.900390625\n",
            "Batch 26: loss = 0.2633973956108093, acc = 0.8994140625\n",
            "Batch 27: loss = 0.2945072054862976, acc = 0.9052734375\n",
            "Batch 28: loss = 0.31080707907676697, acc = 0.8994140625\n",
            "Batch 29: loss = 0.2835686206817627, acc = 0.904296875\n",
            "Batch 30: loss = 0.2845271825790405, acc = 0.8974609375\n",
            "Batch 31: loss = 0.2965152859687805, acc = 0.91015625\n",
            "Batch 32: loss = 0.34349340200424194, acc = 0.875\n",
            "Batch 33: loss = 0.29341691732406616, acc = 0.90625\n",
            "Batch 34: loss = 0.3362114429473877, acc = 0.890625\n",
            "Batch 35: loss = 0.314630925655365, acc = 0.8984375\n",
            "Batch 36: loss = 0.2463163435459137, acc = 0.9208984375\n",
            "Batch 37: loss = 0.253531813621521, acc = 0.9326171875\n",
            "Batch 38: loss = 0.2632819414138794, acc = 0.908203125\n",
            "Batch 39: loss = 0.2676340639591217, acc = 0.9130859375\n",
            "Batch 40: loss = 0.2642057240009308, acc = 0.9169921875\n",
            "Batch 41: loss = 0.2586170434951782, acc = 0.91015625\n",
            "Batch 42: loss = 0.2887459695339203, acc = 0.89453125\n",
            "Batch 43: loss = 0.3565782606601715, acc = 0.8828125\n",
            "Batch 44: loss = 0.22287002205848694, acc = 0.923828125\n",
            "Batch 45: loss = 0.2596599757671356, acc = 0.912109375\n",
            "Batch 46: loss = 0.26118993759155273, acc = 0.900390625\n",
            "Batch 47: loss = 0.2958044111728668, acc = 0.90234375\n",
            "Batch 48: loss = 0.26212918758392334, acc = 0.9169921875\n",
            "Batch 49: loss = 0.273764431476593, acc = 0.9248046875\n",
            "Batch 50: loss = 0.2671009302139282, acc = 0.9130859375\n",
            "Batch 51: loss = 0.29365018010139465, acc = 0.8974609375\n",
            "Batch 52: loss = 0.2675984501838684, acc = 0.9140625\n",
            "Batch 53: loss = 0.2718617916107178, acc = 0.90625\n",
            "Batch 54: loss = 0.22581949830055237, acc = 0.91796875\n",
            "Batch 55: loss = 0.22688578069210052, acc = 0.9326171875\n",
            "Batch 56: loss = 0.32114169001579285, acc = 0.8916015625\n",
            "Batch 57: loss = 0.3121737837791443, acc = 0.892578125\n",
            "Batch 58: loss = 0.32075729966163635, acc = 0.8837890625\n",
            "Batch 59: loss = 0.22529147565364838, acc = 0.927734375\n",
            "Batch 60: loss = 0.2784716486930847, acc = 0.908203125\n",
            "Batch 61: loss = 0.253648966550827, acc = 0.921875\n",
            "Batch 62: loss = 0.28909602761268616, acc = 0.9033203125\n",
            "Batch 63: loss = 0.27655282616615295, acc = 0.912109375\n",
            "Batch 64: loss = 0.24285142123699188, acc = 0.921875\n",
            "Batch 65: loss = 0.28002941608428955, acc = 0.9130859375\n",
            "Batch 66: loss = 0.28997451066970825, acc = 0.9033203125\n",
            "Batch 67: loss = 0.2695268988609314, acc = 0.9033203125\n",
            "Batch 68: loss = 0.2811056077480316, acc = 0.90234375\n",
            "Batch 69: loss = 0.2951190769672394, acc = 0.9052734375\n",
            "Batch 70: loss = 0.3198608160018921, acc = 0.8955078125\n",
            "Batch 71: loss = 0.3006414771080017, acc = 0.890625\n",
            "Batch 72: loss = 0.2837810218334198, acc = 0.9111328125\n",
            "Batch 73: loss = 0.3068907558917999, acc = 0.9033203125\n",
            "Batch 74: loss = 0.31509795784950256, acc = 0.8974609375\n",
            "Batch 75: loss = 0.3358650803565979, acc = 0.87890625\n",
            "Batch 76: loss = 0.3290559947490692, acc = 0.8876953125\n",
            "Batch 77: loss = 0.31082475185394287, acc = 0.890625\n",
            "Batch 78: loss = 0.30446526408195496, acc = 0.8916015625\n",
            "Batch 79: loss = 0.33017870783805847, acc = 0.880859375\n",
            "Batch 80: loss = 0.287392258644104, acc = 0.90625\n",
            "Batch 81: loss = 0.27817317843437195, acc = 0.90625\n",
            "Batch 82: loss = 0.30035915970802307, acc = 0.9072265625\n",
            "Batch 83: loss = 0.2753385305404663, acc = 0.9111328125\n",
            "Batch 84: loss = 0.3014167547225952, acc = 0.904296875\n",
            "Batch 85: loss = 0.31483447551727295, acc = 0.8916015625\n",
            "Batch 86: loss = 0.2825724184513092, acc = 0.9052734375\n",
            "Batch 87: loss = 0.3004513084888458, acc = 0.9013671875\n",
            "Batch 88: loss = 0.3264544904232025, acc = 0.8876953125\n",
            "Batch 89: loss = 0.25877171754837036, acc = 0.9169921875\n",
            "Batch 90: loss = 0.29058364033699036, acc = 0.8984375\n",
            "Batch 91: loss = 0.31213411688804626, acc = 0.896484375\n",
            "Batch 92: loss = 0.30420881509780884, acc = 0.900390625\n",
            "Batch 93: loss = 0.25081002712249756, acc = 0.919921875\n",
            "Batch 94: loss = 0.3092152774333954, acc = 0.896484375\n",
            "Batch 95: loss = 0.2973349690437317, acc = 0.900390625\n",
            "Batch 96: loss = 0.3208845853805542, acc = 0.8935546875\n",
            "Batch 97: loss = 0.3026512861251831, acc = 0.9150390625\n",
            "Batch 98: loss = 0.3122612237930298, acc = 0.9013671875\n",
            "Batch 99: loss = 0.3216434717178345, acc = 0.8984375\n",
            "Batch 100: loss = 0.35375407338142395, acc = 0.875\n",
            "Batch 101: loss = 0.2981036901473999, acc = 0.90625\n",
            "Batch 102: loss = 0.3106081485748291, acc = 0.8955078125\n",
            "Batch 103: loss = 0.3046529293060303, acc = 0.90625\n",
            "Batch 104: loss = 0.2668614685535431, acc = 0.91015625\n",
            "Batch 105: loss = 0.25860464572906494, acc = 0.9072265625\n",
            "Batch 106: loss = 0.296217679977417, acc = 0.8935546875\n",
            "Batch 107: loss = 0.3007511794567108, acc = 0.88671875\n",
            "Batch 108: loss = 0.31567686796188354, acc = 0.896484375\n",
            "Batch 109: loss = 0.2824982702732086, acc = 0.9013671875\n",
            "Batch 110: loss = 0.27325230836868286, acc = 0.9052734375\n",
            "Batch 111: loss = 0.34174081683158875, acc = 0.87890625\n",
            "Batch 112: loss = 0.31698811054229736, acc = 0.8896484375\n",
            "Batch 113: loss = 0.31215065717697144, acc = 0.8984375\n",
            "Batch 114: loss = 0.30673521757125854, acc = 0.900390625\n",
            "Batch 115: loss = 0.2882143259048462, acc = 0.912109375\n",
            "Batch 116: loss = 0.35701319575309753, acc = 0.8876953125\n",
            "Batch 117: loss = 0.27889564633369446, acc = 0.908203125\n",
            "Batch 118: loss = 0.2896231710910797, acc = 0.90234375\n",
            "Batch 119: loss = 0.2896757423877716, acc = 0.8994140625\n",
            "Batch 120: loss = 0.2699642777442932, acc = 0.919921875\n",
            "Batch 121: loss = 0.3136036992073059, acc = 0.8974609375\n",
            "Batch 122: loss = 0.2923613488674164, acc = 0.9052734375\n",
            "Batch 123: loss = 0.3187602758407593, acc = 0.8955078125\n",
            "Batch 124: loss = 0.3188197612762451, acc = 0.890625\n",
            "Batch 125: loss = 0.308072566986084, acc = 0.8876953125\n",
            "Batch 126: loss = 0.30216994881629944, acc = 0.8955078125\n",
            "\n",
            "Epoch 95/100\n",
            "Batch 1: loss = 0.41540414094924927, acc = 0.8818359375\n",
            "Batch 2: loss = 0.3134441077709198, acc = 0.892578125\n",
            "Batch 3: loss = 0.3072327673435211, acc = 0.8974609375\n",
            "Batch 4: loss = 0.28344738483428955, acc = 0.904296875\n",
            "Batch 5: loss = 0.2965860366821289, acc = 0.9052734375\n",
            "Batch 6: loss = 0.30751579999923706, acc = 0.900390625\n",
            "Batch 7: loss = 0.30192697048187256, acc = 0.890625\n",
            "Batch 8: loss = 0.311710387468338, acc = 0.9052734375\n",
            "Batch 9: loss = 0.2792433500289917, acc = 0.9150390625\n",
            "Batch 10: loss = 0.25362762808799744, acc = 0.90625\n",
            "Batch 11: loss = 0.3080434203147888, acc = 0.900390625\n",
            "Batch 12: loss = 0.267589807510376, acc = 0.9130859375\n",
            "Batch 13: loss = 0.27449649572372437, acc = 0.904296875\n",
            "Batch 14: loss = 0.30655592679977417, acc = 0.90234375\n",
            "Batch 15: loss = 0.25320810079574585, acc = 0.9189453125\n",
            "Batch 16: loss = 0.3069073259830475, acc = 0.896484375\n",
            "Batch 17: loss = 0.2944158911705017, acc = 0.9033203125\n",
            "Batch 18: loss = 0.35302430391311646, acc = 0.8857421875\n",
            "Batch 19: loss = 0.2694272994995117, acc = 0.9130859375\n",
            "Batch 20: loss = 0.3037990927696228, acc = 0.8974609375\n",
            "Batch 21: loss = 0.2905522882938385, acc = 0.8974609375\n",
            "Batch 22: loss = 0.2885255813598633, acc = 0.9052734375\n",
            "Batch 23: loss = 0.2857931852340698, acc = 0.90625\n",
            "Batch 24: loss = 0.2667161226272583, acc = 0.916015625\n",
            "Batch 25: loss = 0.3458450436592102, acc = 0.8916015625\n",
            "Batch 26: loss = 0.25187012553215027, acc = 0.91796875\n",
            "Batch 27: loss = 0.29911187291145325, acc = 0.90234375\n",
            "Batch 28: loss = 0.27832740545272827, acc = 0.9091796875\n",
            "Batch 29: loss = 0.3132084608078003, acc = 0.8857421875\n",
            "Batch 30: loss = 0.3057156503200531, acc = 0.8896484375\n",
            "Batch 31: loss = 0.3194935619831085, acc = 0.896484375\n",
            "Batch 32: loss = 0.32119253277778625, acc = 0.896484375\n",
            "Batch 33: loss = 0.3057195842266083, acc = 0.8994140625\n",
            "Batch 34: loss = 0.27854999899864197, acc = 0.912109375\n",
            "Batch 35: loss = 0.2618500292301178, acc = 0.9150390625\n",
            "Batch 36: loss = 0.25680145621299744, acc = 0.9189453125\n",
            "Batch 37: loss = 0.25087931752204895, acc = 0.9189453125\n",
            "Batch 38: loss = 0.2540484666824341, acc = 0.919921875\n",
            "Batch 39: loss = 0.24622775614261627, acc = 0.923828125\n",
            "Batch 40: loss = 0.2555328905582428, acc = 0.9130859375\n",
            "Batch 41: loss = 0.2623504102230072, acc = 0.9091796875\n",
            "Batch 42: loss = 0.294023722410202, acc = 0.90625\n",
            "Batch 43: loss = 0.31199485063552856, acc = 0.88671875\n",
            "Batch 44: loss = 0.2518307864665985, acc = 0.9111328125\n",
            "Batch 45: loss = 0.2583392858505249, acc = 0.912109375\n",
            "Batch 46: loss = 0.24682669341564178, acc = 0.91796875\n",
            "Batch 47: loss = 0.2827950417995453, acc = 0.908203125\n",
            "Batch 48: loss = 0.28686460852622986, acc = 0.904296875\n",
            "Batch 49: loss = 0.28026264905929565, acc = 0.908203125\n",
            "Batch 50: loss = 0.25697144865989685, acc = 0.921875\n",
            "Batch 51: loss = 0.28847670555114746, acc = 0.8984375\n",
            "Batch 52: loss = 0.27768415212631226, acc = 0.9013671875\n",
            "Batch 53: loss = 0.2973271906375885, acc = 0.9033203125\n",
            "Batch 54: loss = 0.21979352831840515, acc = 0.927734375\n",
            "Batch 55: loss = 0.2548663020133972, acc = 0.9111328125\n",
            "Batch 56: loss = 0.33751335740089417, acc = 0.884765625\n",
            "Batch 57: loss = 0.28564247488975525, acc = 0.904296875\n",
            "Batch 58: loss = 0.3285996913909912, acc = 0.892578125\n",
            "Batch 59: loss = 0.23351553082466125, acc = 0.9208984375\n",
            "Batch 60: loss = 0.2758828103542328, acc = 0.9052734375\n",
            "Batch 61: loss = 0.24410131573677063, acc = 0.931640625\n",
            "Batch 62: loss = 0.3212331235408783, acc = 0.8876953125\n",
            "Batch 63: loss = 0.27896371483802795, acc = 0.904296875\n",
            "Batch 64: loss = 0.25814491510391235, acc = 0.9189453125\n",
            "Batch 65: loss = 0.31332677602767944, acc = 0.9033203125\n",
            "Batch 66: loss = 0.2971421480178833, acc = 0.9013671875\n",
            "Batch 67: loss = 0.2832691967487335, acc = 0.896484375\n",
            "Batch 68: loss = 0.28680962324142456, acc = 0.9130859375\n",
            "Batch 69: loss = 0.29328227043151855, acc = 0.8984375\n",
            "Batch 70: loss = 0.3135223686695099, acc = 0.9033203125\n",
            "Batch 71: loss = 0.2878999710083008, acc = 0.908203125\n",
            "Batch 72: loss = 0.2725919485092163, acc = 0.9150390625\n",
            "Batch 73: loss = 0.31186509132385254, acc = 0.890625\n",
            "Batch 74: loss = 0.3249123990535736, acc = 0.8828125\n",
            "Batch 75: loss = 0.36880382895469666, acc = 0.8837890625\n",
            "Batch 76: loss = 0.32913562655448914, acc = 0.875\n",
            "Batch 77: loss = 0.28770992159843445, acc = 0.9091796875\n",
            "Batch 78: loss = 0.29755035042762756, acc = 0.9013671875\n",
            "Batch 79: loss = 0.30821219086647034, acc = 0.904296875\n",
            "Batch 80: loss = 0.3086540400981903, acc = 0.8896484375\n",
            "Batch 81: loss = 0.2923123240470886, acc = 0.89453125\n",
            "Batch 82: loss = 0.2933664321899414, acc = 0.9033203125\n",
            "Batch 83: loss = 0.27607133984565735, acc = 0.900390625\n",
            "Batch 84: loss = 0.3096603453159332, acc = 0.8935546875\n",
            "Batch 85: loss = 0.30910593271255493, acc = 0.8935546875\n",
            "Batch 86: loss = 0.2970712184906006, acc = 0.89453125\n",
            "Batch 87: loss = 0.31733018159866333, acc = 0.896484375\n",
            "Batch 88: loss = 0.3610827326774597, acc = 0.8876953125\n",
            "Batch 89: loss = 0.28471776843070984, acc = 0.90625\n",
            "Batch 90: loss = 0.3016480505466461, acc = 0.892578125\n",
            "Batch 91: loss = 0.2978072166442871, acc = 0.8994140625\n",
            "Batch 92: loss = 0.3228989839553833, acc = 0.8984375\n",
            "Batch 93: loss = 0.2777354121208191, acc = 0.9111328125\n",
            "Batch 94: loss = 0.3196408152580261, acc = 0.890625\n",
            "Batch 95: loss = 0.3246782124042511, acc = 0.890625\n",
            "Batch 96: loss = 0.3066766560077667, acc = 0.8994140625\n",
            "Batch 97: loss = 0.27174970507621765, acc = 0.9140625\n",
            "Batch 98: loss = 0.2697390019893646, acc = 0.9111328125\n",
            "Batch 99: loss = 0.3182966709136963, acc = 0.90234375\n",
            "Batch 100: loss = 0.3431890904903412, acc = 0.8798828125\n",
            "Batch 101: loss = 0.2952268123626709, acc = 0.9033203125\n",
            "Batch 102: loss = 0.3283501863479614, acc = 0.8837890625\n",
            "Batch 103: loss = 0.31691214442253113, acc = 0.896484375\n",
            "Batch 104: loss = 0.27250173687934875, acc = 0.904296875\n",
            "Batch 105: loss = 0.2627008259296417, acc = 0.904296875\n",
            "Batch 106: loss = 0.2697022557258606, acc = 0.9140625\n",
            "Batch 107: loss = 0.3056623339653015, acc = 0.8984375\n",
            "Batch 108: loss = 0.30777549743652344, acc = 0.896484375\n",
            "Batch 109: loss = 0.3109850287437439, acc = 0.88671875\n",
            "Batch 110: loss = 0.30034884810447693, acc = 0.888671875\n",
            "Batch 111: loss = 0.32260608673095703, acc = 0.888671875\n",
            "Batch 112: loss = 0.30036404728889465, acc = 0.8876953125\n",
            "Batch 113: loss = 0.29813864827156067, acc = 0.9033203125\n",
            "Batch 114: loss = 0.3128935992717743, acc = 0.896484375\n",
            "Batch 115: loss = 0.3070417642593384, acc = 0.900390625\n",
            "Batch 116: loss = 0.30128222703933716, acc = 0.900390625\n",
            "Batch 117: loss = 0.31705185770988464, acc = 0.896484375\n",
            "Batch 118: loss = 0.28349006175994873, acc = 0.908203125\n",
            "Batch 119: loss = 0.30983805656433105, acc = 0.8857421875\n",
            "Batch 120: loss = 0.2838459610939026, acc = 0.9033203125\n",
            "Batch 121: loss = 0.289956271648407, acc = 0.8955078125\n",
            "Batch 122: loss = 0.2632601261138916, acc = 0.9208984375\n",
            "Batch 123: loss = 0.28122323751449585, acc = 0.900390625\n",
            "Batch 124: loss = 0.3000549077987671, acc = 0.8955078125\n",
            "Batch 125: loss = 0.29354235529899597, acc = 0.9091796875\n",
            "Batch 126: loss = 0.30542829632759094, acc = 0.90234375\n",
            "\n",
            "Epoch 96/100\n",
            "Batch 1: loss = 0.39066240191459656, acc = 0.8935546875\n",
            "Batch 2: loss = 0.32032719254493713, acc = 0.9033203125\n",
            "Batch 3: loss = 0.30817896127700806, acc = 0.9111328125\n",
            "Batch 4: loss = 0.3017892837524414, acc = 0.8984375\n",
            "Batch 5: loss = 0.289386510848999, acc = 0.91796875\n",
            "Batch 6: loss = 0.31284934282302856, acc = 0.8974609375\n",
            "Batch 7: loss = 0.2788113057613373, acc = 0.912109375\n",
            "Batch 8: loss = 0.3037247359752655, acc = 0.912109375\n",
            "Batch 9: loss = 0.28602737188339233, acc = 0.908203125\n",
            "Batch 10: loss = 0.2539672553539276, acc = 0.9169921875\n",
            "Batch 11: loss = 0.33146142959594727, acc = 0.896484375\n",
            "Batch 12: loss = 0.2780294120311737, acc = 0.8994140625\n",
            "Batch 13: loss = 0.30466049909591675, acc = 0.8974609375\n",
            "Batch 14: loss = 0.26687806844711304, acc = 0.9208984375\n",
            "Batch 15: loss = 0.25507235527038574, acc = 0.91796875\n",
            "Batch 16: loss = 0.28760117292404175, acc = 0.9033203125\n",
            "Batch 17: loss = 0.30739933252334595, acc = 0.90234375\n",
            "Batch 18: loss = 0.31648027896881104, acc = 0.89453125\n",
            "Batch 19: loss = 0.2752995789051056, acc = 0.90234375\n",
            "Batch 20: loss = 0.2813940942287445, acc = 0.8935546875\n",
            "Batch 21: loss = 0.3174147605895996, acc = 0.8828125\n",
            "Batch 22: loss = 0.3005010485649109, acc = 0.90625\n",
            "Batch 23: loss = 0.26072371006011963, acc = 0.90625\n",
            "Batch 24: loss = 0.2651696801185608, acc = 0.912109375\n",
            "Batch 25: loss = 0.28689026832580566, acc = 0.912109375\n",
            "Batch 26: loss = 0.27269411087036133, acc = 0.912109375\n",
            "Batch 27: loss = 0.30745571851730347, acc = 0.8935546875\n",
            "Batch 28: loss = 0.29461419582366943, acc = 0.90234375\n",
            "Batch 29: loss = 0.26924893260002136, acc = 0.9033203125\n",
            "Batch 30: loss = 0.2849098742008209, acc = 0.9169921875\n",
            "Batch 31: loss = 0.3007658123970032, acc = 0.90625\n",
            "Batch 32: loss = 0.3398442566394806, acc = 0.8857421875\n",
            "Batch 33: loss = 0.29865553975105286, acc = 0.90625\n",
            "Batch 34: loss = 0.2603924572467804, acc = 0.9189453125\n",
            "Batch 35: loss = 0.2939818501472473, acc = 0.8955078125\n",
            "Batch 36: loss = 0.2565111815929413, acc = 0.92578125\n",
            "Batch 37: loss = 0.25481095910072327, acc = 0.91796875\n",
            "Batch 38: loss = 0.24489030241966248, acc = 0.9228515625\n",
            "Batch 39: loss = 0.24572724103927612, acc = 0.9228515625\n",
            "Batch 40: loss = 0.2610052227973938, acc = 0.9169921875\n",
            "Batch 41: loss = 0.24307847023010254, acc = 0.9130859375\n",
            "Batch 42: loss = 0.29731452465057373, acc = 0.88671875\n",
            "Batch 43: loss = 0.3151663541793823, acc = 0.8984375\n",
            "Batch 44: loss = 0.22821345925331116, acc = 0.9189453125\n",
            "Batch 45: loss = 0.26571765542030334, acc = 0.9130859375\n",
            "Batch 46: loss = 0.2334272414445877, acc = 0.919921875\n",
            "Batch 47: loss = 0.2772279679775238, acc = 0.9150390625\n",
            "Batch 48: loss = 0.26753097772598267, acc = 0.9140625\n",
            "Batch 49: loss = 0.26515600085258484, acc = 0.9189453125\n",
            "Batch 50: loss = 0.26829656958580017, acc = 0.904296875\n",
            "Batch 51: loss = 0.27220094203948975, acc = 0.9052734375\n",
            "Batch 52: loss = 0.2779657542705536, acc = 0.8984375\n",
            "Batch 53: loss = 0.2649742364883423, acc = 0.90234375\n",
            "Batch 54: loss = 0.22696658968925476, acc = 0.92578125\n",
            "Batch 55: loss = 0.23463217914104462, acc = 0.9287109375\n",
            "Batch 56: loss = 0.3306690454483032, acc = 0.8857421875\n",
            "Batch 57: loss = 0.281807005405426, acc = 0.9052734375\n",
            "Batch 58: loss = 0.31767958402633667, acc = 0.8994140625\n",
            "Batch 59: loss = 0.24280047416687012, acc = 0.9228515625\n",
            "Batch 60: loss = 0.2858941853046417, acc = 0.91015625\n",
            "Batch 61: loss = 0.2272009700536728, acc = 0.9345703125\n",
            "Batch 62: loss = 0.32168495655059814, acc = 0.904296875\n",
            "Batch 63: loss = 0.270348459482193, acc = 0.919921875\n",
            "Batch 64: loss = 0.25290149450302124, acc = 0.9150390625\n",
            "Batch 65: loss = 0.27693164348602295, acc = 0.908203125\n",
            "Batch 66: loss = 0.301145076751709, acc = 0.88671875\n",
            "Batch 67: loss = 0.2689337730407715, acc = 0.912109375\n",
            "Batch 68: loss = 0.272583544254303, acc = 0.9072265625\n",
            "Batch 69: loss = 0.259321004152298, acc = 0.919921875\n",
            "Batch 70: loss = 0.3208884000778198, acc = 0.892578125\n",
            "Batch 71: loss = 0.30583930015563965, acc = 0.89453125\n",
            "Batch 72: loss = 0.27316814661026, acc = 0.9091796875\n",
            "Batch 73: loss = 0.28246891498565674, acc = 0.9013671875\n",
            "Batch 74: loss = 0.3313596844673157, acc = 0.8798828125\n",
            "Batch 75: loss = 0.3274836242198944, acc = 0.8896484375\n",
            "Batch 76: loss = 0.3305477201938629, acc = 0.888671875\n",
            "Batch 77: loss = 0.28407803177833557, acc = 0.908203125\n",
            "Batch 78: loss = 0.3075316548347473, acc = 0.896484375\n",
            "Batch 79: loss = 0.31005412340164185, acc = 0.8896484375\n",
            "Batch 80: loss = 0.276474267244339, acc = 0.900390625\n",
            "Batch 81: loss = 0.2870612144470215, acc = 0.8974609375\n",
            "Batch 82: loss = 0.27568405866622925, acc = 0.90234375\n",
            "Batch 83: loss = 0.26654353737831116, acc = 0.9072265625\n",
            "Batch 84: loss = 0.30801844596862793, acc = 0.900390625\n",
            "Batch 85: loss = 0.27876707911491394, acc = 0.90625\n",
            "Batch 86: loss = 0.27213266491889954, acc = 0.9091796875\n",
            "Batch 87: loss = 0.2852948307991028, acc = 0.90234375\n",
            "Batch 88: loss = 0.3358970582485199, acc = 0.8837890625\n",
            "Batch 89: loss = 0.24681934714317322, acc = 0.9228515625\n",
            "Batch 90: loss = 0.29564616084098816, acc = 0.8916015625\n",
            "Batch 91: loss = 0.3221885561943054, acc = 0.892578125\n",
            "Batch 92: loss = 0.31016063690185547, acc = 0.8955078125\n",
            "Batch 93: loss = 0.2859327793121338, acc = 0.9033203125\n",
            "Batch 94: loss = 0.27979210019111633, acc = 0.904296875\n",
            "Batch 95: loss = 0.3071255683898926, acc = 0.890625\n",
            "Batch 96: loss = 0.2895415425300598, acc = 0.89453125\n",
            "Batch 97: loss = 0.26427459716796875, acc = 0.9208984375\n",
            "Batch 98: loss = 0.29092293977737427, acc = 0.8955078125\n",
            "Batch 99: loss = 0.33245140314102173, acc = 0.8818359375\n",
            "Batch 100: loss = 0.3154353201389313, acc = 0.88671875\n",
            "Batch 101: loss = 0.2805122137069702, acc = 0.9150390625\n",
            "Batch 102: loss = 0.3235521912574768, acc = 0.890625\n",
            "Batch 103: loss = 0.31600767374038696, acc = 0.8955078125\n",
            "Batch 104: loss = 0.25638437271118164, acc = 0.9111328125\n",
            "Batch 105: loss = 0.24568533897399902, acc = 0.9189453125\n",
            "Batch 106: loss = 0.2621970772743225, acc = 0.908203125\n",
            "Batch 107: loss = 0.29047664999961853, acc = 0.8984375\n",
            "Batch 108: loss = 0.2822403609752655, acc = 0.9130859375\n",
            "Batch 109: loss = 0.2901316285133362, acc = 0.904296875\n",
            "Batch 110: loss = 0.27051252126693726, acc = 0.9111328125\n",
            "Batch 111: loss = 0.33853134512901306, acc = 0.88671875\n",
            "Batch 112: loss = 0.32928362488746643, acc = 0.8818359375\n",
            "Batch 113: loss = 0.32965150475502014, acc = 0.888671875\n",
            "Batch 114: loss = 0.29970359802246094, acc = 0.90234375\n",
            "Batch 115: loss = 0.27014437317848206, acc = 0.92578125\n",
            "Batch 116: loss = 0.3023173213005066, acc = 0.90234375\n",
            "Batch 117: loss = 0.2944534718990326, acc = 0.9072265625\n",
            "Batch 118: loss = 0.2847353219985962, acc = 0.900390625\n",
            "Batch 119: loss = 0.2781878411769867, acc = 0.896484375\n",
            "Batch 120: loss = 0.28014233708381653, acc = 0.908203125\n",
            "Batch 121: loss = 0.298080712556839, acc = 0.8984375\n",
            "Batch 122: loss = 0.26560771465301514, acc = 0.9140625\n",
            "Batch 123: loss = 0.2661573886871338, acc = 0.9169921875\n",
            "Batch 124: loss = 0.2933124601840973, acc = 0.896484375\n",
            "Batch 125: loss = 0.3120657801628113, acc = 0.8955078125\n",
            "Batch 126: loss = 0.30851656198501587, acc = 0.8935546875\n",
            "\n",
            "Epoch 97/100\n",
            "Batch 1: loss = 0.38825392723083496, acc = 0.8916015625\n",
            "Batch 2: loss = 0.2974872291088104, acc = 0.90625\n",
            "Batch 3: loss = 0.33822253346443176, acc = 0.888671875\n",
            "Batch 4: loss = 0.3203011751174927, acc = 0.8916015625\n",
            "Batch 5: loss = 0.2704252600669861, acc = 0.9091796875\n",
            "Batch 6: loss = 0.30908986926078796, acc = 0.892578125\n",
            "Batch 7: loss = 0.27583426237106323, acc = 0.908203125\n",
            "Batch 8: loss = 0.294483482837677, acc = 0.892578125\n",
            "Batch 9: loss = 0.28320470452308655, acc = 0.9091796875\n",
            "Batch 10: loss = 0.24686479568481445, acc = 0.91015625\n",
            "Batch 11: loss = 0.2897445559501648, acc = 0.9072265625\n",
            "Batch 12: loss = 0.2818726897239685, acc = 0.896484375\n",
            "Batch 13: loss = 0.28317344188690186, acc = 0.9111328125\n",
            "Batch 14: loss = 0.2679908871650696, acc = 0.9150390625\n",
            "Batch 15: loss = 0.2667240500450134, acc = 0.9228515625\n",
            "Batch 16: loss = 0.3022296130657196, acc = 0.8984375\n",
            "Batch 17: loss = 0.2850332260131836, acc = 0.9033203125\n",
            "Batch 18: loss = 0.32876765727996826, acc = 0.8876953125\n",
            "Batch 19: loss = 0.31002646684646606, acc = 0.90234375\n",
            "Batch 20: loss = 0.3054259717464447, acc = 0.88671875\n",
            "Batch 21: loss = 0.3040134012699127, acc = 0.9033203125\n",
            "Batch 22: loss = 0.3132506310939789, acc = 0.888671875\n",
            "Batch 23: loss = 0.2883252799510956, acc = 0.8994140625\n",
            "Batch 24: loss = 0.266393780708313, acc = 0.9052734375\n",
            "Batch 25: loss = 0.31327274441719055, acc = 0.8974609375\n",
            "Batch 26: loss = 0.25497931241989136, acc = 0.912109375\n",
            "Batch 27: loss = 0.33073076605796814, acc = 0.8837890625\n",
            "Batch 28: loss = 0.31003710627555847, acc = 0.9072265625\n",
            "Batch 29: loss = 0.2908594310283661, acc = 0.9013671875\n",
            "Batch 30: loss = 0.27858859300613403, acc = 0.89453125\n",
            "Batch 31: loss = 0.2980584502220154, acc = 0.9013671875\n",
            "Batch 32: loss = 0.31169208884239197, acc = 0.8984375\n",
            "Batch 33: loss = 0.27535390853881836, acc = 0.912109375\n",
            "Batch 34: loss = 0.30008766055107117, acc = 0.9111328125\n",
            "Batch 35: loss = 0.2865345776081085, acc = 0.9033203125\n",
            "Batch 36: loss = 0.26070064306259155, acc = 0.9150390625\n",
            "Batch 37: loss = 0.24515998363494873, acc = 0.9326171875\n",
            "Batch 38: loss = 0.24715830385684967, acc = 0.919921875\n",
            "Batch 39: loss = 0.2518085241317749, acc = 0.91796875\n",
            "Batch 40: loss = 0.2792176902294159, acc = 0.912109375\n",
            "Batch 41: loss = 0.25652390718460083, acc = 0.9052734375\n",
            "Batch 42: loss = 0.3178480267524719, acc = 0.8974609375\n",
            "Batch 43: loss = 0.35357680916786194, acc = 0.888671875\n",
            "Batch 44: loss = 0.2582239508628845, acc = 0.9072265625\n",
            "Batch 45: loss = 0.26662677526474, acc = 0.9140625\n",
            "Batch 46: loss = 0.2579795718193054, acc = 0.9150390625\n",
            "Batch 47: loss = 0.27290722727775574, acc = 0.9140625\n",
            "Batch 48: loss = 0.2590138912200928, acc = 0.916015625\n",
            "Batch 49: loss = 0.2673565149307251, acc = 0.9228515625\n",
            "Batch 50: loss = 0.270271360874176, acc = 0.91015625\n",
            "Batch 51: loss = 0.31228023767471313, acc = 0.9091796875\n",
            "Batch 52: loss = 0.2742631137371063, acc = 0.91015625\n",
            "Batch 53: loss = 0.26595446467399597, acc = 0.9150390625\n",
            "Batch 54: loss = 0.24284568428993225, acc = 0.9306640625\n",
            "Batch 55: loss = 0.24744150042533875, acc = 0.9248046875\n",
            "Batch 56: loss = 0.2930394113063812, acc = 0.900390625\n",
            "Batch 57: loss = 0.28801512718200684, acc = 0.8876953125\n",
            "Batch 58: loss = 0.35132044553756714, acc = 0.8837890625\n",
            "Batch 59: loss = 0.22948530316352844, acc = 0.9208984375\n",
            "Batch 60: loss = 0.2825832962989807, acc = 0.9130859375\n",
            "Batch 61: loss = 0.24161860346794128, acc = 0.9287109375\n",
            "Batch 62: loss = 0.2788672149181366, acc = 0.9248046875\n",
            "Batch 63: loss = 0.2595541179180145, acc = 0.9130859375\n",
            "Batch 64: loss = 0.26509347558021545, acc = 0.91015625\n",
            "Batch 65: loss = 0.2735200524330139, acc = 0.91015625\n",
            "Batch 66: loss = 0.2607778012752533, acc = 0.9130859375\n",
            "Batch 67: loss = 0.28292587399482727, acc = 0.904296875\n",
            "Batch 68: loss = 0.28818538784980774, acc = 0.900390625\n",
            "Batch 69: loss = 0.25491446256637573, acc = 0.9208984375\n",
            "Batch 70: loss = 0.31100571155548096, acc = 0.8994140625\n",
            "Batch 71: loss = 0.3094260096549988, acc = 0.8984375\n",
            "Batch 72: loss = 0.27332761883735657, acc = 0.9091796875\n",
            "Batch 73: loss = 0.29895949363708496, acc = 0.89453125\n",
            "Batch 74: loss = 0.32533764839172363, acc = 0.892578125\n",
            "Batch 75: loss = 0.3435138165950775, acc = 0.880859375\n",
            "Batch 76: loss = 0.29322943091392517, acc = 0.88671875\n",
            "Batch 77: loss = 0.28725698590278625, acc = 0.9052734375\n",
            "Batch 78: loss = 0.31750139594078064, acc = 0.9013671875\n",
            "Batch 79: loss = 0.3477739691734314, acc = 0.8818359375\n",
            "Batch 80: loss = 0.2870336174964905, acc = 0.90625\n",
            "Batch 81: loss = 0.2929566502571106, acc = 0.8955078125\n",
            "Batch 82: loss = 0.2933187186717987, acc = 0.9033203125\n",
            "Batch 83: loss = 0.2668724060058594, acc = 0.90625\n",
            "Batch 84: loss = 0.2936568558216095, acc = 0.8955078125\n",
            "Batch 85: loss = 0.3250173032283783, acc = 0.88671875\n",
            "Batch 86: loss = 0.27714601159095764, acc = 0.912109375\n",
            "Batch 87: loss = 0.3235454261302948, acc = 0.8955078125\n",
            "Batch 88: loss = 0.308117538690567, acc = 0.9013671875\n",
            "Batch 89: loss = 0.2438284158706665, acc = 0.921875\n",
            "Batch 90: loss = 0.28662624955177307, acc = 0.9033203125\n",
            "Batch 91: loss = 0.32393836975097656, acc = 0.8876953125\n",
            "Batch 92: loss = 0.3281223177909851, acc = 0.8837890625\n",
            "Batch 93: loss = 0.2724573612213135, acc = 0.91015625\n",
            "Batch 94: loss = 0.2775270342826843, acc = 0.9169921875\n",
            "Batch 95: loss = 0.2982654571533203, acc = 0.89453125\n",
            "Batch 96: loss = 0.3070808947086334, acc = 0.896484375\n",
            "Batch 97: loss = 0.2956410348415375, acc = 0.9013671875\n",
            "Batch 98: loss = 0.2738621234893799, acc = 0.9130859375\n",
            "Batch 99: loss = 0.3124932646751404, acc = 0.896484375\n",
            "Batch 100: loss = 0.32906338572502136, acc = 0.8916015625\n",
            "Batch 101: loss = 0.26752981543540955, acc = 0.9140625\n",
            "Batch 102: loss = 0.30175137519836426, acc = 0.8984375\n",
            "Batch 103: loss = 0.33807432651519775, acc = 0.8896484375\n",
            "Batch 104: loss = 0.28595900535583496, acc = 0.9111328125\n",
            "Batch 105: loss = 0.2526881992816925, acc = 0.908203125\n",
            "Batch 106: loss = 0.2894992530345917, acc = 0.8974609375\n",
            "Batch 107: loss = 0.28558486700057983, acc = 0.90234375\n",
            "Batch 108: loss = 0.28889521956443787, acc = 0.892578125\n",
            "Batch 109: loss = 0.28425174951553345, acc = 0.91015625\n",
            "Batch 110: loss = 0.2926378548145294, acc = 0.90234375\n",
            "Batch 111: loss = 0.2852155566215515, acc = 0.8974609375\n",
            "Batch 112: loss = 0.326517790555954, acc = 0.89453125\n",
            "Batch 113: loss = 0.2738489508628845, acc = 0.91015625\n",
            "Batch 114: loss = 0.30784520506858826, acc = 0.904296875\n",
            "Batch 115: loss = 0.30767858028411865, acc = 0.8994140625\n",
            "Batch 116: loss = 0.3250848054885864, acc = 0.896484375\n",
            "Batch 117: loss = 0.2802819013595581, acc = 0.9033203125\n",
            "Batch 118: loss = 0.24918439984321594, acc = 0.916015625\n",
            "Batch 119: loss = 0.27614626288414, acc = 0.904296875\n",
            "Batch 120: loss = 0.25386983156204224, acc = 0.908203125\n",
            "Batch 121: loss = 0.26649710536003113, acc = 0.9140625\n",
            "Batch 122: loss = 0.2785608172416687, acc = 0.9072265625\n",
            "Batch 123: loss = 0.2903825044631958, acc = 0.9111328125\n",
            "Batch 124: loss = 0.33322250843048096, acc = 0.8837890625\n",
            "Batch 125: loss = 0.276593953371048, acc = 0.9072265625\n",
            "Batch 126: loss = 0.34818971157073975, acc = 0.87890625\n",
            "\n",
            "Epoch 98/100\n",
            "Batch 1: loss = 0.385152131319046, acc = 0.888671875\n",
            "Batch 2: loss = 0.350600004196167, acc = 0.8857421875\n",
            "Batch 3: loss = 0.3219112753868103, acc = 0.9013671875\n",
            "Batch 4: loss = 0.2764635384082794, acc = 0.916015625\n",
            "Batch 5: loss = 0.2763197720050812, acc = 0.90625\n",
            "Batch 6: loss = 0.303970605134964, acc = 0.90234375\n",
            "Batch 7: loss = 0.2640310227870941, acc = 0.91015625\n",
            "Batch 8: loss = 0.2783636152744293, acc = 0.908203125\n",
            "Batch 9: loss = 0.30758705735206604, acc = 0.8994140625\n",
            "Batch 10: loss = 0.2619088888168335, acc = 0.9052734375\n",
            "Batch 11: loss = 0.302574098110199, acc = 0.90625\n",
            "Batch 12: loss = 0.26453301310539246, acc = 0.90625\n",
            "Batch 13: loss = 0.27087587118148804, acc = 0.900390625\n",
            "Batch 14: loss = 0.2969200015068054, acc = 0.900390625\n",
            "Batch 15: loss = 0.27372801303863525, acc = 0.9169921875\n",
            "Batch 16: loss = 0.2994641363620758, acc = 0.9169921875\n",
            "Batch 17: loss = 0.3118727207183838, acc = 0.904296875\n",
            "Batch 18: loss = 0.3356262445449829, acc = 0.880859375\n",
            "Batch 19: loss = 0.2811571955680847, acc = 0.8955078125\n",
            "Batch 20: loss = 0.3112461566925049, acc = 0.8837890625\n",
            "Batch 21: loss = 0.28627344965934753, acc = 0.890625\n",
            "Batch 22: loss = 0.3014514446258545, acc = 0.8994140625\n",
            "Batch 23: loss = 0.25033512711524963, acc = 0.9140625\n",
            "Batch 24: loss = 0.2792816460132599, acc = 0.9013671875\n",
            "Batch 25: loss = 0.32039210200309753, acc = 0.8955078125\n",
            "Batch 26: loss = 0.2641837000846863, acc = 0.91796875\n",
            "Batch 27: loss = 0.3055460453033447, acc = 0.8955078125\n",
            "Batch 28: loss = 0.30290645360946655, acc = 0.9033203125\n",
            "Batch 29: loss = 0.3036556839942932, acc = 0.892578125\n",
            "Batch 30: loss = 0.29652702808380127, acc = 0.8984375\n",
            "Batch 31: loss = 0.319166898727417, acc = 0.9072265625\n",
            "Batch 32: loss = 0.30697140097618103, acc = 0.91015625\n",
            "Batch 33: loss = 0.264100581407547, acc = 0.9150390625\n",
            "Batch 34: loss = 0.2961381673812866, acc = 0.90625\n",
            "Batch 35: loss = 0.3059653639793396, acc = 0.896484375\n",
            "Batch 36: loss = 0.24492114782333374, acc = 0.9169921875\n",
            "Batch 37: loss = 0.2617202401161194, acc = 0.9150390625\n",
            "Batch 38: loss = 0.2593684494495392, acc = 0.9228515625\n",
            "Batch 39: loss = 0.23692728579044342, acc = 0.9296875\n",
            "Batch 40: loss = 0.2613934874534607, acc = 0.9169921875\n",
            "Batch 41: loss = 0.24178171157836914, acc = 0.91796875\n",
            "Batch 42: loss = 0.2602071166038513, acc = 0.9140625\n",
            "Batch 43: loss = 0.29577216506004333, acc = 0.8984375\n",
            "Batch 44: loss = 0.2388438880443573, acc = 0.927734375\n",
            "Batch 45: loss = 0.28099682927131653, acc = 0.8994140625\n",
            "Batch 46: loss = 0.2784893214702606, acc = 0.908203125\n",
            "Batch 47: loss = 0.28435444831848145, acc = 0.9091796875\n",
            "Batch 48: loss = 0.2677760124206543, acc = 0.9091796875\n",
            "Batch 49: loss = 0.26047301292419434, acc = 0.919921875\n",
            "Batch 50: loss = 0.2832677364349365, acc = 0.9091796875\n",
            "Batch 51: loss = 0.2808275520801544, acc = 0.9072265625\n",
            "Batch 52: loss = 0.2905413806438446, acc = 0.8955078125\n",
            "Batch 53: loss = 0.2730233371257782, acc = 0.9140625\n",
            "Batch 54: loss = 0.25090596079826355, acc = 0.908203125\n",
            "Batch 55: loss = 0.25989261269569397, acc = 0.90625\n",
            "Batch 56: loss = 0.3043050169944763, acc = 0.8896484375\n",
            "Batch 57: loss = 0.26544663310050964, acc = 0.91015625\n",
            "Batch 58: loss = 0.3088250756263733, acc = 0.8974609375\n",
            "Batch 59: loss = 0.22347496449947357, acc = 0.93359375\n",
            "Batch 60: loss = 0.2725598216056824, acc = 0.90625\n",
            "Batch 61: loss = 0.25309792160987854, acc = 0.9111328125\n",
            "Batch 62: loss = 0.3188522458076477, acc = 0.8935546875\n",
            "Batch 63: loss = 0.24223099648952484, acc = 0.9208984375\n",
            "Batch 64: loss = 0.2733665108680725, acc = 0.9150390625\n",
            "Batch 65: loss = 0.2895408570766449, acc = 0.9013671875\n",
            "Batch 66: loss = 0.2904216945171356, acc = 0.8974609375\n",
            "Batch 67: loss = 0.27446138858795166, acc = 0.90234375\n",
            "Batch 68: loss = 0.27402207255363464, acc = 0.904296875\n",
            "Batch 69: loss = 0.2859438955783844, acc = 0.9033203125\n",
            "Batch 70: loss = 0.3436521589756012, acc = 0.8798828125\n",
            "Batch 71: loss = 0.3091430366039276, acc = 0.8896484375\n",
            "Batch 72: loss = 0.28324535489082336, acc = 0.9150390625\n",
            "Batch 73: loss = 0.2744734287261963, acc = 0.9169921875\n",
            "Batch 74: loss = 0.293954074382782, acc = 0.900390625\n",
            "Batch 75: loss = 0.33949390053749084, acc = 0.8876953125\n",
            "Batch 76: loss = 0.3228037655353546, acc = 0.8818359375\n",
            "Batch 77: loss = 0.27669140696525574, acc = 0.9072265625\n",
            "Batch 78: loss = 0.30712175369262695, acc = 0.900390625\n",
            "Batch 79: loss = 0.3056551218032837, acc = 0.892578125\n",
            "Batch 80: loss = 0.316161185503006, acc = 0.8818359375\n",
            "Batch 81: loss = 0.29005393385887146, acc = 0.8974609375\n",
            "Batch 82: loss = 0.31419360637664795, acc = 0.90625\n",
            "Batch 83: loss = 0.25731387734413147, acc = 0.908203125\n",
            "Batch 84: loss = 0.25801244378089905, acc = 0.9150390625\n",
            "Batch 85: loss = 0.3239697515964508, acc = 0.8798828125\n",
            "Batch 86: loss = 0.3088434636592865, acc = 0.9052734375\n",
            "Batch 87: loss = 0.313848078250885, acc = 0.896484375\n",
            "Batch 88: loss = 0.32607877254486084, acc = 0.884765625\n",
            "Batch 89: loss = 0.2837477922439575, acc = 0.90625\n",
            "Batch 90: loss = 0.29300880432128906, acc = 0.8974609375\n",
            "Batch 91: loss = 0.32135167717933655, acc = 0.888671875\n",
            "Batch 92: loss = 0.30386883020401, acc = 0.8955078125\n",
            "Batch 93: loss = 0.2666321396827698, acc = 0.912109375\n",
            "Batch 94: loss = 0.2936137318611145, acc = 0.9091796875\n",
            "Batch 95: loss = 0.2926456332206726, acc = 0.8994140625\n",
            "Batch 96: loss = 0.30459675192832947, acc = 0.8935546875\n",
            "Batch 97: loss = 0.2683548331260681, acc = 0.9091796875\n",
            "Batch 98: loss = 0.27890995144844055, acc = 0.9052734375\n",
            "Batch 99: loss = 0.3210092782974243, acc = 0.8916015625\n",
            "Batch 100: loss = 0.3212546706199646, acc = 0.8798828125\n",
            "Batch 101: loss = 0.28675198554992676, acc = 0.8935546875\n",
            "Batch 102: loss = 0.3233175575733185, acc = 0.896484375\n",
            "Batch 103: loss = 0.34927698969841003, acc = 0.8837890625\n",
            "Batch 104: loss = 0.24610817432403564, acc = 0.919921875\n",
            "Batch 105: loss = 0.2760683596134186, acc = 0.9013671875\n",
            "Batch 106: loss = 0.2855282723903656, acc = 0.908203125\n",
            "Batch 107: loss = 0.29729223251342773, acc = 0.896484375\n",
            "Batch 108: loss = 0.29881584644317627, acc = 0.900390625\n",
            "Batch 109: loss = 0.3169999420642853, acc = 0.90234375\n",
            "Batch 110: loss = 0.30504560470581055, acc = 0.896484375\n",
            "Batch 111: loss = 0.2738027274608612, acc = 0.9052734375\n",
            "Batch 112: loss = 0.29328393936157227, acc = 0.912109375\n",
            "Batch 113: loss = 0.28837141394615173, acc = 0.9072265625\n",
            "Batch 114: loss = 0.3282308578491211, acc = 0.8876953125\n",
            "Batch 115: loss = 0.29704850912094116, acc = 0.904296875\n",
            "Batch 116: loss = 0.33672621846199036, acc = 0.8955078125\n",
            "Batch 117: loss = 0.2740297019481659, acc = 0.9130859375\n",
            "Batch 118: loss = 0.26403969526290894, acc = 0.9140625\n",
            "Batch 119: loss = 0.2793196439743042, acc = 0.8916015625\n",
            "Batch 120: loss = 0.24572166800498962, acc = 0.923828125\n",
            "Batch 121: loss = 0.3105429410934448, acc = 0.8896484375\n",
            "Batch 122: loss = 0.27022597193717957, acc = 0.916015625\n",
            "Batch 123: loss = 0.30560821294784546, acc = 0.9091796875\n",
            "Batch 124: loss = 0.320781946182251, acc = 0.8818359375\n",
            "Batch 125: loss = 0.27541905641555786, acc = 0.90625\n",
            "Batch 126: loss = 0.27490371465682983, acc = 0.91015625\n",
            "\n",
            "Epoch 99/100\n",
            "Batch 1: loss = 0.4294278621673584, acc = 0.873046875\n",
            "Batch 2: loss = 0.3302612900733948, acc = 0.8974609375\n",
            "Batch 3: loss = 0.311670184135437, acc = 0.8994140625\n",
            "Batch 4: loss = 0.2817434072494507, acc = 0.916015625\n",
            "Batch 5: loss = 0.2780950367450714, acc = 0.916015625\n",
            "Batch 6: loss = 0.2792515754699707, acc = 0.912109375\n",
            "Batch 7: loss = 0.2868484854698181, acc = 0.904296875\n",
            "Batch 8: loss = 0.26586246490478516, acc = 0.9150390625\n",
            "Batch 9: loss = 0.2636576294898987, acc = 0.91796875\n",
            "Batch 10: loss = 0.25843656063079834, acc = 0.9140625\n",
            "Batch 11: loss = 0.28624197840690613, acc = 0.900390625\n",
            "Batch 12: loss = 0.2634435296058655, acc = 0.8994140625\n",
            "Batch 13: loss = 0.28533533215522766, acc = 0.90234375\n",
            "Batch 14: loss = 0.2553487718105316, acc = 0.9248046875\n",
            "Batch 15: loss = 0.26645389199256897, acc = 0.9033203125\n",
            "Batch 16: loss = 0.29589441418647766, acc = 0.9091796875\n",
            "Batch 17: loss = 0.30373451113700867, acc = 0.9033203125\n",
            "Batch 18: loss = 0.31984198093414307, acc = 0.900390625\n",
            "Batch 19: loss = 0.28684428334236145, acc = 0.89453125\n",
            "Batch 20: loss = 0.2690652012825012, acc = 0.9033203125\n",
            "Batch 21: loss = 0.32055410742759705, acc = 0.8876953125\n",
            "Batch 22: loss = 0.304299533367157, acc = 0.892578125\n",
            "Batch 23: loss = 0.2905234396457672, acc = 0.8994140625\n",
            "Batch 24: loss = 0.24113035202026367, acc = 0.9208984375\n",
            "Batch 25: loss = 0.28738898038864136, acc = 0.90625\n",
            "Batch 26: loss = 0.2816949486732483, acc = 0.90234375\n",
            "Batch 27: loss = 0.3201030492782593, acc = 0.896484375\n",
            "Batch 28: loss = 0.29681435227394104, acc = 0.8974609375\n",
            "Batch 29: loss = 0.2749968469142914, acc = 0.9091796875\n",
            "Batch 30: loss = 0.28492021560668945, acc = 0.9013671875\n",
            "Batch 31: loss = 0.2796914577484131, acc = 0.9140625\n",
            "Batch 32: loss = 0.33687227964401245, acc = 0.8984375\n",
            "Batch 33: loss = 0.28048044443130493, acc = 0.9033203125\n",
            "Batch 34: loss = 0.2926930785179138, acc = 0.90234375\n",
            "Batch 35: loss = 0.30437371134757996, acc = 0.904296875\n",
            "Batch 36: loss = 0.24227619171142578, acc = 0.9189453125\n",
            "Batch 37: loss = 0.23407573997974396, acc = 0.9326171875\n",
            "Batch 38: loss = 0.2680921256542206, acc = 0.9111328125\n",
            "Batch 39: loss = 0.2458593249320984, acc = 0.9150390625\n",
            "Batch 40: loss = 0.29578837752342224, acc = 0.8994140625\n",
            "Batch 41: loss = 0.26750850677490234, acc = 0.9052734375\n",
            "Batch 42: loss = 0.28763023018836975, acc = 0.9033203125\n",
            "Batch 43: loss = 0.2939037084579468, acc = 0.8994140625\n",
            "Batch 44: loss = 0.25304079055786133, acc = 0.9091796875\n",
            "Batch 45: loss = 0.2459934502840042, acc = 0.921875\n",
            "Batch 46: loss = 0.2419193983078003, acc = 0.9091796875\n",
            "Batch 47: loss = 0.25498098134994507, acc = 0.919921875\n",
            "Batch 48: loss = 0.27654901146888733, acc = 0.904296875\n",
            "Batch 49: loss = 0.2859404683113098, acc = 0.908203125\n",
            "Batch 50: loss = 0.2547566294670105, acc = 0.919921875\n",
            "Batch 51: loss = 0.29702475666999817, acc = 0.89453125\n",
            "Batch 52: loss = 0.3066157102584839, acc = 0.892578125\n",
            "Batch 53: loss = 0.2785055637359619, acc = 0.904296875\n",
            "Batch 54: loss = 0.2821318805217743, acc = 0.90625\n",
            "Batch 55: loss = 0.2360522449016571, acc = 0.91796875\n",
            "Batch 56: loss = 0.30011582374572754, acc = 0.900390625\n",
            "Batch 57: loss = 0.2813350558280945, acc = 0.904296875\n",
            "Batch 58: loss = 0.30845510959625244, acc = 0.8876953125\n",
            "Batch 59: loss = 0.23000174760818481, acc = 0.9306640625\n",
            "Batch 60: loss = 0.3071939945220947, acc = 0.8935546875\n",
            "Batch 61: loss = 0.2500976026058197, acc = 0.919921875\n",
            "Batch 62: loss = 0.2971106767654419, acc = 0.9111328125\n",
            "Batch 63: loss = 0.2686181664466858, acc = 0.9013671875\n",
            "Batch 64: loss = 0.26622095704078674, acc = 0.9140625\n",
            "Batch 65: loss = 0.2941092848777771, acc = 0.9033203125\n",
            "Batch 66: loss = 0.32349342107772827, acc = 0.8974609375\n",
            "Batch 67: loss = 0.2893671989440918, acc = 0.90234375\n",
            "Batch 68: loss = 0.3038460314273834, acc = 0.8994140625\n",
            "Batch 69: loss = 0.3078681528568268, acc = 0.89453125\n",
            "Batch 70: loss = 0.28982144594192505, acc = 0.8984375\n",
            "Batch 71: loss = 0.28527894616127014, acc = 0.900390625\n",
            "Batch 72: loss = 0.24658317863941193, acc = 0.9248046875\n",
            "Batch 73: loss = 0.2939991056919098, acc = 0.90234375\n",
            "Batch 74: loss = 0.27860119938850403, acc = 0.9072265625\n",
            "Batch 75: loss = 0.32586491107940674, acc = 0.8974609375\n",
            "Batch 76: loss = 0.2978280186653137, acc = 0.900390625\n",
            "Batch 77: loss = 0.27340182662010193, acc = 0.90234375\n",
            "Batch 78: loss = 0.25538021326065063, acc = 0.9111328125\n",
            "Batch 79: loss = 0.2919495105743408, acc = 0.8984375\n",
            "Batch 80: loss = 0.2433326244354248, acc = 0.9267578125\n",
            "Batch 81: loss = 0.29234373569488525, acc = 0.9033203125\n",
            "Batch 82: loss = 0.30729755759239197, acc = 0.8974609375\n",
            "Batch 83: loss = 0.2561972141265869, acc = 0.9189453125\n",
            "Batch 84: loss = 0.2729122042655945, acc = 0.91015625\n",
            "Batch 85: loss = 0.2939241826534271, acc = 0.90234375\n",
            "Batch 86: loss = 0.27096888422966003, acc = 0.9150390625\n",
            "Batch 87: loss = 0.297903835773468, acc = 0.896484375\n",
            "Batch 88: loss = 0.2984277606010437, acc = 0.890625\n",
            "Batch 89: loss = 0.2717101275920868, acc = 0.9072265625\n",
            "Batch 90: loss = 0.30098772048950195, acc = 0.8974609375\n",
            "Batch 91: loss = 0.3060888648033142, acc = 0.8955078125\n",
            "Batch 92: loss = 0.31844717264175415, acc = 0.8916015625\n",
            "Batch 93: loss = 0.2499121129512787, acc = 0.916015625\n",
            "Batch 94: loss = 0.2952766418457031, acc = 0.89453125\n",
            "Batch 95: loss = 0.28944528102874756, acc = 0.900390625\n",
            "Batch 96: loss = 0.3246772885322571, acc = 0.8955078125\n",
            "Batch 97: loss = 0.28071463108062744, acc = 0.912109375\n",
            "Batch 98: loss = 0.2669205367565155, acc = 0.9111328125\n",
            "Batch 99: loss = 0.29539424180984497, acc = 0.904296875\n",
            "Batch 100: loss = 0.3277072608470917, acc = 0.884765625\n",
            "Batch 101: loss = 0.2611601948738098, acc = 0.9033203125\n",
            "Batch 102: loss = 0.2757076621055603, acc = 0.8955078125\n",
            "Batch 103: loss = 0.30043670535087585, acc = 0.9052734375\n",
            "Batch 104: loss = 0.2676616609096527, acc = 0.9130859375\n",
            "Batch 105: loss = 0.28884971141815186, acc = 0.9033203125\n",
            "Batch 106: loss = 0.299038290977478, acc = 0.8984375\n",
            "Batch 107: loss = 0.2753845453262329, acc = 0.900390625\n",
            "Batch 108: loss = 0.3128732442855835, acc = 0.8974609375\n",
            "Batch 109: loss = 0.2927402853965759, acc = 0.8916015625\n",
            "Batch 110: loss = 0.26378583908081055, acc = 0.912109375\n",
            "Batch 111: loss = 0.3075464367866516, acc = 0.8984375\n",
            "Batch 112: loss = 0.3053382635116577, acc = 0.892578125\n",
            "Batch 113: loss = 0.2812371850013733, acc = 0.9072265625\n",
            "Batch 114: loss = 0.2735121548175812, acc = 0.9150390625\n",
            "Batch 115: loss = 0.2718521058559418, acc = 0.908203125\n",
            "Batch 116: loss = 0.3127514123916626, acc = 0.8984375\n",
            "Batch 117: loss = 0.2944280207157135, acc = 0.8994140625\n",
            "Batch 118: loss = 0.24951262772083282, acc = 0.916015625\n",
            "Batch 119: loss = 0.2877371609210968, acc = 0.9052734375\n",
            "Batch 120: loss = 0.2601575255393982, acc = 0.916015625\n",
            "Batch 121: loss = 0.3222329020500183, acc = 0.8857421875\n",
            "Batch 122: loss = 0.2985166311264038, acc = 0.8974609375\n",
            "Batch 123: loss = 0.31548941135406494, acc = 0.89453125\n",
            "Batch 124: loss = 0.31538689136505127, acc = 0.890625\n",
            "Batch 125: loss = 0.3005646765232086, acc = 0.9013671875\n",
            "Batch 126: loss = 0.3199799656867981, acc = 0.8837890625\n",
            "\n",
            "Epoch 100/100\n",
            "Batch 1: loss = 0.41837629675865173, acc = 0.8740234375\n",
            "Batch 2: loss = 0.3482246398925781, acc = 0.8818359375\n",
            "Batch 3: loss = 0.306845486164093, acc = 0.9052734375\n",
            "Batch 4: loss = 0.2819790244102478, acc = 0.9140625\n",
            "Batch 5: loss = 0.2811128497123718, acc = 0.900390625\n",
            "Batch 6: loss = 0.32450175285339355, acc = 0.896484375\n",
            "Batch 7: loss = 0.2708984613418579, acc = 0.908203125\n",
            "Batch 8: loss = 0.29774999618530273, acc = 0.896484375\n",
            "Batch 9: loss = 0.27622365951538086, acc = 0.9013671875\n",
            "Batch 10: loss = 0.2670717239379883, acc = 0.8984375\n",
            "Batch 11: loss = 0.302837610244751, acc = 0.908203125\n",
            "Batch 12: loss = 0.25526565313339233, acc = 0.916015625\n",
            "Batch 13: loss = 0.2754592299461365, acc = 0.91015625\n",
            "Batch 14: loss = 0.2603626549243927, acc = 0.919921875\n",
            "Batch 15: loss = 0.264788419008255, acc = 0.916015625\n",
            "Batch 16: loss = 0.3043719232082367, acc = 0.8984375\n",
            "Batch 17: loss = 0.2820325195789337, acc = 0.9072265625\n",
            "Batch 18: loss = 0.34225761890411377, acc = 0.8896484375\n",
            "Batch 19: loss = 0.27833181619644165, acc = 0.904296875\n",
            "Batch 20: loss = 0.30174052715301514, acc = 0.900390625\n",
            "Batch 21: loss = 0.28723835945129395, acc = 0.8955078125\n",
            "Batch 22: loss = 0.31318211555480957, acc = 0.892578125\n",
            "Batch 23: loss = 0.2808639109134674, acc = 0.908203125\n",
            "Batch 24: loss = 0.2813358008861542, acc = 0.8955078125\n",
            "Batch 25: loss = 0.28667303919792175, acc = 0.9091796875\n",
            "Batch 26: loss = 0.26780444383621216, acc = 0.9052734375\n",
            "Batch 27: loss = 0.2771267294883728, acc = 0.912109375\n",
            "Batch 28: loss = 0.26755601167678833, acc = 0.9140625\n",
            "Batch 29: loss = 0.27879151701927185, acc = 0.8994140625\n",
            "Batch 30: loss = 0.28395751118659973, acc = 0.9033203125\n",
            "Batch 31: loss = 0.2752763628959656, acc = 0.916015625\n",
            "Batch 32: loss = 0.3603453040122986, acc = 0.8876953125\n",
            "Batch 33: loss = 0.2581893801689148, acc = 0.9072265625\n",
            "Batch 34: loss = 0.30611708760261536, acc = 0.9033203125\n",
            "Batch 35: loss = 0.3035835027694702, acc = 0.8955078125\n",
            "Batch 36: loss = 0.27681422233581543, acc = 0.9052734375\n",
            "Batch 37: loss = 0.2610241174697876, acc = 0.9228515625\n",
            "Batch 38: loss = 0.250084787607193, acc = 0.9208984375\n",
            "Batch 39: loss = 0.25743526220321655, acc = 0.923828125\n",
            "Batch 40: loss = 0.2609812021255493, acc = 0.9091796875\n",
            "Batch 41: loss = 0.248256653547287, acc = 0.916015625\n",
            "Batch 42: loss = 0.2854803502559662, acc = 0.9033203125\n",
            "Batch 43: loss = 0.3226521909236908, acc = 0.87890625\n",
            "Batch 44: loss = 0.22070106863975525, acc = 0.92578125\n",
            "Batch 45: loss = 0.2713131010532379, acc = 0.908203125\n",
            "Batch 46: loss = 0.23970064520835876, acc = 0.9150390625\n",
            "Batch 47: loss = 0.2855081558227539, acc = 0.8974609375\n",
            "Batch 48: loss = 0.23507370054721832, acc = 0.9306640625\n",
            "Batch 49: loss = 0.25660020112991333, acc = 0.912109375\n",
            "Batch 50: loss = 0.2503840923309326, acc = 0.9130859375\n",
            "Batch 51: loss = 0.27951234579086304, acc = 0.9013671875\n",
            "Batch 52: loss = 0.2551088035106659, acc = 0.9130859375\n",
            "Batch 53: loss = 0.2631976008415222, acc = 0.904296875\n",
            "Batch 54: loss = 0.2462363839149475, acc = 0.9111328125\n",
            "Batch 55: loss = 0.23178504407405853, acc = 0.9228515625\n",
            "Batch 56: loss = 0.3180326521396637, acc = 0.896484375\n",
            "Batch 57: loss = 0.2954023480415344, acc = 0.896484375\n",
            "Batch 58: loss = 0.31134289503097534, acc = 0.8916015625\n",
            "Batch 59: loss = 0.21668623387813568, acc = 0.9287109375\n",
            "Batch 60: loss = 0.25921618938446045, acc = 0.9228515625\n",
            "Batch 61: loss = 0.25307777523994446, acc = 0.921875\n",
            "Batch 62: loss = 0.31710946559906006, acc = 0.8896484375\n",
            "Batch 63: loss = 0.25559067726135254, acc = 0.9150390625\n",
            "Batch 64: loss = 0.2520226538181305, acc = 0.9072265625\n",
            "Batch 65: loss = 0.29087889194488525, acc = 0.908203125\n",
            "Batch 66: loss = 0.2619486451148987, acc = 0.912109375\n",
            "Batch 67: loss = 0.2618796229362488, acc = 0.91796875\n",
            "Batch 68: loss = 0.24697363376617432, acc = 0.92578125\n",
            "Batch 69: loss = 0.2950064539909363, acc = 0.9072265625\n",
            "Batch 70: loss = 0.27974462509155273, acc = 0.904296875\n",
            "Batch 71: loss = 0.27432432770729065, acc = 0.9033203125\n",
            "Batch 72: loss = 0.28266048431396484, acc = 0.9150390625\n",
            "Batch 73: loss = 0.27396342158317566, acc = 0.9111328125\n",
            "Batch 74: loss = 0.3063550591468811, acc = 0.8935546875\n",
            "Batch 75: loss = 0.3485307991504669, acc = 0.8798828125\n",
            "Batch 76: loss = 0.2854398190975189, acc = 0.8974609375\n",
            "Batch 77: loss = 0.28741517663002014, acc = 0.90625\n",
            "Batch 78: loss = 0.28791722655296326, acc = 0.9091796875\n",
            "Batch 79: loss = 0.2924880385398865, acc = 0.8974609375\n",
            "Batch 80: loss = 0.2791557312011719, acc = 0.9052734375\n",
            "Batch 81: loss = 0.29666921496391296, acc = 0.8876953125\n",
            "Batch 82: loss = 0.2837619483470917, acc = 0.9072265625\n",
            "Batch 83: loss = 0.2502076029777527, acc = 0.9150390625\n",
            "Batch 84: loss = 0.31485986709594727, acc = 0.8984375\n",
            "Batch 85: loss = 0.2584132254123688, acc = 0.916015625\n",
            "Batch 86: loss = 0.2603399455547333, acc = 0.923828125\n",
            "Batch 87: loss = 0.27381303906440735, acc = 0.904296875\n",
            "Batch 88: loss = 0.3312363624572754, acc = 0.884765625\n",
            "Batch 89: loss = 0.30162644386291504, acc = 0.9052734375\n",
            "Batch 90: loss = 0.3040944039821625, acc = 0.904296875\n",
            "Batch 91: loss = 0.3101847171783447, acc = 0.8994140625\n",
            "Batch 92: loss = 0.29432862997055054, acc = 0.8916015625\n",
            "Batch 93: loss = 0.29885798692703247, acc = 0.8994140625\n",
            "Batch 94: loss = 0.2818080186843872, acc = 0.9013671875\n",
            "Batch 95: loss = 0.26561346650123596, acc = 0.923828125\n",
            "Batch 96: loss = 0.31240546703338623, acc = 0.896484375\n",
            "Batch 97: loss = 0.2565973103046417, acc = 0.921875\n",
            "Batch 98: loss = 0.27396222949028015, acc = 0.91015625\n",
            "Batch 99: loss = 0.2833845317363739, acc = 0.8994140625\n",
            "Batch 100: loss = 0.30358755588531494, acc = 0.8896484375\n",
            "Batch 101: loss = 0.27347517013549805, acc = 0.9033203125\n",
            "Batch 102: loss = 0.28286421298980713, acc = 0.9033203125\n",
            "Batch 103: loss = 0.3259013295173645, acc = 0.8984375\n",
            "Batch 104: loss = 0.2658942937850952, acc = 0.9072265625\n",
            "Batch 105: loss = 0.26801764965057373, acc = 0.9072265625\n",
            "Batch 106: loss = 0.2882986068725586, acc = 0.9111328125\n",
            "Batch 107: loss = 0.29619255661964417, acc = 0.8916015625\n",
            "Batch 108: loss = 0.28712573647499084, acc = 0.904296875\n",
            "Batch 109: loss = 0.28756049275398254, acc = 0.9072265625\n",
            "Batch 110: loss = 0.2737766206264496, acc = 0.9033203125\n",
            "Batch 111: loss = 0.283206969499588, acc = 0.8994140625\n",
            "Batch 112: loss = 0.3142510652542114, acc = 0.8955078125\n",
            "Batch 113: loss = 0.29159536957740784, acc = 0.9013671875\n",
            "Batch 114: loss = 0.3078712821006775, acc = 0.8955078125\n",
            "Batch 115: loss = 0.28053581714630127, acc = 0.9130859375\n",
            "Batch 116: loss = 0.30200517177581787, acc = 0.8955078125\n",
            "Batch 117: loss = 0.250092089176178, acc = 0.9140625\n",
            "Batch 118: loss = 0.2511340379714966, acc = 0.923828125\n",
            "Batch 119: loss = 0.2513832151889801, acc = 0.9150390625\n",
            "Batch 120: loss = 0.2440679967403412, acc = 0.927734375\n",
            "Batch 121: loss = 0.2867553234100342, acc = 0.9140625\n",
            "Batch 122: loss = 0.2684958577156067, acc = 0.9052734375\n",
            "Batch 123: loss = 0.29295241832733154, acc = 0.896484375\n",
            "Batch 124: loss = 0.27141281962394714, acc = 0.900390625\n",
            "Batch 125: loss = 0.2779392600059509, acc = 0.908203125\n",
            "Batch 126: loss = 0.28295382857322693, acc = 0.90625\n",
            "Saved checkpoint to weights.100.h5\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (16, 64, 256)             22016     \n",
            "_________________________________________________________________\n",
            "lstm_25 (LSTM)               (16, 64, 256)             525312    \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (16, 64, 256)             0         \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (16, 64, 86)              22102     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (16, 64, 86)              0         \n",
            "=================================================================\n",
            "Total params: 569,430\n",
            "Trainable params: 569,430\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZyckrmUOaw5"
      },
      "source": [
        "## Traing Loss "
      ],
      "id": "hZyckrmUOaw5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "EDFSkQIkt6r6",
        "outputId": "984a2555-f254-4714-839b-0da0b0cb478c"
      },
      "source": [
        "plt.plot(Train_epoch_loss_acc['Losses'])\n",
        "plt.title('LSTM Model train')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train_loss'], loc='upper right')"
      ],
      "id": "EDFSkQIkt6r6",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0606e63890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIdQe6ZOOex6"
      },
      "source": [
        "## Model Output\n",
        "### Enter epoch number to select desired weights of that epoch, starting character index, length of output characters"
      ],
      "id": "PIdQe6ZOOex6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8rZ54lMtECo",
        "outputId": "b7adfbb5-6976-45c0-927a-0ac5d2886fdd"
      },
      "source": [
        "epoch_number = int(input(\"Enter epoch number between 1 to 100 only multiples of 10: \"))\n",
        "character_index = int(input(\"Enter any number between 0 to 86 to sequence generation: \"))\n",
        "Sequence_length = int(input(\"Number of characters to generate: \"))\n",
        "\n",
        "Generated_music_sequence = sample_seq_generator(epoch_number, character_index, Sequence_length)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(Generated_music_sequence)"
      ],
      "id": "w8rZ54lMtECo",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter epoch number between 1 to 100 only multiples of 10: 100\n",
            "Enter any number between 0 to 86 to sequence generation: 1\n",
            "Number of characters to generate: 600\n",
            "\n",
            "\n",
            " [1\"F\"\"F#7\"k'3 fe|\"Bb\"d3 -d3|\"F\"A2B \"C7\"_ed|\"F\"c3 -cAF||\n",
            "|:\"C\"c3 c3|c3 -c3||\n",
            "\n",
            "\n",
            "X: 100\n",
            "T:Raisher's Brigg\n",
            "% Nottingham Music Database\n",
            "S:Trad, arr Phil Rowe\n",
            "M:6/8\n",
            "K:D\n",
            "|:d|\"G\"d2g gfe|dgd B3|\"Am\"cBc \"D7\"A2c|\"G\"Bdd dgg|\n",
            "\"Am\"ecA Ace|\"D\"fef fed|\"A7\"ABc ABc|\"D\"d3 -d2||\n",
            "\n",
            "\n",
            "X: 129\n",
            "T:The Maggan's Aanbarie\n",
            "% Nottingham Music Database\n",
            "S:Bob McQuillen 19 976, via Phil Rowe\n",
            "M:6/8\n",
            "K:C\n",
            "G3 |\"C\"cBc G^FG|CGE C3|\"C\"EG^F \"G7\"G2B|\"C\"e^de c2B|\n",
            "\"F\"AyA A2G|\"F\"FGA \"D7\"AFA|\"Gm\"G3 G2:|\n",
            "\n",
            "\n",
            "X: 81\n",
            "T:Dan Hunting\n",
            "% Nottingham Music Database\n",
            "S:Dennis Salter, via Phil Rowe\n",
            "M:6/8\n",
            "K:G\n",
            "D2|:\"G\"GFG \"D7\"A2F|\"G\"GAB \"C\"d2c|\"G\"Bdg \"Am\"ecA|\"D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNzWhbyqprJq"
      },
      "source": [
        ""
      ],
      "id": "BNzWhbyqprJq",
      "execution_count": null,
      "outputs": []
    }
  ]
}